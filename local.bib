@article{Overstall2017,
author = {Antony M. Overstall and David C. Woods},
title = {Bayesian Design of Experiments Using Approximate Coordinate Exchange},
journal = {Technometrics},
volume = {59},
number = {4},
pages = {458-470},
year = {2017},
publisher = {Taylor & Francis},
doi = {10.1080/00401706.2016.1251495},
URL = {https://doi.org/10.1080/00401706.2016.1251495},
eprint = {https://doi.org/10.1080/00401706.2016.1251495}
}

@article{Weaver2016,
author = {Brian P. Weaver and Brian J. Williams and Christine M. Anderson-Cook and David M. Higdon},
title = {{Computational Enhancements to Bayesian Design of Experiments Using Gaussian Processes}},
volume = {11},
journal = {Bayesian Analysis},
number = {1},
publisher = {International Society for Bayesian Analysis},
pages = {191 -- 213},
keywords = {accelerated life tests, Bayesian design of experiments, expected quantile improvement, Gaussian processes, preposterior expectation},
year = {2016},
doi = {10.1214/15-BA945},
URL = {https://doi.org/10.1214/15-BA945}
}


@InProceedings{alvarez09a,
  title = 	 {Latent Force Models},
  author = 	 {Álvarez, Mauricio and Luengo, David and Lawrence, Neil D.},
  booktitle = 	 {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {9--16},
  year = 	 {2009},
  editor = 	 {van Dyk, David and Welling, Max},
  volume = 	 {5},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v5/alvarez09a/alvarez09a.pdf},
  url = 	 {https://proceedings.mlr.press/v5/alvarez09a.html},
  abstract = 	 {Purely data driven approaches for machine learning present difficulties when data is scarce relative to the complexity of the model or when the model is forced to extrapolate. On the other hand, purely mechanistic approaches  need to identify and specify all the interactions in the problem at hand (which  may not be feasible) and still leave the issue of how to parameterize the system. In this paper, we present a hybrid approach using Gaussian processes and differential equations to combine data driven modeling with a physical model of the system. We show how different, physically-inspired, kernel functions  can be developed through sensible, simple, mechanistic assumptions about the underlying system. The versatility of our approach is illustrated with three case studies from computational biology, motion capture and geostatistics.}
}

@misc{barnard2000,
    title = {Modeling covariance matrices in terms of standard deviations and correlations, with application to shrinkage},
    author = {Barnard, John and McCulloch, Robert and Meng, Xiao-Li},
    url = {https://www3.stat.sinica.edu.tw/statistica/j10n4/j10n416/j10n416.htm},
    volume= {10},
    number= {4},
    journal={Statistica Sinica},
    year={2000},
    pages={1281--1311}
}

@article{bomarito_optimization_2022,
	title = {On the optimization of approximate control variates with parametrically defined estimators},
	volume = {451},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999121007774},
	doi = {10.1016/j.jcp.2021.110882},
	language = {en},
	urldate = {2022-05-16},
	journal = {Journal of Computational Physics},
	author = {Bomarito, G.F. and Leser, P.E. and Warner, J.E. and Leser, W.P.},
	month = feb,
	year = {2022},
	pages = {110882},
	annote = {Adds optimization of parameters in ACV-KL estimator from Gorodetsky}
}

@misc{chakroborty_covariance-free_2024,
	title = {Covariance-free {Multifidelity} {Control} {Variates} {Importance} {Sampling} for {Reliability} {Analysis} of {Rare} {Events}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	url = {https://arxiv.org/abs/2405.03834},
	doi = {10.48550/ARXIV.2405.03834},
	abstract = {Multifidelity modeling has been steadily gaining attention as a tool to address the problem of exorbitant model evaluation costs that makes the estimation of failure probabilities a significant computational challenge for complex real-world problems, particularly when failure is a rare event. To implement multifidelity modeling, estimators that efficiently combine information from multiple models/sources are necessary. In past works, the variance reduction techniques of Control Variates (CV) and Importance Sampling (IS) have been leveraged for this task. In this paper, we present the CVIS framework; a creative take on a coupled Control Variates and Importance Sampling estimator for bifidelity reliability analysis. The framework addresses some of the practical challenges of the CV method by using an estimator for the control variate mean and side-stepping the need to estimate the covariance between the original estimator and the control variate through a clever choice for the tuning constant. The task of selecting an efficient IS distribution is also considered, with a view towards maximally leveraging the bifidelity structure and maintaining expressivity. Additionally, a diagnostic is provided that indicates both the efficiency of the algorithm as well as the relative predictive quality of the models utilized. Finally, the behavior and performance of the framework is explored through analytical and numerical examples.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Chakroborty, Promit and Dhulipala, Somayajulu L. N. and Shields, Michael D.},
	year = {2024},
	note = {Version Number: 1},
	keywords = {62-08 (Primary), 62D99, 62F10, 62H12, 62L12, 62P30, 65C05, 90B25, FOS: Computer and information sciences, G.3, Methodology (stat.ME)},
	annote = {Other
32 pages, 6 figures},
}


@article{pham_ensemble_2022,
	title = {Ensemble {Approximate} {Control} {Variate} {Estimators}: {Applications} to {MultiFidelity} {Importance} {Sampling}},
	volume = {10},
	issn = {2166-2525},
	shorttitle = {Ensemble {Approximate} {Control} {Variate} {Estimators}},
	url = {https://epubs.siam.org/doi/10.1137/21M1390426},
	doi = {10.1137/21M1390426},
	language = {en},
	number = {3},
	urldate = {2023-07-08},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Pham, Trung and Gorodetsky, Alex A.},
	month = sep,
	year = {2022},
	pages = {1250--1292},
	file = {Submitted Version:/Users/me-tcoons/Zotero/storage/HD6SGN5G/Pham and Gorodetsky - 2022 - Ensemble Approximate Control Variate Estimators A.pdf:application/pdf},
}

@article{xu_bandit-learning_2022,
	title = {A {Bandit}-{Learning} {Approach} to {Multifidelity} {Approximation}},
	volume = {44},
	issn = {1064-8275, 1095-7197},
	url = {https://epubs.siam.org/doi/10.1137/21M1408312},
	doi = {10.1137/21M1408312},
	language = {en},
	number = {1},
	urldate = {2023-07-08},
	journal = {SIAM Journal on Scientific Computing},
	author = {Xu, Yiming and Keshavarzzadeh, Vahid and Kirby, Robert M. and Narayan, Akil},
	month = feb,
	year = {2022},
	pages = {A150--A175},
	file = {Submitted Version:/Users/me-tcoons/Zotero/storage/DYAWGKVU/Xu et al. - 2022 - A Bandit-Learning Approach to Multifidelity Approx.pdf:application/pdf},
}

@article{giles_multilevel_2015,
	title = {Multilevel {Monte} {Carlo} methods},
	volume = {24},
	issn = {0962-4929, 1474-0508},
	url = {https://www.cambridge.org/core/product/identifier/S096249291500001X/type/journal_article},
	doi = {10.1017/S096249291500001X},
	abstract = {Monte Carlo methods are a very general and useful approach for the estimation of expectations arising from stochastic simulation. However, they can be computationally expensive, particularly when the cost of generating individual stochastic samples is very high, as in the case of stochastic PDEs. Multilevel Monte Carlo is a recently developed approach which greatly reduces the computational cost by performing most simulations with low accuracy at a correspondingly low cost, with relatively few simulations being performed at high accuracy and a high cost.
            In this article, we review the ideas behind the multilevel Monte Carlo method, and various recent generalizations and extensions, and discuss a number of applications which illustrate the flexibility and generality of the approach and the challenges in developing more efficient implementations with a faster rate of convergence of the multilevel correction variance.},
	language = {en},
	urldate = {2022-09-01},
	journal = {Acta Numerica},
	author = {Giles, Michael B.},
	month = may,
	year = {2015},
	pages = {259--328},
	file = {Submitted Version:/Users/me-tcoons/Zotero/storage/YUW8AJE6/Giles - 2015 - Multilevel Monte Carlo methods.pdf:application/pdf},
}

@article{peherstorfer_optimal_2016,
	title = {Optimal {Model} {Management} for {Multifidelity} {Monte} {Carlo} {Estimation}},
	volume = {38},
	issn = {1064-8275, 1095-7197},
	url = {http://epubs.siam.org/doi/10.1137/15M1046472},
	doi = {10.1137/15M1046472},
	language = {en},
	number = {5},
	urldate = {2022-09-15},
	journal = {SIAM Journal on Scientific Computing},
	author = {Peherstorfer, Benjamin and Willcox, Karen and Gunzburger, Max},
	month = jan,
	year = {2016},
	pages = {A3163--A3194},
	file = {Full Text:/Users/me-tcoons/Zotero/storage/KWMLVXZV/Peherstorfer et al. - 2016 - Optimal Model Management for Multifidelity Monte C.pdf:application/pdf},
}

@article{bomarito_multi_2020,
	title = {Multi {Model} {Monte} {Carlo} with {Python} ({MXMCPy})},
	journal = {NASA/TM–2020–22058},
	author = {Bomarito, G.F. and Warner, J.E. and Leser, P.E. and Leser, W.P. and Morrill, L.},
	year = {2020},
}

@book{gelman_bayesian_2014,
	address = {Boca Raton},
	edition = {Third edition},
	series = {Chapman \& {Hall}/{CRC} texts in statistical science},
	title = {Bayesian data analysis},
	isbn = {9781439840955},
	publisher = {CRC Press},
	author = {Gelman, Andrew},
	year = {2014},
	keywords = {Bayesian statistical decision theory, MATHEMATICS / Probability \& Statistics / General},
}

@article{saxena_separation-based_2023,
	title = {Separation-based parameterization strategies for estimation of restricted covariance matrices in multivariate model systems},
	volume = {47},
	issn = {17555345},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S175553452300012X},
	doi = {10.1016/j.jocm.2023.100411},
	language = {en},
	urldate = {2024-01-19},
	journal = {Journal of Choice Modelling},
	author = {Saxena, Shobhit and Bhat, Chandra R. and Pinjari, Abdul Rawoof},
	month = jun,
	year = {2023},
	pages = {100411},
}

@inproceedings{
ament2023unexpected,
title={Unexpected Improvements to Expected Improvement for Bayesian Optimization},
author={Sebastian Ament and Sam Daulton and David Eriksson and Maximilian Balandat and Eytan Bakshy},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=1vyAG6j9PE}
}

@InProceedings{pmlr-v222-zhou24a,
  title = 	 {A Corrected Expected Improvement Acquisition Function Under Noisy Observations},
  author =       {Zhou, Han and Ma, Xingchen and Blaschko, Matthew B},
  booktitle = 	 {Proceedings of the 15th Asian Conference on Machine Learning},
  pages = 	 {1747--1762},
  year = 	 {2024},
  editor = 	 {Yanıkoğlu, Berrin and Buntine, Wray},
  volume = 	 {222},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {11--14 Nov},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v222/zhou24a/zhou24a.pdf},
  url = 	 {https://proceedings.mlr.press/v222/zhou24a.html},
  abstract = 	 {Sequential maximization of expected improvement (EI) is one of the most widely used policies in Bayesian optimization because of its simplicity and ability to handle noisy observations. In particular, the improvement function often uses the best posterior mean as the best incumbent in noisy settings. However, the uncertainty associated with the incumbent solution is often neglected in many analytic EI-type methods: a closed-form acquisition function is derived in the noise-free setting, but then applied to the setting with noisy observations. To address this limitation, we propose a modification of EI that corrects its closed-form expression by incorporating the covariance information provided by the Gaussian Process (GP) model. This acquisition function specializes to the classical noise-free result, and we argue should replace that formula in Bayesian optimization software packages, tutorials, and textbooks. This enhanced acquisition provides good generality for noisy and noiseless settings. We show that our method achieves a sublinear convergence rate on the cumulative regret bound under heteroscedastic observation noise. Our empirical results demonstrate that our proposed acquisition function can outperform EI in the presence of noisy observations on benchmark functions for black-box optimization, as well as on parameter search for neural network model compression.}
}


@article{ng_multifidelity_2014,
	title = {Multifidelity approaches for optimization under uncertainty},
	volume = {100},
	copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
	issn = {00295981},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/nme.4761},
	doi = {10.1002/nme.4761},
	language = {en},
	number = {10},
	urldate = {2024-08-13},
	journal = {International Journal for Numerical Methods in Engineering},
	author = {Ng, Leo W. T. and Willcox, Karen E.},
	month = dec,
	year = {2014},
	pages = {746--772},
	file = {Full Text:/Users/me-tcoons/Zotero/storage/Q96TYE46/Ng and Willcox - 2014 - Multifidelity approaches for optimization under un.pdf:application/pdf},
}

@article{barnard_modeling_2000,
	title = {Modeling covariance matrices in terms of standard deviations and correlations, with application to shrinkage},
	volume = {10},
	number = {4},
	journal = {Statistica Sinica},
	author = {Barnard, John and McCulloch, Robert and Meng, Xiao-Li},
	year = {2000},
	pages = {1281--1311},
}

@article{pinheiro_unconstrained_1996,
	title = {Unconstrained parametrizations for variance-covariance matrices},
	volume = {6},
	copyright = {http://www.springer.com/tdm},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/10.1007/BF00140873},
	doi = {10.1007/BF00140873},
	language = {en},
	number = {3},
	urldate = {2024-09-24},
	journal = {Statistics and Computing},
	author = {Pinheiro, Jos� C. and Bates, Douglas M.},
	month = sep,
	year = {1996},
	pages = {289--296},
}

@article{archakov_new_2021a,
	title = {A {New} {Parametrization} of {Correlation} {Matrices}},
	volume = {89},
	issn = {0012-9682},
	url = {https://www.econometricsociety.org/doi/10.3982/ECTA16910},
	doi = {10.3982/ECTA16910},
	abstract = {We introduce a novel parametrization of the correlation matrix. The reparametrization facilitates modeling of correlation and covariance matrices by an unrestricted vector, where positive definiteness is an innate property. This parametrization can be viewed as a generalization of Fisher's
              Z‐transformation to higher dimensions and has a wide range of potential applications. An algorithm for reconstructing the unique
              n×n correlation matrix from any vector in R n(n−1)/2 is provided, and we derive its numerical complexity.},
	language = {en},
	number = {4},
	urldate = {2024-09-20},
	journal = {Econometrica},
	author = {Archakov, Ilya and Hansen, Peter Reinhard},
	year = {2021},
	pages = {1699--1715},
	file = {Submitted Version:/Users/me-tcoons/Zotero/storage/23H8DY62/Archakov and Hansen - 2021 - A New Parametrization of Correlation Matrices.pdf:application/pdf},
}

@article{bucci_comparing_2022a,
	title = {Comparing unconstrained parametrization methods for return covariance matrix prediction},
	volume = {32},
	issn = {0960-3174, 1573-1375},
	url = {https://link.springer.com/10.1007/s11222-022-10157-4},
	doi = {10.1007/s11222-022-10157-4},
	abstract = {Abstract
            Forecasting covariance matrices is a difficult task in many research fields since the predicted matrices should be at least positive semidefinite. This problem can be overcome by including constraints in the predictive model or through a parametrization of the matrices to be predicted. In this paper, we focus on the latter approach in a financial application and analyse four parametrizations of the covariance matrices of asset returns. The aim of the manuscript is to understand if the parametrizations of the covariance matrices exhibit differences in terms of predictive accuracy. To this end, we critically analyse their predictive performance through both a Monte Carlo simulation and an empirical application with daily and weekly realized covariance matrices of stock assets. Our findings highlight that the Cholesky decomposition and the parametrization recently introduced by Archakov and Hansen are the overall best-performing methods in terms of forecasting accuracy.},
	language = {en},
	number = {5},
	urldate = {2024-09-20},
	journal = {Statistics and Computing},
	author = {Bucci, Andrea and Ippoliti, Luigi and Valentini, Pasquale},
	month = oct,
	year = {2022},
	pages = {90},
	file = {Full Text:/Users/me-tcoons/Zotero/storage/NR6GUVSK/Bucci et al. - 2022 - Comparing unconstrained parametrization methods for return covariance matrix prediction.pdf:application/pdf},
}

@article{dixon_covariance_2024,
	title = {Covariance {Expressions} for {Multifidelity} {Sampling} with {Multioutput}, {Multistatistic} {Estimators}: {Application} to {Approximate} {Control} {Variates}},
	volume = {12},
	issn = {2166-2525},
	shorttitle = {Covariance {Expressions} for {Multifidelity} {Sampling} with {Multioutput}, {Multistatistic} {Estimators}},
	url = {https://epubs.siam.org/doi/10.1137/23M1607994},
	doi = {10.1137/23M1607994},
	language = {en},
	number = {3},
	urldate = {2024-09-21},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Dixon, Thomas O. and Warner, James E. and Bomarito, Geoffrey F. and Gorodetsky, Alex A.},
	month = sep,
	year = {2024},
	pages = {1005--1049},
}

@InProceedings{pmlr-v70-loukas17a,
  title = 	 {How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?},
  author =       {Andreas Loukas},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2228--2237},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/loukas17a/loukas17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/loukas17a.html},
  abstract = 	 {How many samples are sufficient to guarantee that the eigenvectors of the sample covariance matrix are close to those of the actual covariance matrix? For a wide family of distributions, including distributions with finite second moment and sub-gaussian distributions supported in a centered Euclidean ball, we prove that the inner product between eigenvectors of the sample and actual covariance matrices decreases proportionally to the respective eigenvalue distance and the number of samples. Our findings imply <em>non-asymptotic</em> concentration bounds for eigenvectors and eigenvalues and carry strong consequences for the non-asymptotic analysis of PCA and its applications. For instance, they provide conditions for separating components estimated from $O(1)$ samples and show that even few samples can be sufficient to perform dimensionality reduction, especially for low-rank covariances.}
}

@techreport{silvennoinen_multivariate_2007,
	type = {Working {Paper}},
	title = {Multivariate {GARCH} models},
	copyright = {http://www.econstor.eu/dspace/Nutzungsbedingungen},
	url = {https://www.econstor.eu/handle/10419/56218},
	abstract = {This article contains a review of multivariate GARCH models. Most common GARCH models are presented and their properties considered. This also includes semiparametric and nonparametric GARCH models. Existing specification and misspecification tests are discussed. Finally, there is an empirical example in which several multivariate GARCH models are fitted to the same data set and the results compared with each other.},
	language = {eng},
	number = {669},
	urldate = {2024-09-17},
	institution = {SSE/EFI Working Paper Series in Economics and Finance},
	author = {Silvennoinen, Annastiina and Teräsvirta, Timo},
	year = {2007},
}

@techreport{weigand_matrix_2014a,
	type = {Working {Paper}},
	title = {Matrix {Box}-{Cox} models for multivariate realized volatility},
	copyright = {http://www.econstor.eu/dspace/Nutzungsbedingungen},
	url = {https://www.econstor.eu/handle/10419/99972},
	abstract = {We propose exible models for multivariate realized volatility dynamics which involve generalizations of the Box-Cox transform to the matrix case. The matrix Box-Cox model of realized covariances (MBC-RCov) is based on transformations of the covariance matrix eigenvalues, while for the Box-Cox dynamic correlation (BC-DC) specification the variances are transformed individually and modeled jointly with the correlations. We estimate transformation parameters by a new multivariate semiparametric estimator and discuss bias-corrected point and density forecasting by simulation. The methods are applied to stock market data where excellent in-sample and out-of-sample performance is found.},
	language = {eng},
	number = {144},
	urldate = {2024-08-13},
	institution = {BGPE Discussion Paper},
	author = {Weigand, Roland},
	year = {2014},
}

@article{Rodriguez2022,
    author =    {Rodriguez, Josue E. AND Williams, Donald R. },
    journal =   {The Quantitative Methods for Psychology},
    publisher = {TQMP},
    title =     {Bayesian Bootstrapped Correlation Coefficients},
    year =      {2022},
    volume =    {18},
    number =    {1},
    url =       {http://www.tqmp.org/RegularArticles/vol18-1/p039/p039.pdf },
    pages =     {39-54},
    abstract =  {We propose the Bayesian bootstrap (BB) as a generic, simple, and accessible method for sampling from the posterior distribution of various correlation coefficients that are commonly used in the social-behavioral sciences. In a series of examples, we demonstrate how the BB can be used to estimate Pearson's, Spearman's, Gaussian rank, Kendall's $\tau $, and polychoric correlations. We also describe an approach based on a region of practical equivalence to evaluate differences and null associations among the estimated correlations. In addition, we have implemented the methodology in the R package BBcor (https://cran.r-project.org/web/packages/BBcor/index.html). Example code and key advantages of the proposed methods are illustrated in an applied example.},
    doi =       {10.20982/tqmp.18.1.p039}
}

@misc{meinert_multivariate_2022,
	title = {Multivariate {Deep} {Evidential} {Regression}},
	url = {http://arxiv.org/abs/2104.06135},
	doi = {10.48550/arXiv.2104.06135},
	abstract = {There is significant need for principled uncertainty reasoning in machine learning systems as they are increasingly deployed in safety-critical domains. A new approach with uncertainty-aware neural networks (NNs), based on learning evidential distributions for aleatoric and epistemic uncertainties, shows promise over traditional deterministic methods and typical Bayesian NNs, yet several important gaps in the theory and implementation of these networks remain. We discuss three issues with a proposed solution to extract aleatoric and epistemic uncertainties from regression-based neural networks. The approach derives a technique by placing evidential priors over the original Gaussian likelihood function and training the NN to infer the hyperparameters of the evidential distribution. Doing so allows for the simultaneous extraction of both uncertainties without sampling or utilization of out-of-distribution data for univariate regression tasks. We describe the outstanding issues in detail, provide a possible solution, and generalize the deep evidential regression technique for multivariate cases.},
	urldate = {2024-03-25},
	publisher = {arXiv},
	author = {Meinert, Nis and Lavin, Alexander},
	month = feb,
	year = {2022},
	note = {arXiv:2104.06135 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{martin_informative_2021a,
	title = {Informative priors for correlation matrices: {An} easy approach},
	shorttitle = {Informative priors for correlation matrices},
	url = {https://srmart.in/informative-priors-for-correlation-matrices-an-easy-approach/},
	language = {en-US},
	urldate = {2024-09-18},
	journal = {Stephen R. Martin, PhD},
	author = {Martin, Stephen},
	month = apr,
	year = {2021},
}

@article{gorodetsky_generalized_2020,
	title = {A generalized approximate control variate framework for multifidelity uncertainty quantification},
	volume = {408},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999120300310},
	doi = {https://doi.org/10.1016/j.jcp.2020.109257},
	abstract = {We describe and analyze a variance reduction approach for Monte Carlo (MC) sampling that accelerates the estimation of statistics of computationally expensive simulation models using an ensemble of models with lower cost. These lower cost models — which are typically lower fidelity with unknown statistics — are used to reduce the variance in statistical estimators relative to a MC estimator with equivalent cost. We derive the conditions under which our proposed approximate control variate framework recovers existing multifidelity variance reduction schemes as special cases. We demonstrate that existing recursive/nested strategies are suboptimal because they use the additional low-fidelity models only to efficiently estimate the unknown mean of the first low-fidelity model. As a result, they cannot achieve variance reduction beyond that of a control variate estimator that uses a single low-fidelity model with known mean. However, there often exists about an order-of-magnitude gap between the maximum achievable variance reduction using all low-fidelity models and that achieved by a single low-fidelity model with known mean. We show that our proposed approach can exploit this gap to achieve greater variance reduction by using non-recursive sampling schemes. The proposed strategy reduces the total cost of accurately estimating statistics, especially in cases where only low-fidelity simulation models are accessible for additional evaluations. Several analytic examples and an example with a hyperbolic PDE describing elastic wave propagation in heterogeneous media are used to illustrate the main features of the methodology.},
	journal = {Journal of Computational Physics},
	author = {Gorodetsky, Alex A. and Geraci, Gianluca and Eldred, Michael S. and Jakeman, John D.},
	year = {2020},
	keywords = {Control variates, Monte Carlo, Multifidelity modeling, Variance reduction},
	pages = {109257},
}

@misc{ERADist2023,
  title = {{ERADist}: Probability Distribution Class},
  author = {{Engineering Risk Analysis Group}},
  year = {2023},
  month = {5},
  institution = {Technical University of Munich},
  url = {https://www.cee.ed.tum.de/era/software/eradist/},
}

@techreport{adams2021dakota,
  author = {Adams, B.M. and Bohnhoff, W.J. and Dalbey, K.R. and Ebeida, M.S. and Eddy, J.P. and Eldred, M.S. and Hooper, R.W. and Hough, P.D. and Hu, K.T. and Jakeman, J.D. and Khalil, M. and Maupin, K.A. and Monschke, J.A. and Ridgway, E.M. and Rushdi, A.A. and Seidl, D.T. and Stephens, J.A. and Swiler, L.P. and Winokur, J.G.},
  title = {Dakota, A Multilevel Parallel Object-Oriented Framework for Design Optimization, Parameter Estimation, Uncertainty Quantification, and Sensitivity Analysis: Version 6.15 User's Manual},
  institution = {Sandia National Laboratories},
  year = {2021},
  month = {November},
  type = {Sandia Technical Report},
  number = {SAND2020-12495}
}

@article{gorodetsky_grouped_2024,
	title = {Grouped approximate control variate estimators},
journal = {arXiv preprint},
volume = {2402.14736},
	eprint = {2402.14736},
	author = {Gorodetsky, Alex A. and Jakeman, John D. and Eldred, Michael S.},
	year = {2024},
}

@article{schaden_multilevel_2020,
	title = {On {Multilevel} {Best} {Linear} {Unbiased} {Estimators}},
	volume = {8},
	issn = {2166-2525},
	doi = {10.1137/19M1263534},
	language = {en},
	number = {2},
	urldate = {2025-01-13},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Schaden, Daniel and Ullmann, Elisabeth},
	year = {2020},
	pages = {601--635},
}
@misc{wang_stein_2019,
    title = {Stein {Variational} {Gradient} {Descent} {With} {Matrix}-{Valued} {Kernels}},
    url = {http://arxiv.org/abs/1910.12794},
    doi = {10.48550/arXiv.1910.12794},
    abstract = {Stein variational gradient descent (SVGD) is a particle-based inference algorithm that leverages gradient information for efficient approximate inference. In this work, we enhance SVGD by leveraging preconditioning matrices, such as the Hessian and Fisher information matrix, to incorporate geometric information into SVGD updates. We achieve this by presenting a generalization of SVGD that replaces the scalar-valued kernels in vanilla SVGD with more general matrix-valued kernels. This yields a significant extension of SVGD, and more importantly, allows us to flexibly incorporate various preconditioning matrices to accelerate the exploration in the probability landscape. Empirical results show that our method outperforms vanilla SVGD and a variety of baseline approaches over a range of real-world Bayesian inference tasks.},
    urldate = {2025-06-09},
    publisher = {arXiv},
    author = {Wang, Dilin and Tang, Ziyang and Bajaj, Chandrajit and Liu, Qiang},
    month = nov,
    year = {2019},
    note = {arXiv:1910.12794 [stat]},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}