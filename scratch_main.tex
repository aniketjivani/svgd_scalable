\documentclass[12pt]{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[utf8]{inputenc}

\usepackage{hyperref}
\usepackage{xcolor}

\hypersetup{
  colorlinks=true,
  citecolor=blue,
  linkcolor=red,
  urlcolor=magenta,
  }
  % citebordercolor=false}

\usepackage{float}
\usepackage{url}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{xurl}
\usepackage{lineno}
\usepackage{amsthm}
\usepackage{epsfig}
\usepackage[linesnumbered, ruled]{algorithm2e}

\usepackage{ulem}
\newcommand{\stkout}[1]{\ifmmode\text{\sout{\ensuremath{#1}}}\else\sout{#1}\fi}

\usepackage{multirow}
\usepackage{longtable}
\setlength\LTleft{0pt}
\usepackage{tabularx}

\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{float}
\usepackage{url}
% \setlength\LTleft{0pt}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{natbib}
% \bibliographystyle{unsrt}
\bibliographystyle{agu}
% \bibliographystyle{plainnat}

\newcommand\prob{\mathbb{P}}

\usepackage{xurl} %
%
\usepackage{lineno}

\usepackage{soul}
%
%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{float}
%
\usepackage{epsfig}
\usepackage[linesnumbered, ruled]{algorithm2e}

\usepackage{natbib}
\usepackage{color}

%
%
%

\usepackage{multirow}
\usepackage{longtable}
\setlength\LTleft{0pt}
\usepackage{tabularx}
%
\clearpage{}%


\newcommand{\xh}[1]{\textcolor{orange}{\textbf{(xh:)} #1}}

\newcommand{\aj}[1]{\textcolor{magenta}{\textbf{(aj:)} #1}}

%
\newcommand{\resp}[1]{\textcolor{red}{\textbf{Response: } #1}}
\newcommand{\respc}[1]{\textcolor{red}{#1}}


%
\renewcommand{\[}{\left[}
\renewcommand{\]}{\right]}
\renewcommand{\(}{\left(}
\renewcommand{\)}{\right)}

%
\newcommand{\dd}[2]{\frac{d #1}{d #2}}
\newcommand{\ddt}[1]{\frac{d #1}{d t}}
\newcommand{\ddd}[2]{\frac{d^2 #1}{d #2^2}}
\newcommand{\dddt}[1]{\frac{d^2 #1}{d t^2}}
\newcommand{\pp}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\ppp}[2]{\frac{\partial^2 #1}{\partial #2^2}}
\newcommand{\pppp}[2]{\frac{\partial^3 #1}{\partial #2^3}}
\newcommand{\ppppp}[2]{\frac{\partial^4 #1}{\partial #2^4}}

\newcommand{\adj}[1]{#1^{*}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\divergence}[1]{\nabla \cdot #1}
\newcommand{\enorm}[1]{\vvvert #1 \vvvert}
\newcommand{\grad}[1]{\nabla #1}
\newcommand{\laplace}[1]{\nabla^2 #1}
\newcommand{\norm}[2]{\left\|\, #1 \,\right\|_{#2}}
\newcommand{\order}[1]{\mathcal{O}\(#1\)}
\newcommand{\supp}{\mathop{\mathrm{supp}}}
\newcommand{\vvvert}{|\kern-1pt|\kern-1pt|}

\newcommand{\eq}[1]{\mathop{\,{\buildrel #1 \over =}\,}}
\newcommand{\ap}[1]{\mathop{\,{\buildrel #1 \over \approx}\,}}

%

%
\newcommand{\hg}{\hat{g}}
\newcommand{\hh}{\hat{h}}
\newcommand{\hi}{\hat{i}}
\newcommand{\hj}{\hat{j}}
\newcommand{\hk}{\hat{k}}
\newcommand{\hm}{\hat{m}}
\newcommand{\hn}{\hat{n}}
\newcommand{\hs}{\hat{s}}
\newcommand{\hu}{\hat{u}}
\newcommand{\hv}{\hat{v}}
\newcommand{\hx}{\hat{x}}

\newcommand{\hA}{\hat{A}}
\newcommand{\hC}{\hat{C}}
\newcommand{\hI}{\hat{I}}
\newcommand{\hJ}{\hat{J}}
\newcommand{\hN}{\hat{N}}
\newcommand{\hT}{\hat{T}}
\newcommand{\hU}{\hat{U}}

\newcommand{\hbf}{\boldsymbol{\hat{f}}}
\newcommand{\hbg}{\boldsymbol{\hat{g}}}
\newcommand{\hsig}{\hat{\sigma}}

%
\newcommand{\barg}{\bar{g}}
\newcommand{\barh}{\bar{h}}
\newcommand{\barm}{\bar{m}}
\newcommand{\barv}{\bar{v}}
\newcommand{\barw}{\bar{w}}
\newcommand{\barx}{\bar{x}}
\newcommand{\bary}{\bar{y}}

\newcommand{\barB}{\bar{B}}
\newcommand{\barC}{\bar{C}}
\newcommand{\barH}{\bar{H}}
\newcommand{\barJ}{\bar{J}}
\newcommand{\barN}{\bar{N}}
\newcommand{\barR}{\bar{R}}

\newcommand{\barmu}{\bar{\mu}}

%
\newcommand{\tf}{\tilde{f}}
\newcommand{\thh}{\tilde{h}}

\newcommand{\tA}{\tilde{A}}
\newcommand{\tg}{\tilde{g}}
\newcommand{\tH}{\tilde{H}}
\newcommand{\tJ}{\tilde{J}}
\newcommand{\tQ}{\tilde{Q}}
\newcommand{\tT}{\tilde{T}}

\newcommand{\tmu}{\tilde{\mu}}
\newcommand{\ttheta}{\tilde{\theta}}
\newcommand{\txi}{\tilde{\xi}}

\newcommand{\tTheta}{\tilde{\Theta}}
\newcommand{\tXi}{\tilde{\Xi}}

%
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\sbf}[1]{\boldsymbol{#1}}

\newcommand{\bb}{\textbf{b}}
\newcommand{\bd}{\textbf{d}}
\newcommand{\bee}{\textbf{e}}
\newcommand{\bff}{\textbf{f}}
\newcommand{\bh}{\textbf{h}}
\newcommand{\bg}{\textbf{g}}
\newcommand{\bk}{\textbf{k}}
\newcommand{\bii}{\textbf{i}}
\newcommand{\bj}{\textbf{j}}
\newcommand{\bl}{\textbf{l}}
\newcommand{\bn}{\textbf{n}}
\newcommand{\bp}{\textbf{p}}
\newcommand{\br}{\textbf{r}}
\newcommand{\bs}{\textbf{s}}
\newcommand{\bt}{\textbf{t}}
\newcommand{\bu}{\textbf{u}}
\newcommand{\bv}{\textbf{v}}
\newcommand{\bw}{\textbf{w}}
\newcommand{\bx}{\textbf{x}}
\newcommand{\by}{\textbf{y}}

\newcommand{\bA}{\mathbf{A}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bN}{\mathbf{N}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bY}{\mathbf{Y}}

\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bepsilon}{\boldsymbol{\epsilon}}
\newcommand{\bhsig}{\boldsymbol{\hsig}}
\newcommand{\bpsi}{\boldsymbol{\psi}}
\newcommand{\bsig}{\boldsymbol{\sigma}}
\newcommand{\btau}{\boldsymbol{\tau}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bxi}{\boldsymbol{\xi}}

\newcommand{\bDelta}{\boldsymbol{\Delta}}
\newcommand{\bTheta}{\boldsymbol{\Theta}}
\newcommand{\bXi}{\boldsymbol{\Xi}}
\newcommand{\bOmega}{\boldsymbol{\Omega}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}

%
\newcommand{\EE}{\mathbb{E}}
\newcommand{\II}{\mathbb{I}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\RR}{\mathbb{R}}

%
\newcommand{\vt}{\vec{t}}
\newcommand{\vu}{\vec{u}}
\newcommand{\vv}{\vec{v}}
\newcommand{\vx}{\vec{x}}

\newcommand{\vV}{\vec{V}}

\newcommand{\vo}{\vec{\omega}}

%
\newcommand{\dotk}{\dot{k}}
\newcommand{\dotm}{\dot{m}}
\newcommand{\dotx}{\dot{x}}

\newcommand{\dotomega}{\dot{\omega}}

%
\newcommand{\CA}{\mathcal{A}}
\newcommand{\CB}{\mathcal{B}}
\newcommand{\CD}{\mathcal{D}}
\newcommand{\CE}{\mathcal{E}}
\newcommand{\CF}{\mathcal{F}}
\newcommand{\CH}{\mathcal{H}}
\newcommand{\CJ}{\mathcal{J}}
\newcommand{\CK}{\mathcal{K}}
\newcommand{\CL}{\mathcal{L}}
\newcommand{\CM}{\mathcal{M}}
\newcommand{\CN}{\mathcal{N}}
\newcommand{\CR}{\mathcal{R}}
\newcommand{\CS}{\mathcal{S}}
\newcommand{\CT}{\mathcal{T}}
\newcommand{\CU}{\mathcal{U}}
\newcommand{\CV}{\mathcal{V}}
\newcommand{\CW}{\mathcal{W}}
\newcommand{\CX}{\mathcal{X}}
\newcommand{\CY}{\mathcal{Y}}

\newcommand{\CbarJ}{\bar{\mathcal{J}}}
\newcommand{\CbarL}{\bar{\mathcal{L}}}
\newcommand{\CbarR}{\bar{\mathcal{R}}}

%
\newcommand{\du}{\delta{u}}

\newcommand{\dbeta}{\delta{\beta}}
\newcommand{\dxi}{\delta{\xi}}
\newcommand{\deta}{\delta{\eta}}
\newcommand{\drho}{\delta{\rho}}
\newcommand{\dtau}{\delta{\tau}}

\newcommand{\dbu}{\delta{\boldsymbol{u}}}
\newcommand{\dbp}{\delta{\boldsymbol{p}}}
\newcommand{\dbx}{\delta{\boldsymbol{x}}}

\newcommand{\Dx}{\Delta{x}}
\newcommand{\Dy}{\Delta{y}}
\newcommand{\Dt}{\Delta{t}}

%
\newcommand{\myblue}[1]{{\color[rgb]{0,0,0.65} #1}}
\newcommand{\mygreen}[1]{{\color[rgb]{0,.65,0} #1}}
\newcommand{\mywhite}[1]{{\color[rgb]{1.0,1.0,1.0} #1}}
\newcommand{\myred}[1]{{\color[rgb]{0.65,0.0,0.0} #1}}
\newcommand{\myblack}[1]{{\color[rgb]{0.0,0.0,0.0} #1}}
\newcommand{\mygrey}[1]{{\color[rgb]{0.6,0.6,0.6} #1}}

%
% \newcommand{\coo}{CO$_2$}
% \newcommand{\hho}{H$_2$O}
% \newcommand{\oo}{O$_2$}
% \newcommand{\nn}{N$_2$}
% \newcommand{\mwe}{MW$_e$}
% \newcommand{\nox}{NO$_{\textrm{x}}$}
% \newcommand{\cooe}{CO$_{2e}$}
% \newcommand{\nno}{N$_2$O}
% \newcommand{\noo}{NO$_2$ }
% \newcommand{\chhhh}{CH$_4$ }
% \newcommand{\hhoo}{H$_2$O$_2$}
% \newcommand{\hhoor}{H$_2$-O$_2$}
%
\newcommand{\degs}{^\circ}
% \newcommand{\Jpkmol}{\frac{J}{kmol}}
% \newcommand{\JpkmolK}{\frac{J}{kmol K}}
% \newcommand{\Jpkg}{\frac{J}{kg}}
% \newcommand{\JpkgK}{\frac{J}{kg K}}

\newcommand{\ra}{\rightarrow}
\newcommand{\Ra}{\Rightarrow}
\newcommand{\LRa}{\Longrightarrow}
\newcommand{\lra}{\longrightarrow}

\newcommand{\pe}{\,{\scriptstyle +}\!\!=}
\newcommand{\me}{\,{\scriptstyle -}\!\!=}
\newcommand{\Var}{\textrm{Var}}
\newcommand{\Cov}{\textrm{Cov}}
\newcommand{\diag}{\textrm{diag}}

\newcommand{\etal}{\textit{et al.}}

% \newcommand{\dkl}

\def\sgn{\mathop{\rm sgn}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\DKL}{D_{\mathrm{KL}}}

\newcommand{\iid}{\stackrel{\textrm{iid}}{\sim}}
\newcommand{\ti}[1]{\textbf{Title: }\textit{{#1}}}

\newcommand{\alf}{Alfv\'{e}n}
\newcommand{\Rs}{R$_{\odot}$}


\usepackage{algorithmic}
% \usepackage{algpseudocode}



\linenumbers

\title{SVGD via Continuous Kernel Normalizing Flows for Bayesian Inference}
\author{Aniket}
\date{\today}

\begin{document}

\maketitle

(many thanks to Thomas Coons for sparking my interest in this topic - this is a very rich area of computational statistics that needs more attention and some cool new ideas!!!)

\section{Abstract}

(main motivation is to have a solid and reliable way of building BNNs - also maybe needed for BOED surrogate training if we are looking for MCMC alternatives -  we need to understand the theory behind Stein a bit better for that)

(we will review the use of SVGD for Bayesian Inference, the challenges in scaling it up and if ideas from sparse GPs could be applied to make it more usable with many particles)

(another perspective is that of continuous normalizing flows. Can tricks from these e.g. trace estimators help with the computation for many particles? also, is there an elegant and simple way to tie together the framework of continuous time NNs and weight updates as a single augmented system?)

\begin{enumerate}
    \item SVGD review \cite{liu_short_2016,liu_kernelized_2016,liu_stein_2019}

    \item SVGD drawbacks (mode collapse?) and projected SVGD \cite{chen_projected_2020}

    \item Ideas from sparse GP + VI \url{https://gpss.cc/gpss17/slides/gp-approx-new.pdf} and \cite{noack_unifying_2023}. Slightly better: start from the simplest case i.e. \cite{snelson_sparse_2005} and then move on to \cite{titsias_variational_2009,titsias_bayesian_2010}

    \item Accelerated SVGD? SVI for Deep GPs? \cite{hoffman_stochastic_2013}

    \item are there any ideas from structure preserving methods (is HMC structure preserving?) that can be seamlessly incorporated into SVGD? here are structure preserving Gaussian processes as an example: \url{https://arxiv.org/pdf/2102.01606.pdf}
    
    \item (hopefully) success!
\end{enumerate}

\begin{enumerate}
    \item Spectral Delta Kernels \cite{lazaro-gredilla_sparse_2010}

    \item Active Subspace based reduction for BNNs \cite{jantre_learning_2023}

    \item Continuous Normalizing Flows \cite{grathwohl_ffjord_2018}

    \item Kernelized Normalizing Flows \cite{english_kernelised_2024}

    \item SVGD as Gradient Flow \cite{liu_stein_2017}

    \item Stein breakdown in high dimensions \cite{ba_towards_2019}

    % \item Stein's Method for High Dimensions \cite{chang_kernel_2020} - while the paper is almost unreadable, the authors try to use ideas from score matching such as Anneal-SGLD on the vanilla SVGD procedure and find that the kernel choice is inadequate because the bandwidth is not changing with respect to the noise level. Their proposed kernel includes an autoencoder for dimensionality reduction and conditions the hyperparameters on $\sigma$.

    \item Multilevel SVGD \cite{alsup_multilevel_2022}

    \item Sparse Sinkhorn Attention \cite{tay_sparse_2020}
\end{enumerate}

\section{Literature}
We motivate our method by first introducing the concepts of Stein discrepancy, Stein gradient descent, learning kernel operators from Gaussian Processes and the scope for introducing more scalable methods for the same. 

GPs are specified by a placing a Gaussian prior on the space of functions $f$ that model the mapping between inputs $X$ and outputs $y$:

$$p(f | X) = \mathcal{N}(0, K_N)$$

Here $K$ is the (parametrized) covariance kernel which expresses some prior notion of smoothness of the underlying function.

The marginal likelihood $p(y | X, \mathcal{D}, \theta)$ is used to train the GP and learn the hyperparameters. The posterior predictive distribution uses standard formulas for update of the mean and covariance of the prior GP, and is generally prohibitive for vanilla GPs because of the $O(N^3)$ cost of the covariance matrix.

\subsection{Sparse GPs}

\citep{snelson_sparse_2005} To fit a sparse GP, we consider a pseudo-dataset $\bar{\mathcal{D}} = \{(\bar{X}, \bar{f})_m\}_{m=1}^{M}$, with $M<N$ Since the targets are not real observations, they are denoted by $\bar{f}$ instead of $\bar{y}$ and set equal to latent function values.

We have to determine the following:

\begin{enumerate}
    \item Posterior distribution over pseudo-targets $\bar{f}$

    \item Predictive distribution for new $x_{\ast}$

    \item Marginal likelihood expression to find pseudo-input locations!
\end{enumerate}

\subsection{SVGD Algorithm}
Most of the math for SVGD comes from \cite{liu_short_2016,liu_kernelized_2016}.

Consider set of initial particles $\{x_i^0\}_{i=1}^n$ and target density $p(x)$. Then for $l$ update steps, the following transports points from initial distribution to target distribution $p(x)$ (or its approximation $q(x)$)

$$x_i^{l+1} \leftarrow x_i^{l} + \epsilon \frac{1}{n}\sum_{i=1}^{n}\left[k(x_j^l, x) \nabla_{x_j^l}(p(x_j^l)) + \nabla_{x_j^l}k(x_j^l, x)\right]$$

% \begin{algorithm}
% \caption{SVGD from \cite{liu_kernelized_2016}}\label{alg: svgd_vanilla}
% \begin{algorithmic}
% \KwData{$p(x)$}

% \For{\texttt{blah}}
% \STATE $i\gets 10$
% \IF {$i\geq 5$} 
%   \STATE $i\gets i-1$
% \ELSE
%   \IF {$i\leq 3$}
%     \STATE $i\gets i+2$
%   \ENDIF
% \ENDIF

% \end{algorithmic}
% \end{algorithm}

\subsection{Projected SVGD}
\citep{chen_projected_2020} Projected SVGD finds a low-dimensional subspace for projecting $x$ and performs SVGD on a lower dimensional $x$ to avoid the kernel function becoming degenerate for large $d$. This feels like it solves the major problems of SVGD already, but we are probably coming at it from a different angle i.e. just the steps and approximations of the original algorithm and if there are any generalizations or improvements to be pursued there.

\subsection{Normalizing Flows}
Understanding CNFs also requires that we obtain a detailed understanding of how normalizing flows are proposed and solved, as well as the divergence loss depending on the target application. \cite{papamakarios_normalizing_2021}

\section{Methodology}

Let's start by asking the following questions:
\begin{enumerate}
    \item What kinds of kernels does SVGD hold for? What alternatives exist to avoid degeneracy? The original paper still claims that a lot of the problematic parts of large $n$ can be easily bypassed.

    \item Is there a meaningful connection between determination of pseudo-inputs for sparse GPs and GD style updates? How do we find pseudo-inputs for the SVGD case? \textcolor{red}{remember that this is most likely a fruitless exercise, these ideas are defined for very different applications: entirely non-parametric methods vs NNs where people look for nice ways to combine optimization over NN parameters (data-fitting) with posterior updates for the weights.}

    \textcolor{red}{We either need stand-alone kernel approximations - which don't involve learning inducing points, or a clever way to learn inducing point locations}

    \begin{enumerate}
        \item Initialize inducing points

        \item (some update step for transporting inducing points)

        \item (some correction step for moving them around so they represent the original dataset)
    \end{enumerate}

    \item What are the precise connections between SVGD and flow-based models like CNFs? Can we combine neural network weight updates in a flow-based model with posterior computations?

    \textcolor{blue}{The simplest thing we can actually try, funnily enough, is just replacing $p(.)$ by some MLP and / or a richer kernel representation for $k$ - the points being that}
    
    \begin{enumerate}
        \item we can get rid of the variational family and consider a more general case - if there is a strong case for the quality of posterior learnt by VI

        \item Right now a lot of the methodology here feels like a bunch of tricks to learn the kernel hyperparameters - but surely there is a rigorous way to actually learn these, or a more general and flexible form of Stein that accomodates different choices - again, if there are advantages for a different kernel family (how do we show this?) - the theory and literature on spectral delta kernels makes it clear that there are problems where composing kernels or learning an arbitrary stationary kernel is beneficial. 
    \end{enumerate}

    \item SVGD for Monte-Carlo integration (in combination with level-set methods)
\end{enumerate}



% Here is the original algorithm for performing SVGD (assume RBF kernel):


% \noindent Here's our approximation.


% \noindent Here's the modified algorithm.

Let's write down some notes for answering question 3 first:

First we remember that the map is usually constructed to be invertible, starting from the data-generating distribution $x$ and mapping to a standard base distribution (e.g. particles from $q_0$) i.e. the flow direction is reversed. The invertibility ensures we can then draw from the base distribution to sample the posterior distribution.

Let $l$ represent the artificial depth variable or the time variable for integration, and $y_i$ some state variable we wish to update then:

$$y_i^l = y_i^0 + \int_0^{l}g_\theta(y_0, l)dl$$

where $g_\theta$ is the MLP for the flow dynamics.

For CNFs the above can be written as:

$$\log p(x) = \log p(x_0) + \int_l^0 -\operatorname{Tr}\left(\frac{\partial f}{\partial x}\right) dl$$

\stkout{The base distribution gets approximated by particles over here as well, so we end up with (for a single step update):}

$$x_i^{l+1} = x_i^{l} + \int_{t_{l + 1}}^{t_l} -\operatorname{Tr}\left(\frac{\partial f}{\partial x}\right) dl$$


and at the final step:

$$x_i^{l} = x_i^{0} + \int_{l}^{0}\operatorname{Tr}\left(\frac{\partial f}{\partial x}\right) dl $$

\textcolor{red}{okay, so the above is definitely wrong. Its not that the base distribution is approximated by particles, rather CNF solves an augmented system where we find the particle that generates a specific $x$ and solve for the instantaneous change of variables formula that maximizes the log-likelihood of $x$ under that transformation.}


\stkout{(doing what's already been done before and probably isn't very remarkable once I think about it a little more, unless I am very mistaken). Let's take a small step back. Before we introduce the kernel function, the original Stein operator based update, when using a variational family $q(x)$ to approximate $p(x)$ reads as:}

% $$x_i^{l+1} = x_i^{l} + \mathbb{E}_{x \sim q}\left[-\operatorname{Tr}(\mathcal{A}_p \phi(x))\right]$$

\begin{equation*}
    x_i^{l+1} = x_i^{l} + \epsilon \mathbf{\phi}_{\hat{\mu}_l^n, p}^{\ast}(x_l^i)
\end{equation*}

the problematic part of the Stein approximation is that of the non-linear velocity field $\phi$ which should maximally decrease KL divergence w.r.t. target distribution. 

Another way to write it is through the operation of a fixed point non-linear map on empirical measure $\hat{\mu}$:

$$\hat{\mu}_{l + 1}^n = \Phi_p (\hat{\mu}_l^n)$$

% or,

% $$x_i^{l+1} = x_i^{l} + \int \left[-\operatorname{Tr}(\mathcal{A}_p \phi(x))\right] q(x) dx$$


\subsection{Metrics for normalizing flows}

(from \cite{papamakarios_normalizing_2021}):
Options for comparing distributions are grouped into two general families (density ratios vs differences):

\begin{enumerate}
    \item $f$-divergence:

    $$D_f\left[p_\mathrm{x}^*(\mathbf{x})\parallel p_\mathrm{x}(\mathbf{x};\boldsymbol{\theta})\right]=\mathbb{E}_{p_\mathrm{x}(\mathbf{x};\boldsymbol{\theta})}\left[f\left(\frac{p_\mathrm{x}^*(\mathbf{x})}{p_\mathrm{x}(\mathbf{x};\boldsymbol{\theta})}\right)\right]$$

    \item Integral Probability Metrics (IPMs) (\textcolor{red}{Stein uses maximum mean discrepancy?})

    $$\delta_s\left[p_\mathrm{x}^*(\mathbf{x})\left\|p_\mathrm{x}(\mathbf{x};\boldsymbol{\theta})\right.\right]=\mathbb{E}_{p_\mathrm{x}^*(\mathbf{x})}\left[\left.s(\mathbf{x})\right.\right]-\mathbb{E}_{p_\mathrm{x}(\mathbf{x};\boldsymbol{\theta})}\left[\left.s(\mathbf{x})\right.\right]$$
\end{enumerate}

(using example of divergence) - we have to choose between forward and reverse KL. The forward KL is well suited for when we have samples from the target distribution (or we can generate them) but we cannot necessarily evaluate the target density. Minimizing the MC approximation of the KL divergence here is equivalent to fitting the flow-based model by MLE.

The reverse KL is often used for variational inference - we can fit a flow based model even if we cannot evaluate the base density or compute inverse transformation - it is useful when samples cannot be drawn efficiently from the target model, so the flow model essentially replaces the target model as a fast surrogate.

\subsection{Continuous normalizing flows for BI}


\section{General Literature}

\noindent \textbf{Latent Force Models:} Connecting Differential Equations and GP Kernels - see \cite{alvarez09a} for details.

\noindent This assumes latent forcing functions that give rise to the state evolution of the system, and assuming a GP prior on those functions leads to closed form expressions for the output kernel. Then the log-likelihood can be minimized, leading to standard inference based on GP regression.

\noindent \textbf{Sparse Spectrum Kernels:} Based on trigonometric basis functions, Fourier duals - see \cite{lazaro-gredilla_sparse_2010}

Details of Stein's Operator and Applications: See \cite{anastasiou_steins_2023}

\noindent Derivation of SVGD!


\noindent Rewriting the Variational part with a different approximation (\textsc{REINFORCE} vs the reparametrization trick! - see \url{https://stillbreeze.github.io/REINFORCE-vs-Reparameterization-trick/} for details


\noindent Details of amortized Stein for training probabilistic NNs: See \cite{feng_learning_2017}


\noindent \textcolor{red}{\textbf{Stein operators for other divergences:}} This basically means that we come up with a viable form of the Stein operator s.t. the maximization of the Stein discrepancy can be linked with other divergence formulas. See \cite{zozor_debruijn_2015} and functional gradients for ways to evaluate arbitrary divergences. \textcolor{red}{This would give rise to a generalized version of SVGD.} 


\noindent \textcolor{red}{Other divergences?}: Also see - \cite{kattumannil_steins_2009} for generalization of Stein's identity to exponential families. Finally, if there is a reason to favour Wasserstein distance over regular KL, \cite{huggins_practical_2018} bounds Wasserstein distance via the $(p, \nu)-$Fisher distance:

$$d_{p, \nu} (\eta, \hat{\eta}) = \| \| \nabla U - \nabla \hat{U}\|_2\|_{L^p(\nu)} = \left\{\int \| \nabla U(\theta) - \nabla \hat{U}(\theta) \|_2^p \nu(d\theta)\right\}^{1/p}$$ where $U = -\log d\hat{\eta}/d\theta$.

A more simplified notation for the above (called in other places as the \emph{Fisher divergence} or the \emph{Fisher information distance} is given as \citep{sriperumbudur_density_2017}:

$$J(p || q) = \frac{1}{2} \int_{\Omega} p(x) ||\nabla \log p(x) - \nabla \log q(x)||_2^2 dx$$

Funnily, Fisher is closely related to KL through the de Bruijn's identity (see the above reference for more details), and ostensibly its a stronger form of convergence than in KL (total variation distance?) (also see \url{https://math.stackexchange.com/a/4711339})

A final interesting point is using variational approximations for Fisher divergence instead of KL - see \cite{yang_variational_2019} for a proposed method that does away with mean-field assumptions (we are not picky about using KL for Bayesian inference unlike for OED because the latter has theoretical constraints for what counts as a measure of information when measuring information gain in parameters / objective function).


\noindent \textbf{Particle transport methods} \emph{``Particles may be regarded as Dirac-delta approximations of otherwise continuous measures or densities. However, continuum transport problems differ from true particle-dynamics problems in one important respect $\cdots$ The velocity field that governs the instantaneous motion of the particles depends on local density gradients, not just particle positions.''} - from paper on OT methods for advection-diffusion \cite{fedeli_geometrically-exact_2017}. An example of advection using particle based methods can be found here: \href{https://docs.oceanparcels.org/en/latest/examples/tutorial_analyticaladvection.html}{Lagrangian simulators!} and some more thoughts on this over here: \url{https://math.temple.edu/~seibold/research/meshfree/}

\noindent \textbf{SVGD as Gradient Flow} Stein is a particle-based approximation to an evolutionary PDE of densities or measures...\emph{Empirical measures of SVGD samples weakly converge to the target distribution, $\cdots$ asymptotic behaviour $\cdots$ characterized by a nonlinear Fokker-Planck equation known as Vlasov equation in physics.} \cite{liu_stein_2017}

Our understanding here is still incomplete, mainly w.r.t the following few points:

\begin{enumerate}
    \item Derivation of the gradient flow equation - I am completely lost in the steps there - some trace trick?

    \item Derivation of the gradient step of KL in terms of the kernel and the Fisher score

    \item Minimization of other Stein discrepancies (non-kernelized) (see \cite{anastasiou_steins_2023} for overview)

    \item Theorem 3.3 from \cite{liu_stein_2017}

    \item Fixed point iteration perspective vs differential equation perspective (one is for particles while the other is for densities??)
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/stein_overview.jpg}
    \caption{My rudimentary understanding of the ideas involved in Stein}
    \label{fig:stein-big-picture}
\end{figure}

Based on the above, however, we can draw up a very rough action plan!

\begin{enumerate}
    \item \stkout{How many particles are too many? i.e. when does the kernel approximation really break down? Probably need to stress test on some tougher canonical example for this.} How many particles are too little? How costly is it to run more particles? (with all tricks related to subsampling etc.)? Stress test on a tougher canonical example. Definitely , pSVGD could also be an interesting use case (numerical stability of active subspace eigenvectors?? - see \cite{hauth_advances_2024} for details.)

    \item Start with the augmented CNF \cite{grathwohl_ffjord_2018}:
    $$\underbrace{\begin{bmatrix}\mathbf{z}_0\\\log p(\mathbf{x})-\log p_{z_0}(\mathbf{z}_0)\end{bmatrix}}_{\text{solutions}}=\underbrace{\int_{t_1}^{t_0}\begin{bmatrix}f(\mathbf{z}(t),t;\theta)\\-\operatorname{Tr}\left(\frac{\partial f}{\partial\mathbf{z}(t)}\right)\end{bmatrix}dt}_{\text{dynamics}},\quad\underbrace{\begin{bmatrix}\mathbf{z}(t_1)\\\log p(\mathbf{x})-\log p(\mathbf{z}(t_1))\end{bmatrix}=\begin{bmatrix}\mathbf{x}\\0\end{bmatrix}}_{\text{initial values}}$$

    and replace $f$ by the Stein update (so we need to find the expression with the kernel, and also get its trace!) - if we are not doing something silly here.

    \textcolor{red}{Sep 4: }One interesting point here is that the $f$ over here can be replaced by the kernel function based update. \textcolor{red}{One idea is that a spectral mixture kernel may not be degenerate for large $d$, and we could also parametrize the kernel as a neural network (think of Deep Kernel Learning style papers), because the CNF essentially gives us the ability to deal with an unrestricted Jacobian via the trace approximation. We will look at these methods next.}

    \textcolor{red}{Sep 4: }Then we can run a few comparisons right off the bat with CNFs: Number of function evaluations needed in forward and backward pass, plots of learnt dynamics (are they regularized out of the box?!) and also try experiments with a bunch of kernels. 
    \textcolor{red}{Hopefully, we get some indication of whether this is adding something useful in terms of being able to handle more particles, which would be one of the main proposed contributions}.

    \textcolor{red}{Sep 4: One more gotcha is that kernelized flows are not a new subject. Here's a recent paper introducing them as a parameter efficient means to do NF - see \cite{english_kernelised_2024} for more!}

    To be clear, we also need to investigate some claims in greater detail, notably performance improvements of CNFs over NFs, 
    % what freedom it allows in terms of choice of $f$ and trying to find more robust links between the OT-based regularization and any `implicit' regularization in Stein's method. The motivation is also not completely clear, and we should clarify how it fits into the broader aims of the thesis too.

    \textcolor{red}{One key advantage may be that once the change of variables formula has been simplified to use the trace, unbiased estimators of the trace can be deployed, resulting in a free-form Jacobian.}

    \textcolor{red}{What then about the form of $f$?} One idea is that $f = \grad_x \log p(x) \phi(x)^T + \grad_x \phi(x)$ where $\phi$ can now be replaced by any suitable choice of kernel. I wonder if this becomes a two-step update in that case, where we first solve for $z$ with some initialized / older value for $p(x)$ and then update the logpdf of the same in the second step (this suddenly looks suspiciously like symplectic integration).

    \item More theoretical stuff, quality of posterior compared to other methods, more (faraway) thoughts on kernel approximations.

    \item 
\end{enumerate}

\noindent \textbf{Connections to transport maps}

Let $\mu_{ref}$ be a reference distributions from which we can generate independent and unweighted samples, e.g. a standard Gaussian. Then a transport map $T$ pushes forward $\mu_{ref}$ to $\mu_{tar}$ (a target measure we wish to characterize). 

Compactly, this can be written as:

$$T_{\#}\mu_{ref} = \mu_{tar}$$

One example of a valid transport map may be found by the global minimizer of the optimization problem based on KL divergence i.e.

$$\min \DKL (T_{\#} \eta || \pi)$$

s.t. $$\det \nabla T > 0$$

We can also address problems of inverse transport e.g. where the target density is unknown (ABC, density estimation) and compute a transport to push forward the target to the reference measure via convex optimization. For triangular maps, the structure can be exploited to efficiently compute the inverse map from reference to the target.

\textcolor{red}{Iterative construction of maps to minimize KL}

\section{Flow Matching}
% \subsection{Flow Matching}

Below, we briefly review the current practice in conditional flow matching algorithms and motivate their use in learning maps between samples from multiple-fidelity model likelihoods.

Generative modeling tasks consider the problem of approximating and sampling from a probability distribution. For instance, continuous normalizing flows (CNFs) proposed in \cite{grathwohl_ffjord_2018} express an invertible mapping between a fixed and tractable source distribution and the data distribution using Neural Ordinary Differential Equations (NODEs). CNFs can be trained and scaled to larger datasets better through a regression of the ODE drift, a so-called Flow Matching (FM) objective. The advances proposed in \cite{tong_improving_2024} generalize the flow matching framework proposed by \cite{lipman_flow_2023} to use transport maps between arbitrary distributions and approximate the dynamic Optimal Transport problem during sampling of conditional paths to improve the efficiency of training and inference. 

A smooth time-varying vector field $u: [0, 1] \times \RR^d \to \RR^d$ is defined by the following ordinary differential equation (ODE):
\begin{equation}
    dx = u(t, x)dt
\end{equation}

Given density $p_0$ over $\RR^d$, if $\phi_t(x)$ is the solution for the above ODE (i.e. it denotes the position of point $x$ transported along $u$ from time $0$ up to time $t$), this integration map introduces a pushforward density $p_t$ ($p_t$ is the density of points $x \sim p_0$ transported along $u$ from time 0 to time $t$) that is characterized by the continuity equation:

\begin{equation}
    \frac{\partial p}{\partial t} = -\grad.\left(p_t u_t\right)
\end{equation}

We can characterize the marginal probability path $p_t$ as a mixture of conditional probability paths:

\begin{equation}
    p_t(x) = \int p_t(x | z) q(z) dz
\end{equation}

If a path $p_t(x|z)$ is generated from $u_t(x | z)$ from initial conditions $p_0(x | z)$, then the vector field $u_t(x)$ given by:

\begin{equation}
    u_t(x) = \EE_{q(z)}\left[\frac{u_t(x|z)p_t(x|z)}{p_t(x)}\right]\label{eq: marg_cond_velocity_field}
\end{equation}

generates probability path $p_t(x)$ from initial conditions $p_0(x)$.

The key ideas behind flow matching rely on this connection between marginal vector fields and marginal probability paths to decompose the intractable marginal vector field into conditional vector fields.

The original flow matching objective considered in \cite{lipman_flow_2023} is intractable for general source density $q_0$ mapping to target $q_1$:

\begin{equation}
    \CL_{\text{FM}}(\theta)= \EE_{t \sim \mathcal{U}(0, 1), x \sim p_t(x)} ||v_\theta(t, x) - u_t(x)||^2
\end{equation}

In the special case where the marginal densities $p_t(x) = \CN(x | \mu_t, \sigma_t^2)$ are Gaussian, a possible simple (but non-unique) ODE satisfies:

\begin{equation}
    \phi_t(x_0) = \mu_t + \sigma_t \left(\frac{x_0 - \mu_0}{\sigma_0}\right) \label{eq: gaussian_ode_sol}
\end{equation}

and the unique vector field whose integration map satisfies \eqref{eq: gaussian_ode_sol} has the form:

\begin{equation}
    u_t(x) = \frac{\sigma_t'}{\sigma_t} (x - \mu_t) + \mu_t' \label{eq: vector_field_gauss_dens}
\end{equation}

where $\sigma_t'$ and $\mu_t'$ denote the time derivatives of $\sigma_t$ and $\mu_t$ respectively, and the vector field $u$ with initial conditions $\CN(\mu_0, \sigma_0^2)$ generates the Gaussian probability path $p_t(x) = \CN(x | \mu_t, \sigma_t^2)$.

The flow matching loss under assumptions of existence and exchange of different integrals, and boundedness of the solution fields can be switched out for an easier-to-regress conditional flow matching loss with the loss gradient only changing by a constant factor that is independent of $\theta$:

\begin{equation}
    \CL_{\text{CFM}}(\theta) = \EE_{t, q(z), p_t(x|z)}||v_\theta(t, x) - u_t(x|z)||^2 \label{eq: cfm_loss}
\end{equation}

For all practical purposes, we can solve the much more useful CFM objective, under the condition that we can:

\begin{enumerate}
    \item Sample from $q(z)$

    \item Sample from $p_t(x|z)$

    \item Calculate $u_t(x|z)$
\end{enumerate}

\begin{equation}
    \nabla_{\theta}\CL_{\text{FM}}(\theta) =\nabla_{\theta}\CL_{\text{CFM}}(\theta) \label{eq: cfm_fm_grad}
\end{equation}

A short proof of this is provided below from \cite{tong_improving_2024}:

\begin{align*}
    \grad_{\theta}\EE_{p_t(x)}||v_\theta(t, x) - u_t(x)||^2 &= \grad_\theta \EE_{p_t(x)}(||v_\theta(t, x)||^2 - 2\langle v_\theta(t, x) u_t(x)\rangle) \quad \textcolor{red}{\text{($u_t(x)
    $ does not depend on $\theta$})}
\end{align*}


\begin{align*}
    \grad_{\theta}\EE_{q(z),p_t(x|z)}||v_\theta(t, x) - u_t(x, z)||^2 &= \EE_{q(z), p_t(x|z)}\grad_{\theta}(||v_\theta(t, x)||^2 - 2 \langle v_\theta(t, x) , u_t(x|z)\rangle)
\end{align*}

The first term in each loss function can be reparametrized as:
\begin{equation*}
    \EE_{p_t(x)}||v_\theta(t, x)||^2 = \iint ||v_\theta(t, x)||^2 p_t(x|z)q(z)dz dx = \EE_{q(z), p_t(x|z)}||v_\theta(t, x)||^2
\end{equation*}

while we substitute \eqref{eq: marg_cond_velocity_field} in the inner product expression for FM to recover the CFM inner product and thus equalize the loss gradients:

\begin{align*}
    \EE_{p_t(x)}\langle v_\theta(t, x), u_t(x)\rangle &= \int \biggl<v_\theta(t, x), \frac{\int u_t(x|z)p_t(x|z)q(z)dz}{p_t(x)}\biggr>p_t(x)dx \\
    &=\iint \langle v_\theta(t, x), u_t(x|z)\rangle p_t(x|z)q(z)dzdx \quad \textcolor{red}{\text{(inner product distributed over integral argument)}}\\
    &=\EE_{q(z), p_t(x|z)}\langle v_\theta(t, x) u_t(x | z) \rangle
\end{align*}

\medskip
\emph{I-CFM (via Independent Coupling):}
For source point $x_0$ and target point $x_1$:

\begin{enumerate}
    \item $q(z) = q(x_0)q(x_1)$

    \item $p_t(x|z) = \CN(x | tx_1 + (1 - t)  x_0, \sigma^2)$

    \item Using \eqref{eq: vector_field_gauss_dens} for the conditional vector field with $\mu_t = tx_1 + (1 - t)x_0 $ and $\sigma_t = \sigma$, $u_t(x|z) = x_1 - x_0$
\end{enumerate}

\medskip
\emph{OT-CFM (sampling via OT Map):}

The key difference from I-CFM is that instead of $x_0, x_1$ being sampled independently from their marginal distributions, they are sampled jointly according to the optimal transport map $\pi$:

$$q(z) = \pi(x_0, x_1)$$

In cases where a static OT plan is computationally infeasible to determine exactly, a minibatch OT approximation \citep{fatras_learning_2020} shows improvements over random sampling plans in terms of model performance and training times. 
Related work such as that of \cite{finlay_how_2020} use a regularized CNF with dynamic OT objectives though these are difficult to train and scale.


\emph{SB-CFM (entropy-regularized OT map):}
Recent efforts such as \citep{de_bortoli_diffusion_2021,heng_diffusion_2024} have also focused on efficient inference in diffusion models and general Bayesian computation by reformulating them as a Schrödinger bridge (SB) problem where the forward process need not be run for large number of steps to ensure $p_N = p_{\text{prior}}$. 
The static SB problem can be seen as an entropy-regularized quadratic cost OT problem that is an attractive choice for high-dimensional OT between arbitrary data distributions. 
These can be applied to an entropic variant of OT-CFM, SB-CFM, to match probability flow of a Schrödinger bridge with a Brownian motion refererence process.

The SB problem seeks a process $\pi$ that is closest to the initial time marginal $p_{\text{ref}}$ while having initial and terminal marginal distributions specified by $q(x_0)$ and $q(x_1)$ respectively i.e.:
\begin{equation*}
\pi^{\ast} = \argmin_{\pi(x_0) = q(x_0), \pi(x_1)=q(x_1)} \DKL(\pi, p_{\text{ref}})
\end{equation*}

Then the above solution can be recovered through the marginal vector field $u_t(x)$ defined via:

\begin{enumerate}
\item $q(z) = \pi_{2\sigma^2}(x_0, x_1)$ where $\pi_{2\sigma^2}(x_0, x_1)$ solves the entropy-regularized optimal transport problem with cost $||x_0 - x_1||$ and regularization $\lambda = 2\sigma^2$

\item $p_t(x|z) = \CN(x | tx_1 + (1 - t)x_0, t(1-t)\sigma^2$ \textcolor{red}{(Brownian bridge between $x_0$ and $x_1$)}

\item $u_t(x|z) = \frac{1 - 2t}{2t(1-t)}\left(x - (tx_1 + (1 - t) x_0) \right) + (x_1 - x_0)$
\end{enumerate}

The ability to establish correlations through reasonably sized pilot datasets may be enhanced through the use of flow matching. 
In cases where the high-fidelity model can only be evaluated a limited number of times, the flow matching surrogate can enable a fast mapping from the particles propagated under a lower-fidelity model likelihood to those under the highest fidelity likelihood. 
Moreover, it can be parametrized to use particles based on multiple lower-fidelity models for even greater savings.

\section{Results}

\section{Conclusions}


% \textcolor{red}{Online vs offline pilot sampling for correlations. To get correlations avg over all neighbours vs compute for individual neighbours? ACV mean should change based on x particle, though the score remain unchanged for a fixed sample allocation and also rewrite notation $\phi(x)$}



% \begin{equation*}
%     \theta^{(0)}_i \leftarrow \theta^{(0)}_i + \epsilon \phi^{\ast}(\theta^{(0)})
% \end{equation*}

% And similarly, for lower-fidelity models indexed from $1$ to $m$, we can compute the particle positions as:

% \begin{equation*}
%     \theta^{(l)}_i \leftarrow \theta^{(l)}_i + \epsilon \phi^{\ast}(\theta^{(l)}), \quad l=1, \cdots, m
% \end{equation*}

% where

% \begin{align}
%     \phi^{\ast}(\theta^{(0)}) &= \frac{1}{M^{(0)}}\sum_{j=1}^{M^{(0)}}[k(\theta_j, \theta) \grad_{\theta_j}[L_{0} + \log p(\theta_j)] + \grad_{\theta_j} k(\theta_j, \theta)] \\
%     &= \frac{1}{M^{(0)}}\sum_{j=1}^{M^{(0)}}[(k(\theta_j, \theta^{(0)})  + k(\theta_j, \theta^{(1)}) + \cdots + \nonumber\\ 
%     &k(\theta_j, \theta^{(m)})) \grad_{\theta_j}[L_{0} + \log p(\theta_j)]] + \grad_{\theta_j} [k(\theta_j, \theta^{(0)}) + \cdots + k(\theta_j, \theta^{(m)})]\\ \nonumber
% \end{align}

% \begin{align}
%     \phi^{\ast}(\theta^{(l)}) &= \frac{1}{M^{(l)}}\sum_{j=1}^{M^{(l)}}[k(\theta_j, \theta) \grad_{\theta_j}[L_{l} + \log p(\theta_j)] + \grad_{\theta_j} k(\theta_j, \theta)] \\
%     &= \frac{1}{M^{(l)}}\sum_{j=1}^{M^{(l)}}[(k(\theta_j, \theta^{(0)})  + k(\theta_j, \theta^{(1)}) + \cdots + \nonumber \\
%     &k(\theta_j, \theta^{(m)})) \grad_{\theta_j}[L_{l} + \log p(\theta_j)] + \log p(\theta_j)]] + \grad_{\theta_j} k(\theta_j, \theta) \nonumber \\ 
%     &= \frac{1}{M^{(l)}}\sum_{j=1}^{M^{(l)}}[(k(\theta_j, \theta^{(0)})  + k(\theta_j, \theta^{(1)}) + \cdots + \nonumber\\ 
%     &k(\theta_j, \theta^{(m)})) \grad_{\theta_j}[L_{l} + \log p(\theta_j)]] + \grad_{\theta_j} [k(\theta_j, \theta^{(0)}) + \cdots + k(\theta_j, \theta^{(m)})]\\ \nonumber
% \end{align}

% While the $M$ particles are allocated to different models as $M_0, M_1, \cdots, M_m$, the above updates still use all the particles to compute the map at each iteration - this introduces interactions between the multifidelity particles in \stkout{\emph{both} terms of the update: the first term driving the particles to regions of high-probability} \textcolor{red}{(unfortunately, this is not true. When you cut off the kernel matrix to remove the particle interactions amongst the other fidelities, it turns out that only the particle distances corresponding to that fidelity weigh the likelihood)} the second term of the update accounting for the repulsive force that prevents particle collapse into local modes. 
% The repulsive force i.e. the kernel gradient evaluates a shared kernel function over different subsets of $\theta$ in each term, while the force driving particles towards high-probability regions weighs the kernel function by the individual log-likelihoods.
% \textcolor{red}{Interestingly, another algorithmic choice here is the choice of particle positions to use in a given fidelity's updates. For example, if we updated the low-fidelity particle's positions first, these updates could be transmitted to and used by the high fidelity update mechanism in the same step instead of both updates using previously computed positions.}

% The above formulation continues to allow flexibility with regards to the specification and the number of lower-fidelity model; but it also raises additional open questions regarding the following aspects:

% \begin{enumerate}
%     \item The choice of the kernel function for individual model fidelities

%     \item The allocation of particles to different model fidelities (disjoint / overlapping) and how it changes as we step through the method.

%     \item The number and allocation of particles used in computations of the kernel function i.e. neighbourhood selection for particles.

%     \item The frequency of updates to particles of different fidelities, and the update sharing frequency within a single iteration of the outer loop.

%     \item The stopping criterion that assesses convergence to the target distribution.

%     \item Quantitative measurements of discrepancy between distributions fitted via single-fidelity and multiple-fidelity methods.
% \end{enumerate}

% \subsection{KL-Divergence Based Correction with kNN Estimator}





% Perhaps the more pertinent question before we discuss the finer details of the implementation is why we expect this to work at all. Consider the elementary case where we only have access to a single model for likelihood computation. 
% There are two possible bottlenecks: 1. the full data log-likelihood computation is prohibitive on account of the need to process very large-scale data, requiring the modification of standard inference algorithms to handle distributed or streaming data settings 2. the high costs of running the forward model renders the inference intractable without resorting to surrogate approximations. 

% A prominent line of work that deals with the first case uses the so-called `consensus Monte Carlo methods' \citep{rabinovich_variational_2015,scott_bayes_2016}. Here a data sharding (partitioning) step is followed by independent posterior sampling on multiple machines conditioned on the data partitions. 
% The communication overhead is avoided by combining the posterior draws to form a `consensus' belief about the model unknowns.
% The algorithm is exact for Gaussian posteriors, but has been found to be broadly useful for non-Gaussian settings too.

% Rather than modifying the inference algorithm, practitioners have recently focused on exploiting redundancies in the dataset by identifying a weighted subset of the data known as the \emph{coreset}, that originated in computational geometry \citep{agarwal_geometric_2007} and later found popularity in scalable clustering methods.
% Discussions on Bayesian coreset methods \citep{huggins_coresets_2016,campbell_automated_2019} focus on high-probability guarantees for the approximation quality of the resulting log-likelihood and demonstrate their superior performance over random subsampling methods. A superficial analogy we can draw is the treatment of the high-fidelity particles as our coreset, and the kernel function that accounts for all model fidelities as a means to enforce a weighted log-likelihood.

% Akin to coresets, the concept of `graph coarsening' is ubiquitous all across scientific computing and machine learning methods for graph representation \citep{chen_graph_2022}. The goal of graph coarsening is to uncover a smaller graph that faithfully reflects the structure of the original graph. The coarse graph/s is/are typically determined using notions of pairwise similarity, algebraic distance, independent sets, sketching based on leverage scores and other criteria. 
% In this framework, we can treat the selection of high-fidelity particles as a form of graph coarsening, if the features for these particles are constructed from aggregation of lower-fidelity particles rather than assignment of existing particles.

% While these techniques do not directly relate to the workings or performance of our proposed method relying on the non-parametric version of the SVGD method, they serve as useful guiding principles to improve practical implementations which typically rely on random particle subsampling for efficient kernel computations. 

% A simpler analogy arises from the purview of dynamical systems: the asymptotic behaviour of SVGD is characterized by the Vlasov equation \citep{liu_stein_2017}. 
% Then, deploying a multistep predictor-corrector method to solve the resulting ordinary differential equation would be akin to performing a sequence of low-fidelity predictions for particle positions to serve as the initial guess, followed by a correction step for some or all particles via the high-fidelity likelihood i.e. the fidelity assignment \emph{alternates} rather than preceding or following the model updates .
% The benefits of such an approach would be practically realized when the equivalent cost in terms of single-fidelity evaluations is measurably brought down in implementations. 
% However, determination of `when to correct, what to correct' is also a non-trivial problem.

% In the following sections, we will lay out a framework to evaluate some of these key choices and provide more details of our proposed method.

% \subsection{Particle assignment to various fidelities}

% \emph{Method 0: Non-randomized Assignment}
% The simplest possible setting does not attempt to reassign particles, instead, at every iteration, a fixed proportion of the particles is treated as the lower-fidelity likelihood update while the remaining are high-fidelity updates. 
% We look at the number of iterations required for convergence by monitoring the hyperparameters of the kernel function.

% \noindent \emph{Method 1: Randomized Assignment}
% To correct for possible bias due to incorrect particle assignment, randomized assignment shuffles the particles between the different fidelities after one or more update steps.

% \noindent \emph{Method 2: Assignment based on clustering methods}
% For the first time, we consider the setting where a small subset of particles is deemed to be non-redundant and a summarization of the dataset. Here, we incorporate a clustering-based heuristic that determines the high-fidelity particles based on a suitable measure of distance.
% It is of particular importance to ensure that the cost of pairwise distance computations does not grow or outweigh the cost of simpler methods such as randomized assignment i.e. there is possibility of reuse for these when we actually perform the Stein updates.


% \noindent \emph{Method 3: Resource allocation using model correlations}

% \subsection{Graph Network Based Propagation}
% Deep learning methods that model arbitrary relational structures between elements, such as graph networks (GNs), are surveyed in \cite{battaglia_relational_2018}. 

% GNs model a domain by decomposing it into a graphical representation $\CG=\{\CV, \CE\}$ - nodes $\CV$ that represent individual features and edges $\CE$ that describe directed or undirected interactions between nodes. 
% The learning proceeds by first defining a neighbourhood for individual nodes. This is followed by a message-passing block that updates node features based on aggregating interactions with all or a subset of its possible neighbours. Chaining several such blocks with intermediate nonlinear activations results in a GN which accepts an arbitrarily sized graph as input and returns either a transformed graph or some global property of the graph as the output. 

% Superficially translating the graphical representation to a particle-based updates / normalizing flow setup suggests the propagation of augmented node features $[x, \log p(x)]$ where both the particle positions $x$ and their log-likelihoods $\log p(x)$ are updated as they pass through the message-passing blocks. Two key advantages offered by graph-based methods are their inherent invariance to permutations (unlike MLPs) and their ability to operate on graphs of arbitrary sizes without modifications to the architecture. 
% This feature translates to so-called `discretization invariance' in expressive methods for learning PDEs such as neural operators \citep{Li2020} which exhibit good generalization properties even when trained on low-resolution data. GNs have been successfully used to simulate complex physics with long rollout times, e.g. \cite{sanchez-gonzalez_learning_2020}.

% Without additional constraints however, a GN is not directly compatible with the workflow of particle-based methods for inference, where the key goal is to constrain the transport of individual particles by minimizing a particular discrepancy function that characterizes the quality of the posterior approximation. 
% It is also unclear if the discretization-invariance property grants any advantages in better representation of the posterior density in low-data settings. 
% However, the advances in scaling GNs to very large graphs with millions of nodes suggest opportunities to influence the development of scalable particle methods for inference and offer a more flexible workflow. 
% We begin with a brief overview of our proposed changes below:

% \subsection{Particle Swarm Optimization based methods}
% (with a specific interest in multi-population cooperative particle swarm optimization, even though we are not specifically targeting optimization) - see \url{https://ieeexplore.ieee.org/document/9967774} for an example.


% \subsection{Sparse Kernel Updates}

\bibliography{local,references}

\end{document}

