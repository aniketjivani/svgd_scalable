@misc{weissmann_mean-field_2024,
	title = {On the mean-field limit for {Stein} variational gradient descent: stability and multilevel approximation},
	shorttitle = {On the mean-field limit for {Stein} variational gradient descent},
	url = {http://arxiv.org/abs/2402.01320},
	doi = {10.48550/arXiv.2402.01320},
	abstract = {In this paper we propose and analyze a novel multilevel version of Stein variational gradient descent (SVGD). SVGD is a recent particle based variational inference method. For Bayesian inverse problems with computationally expensive likelihood evaluations, the method can become prohibitive as it requires to evolve a discrete dynamical system over many time steps, each of which requires likelihood evaluations at all particle locations. To address this, we introduce a multilevel variant that involves running several interacting particle dynamics in parallel corresponding to different approximation levels of the likelihood. By carefully tuning the number of particles at each level, we prove that a significant reduction in computational complexity can be achieved. As an application we provide a numerical experiment for a PDE driven inverse problem, which confirms the speed up suggested by our theoretical results.},
	urldate = {2025-10-27},
	publisher = {arXiv},
	author = {Weissmann, Simon and Zech, Jakob},
	month = feb,
	year = {2024},
	note = {arXiv:2402.01320},
	keywords = {Mathematics - Numerical Analysis, Computer Science - Numerical Analysis, Statistics - Methodology},
}

@inproceedings{alsup_multilevel_2022,
	title = {Multilevel {Stein} variational gradient descent with applications to {Bayesian} inverse problems},
	language = {en},
	booktitle = {Proceedings of the 2nd {Mathematical} and {Scientific} {Machine} {Learning} {Conference}},
	publisher = {PMLR},
	author = {Alsup, Terrence and Venturi, Luca and Peherstorfer, Benjamin},
	year = {2022},
	pages = {93--117},
}

@misc{liu_stein_2017,
	title = {Stein {Variational} {Gradient} {Descent} as {Gradient} {Flow}},
	doi = {10.48550/arXiv.1704.07520},
	publisher = {arXiv},
	author = {Liu, Qiang},
	month = nov,
	year = {2017},
	keywords = {Statistics - Machine Learning},
}

@inproceedings{shi_mackey_2023,
 author = {Shi, Jiaxin and Mackey, Lester},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {26831--26844},
 publisher = {Curran Associates, Inc.},
 title = {A Finite-Particle Convergence Rate for Stein Variational Gradient Descent},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/54e5d7af6250ccab796ad7fe75663ba5-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{detomasso_marzouk2018,
 author = {Detommaso, Gianluca and Cui, Tiangang and Marzouk, Youssef and Spantini, Alessio and Scheichl, Robert},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 publisher = {Curran Associates, Inc.},
 title = {A Stein variational Newton method},
 volume = {31},
 year = {2018}
}

@misc{takamoto_pdebench_2024,
	title = {{PDEBENCH}: {An} {Extensive} {Benchmark} for {Scientific} {Machine} {Learning}},
	doi = {10.48550/arXiv.2210.07182},
	publisher = {arXiv},
	author = {Takamoto, Makoto and Praditia, Timothy and Leiteritz, Raphael and MacKinlay, Dan and Alesiani, Francesco and Pflüger, Dirk and Niepert, Mathias},
	year = {2024},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Physics - Fluid Dynamics, Physics - Geophysics},
}

@inproceedings{
dangelo2021annealed,
title={Annealed Stein Variational Gradient Descent},
author={Francesco D'Angelo and Vincent Fortuin},
booktitle={Third Symposium on Advances in Approximate Bayesian Inference},
year={2021},
url={https://openreview.net/forum?id=pw2v8HFJIYg}
}

@article{blei_variational_2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	issn = {0162-1459},
	shorttitle = {Variational {Inference}},
	doi = {10.1080/01621459.2017.1285773},
	number = {518},
	urldate = {2025-07-24},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = apr,
	year = {2017},
	keywords = {Algorithms, Computationally intensive methods, Statistical computing},
	pages = {859--877},
}

@article{bomarito_optimization_2022,
	title = {On the optimization of approximate control variates with parametrically defined estimators},
	volume = {451},
	issn = {00219991},
	language = {en},
	urldate = {2022-05-16},
	journal = {Journal of Computational Physics},
	author = {Bomarito, G.F. and Leser, P.E. and Warner, J.E. and Leser, W.P.},
	month = feb,
	year = {2022},
	pages = {110882},
}

@article{bomarito_multi_2020,
	title = {Multi {Model} {Monte} {Carlo} with {Python} ({MXMCPy})},
	journal = {NASA/TM–2020–22058},
	author = {Bomarito, G.F. and Warner, J.E. and Leser, P.E. and Leser, W.P. and Morrill, L.},
	year = {2020},
}

@inproceedings{das2023,
     author = {Das, Aniket and Nagaraj, Dheeraj},
     booktitle = {Advances in Neural Information Processing Systems},
     editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
     pages = {49748--49760},
     publisher = {Curran Associates, Inc.},
     title = {Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation},
     volume = {36},
     year = {2023}
}

@misc{detommaso_stein_2018,
	title = {A {Stein} variational {Newton} method},
	doi = {10.48550/arXiv.1806.03085},
	publisher = {arXiv},
	author = {Detommaso, Gianluca and Cui, Tiangang and Spantini, Alessio and Marzouk, Youssef and Scheichl, Robert},
	month = oct,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Computer Science - Numerical Analysis, Statistics - Machine Learning},
}

@article{dixon_covariance_2024,
	title = {Covariance {Expressions} for {Multifidelity} {Sampling} with {Multioutput}, {Multistatistic} {Estimators}: {Application} to {Approximate} {Control} {Variates}},
	volume = {12},
	shorttitle = {Covariance {Expressions} for {Multifidelity} {Sampling} with {Multioutput}, {Multistatistic} {Estimators}},
	doi = {10.1137/23M1607994},
	number = {3},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Dixon, Thomas O. and Warner, James E. and Bomarito, Geoffrey F. and Gorodetsky, Alex A.},
	month = sep,
	year = {2024},
	pages = {1005--1049},
}

@misc{fop_model-based_2018,
	title = {Model-based {Clustering} with {Sparse} {Covariance} {Matrices}},
	url = {http://arxiv.org/abs/1711.07748},
	doi = {10.48550/arXiv.1711.07748},
	publisher = {arXiv},
	author = {Fop, Michael and Murphy, Thomas Brendan and Scrucca, Luca},
	month = sep,
	year = {2018},
	keywords = {Statistics - Computation, Statistics - Methodology},
}

@article{giles_multilevel_2015,
	title = {Multilevel {Monte} {Carlo} methods},
	volume = {24},
	issn = {0962-4929, 1474-0508},
	doi = {10.1017/S096249291500001X},
	language = {en},
	journal = {Acta Numerica},
	author = {Giles, Michael B.},
	month = may,
	year = {2015},
	pages = {259--328},
}

@inproceedings{gorham_measuring_2017,
	title = {Measuring {Sample} {Quality} with {Kernels}},
	language = {en},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gorham, Jackson and Mackey, Lester},
	month = jul,
	year = {2017},
	pages = {1292--1301},
}

@misc{gorham_stochastic_2020,
	title = {Stochastic {Stein} {Discrepancies}},
	doi = {10.48550/arXiv.2007.02857},
	publisher = {arXiv},
	author = {Gorham, Jackson and Raj, Anant and Mackey, Lester},
	month = oct,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability, Statistics - Machine Learning, Statistics - Methodology},
}

@article{gorodetsky_generalized_2020,
	title = {A generalized approximate control variate framework for multifidelity uncertainty quantification},
	volume = {408},
	issn = {0021-9991},
	doi = {https://doi.org/10.1016/j.jcp.2020.109257},
	journal = {Journal of Computational Physics},
	author = {Gorodetsky, Alex A. and Geraci, Gianluca and Eldred, Michael S. and Jakeman, John D.},
	year = {2020},
	keywords = {Control variates, Monte Carlo, Multifidelity modeling, Variance reduction},
	pages = {109257},
}

@article{jakeman_pyapprox_2023,
	title = {{PyApprox}: {A} software package for sensitivity analysis, {Bayesian} inference, optimal experimental design, and multi-fidelity uncertainty quantification and surrogate modeling},
	volume = {170},
	issn = {1364-8152},
	shorttitle = {{PyApprox}},
	doi = {10.1016/j.envsoft.2023.105825},
	journal = {Environmental Modelling \& Software},
	author = {Jakeman, J. D.},
	year = {2023},
	keywords = {Bayesian inference, Decision making, Experimental design, Modeling, Multi-fidelity, Sensitivity analysis, Surrogate models, Uncertainty quantification},
	pages = {105825},
}

@article{gorodetsky_grouped_2024,
	title = {Grouped approximate control variate estimators},
    journal = {arXiv preprint},
    volume = {2402.14736},
	eprint = {2402.14736},
	author = {Gorodetsky, Alex A. and Jakeman, John D. and Eldred, Michael S.},
	year = {2024},
}

@article{jordan_introduction_1999,
	title = {An {Introduction} to {Variational} {Methods} for {Graphical} {Models}},
	volume = {37},
	issn = {1573-0565},
	doi = {10.1023/A:1007665907178},
	language = {en},
	number = {2},
	urldate = {2025-07-25},
	journal = {Machine Learning},
	author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
	year = {1999},
	pages = {183--233},
}

@misc{liu_stein_2019,
	title = {Stein {Variational} {Gradient} {Descent}: {A} {General} {Purpose} {Bayesian} {Inference} {Algorithm}},
	shorttitle = {Stein {Variational} {Gradient} {Descent}},
	doi = {10.48550/arXiv.1608.04471},
	publisher = {arXiv},
	author = {Liu, Qiang and Wang, Dilin},
	month = sep,
	year = {2019},
	note = {arXiv:1608.04471 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{oates_control_2017,
	title = {Control functionals for {Monte} {Carlo} integration},
	volume = {79},
	copyright = {© 2016 Royal Statistical Society},
	issn = {1467-9868},
	doi = {10.1111/rssb.12185},
	language = {en},
	number = {3},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Oates, Chris J. and Girolami, Mark and Chopin, Nicolas},
	year = {2017},
	pages = {695--718},
}

@article{papamarkou_zero_2014,
	title = {Zero {Variance} {Differential} {Geometric} {Markov} {Chain} {Monte} {Carlo} {Algorithms}},
	volume = {9},
	issn = {1936-0975, 1931-6690},
	doi = {10.1214/13-BA848},
	number = {1},
	journal = {Bayesian Analysis},
	author = {Papamarkou, Theodore and Mira, Antonietta and Girolami, Mark},
	month = mar,
	year = {2014},
	keywords = {Hamiltonian Monte Carlo, Metropolis adjusted Langevin algorithms, Metropolis-Hastings, control variates},
	pages = {97--128},
}

@article{peherstorfer_optimal_2016,
	title = {Optimal {Model} {Management} for {Multifidelity} {Monte} {Carlo} {Estimation}},
	volume = {38},
	issn = {1064-8275, 1095-7197},
	doi = {10.1137/15M1046472},
	language = {en},
	number = {5},
	journal = {SIAM Journal on Scientific Computing},
	author = {Peherstorfer, Benjamin and Willcox, Karen and Gunzburger, Max},
	year = {2016},
	pages = {A3163--A3194},
}

@article{schaden_multilevel_2020,
	title = {On {Multilevel} {Best} {Linear} {Unbiased} {Estimators}},
	volume = {8},
	issn = {2166-2525},
	doi = {10.1137/19M1263534},
	language = {en},
	number = {2},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Schaden, Daniel and Ullmann, Elisabeth},
	year = {2020},
	pages = {601--635},
}

@inproceedings{sharrock_coin_2023,
	title = {Coin {Sampling}: {Gradient}-{Based} {Bayesian} {Inference} without {Learning} {Rates}},
	shorttitle = {Coin {Sampling}},
	language = {en},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sharrock, Louis and Nemeth, Christopher},
	month = jul,
	year = {2023},
	pages = {30850--30882},
}

@misc{wang_stein_2019,
	title = {Stein {Variational} {Gradient} {Descent} {With} {Matrix}-{Valued} {Kernels}},
	doi = {10.48550/arXiv.1910.12794},
	publisher = {arXiv},
	author = {Wang, Dilin and Tang, Ziyang and Bajaj, Chandrajit and Liu, Qiang},
	month = nov,
	year = {2019},
	note = {arXiv:1910.12794 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{si_scalable_2021,
	title = {Scalable {Control} {Variates} for {Monte} {Carlo} {Methods} via {Stochastic} {Optimization}},
	doi = {10.48550/arXiv.2006.07487},
	publisher = {arXiv},
	author = {Si, Shijing and Oates, Chris J. and Duncan, Andrew B. and Carin, Lawrence and Briol, François-Xavier},
	month = jul,
	year = {2021},
	note = {arXiv:2006.07487 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ye_stein_2020,
	title = {Stein {Self}-{Repulsive} {Dynamics}: {Benefits} {From} {Past} {Samples}},
	shorttitle = {Stein {Self}-{Repulsive} {Dynamics}},
	doi = {10.48550/arXiv.2002.09070},
	publisher = {arXiv},
	author = {Ye, Mao and Ren, Tongzheng and Liu, Qiang},
	month = dec,
	year = {2020},
	note = {arXiv:2002.09070 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{brooks_handbook_2011,
	address = {New York},
	title = {Handbook of {Markov} {Chain} {Monte} {Carlo}},
	isbn = {9780429138508},
	abstract = {Since their popularization in the 1990s, Markov chain Monte Carlo (MCMC) methods have revolutionized statistical computing and have had an especially profound impact on the practice of Bayesian statistics. Furthermore, MCMC methods have enabled the development and use of intricate models in an astonishing array of disciplines as diverse as fisherie},
	publisher = {Chapman and Hall/CRC},
	editor = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
	month = may,
	year = {2011},
	doi = {10.1201/b10905},
}

@article{Liu2016,
	title = {Stein variational gradient descent: {A} general purpose {Bayesian} inference algorithm},
	issn = {10495258},
	abstract = {We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Liu, Qiang and Wang, Dilin},
	year = {2016},
	note = {arXiv: 1608.04471},
	pages = {2378--2386},
}

@InProceedings{zhuo18a,
  title = 	 {Message Passing Stein Variational Gradient Descent},
  author =       {Zhuo, Jingwei and Liu, Chang and Shi, Jiaxin and Zhu, Jun and Chen, Ning and Zhang, Bo},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {6018--6027},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/zhuo18a/zhuo18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/zhuo18a.html},
  abstract = 	 {Stein variational gradient descent (SVGD) is a recently proposed particle-based Bayesian inference method, which has attracted a lot of interest due to its remarkable approximation ability and particle efficiency compared to traditional variational inference and Markov Chain Monte Carlo methods. However, we observed that particles of SVGD tend to collapse to modes of the target distribution, and this particle degeneracy phenomenon becomes more severe with higher dimensions. Our theoretical analysis finds out that there exists a negative correlation between the dimensionality and the repulsive force of SVGD which should be blamed for this phenomenon. We propose Message Passing SVGD (MP-SVGD) to solve this problem. By leveraging the conditional independence structure of probabilistic graphical models (PGMs), MP-SVGD converts the original high-dimensional global inference problem into a set of local ones over the Markov blanket with lower dimensions. Experimental results show its advantages of preventing vanishing repulsive force in high-dimensional space over SVGD, and its particle efficiency and approximation flexibility over other inference methods on graphical models.}
}
