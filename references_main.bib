@misc{weissmann_mean-field_2024,
	title = {On the mean-field limit for {Stein} variational gradient descent: stability and multilevel approximation},
	shorttitle = {On the mean-field limit for {Stein} variational gradient descent},
	url = {http://arxiv.org/abs/2402.01320},
	doi = {10.48550/arXiv.2402.01320},
	abstract = {In this paper we propose and analyze a novel multilevel version of Stein variational gradient descent (SVGD). SVGD is a recent particle based variational inference method. For Bayesian inverse problems with computationally expensive likelihood evaluations, the method can become prohibitive as it requires to evolve a discrete dynamical system over many time steps, each of which requires likelihood evaluations at all particle locations. To address this, we introduce a multilevel variant that involves running several interacting particle dynamics in parallel corresponding to different approximation levels of the likelihood. By carefully tuning the number of particles at each level, we prove that a significant reduction in computational complexity can be achieved. As an application we provide a numerical experiment for a PDE driven inverse problem, which confirms the speed up suggested by our theoretical results.},
	urldate = {2025-10-27},
	publisher = {arXiv},
	author = {Weissmann, Simon and Zech, Jakob},
	month = feb,
	year = {2024},
	note = {arXiv:2402.01320},
	keywords = {Mathematics - Numerical Analysis, Computer Science - Numerical Analysis, Statistics - Methodology},
}

@inproceedings{alsup_multilevel_2022,
	title = {Multilevel {Stein} variational gradient descent with applications to {Bayesian} inverse problems},
	language = {en},
	booktitle = {Proceedings of the 2nd {Mathematical} and {Scientific} {Machine} {Learning} {Conference}},
	publisher = {PMLR},
	author = {Alsup, Terrence and Venturi, Luca and Peherstorfer, Benjamin},
	year = {2022},
	pages = {93--117},
}

@misc{liu_stein_2017,
	title = {Stein {Variational} {Gradient} {Descent} as {Gradient} {Flow}},
	doi = {10.48550/arXiv.1704.07520},
	publisher = {arXiv},
	author = {Liu, Qiang},
	month = nov,
	year = {2017},
	keywords = {Statistics - Machine Learning},
}

@inproceedings{shi_mackey_2023,
 author = {Shi, Jiaxin and Mackey, Lester},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {26831--26844},
 publisher = {Curran Associates, Inc.},
 title = {A Finite-Particle Convergence Rate for Stein Variational Gradient Descent},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/54e5d7af6250ccab796ad7fe75663ba5-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{detomasso_marzouk2018,
 author = {Detommaso, Gianluca and Cui, Tiangang and Marzouk, Youssef and Spantini, Alessio and Scheichl, Robert},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 publisher = {Curran Associates, Inc.},
 title = {A Stein variational Newton method},
 volume = {31},
 year = {2018}
}

@misc{takamoto_pdebench_2024,
	title = {{PDEBENCH}: {An} {Extensive} {Benchmark} for {Scientific} {Machine} {Learning}},
	doi = {10.48550/arXiv.2210.07182},
	publisher = {arXiv},
	author = {Takamoto, Makoto and Praditia, Timothy and Leiteritz, Raphael and MacKinlay, Dan and Alesiani, Francesco and Pflüger, Dirk and Niepert, Mathias},
	year = {2024},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Physics - Fluid Dynamics, Physics - Geophysics},
}

@inproceedings{
dangelo2021annealed,
title={Annealed Stein Variational Gradient Descent},
author={Francesco D'Angelo and Vincent Fortuin},
booktitle={Third Symposium on Advances in Approximate Bayesian Inference},
year={2021},
url={https://openreview.net/forum?id=pw2v8HFJIYg}
}

@article{blei_variational_2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	issn = {0162-1459},
	shorttitle = {Variational {Inference}},
	doi = {10.1080/01621459.2017.1285773},
	number = {518},
	urldate = {2025-07-24},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = apr,
	year = {2017},
	keywords = {Algorithms, Computationally intensive methods, Statistical computing},
	pages = {859--877},
}

@article{bomarito_optimization_2022,
	title = {On the optimization of approximate control variates with parametrically defined estimators},
	volume = {451},
	issn = {00219991},
	language = {en},
	urldate = {2022-05-16},
	journal = {Journal of Computational Physics},
	author = {Bomarito, G.F. and Leser, P.E. and Warner, J.E. and Leser, W.P.},
	month = feb,
	year = {2022},
	pages = {110882},
}

@article{bomarito_multi_2020,
	title = {Multi {Model} {Monte} {Carlo} with {Python} ({MXMCPy})},
	journal = {NASA/TM–2020–22058},
	author = {Bomarito, G.F. and Warner, J.E. and Leser, P.E. and Leser, W.P. and Morrill, L.},
	year = {2020},
}

@inproceedings{das2023,
     author = {Das, Aniket and Nagaraj, Dheeraj},
     booktitle = {Advances in Neural Information Processing Systems},
     editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
     pages = {49748--49760},
     publisher = {Curran Associates, Inc.},
     title = {Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation},
     volume = {36},
     year = {2023}
}

@misc{detommaso_stein_2018,
	title = {A {Stein} variational {Newton} method},
	doi = {10.48550/arXiv.1806.03085},
	publisher = {arXiv},
	author = {Detommaso, Gianluca and Cui, Tiangang and Spantini, Alessio and Marzouk, Youssef and Scheichl, Robert},
	month = oct,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Computer Science - Numerical Analysis, Statistics - Machine Learning},
}

@article{dixon_covariance_2024,
	title = {Covariance {Expressions} for {Multifidelity} {Sampling} with {Multioutput}, {Multistatistic} {Estimators}: {Application} to {Approximate} {Control} {Variates}},
	volume = {12},
	shorttitle = {Covariance {Expressions} for {Multifidelity} {Sampling} with {Multioutput}, {Multistatistic} {Estimators}},
	doi = {10.1137/23M1607994},
	number = {3},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Dixon, Thomas O. and Warner, James E. and Bomarito, Geoffrey F. and Gorodetsky, Alex A.},
	month = sep,
	year = {2024},
	pages = {1005--1049},
}

@misc{fop_model-based_2018,
	title = {Model-based {Clustering} with {Sparse} {Covariance} {Matrices}},
	url = {http://arxiv.org/abs/1711.07748},
	doi = {10.48550/arXiv.1711.07748},
	publisher = {arXiv},
	author = {Fop, Michael and Murphy, Thomas Brendan and Scrucca, Luca},
	month = sep,
	year = {2018},
	keywords = {Statistics - Computation, Statistics - Methodology},
}

@article{giles_multilevel_2015,
	title = {Multilevel {Monte} {Carlo} methods},
	volume = {24},
	issn = {0962-4929, 1474-0508},
	doi = {10.1017/S096249291500001X},
	language = {en},
	journal = {Acta Numerica},
	author = {Giles, Michael B.},
	month = may,
	year = {2015},
	pages = {259--328},
}

@inproceedings{gorham_measuring_2017,
	title = {Measuring {Sample} {Quality} with {Kernels}},
	language = {en},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gorham, Jackson and Mackey, Lester},
	month = jul,
	year = {2017},
	pages = {1292--1301},
}

@misc{gorham_stochastic_2020,
	title = {Stochastic {Stein} {Discrepancies}},
	doi = {10.48550/arXiv.2007.02857},
	publisher = {arXiv},
	author = {Gorham, Jackson and Raj, Anant and Mackey, Lester},
	month = oct,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability, Statistics - Machine Learning, Statistics - Methodology},
}

@article{gorodetsky_generalized_2020,
	title = {A generalized approximate control variate framework for multifidelity uncertainty quantification},
	volume = {408},
	issn = {0021-9991},
	doi = {https://doi.org/10.1016/j.jcp.2020.109257},
	journal = {Journal of Computational Physics},
	author = {Gorodetsky, Alex A. and Geraci, Gianluca and Eldred, Michael S. and Jakeman, John D.},
	year = {2020},
	keywords = {Control variates, Monte Carlo, Multifidelity modeling, Variance reduction},
	pages = {109257},
}

@article{jakeman_pyapprox_2023,
	title = {{PyApprox}: {A} software package for sensitivity analysis, {Bayesian} inference, optimal experimental design, and multi-fidelity uncertainty quantification and surrogate modeling},
	volume = {170},
	issn = {1364-8152},
	shorttitle = {{PyApprox}},
	doi = {10.1016/j.envsoft.2023.105825},
	journal = {Environmental Modelling \& Software},
	author = {Jakeman, J. D.},
	year = {2023},
	keywords = {Bayesian inference, Decision making, Experimental design, Modeling, Multi-fidelity, Sensitivity analysis, Surrogate models, Uncertainty quantification},
	pages = {105825},
}

@article{gorodetsky_grouped_2024,
	title = {Grouped approximate control variate estimators},
    journal = {arXiv preprint},
    volume = {2402.14736},
	eprint = {2402.14736},
	author = {Gorodetsky, Alex A. and Jakeman, John D. and Eldred, Michael S.},
	year = {2024},
}

@article{jordan_introduction_1999,
	title = {An {Introduction} to {Variational} {Methods} for {Graphical} {Models}},
	volume = {37},
	issn = {1573-0565},
	doi = {10.1023/A:1007665907178},
	language = {en},
	number = {2},
	urldate = {2025-07-25},
	journal = {Machine Learning},
	author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
	year = {1999},
	pages = {183--233},
}

@misc{liu_stein_2019,
	title = {Stein {Variational} {Gradient} {Descent}: {A} {General} {Purpose} {Bayesian} {Inference} {Algorithm}},
	shorttitle = {Stein {Variational} {Gradient} {Descent}},
	doi = {10.48550/arXiv.1608.04471},
	publisher = {arXiv},
	author = {Liu, Qiang and Wang, Dilin},
	month = sep,
	year = {2019},
	note = {arXiv:1608.04471 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{oates_control_2017,
	title = {Control functionals for {Monte} {Carlo} integration},
	volume = {79},
	copyright = {© 2016 Royal Statistical Society},
	issn = {1467-9868},
	doi = {10.1111/rssb.12185},
	language = {en},
	number = {3},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Oates, Chris J. and Girolami, Mark and Chopin, Nicolas},
	year = {2017},
	pages = {695--718},
}

@article{papamarkou_zero_2014,
	title = {Zero {Variance} {Differential} {Geometric} {Markov} {Chain} {Monte} {Carlo} {Algorithms}},
	volume = {9},
	issn = {1936-0975, 1931-6690},
	doi = {10.1214/13-BA848},
	number = {1},
	journal = {Bayesian Analysis},
	author = {Papamarkou, Theodore and Mira, Antonietta and Girolami, Mark},
	month = mar,
	year = {2014},
	keywords = {Hamiltonian Monte Carlo, Metropolis adjusted Langevin algorithms, Metropolis-Hastings, control variates},
	pages = {97--128},
}

@article{peherstorfer_optimal_2016,
	title = {Optimal {Model} {Management} for {Multifidelity} {Monte} {Carlo} {Estimation}},
	volume = {38},
	issn = {1064-8275, 1095-7197},
	doi = {10.1137/15M1046472},
	language = {en},
	number = {5},
	journal = {SIAM Journal on Scientific Computing},
	author = {Peherstorfer, Benjamin and Willcox, Karen and Gunzburger, Max},
	year = {2016},
	pages = {A3163--A3194},
}

@article{schaden_multilevel_2020,
	title = {On {Multilevel} {Best} {Linear} {Unbiased} {Estimators}},
	volume = {8},
	issn = {2166-2525},
	doi = {10.1137/19M1263534},
	language = {en},
	number = {2},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Schaden, Daniel and Ullmann, Elisabeth},
	year = {2020},
	pages = {601--635},
}

@inproceedings{sharrock_coin_2023,
	title = {Coin {Sampling}: {Gradient}-{Based} {Bayesian} {Inference} without {Learning} {Rates}},
	shorttitle = {Coin {Sampling}},
	language = {en},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sharrock, Louis and Nemeth, Christopher},
	month = jul,
	year = {2023},
	pages = {30850--30882},
}

@misc{wang_stein_2019,
	title = {Stein {Variational} {Gradient} {Descent} {With} {Matrix}-{Valued} {Kernels}},
	doi = {10.48550/arXiv.1910.12794},
	publisher = {arXiv},
	author = {Wang, Dilin and Tang, Ziyang and Bajaj, Chandrajit and Liu, Qiang},
	month = nov,
	year = {2019},
	note = {arXiv:1910.12794 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{si_scalable_2021,
	title = {Scalable {Control} {Variates} for {Monte} {Carlo} {Methods} via {Stochastic} {Optimization}},
	doi = {10.48550/arXiv.2006.07487},
	publisher = {arXiv},
	author = {Si, Shijing and Oates, Chris J. and Duncan, Andrew B. and Carin, Lawrence and Briol, François-Xavier},
	month = jul,
	year = {2021},
	note = {arXiv:2006.07487 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ye_stein_2020,
	title = {Stein {Self}-{Repulsive} {Dynamics}: {Benefits} {From} {Past} {Samples}},
	shorttitle = {Stein {Self}-{Repulsive} {Dynamics}},
	doi = {10.48550/arXiv.2002.09070},
	publisher = {arXiv},
	author = {Ye, Mao and Ren, Tongzheng and Liu, Qiang},
	month = dec,
	year = {2020},
	note = {arXiv:2002.09070 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{brooks_handbook_2011,
	address = {New York},
	title = {Handbook of {Markov} {Chain} {Monte} {Carlo}},
	isbn = {9780429138508},
	abstract = {Since their popularization in the 1990s, Markov chain Monte Carlo (MCMC) methods have revolutionized statistical computing and have had an especially profound impact on the practice of Bayesian statistics. Furthermore, MCMC methods have enabled the development and use of intricate models in an astonishing array of disciplines as diverse as fisherie},
	publisher = {Chapman and Hall/CRC},
	editor = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
	month = may,
	year = {2011},
	doi = {10.1201/b10905},
}

@article{Liu2016,
	title = {Stein variational gradient descent: {A} general purpose {Bayesian} inference algorithm},
	issn = {10495258},
	abstract = {We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Liu, Qiang and Wang, Dilin},
	year = {2016},
	note = {arXiv: 1608.04471},
	pages = {2378--2386},
}

@InProceedings{zhuo18a,
  title = 	 {Message Passing Stein Variational Gradient Descent},
  author =       {Zhuo, Jingwei and Liu, Chang and Shi, Jiaxin and Zhu, Jun and Chen, Ning and Zhang, Bo},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {6018--6027},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/zhuo18a/zhuo18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/zhuo18a.html},
  abstract = 	 {Stein variational gradient descent (SVGD) is a recently proposed particle-based Bayesian inference method, which has attracted a lot of interest due to its remarkable approximation ability and particle efficiency compared to traditional variational inference and Markov Chain Monte Carlo methods. However, we observed that particles of SVGD tend to collapse to modes of the target distribution, and this particle degeneracy phenomenon becomes more severe with higher dimensions. Our theoretical analysis finds out that there exists a negative correlation between the dimensionality and the repulsive force of SVGD which should be blamed for this phenomenon. We propose Message Passing SVGD (MP-SVGD) to solve this problem. By leveraging the conditional independence structure of probabilistic graphical models (PGMs), MP-SVGD converts the original high-dimensional global inference problem into a set of local ones over the Markov blanket with lower dimensions. Experimental results show its advantages of preventing vanishing repulsive force in high-dimensional space over SVGD, and its particle efficiency and approximation flexibility over other inference methods on graphical models.}
}

@misc{muchandimath_accelerating_2025,
	title = {Accelerating {Bayesian} {Inference} via {Multi}-{Fidelity} {Transport} {Map} {Coupling}},
	doi = {10.48550/arXiv.2510.17946},
	publisher = {arXiv},
	author = {Muchandimath, Sanjan C. and Martins, Joaquim R. R. A. and Gorodetsky, Alex A.},
	year = {2025},
	note = {arXiv:2510.17946},
	keywords = {Statistics - Methodology, Statistics - Applications},
}

@misc{corso_particle_2023,
	title = {Particle {Guidance}: non-{I}.{I}.{D}. {Diverse} {Sampling} with {Diffusion} {Models}},
	shorttitle = {Particle {Guidance}},
	doi = {10.48550/arXiv.2310.13102},
	publisher = {arXiv},
	author = {Corso, Gabriele and Xu, Yilun and Bortoli, Valentin de and Barzilay, Regina and Jaakkola, Tommi},
	month = nov,
	year = {2023},
	note = {arXiv:2310.13102},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@article{Han2024,
    author = {Han, Ruijian and Kramer, Boris and Lee, Dongjin and Narayan, Akil and Xu, Yiming},
    title = {An Approximate Control Variates Approach to Multifidelity Distribution Estimation},
    journal = {SIAM/ASA Journal on Uncertainty Quantification},
    volume = {12},
    number = {4},
    pages = {1349-1388},
    year = {2024},
    doi = {10.1137/23M1584307},
}

@article{JMLR:v6:hyvarinen05a,
  author  = {Aapo Hyv{{\"a}}rinen},
  title   = {Estimation of Non-Normalized Statistical Models by Score Matching},
  journal = {Journal of Machine Learning Research},
  year    = {2005},
  volume  = {6},
  number  = {24},
  pages   = {695--709},
  url     = {http://jmlr.org/papers/v6/hyvarinen05a.html}
}

@inproceedings{Korba2020,
 author = {Korba, Anna and Salim, Adil and Arbel, Michael and Luise, Giulia and Gretton, Arthur},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {4672--4682},
 publisher = {Curran Associates, Inc.},
 title = {A Non-Asymptotic Analysis for Stein Variational Gradient Descent},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/3202111cf90e7c816a472aaceb72b0df-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{Burt2020,
  author  = {David R. Burt and Carl Edward Rasmussen and Mark van der Wilk},
  title   = {Convergence of Sparse Variational Inference in Gaussian Processes Regression},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {131},
  pages   = {1--63},
  url     = {http://jmlr.org/papers/v21/19-1015.html}
}

@article{duchi11a,
  author  = {John Duchi and Elad Hazan and Yoram Singer},
  title   = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {61},
  pages   = {2121--2159},
  url     = {http://jmlr.org/papers/v12/duchi11a.html}
}

@inproceedings{peng2020,
 author = {Chen, Peng and Ghattas, Omar},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1947--1958},
 publisher = {Curran Associates, Inc.},
 title = {Projected Stein Variational Gradient Descent},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/14faf969228fc18fcd4fcf59437b0c97-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{anantha_padmanabha_improving_2024,
	title = {Improving the performance of {Stein} variational inference through extreme sparsification of physically-constrained neural network models},
	volume = {432},
	issn = {00457825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045782524006145},
	doi = {10.1016/j.cma.2024.117359},
	language = {en},
	urldate = {2025-12-20},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Anantha Padmanabha, Govinda and Fuhg, Jan Niklas and Safta, Cosmin and Jones, Reese E. and Bouklas, Nikolaos},
	month = dec,
	year = {2024},
	pages = {117359},
}

@article{padmanabha_csvgd_2025,
	title = {{CONCURRENT}, {CONDENSED} {STEIN} {VARIATIONAL} {GRADIENT} {DESCENT} {FOR} {UNCERTAINTY} {QUANTIFICATION} {OF} {NEURAL} {NETWORKS}},
	volume = {6},
	issn = {2689-3967},
	url = {https://www.dl.begellhouse.com/journals/558048804a15188a,24181d765dd9f652,40afaf9b168a212e.html},
	doi = {10.1615/JMachLearnModelComput.2025059347},
	abstract = {We propose a Stein variational gradient descent (SVGD) method to concurrently sparsify, train, and provide uncertainty quantification (UQ) of a complexly parameterized model, such as a neural network (NN). It employs a graph reconciliation and condensation process to reduce complexity and increase similarity in the Stein ensemble of parameterizations. Therefore, the proposed concurrent, condensed SVGD (ccSVGD) method can provide UQ on parameters, not just outputs. Furthermore, the parameter reduction speeds up the convergence of the Stein gradient descent as it reduces the combinatorial complexity by aligning and differentiating the sensitivity to parameters. These properties are demonstrated with an illustrative example and an application to a mechanical response representation problem in solid mechanics.},
	language = {en},
	number = {3},
	urldate = {2025-12-20},
	journal = {Journal of Machine Learning for Modeling and Computing},
	author = {Padmanabha, Govinda Anantha and Safta, Cosmin and Bouklas, Nikolaos and Jones, Reese E.},
	year = {2025},
	pages = {69--94},
}

@misc{sukys_optimal_2017,
	title = {Optimal fidelity multi-level {Monte} {Carlo} for quantification of uncertainty in simulations of cloud cavitation collapse},
	url = {http://arxiv.org/abs/1705.04374},
	doi = {10.48550/arXiv.1705.04374},
	abstract = {We quantify uncertainties in the location and magnitude of extreme pressure spots revealed from large scale multi-phase flow simulations of cloud cavitation collapse. We examine clouds containing 500 cavities and quantify uncertainties related to their initial spatial arrangement. The resulting 2000-dimensional space is sampled using a non-intrusive and computationally efficient Multi-Level Monte Carlo (MLMC) methodology. We introduce novel optimal control variate coefficients to enhance the variance reduction in MLMC. The proposed optimal fidelity MLMC leads to more than two orders of magnitude speedup when compared to standard Monte Carlo methods. We identify large uncertainties in the location and magnitude of the peak pressure pulse and present its statistical correlations and joint probability density functions with the geometrical characteristics of the cloud. Characteristic properties of spatial cloud structure are identified as potential causes of significant uncertainties in exerted collapse pressures.},
	urldate = {2025-12-22},
	publisher = {arXiv},
	author = {Šukys, Jonas and Rasthofer, Ursula and Wermelinger, Fabian and Hadjidoukas, Panagiotis and Koumoutsakos, Petros},
	month = may,
	year = {2017},
	note = {arXiv:1705.04374},
	keywords = {Computer Science - Computational Engineering, Finance, and Science, Computer Science - Distributed, Parallel, and Cluster Computing, Mathematics - Numerical Analysis, Statistics - Computation},
}

@misc{neal_mcmc_2012,
	title = {{MCMC} using {Hamiltonian} dynamics},
	url = {http://arxiv.org/abs/1206.1901},
	doi = {10.48550/arXiv.1206.1901},
	abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard-to-compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, I discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for deciding on acceptance or rejection, computing trajectories using fast approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories from taking much computation time.},
	urldate = {2025-12-23},
	publisher = {arXiv},
	author = {Neal, Radford M.},
	month = jun,
	year = {2012},
	note = {arXiv:1206.1901},
	keywords = {Statistics - Computation, Physics - Computational Physics},
}

@article{JMLR:v15:hoffman14a,
  author  = {Matthew D. Hoffman and Andrew Gelman},
  title   = {The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {47},
  pages   = {1593--1623},
  url     = {http://jmlr.org/papers/v15/hoffman14a.html}
}

@incollection{stein_bound_1972,
	title = {A bound for the error in the normal approximation to the distribution of a sum of dependent random variables},
	volume = {6.2},
	url = {https://projecteuclid.org/ebooks/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Sixth-Berkeley-Symposium-on-Mathematical-Statistics-and/chapter/A-bound-for-the-error-in-the-normal-approximation-to/bsmsp/1200514239},
	urldate = {2025-12-23},
	booktitle = {Proceedings of the {Sixth} {Berkeley} {Symposium} on {Mathematical} {Statistics} and {Probability}, {Volume} 2: {Probability} {Theory}},
	publisher = {University of California Press},
	author = {Stein, Charles},
	month = jan,
	year = {1972},
	pages = {583--603},
}

@article{neal_slice_2003,
	title = {Slice sampling},
	volume = {31},
	issn = {0090-5364},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-31/issue-3/Slice-sampling/10.1214/aos/1056562461.full},
	doi = {10.1214/aos/1056562461},
	number = {3},
	urldate = {2025-12-27},
	journal = {The Annals of Statistics},
	author = {Neal, Radford M.},
	month = jun,
	year = {2003},
}

@article{Quaglino2019,
title = {High-dimensional and higher-order multifidelity Monte Carlo estimators},
journal = {Journal of Computational Physics},
volume = {388},
pages = {300-315},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2019.03.026},
author = {A. Quaglino and S. Pezzuto and R. Krause},
keywords = {Uncertainty quantification, Monte Carlo, Multifidelity, Global sensitivity analysis, Model reduction, Cardiac electrophysiology}
}
