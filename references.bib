
@article{hoffman_stochastic_2013,
	title = {Stochastic {Variational} {Inference}},
	volume = {14},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v14/hoffman13a.html},
	abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
	number = {40},
	urldate = {2024-04-06},
	journal = {Journal of Machine Learning Research},
	author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
	year = {2013},
	pages = {1303--1347},
}

@misc{noack_unifying_2023,
	title = {A {Unifying} {Perspective} on {Non}-{Stationary} {Kernels} for {Deeper} {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/2309.10068},
	doi = {10.48550/arXiv.2309.10068},
	abstract = {The Gaussian process (GP) is a popular statistical technique for stochastic function approximation and uncertainty quantification from data. GPs have been adopted into the realm of machine learning in the last two decades because of their superior prediction abilities, especially in data-sparse scenarios, and their inherent ability to provide robust uncertainty estimates. Even so, their performance highly depends on intricate customizations of the core methodology, which often leads to dissatisfaction among practitioners when standard setups and off-the-shelf software tools are being deployed. Arguably the most important building block of a GP is the kernel function which assumes the role of a covariance operator. Stationary kernels of the Mat{\textbackslash}'ern class are used in the vast majority of applied studies; poor prediction performance and unrealistic uncertainty quantification are often the consequences. Non-stationary kernels show improved performance but are rarely used due to their more complicated functional form and the associated effort and expertise needed to define and tune them optimally. In this perspective, we want to help ML practitioners make sense of some of the most common forms of non-stationarity for Gaussian processes. We show a variety of kernels in action using representative datasets, carefully study their properties, and compare their performances. Based on our findings, we propose a new kernel that combines some of the identified advantages of existing kernels.},
	urldate = {2024-04-06},
	publisher = {arXiv},
	author = {Noack, Marcus M. and Luo, Hengrui and Risser, Mark D.},
	month = sep,
	year = {2023},
	note = {arXiv:2309.10068 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{liu_stein_2019,
	title = {Stein {Variational} {Gradient} {Descent}: {A} {General} {Purpose} {Bayesian} {Inference} {Algorithm}},
	shorttitle = {Stein {Variational} {Gradient} {Descent}},
	url = {http://arxiv.org/abs/1608.04471},
	doi = {10.48550/arXiv.1608.04471},
	abstract = {We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.},
	urldate = {2024-04-06},
	publisher = {arXiv},
	author = {Liu, Qiang and Wang, Dilin},
	month = sep,
	year = {2019},
	note = {arXiv:1608.04471 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{chen_projected_2020,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '20},
	title = {Projected stein variational gradient descent},
	isbn = {978-1-71382-954-6},
	abstract = {The curse of dimensionality is a longstanding challenge in Bayesian inference in high dimensions. In this work, we propose a projected Stein variational gradient descent (pSVGD) method to overcome this challenge by exploiting the fundamental property of intrinsic low dimensionality of the data informed subspace stemming from ill-posedness of such problems. We adaptively construct the subspace using a gradient information matrix of the log-likelihood, and apply pSVGD to the much lower-dimensional coefficients of the parameter projection. The method is demonstrated to be more accurate and efficient than SVGD. It is also shown to be more scalable with respect to the number of parameters, samples, data points, and processor cores via experiments with parameters dimensions ranging from the hundreds to the tens of thousands.},
	urldate = {2024-04-06},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Chen, Peng and Ghattas, Omar},
	month = dec,
	year = {2020},
	pages = {1947--1958},
}

@misc{liu_kernelized_2016,
	title = {A {Kernelized} {Stein} {Discrepancy} for {Goodness}-of-fit {Tests} and {Model} {Evaluation}},
	url = {http://arxiv.org/abs/1602.03253},
	doi = {10.48550/arXiv.1602.03253},
	abstract = {We derive a new discrepancy statistic for measuring differences between two probability distributions based on combining Stein's identity with the reproducing kernel Hilbert space theory. We apply our result to test how well a probabilistic model fits a set of observations, and derive a new class of powerful goodness-of-fit tests that are widely applicable for complex and high dimensional distributions, even for those with computationally intractable normalization constants. Both theoretical and empirical properties of our methods are studied thoroughly.},
	urldate = {2024-04-06},
	publisher = {arXiv},
	author = {Liu, Qiang and Lee, Jason D. and Jordan, Michael I.},
	month = jul,
	year = {2016},
	note = {arXiv:1602.03253 [stat]},
	keywords = {Statistics - Machine Learning},
}

@inproceedings{liu_short_2016,
	title = {A {Short} {Introduction} to {Kernelized} {Stein} {Discrepancy}},
	url = {https://www.semanticscholar.org/paper/A-Short-Introduction-to-Kernelized-Stein-Liu/eef3d4bfac79b383050900ebe8b64f7d9a042b70},
	abstract = {Machine learning and statistics are essentially about understanding data using models (typically probabilistic models). Discrepancy measures that can tell the consistency between data and models are extremely useful, and provide foundations for algorithms for all kinds of tasks, including model evaluation (telling how well a model fits the data), frequentist parameter learning (finding the model that minimizes the discrepancy with data), as well as sampling for Bayesian inference (finding a set of points ("data") to approximate the posterior distribution). See Figure 1.},
	urldate = {2024-04-06},
	author = {Liu, Qiang},
	year = {2016},
}

@article{plumlee_bayesian_2017,
	title = {Bayesian {Calibration} of {Inexact} {Computer} {Models}},
	volume = {112},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2016.1211016},
	doi = {10.1080/01621459.2016.1211016},
	abstract = {Bayesian calibration is used to study computer models in the presence of both a calibration parameter and model bias. The parameter in the predominant methodology is left undefined. This results in an issue, where the posterior of the parameter is suboptimally broad. There has been no generally accepted alternatives to date. This article proposes using Bayesian calibration, where the prior distribution on the bias is orthogonal to the gradient of the computer model. Problems associated with Bayesian calibration are shown to be mitigated through analytic results in addition to examples. Supplementary materials for this article are available online.},
	number = {519},
	urldate = {2024-04-05},
	journal = {Journal of the American Statistical Association},
	author = {Plumlee, Matthew},
	month = jul,
	year = {2017},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2016.1211016},
	keywords = {Calibration, Computer experiments, Deterministic models, Gaussian processes, Identifiability, Kriging, Model inadequacy, Orthogonal processes, Uncertainty quantification},
	pages = {1274--1285},
}

@article{kovachki_neural_2023,
	title = {Neural {Operator}: {Learning} {Maps} {Between} {Function} {Spaces} {With} {Applications} to {PDEs}},
	volume = {24},
	issn = {1533-7928},
	shorttitle = {Neural {Operator}},
	url = {http://jmlr.org/papers/v24/21-1524.html},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite dimensional Euclidean spaces or finite sets. We propose a generalization of neural networks to learn operators, termed neural operators, that map between infinite dimensional function spaces. We formulate the neural operator as a composition of linear integral operators and nonlinear activation functions. We prove a universal approximation theorem for our proposed neural operator, showing that it can approximate any given nonlinear continuous operator. The proposed neural operators are also discretization-invariant, i.e., they share the same model parameters among different discretization of the underlying function spaces. Furthermore, we introduce four classes of efficient parameterization, viz., graph neural operators, multi-pole graph neural operators, low-rank neural operators, and Fourier neural operators. An important application for neural operators is learning surrogate maps for the solution operators of partial differential equations (PDEs). We consider standard PDEs such as the Burgers, Darcy subsurface flow, and the Navier-Stokes equations, and show that the proposed neural operators have superior performance compared to existing machine learning based methodologies, while being several orders of magnitude faster than conventional PDE solvers.},
	number = {89},
	urldate = {2024-04-05},
	journal = {Journal of Machine Learning Research},
	author = {Kovachki, Nikola and Li, Zongyi and Liu, Burigede and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	year = {2023},
	pages = {1--97},
}

@misc{viswanath_neural_2023,
	title = {Neural {Operator}: {Is} data all you need to model the world? {An} insight into the impact of {Physics} {Informed} {Machine} {Learning}},
	shorttitle = {Neural {Operator}},
	url = {http://arxiv.org/abs/2301.13331},
	doi = {10.48550/arXiv.2301.13331},
	abstract = {Numerical approximations of partial differential equations (PDEs) are routinely employed to formulate the solution of physics, engineering and mathematical problems involving functions of several variables, such as the propagation of heat or sound, fluid flow, elasticity, electrostatics, electrodynamics, and more. While this has led to solving many complex phenomena, there are some limitations. Conventional approaches such as Finite Element Methods (FEMs) and Finite Differential Methods (FDMs) require considerable time and are computationally expensive. In contrast, data driven machine learning-based methods such as neural networks provide a faster, fairly accurate alternative, and have certain advantages such as discretization invariance and resolution invariance. This article aims to provide a comprehensive insight into how data-driven approaches can complement conventional techniques to solve engineering and physics problems, while also noting some of the major pitfalls of machine learning-based approaches. Furthermore, we highlight, a novel and fast machine learning-based approach ({\textasciitilde}1000x) to learning the solution operator of a PDE operator learning. We will note how these new computational approaches can bring immense advantages in tackling many problems in fundamental and applied physics.},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Viswanath, Hrishikesh and Rahman, Md Ashiqur and Vyas, Abhijeet and Shor, Andrey and Medeiros, Beatriz and Hernandez, Stephanie and Prameela, Suhas Eswarappa and Bera, Aniket},
	month = sep,
	year = {2023},
	note = {arXiv:2301.13331 [physics]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Physics - Computational Physics},
}

@misc{viswanath_neural_2023-1,
	title = {Neural {Operator}: {Is} data all you need to model the world? {An} insight into the impact of {Physics} {Informed} {Machine} {Learning}},
	shorttitle = {Neural {Operator}},
	url = {http://arxiv.org/abs/2301.13331},
	doi = {10.48550/arXiv.2301.13331},
	abstract = {Numerical approximations of partial differential equations (PDEs) are routinely employed to formulate the solution of physics, engineering and mathematical problems involving functions of several variables, such as the propagation of heat or sound, fluid flow, elasticity, electrostatics, electrodynamics, and more. While this has led to solving many complex phenomena, there are some limitations. Conventional approaches such as Finite Element Methods (FEMs) and Finite Differential Methods (FDMs) require considerable time and are computationally expensive. In contrast, data driven machine learning-based methods such as neural networks provide a faster, fairly accurate alternative, and have certain advantages such as discretization invariance and resolution invariance. This article aims to provide a comprehensive insight into how data-driven approaches can complement conventional techniques to solve engineering and physics problems, while also noting some of the major pitfalls of machine learning-based approaches. Furthermore, we highlight, a novel and fast machine learning-based approach ({\textasciitilde}1000x) to learning the solution operator of a PDE operator learning. We will note how these new computational approaches can bring immense advantages in tackling many problems in fundamental and applied physics.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Viswanath, Hrishikesh and Rahman, Md Ashiqur and Vyas, Abhijeet and Shor, Andrey and Medeiros, Beatriz and Hernandez, Stephanie and Prameela, Suhas Eswarappa and Bera, Aniket},
	month = sep,
	year = {2023},
	note = {arXiv:2301.13331 [physics]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Physics - Computational Physics},
}

@article{prabhudesai_lowering_2023,
	title = {Lowering the computational barrier: {Partially} {Bayesian} neural networks for transparency in medical imaging {AI}},
	volume = {5},
	issn = {2624-9898},
	shorttitle = {Lowering the computational barrier},
	url = {https://www.frontiersin.org/articles/10.3389/fcomp.2023.1071174},
	doi = {10.3389/fcomp.2023.1071174},
	abstract = {Deep Neural Networks (DNNs) can provide clinicians with fast and accurate predictions that are highly valuable for high-stakes medical decision-making, such as in brain tumor segmentation and treatment planning. However, these models largely lack transparency about the uncertainty in their predictions, potentially giving clinicians a false sense of reliability that may lead to grave consequences in patient care. Growing calls for Transparent and Responsible AI have promoted Uncertainty Quantification (UQ) to capture and communicate uncertainty in a systematic and principled manner. However, traditional Bayesian UQ methods remain prohibitively costly for large, million-dimensional tumor segmentation DNNs such as the U-Net. In this work, we discuss a computationally-efficient UQ approach via the partially Bayesian neural networks (pBNN). In pBNN, only a single layer, strategically selected based on gradient-based sensitivity analysis, is targeted for Bayesian inference. We illustrate the effectiveness of pBNN in capturing the full uncertainty for a 7.8-million parameter U-Net. We also demonstrate how practitioners and model developers can use the pBNN's predictions to better understand the model’s capabilities and behavior.},
	language = {English},
	urldate = {2024-04-04},
	journal = {Frontiers in Computer Science},
	author = {Prabhudesai, Snehal and Hauth, Jeremiah and Guo, Dingkun and Rao, Arvind and Banovic, Nikola and Huan, Xun},
	month = feb,
	year = {2023},
	note = {Publisher: Frontiers},
	keywords = {Bayesian uncertainty, Transparency, Tumor segmentation, medical decision-making, responsible ai},
}

@article{zhu_bayesian_2018,
	title = {Bayesian deep convolutional encoder–decoder networks for surrogate modeling and uncertainty quantification},
	volume = {366},
	issn = {0021-9991},
	url = {https://doi.org/10.1016/j.jcp.2018.04.018},
	doi = {10.1016/j.jcp.2018.04.018},
	abstract = {We are interested in the development of surrogate models for uncertainty quantification and propagation in problems governed by stochastic PDEs using a deep convolutional encoder–decoder network in a similar fashion to approaches considered in deep learning for image-to-image regression tasks. Since normal neural networks are data-intensive and cannot provide predictive uncertainty, we propose a Bayesian approach to convolutional neural nets. A recently introduced variational gradient descent algorithm based on Stein's method is scaled to deep convolutional networks to perform approximate Bayesian inference on millions of uncertain network parameters. This approach achieves state of the art performance in terms of predictive accuracy and uncertainty quantification in comparison to other approaches in Bayesian neural networks as well as techniques that include Gaussian processes and ensemble methods even when the training data size is relatively small. To evaluate the performance of this approach, we consider standard uncertainty quantification tasks for flow in heterogeneous media using limited training data consisting of permeability realizations and the corresponding velocity and pressure fields. The performance of the surrogate model developed is very good even though there is no underlying structure shared between the input (permeability) and output (flow/pressure) fields as is often the case in the image-to-image regression models used in computer vision problems. Studies are performed with an underlying stochastic input dimensionality up to 4225 where most other uncertainty quantification methods fail. Uncertainty propagation tasks are considered and the predictive output Bayesian statistics are compared to those obtained with Monte Carlo estimates. • Bayesian Convolutional Encoder–Decoder Deep Networks for Uncertainty Quantification Tasks. • Integrating Stein variational inference for exploring the high-dimensional posterior distribution of the network parameters. • Addressing the curse of dimensionality showing applications in porous media flows with permeability dimensionality of 4225.},
	number = {C},
	urldate = {2024-04-04},
	journal = {Journal of Computational Physics},
	author = {Zhu, Yinhao and Zabaras, Nicholas},
	month = aug,
	year = {2018},
	keywords = {Bayesian neural networks, Convolutional encoder–decoder networks, Deep learning, Porous media flows, Uncertainty quantification},
	pages = {415--447},
}

@article{zhu_bayesian_2018-1,
	title = {Bayesian {Deep} {Convolutional} {Encoder}-{Decoder} {Networks} for {Surrogate} {Modeling} and {Uncertainty} {Quantification}},
	volume = {366},
	issn = {00219991},
	url = {http://arxiv.org/abs/1801.06879},
	doi = {10.1016/j.jcp.2018.04.018},
	abstract = {We are interested in the development of surrogate models for uncertainty quantification and propagation in problems governed by stochastic PDEs using a deep convolutional encoder-decoder network in a similar fashion to approaches considered in deep learning for image-to-image regression tasks. Since normal neural networks are data intensive and cannot provide predictive uncertainty, we propose a Bayesian approach to convolutional neural nets. A recently introduced variational gradient descent algorithm based on Stein's method is scaled to deep convolutional networks to perform approximate Bayesian inference on millions of uncertain network parameters. This approach achieves state of the art performance in terms of predictive accuracy and uncertainty quantification in comparison to other approaches in Bayesian neural networks as well as techniques that include Gaussian processes and ensemble methods even when the training data size is relatively small. To evaluate the performance of this approach, we consider standard uncertainty quantification benchmark problems including flow in heterogeneous media defined in terms of limited data-driven permeability realizations. The performance of the surrogate model developed is very good even though there is no underlying structure shared between the input (permeability) and output (flow/pressure) fields as is often the case in the image-to-image regression models used in computer vision problems. Studies are performed with an underlying stochastic input dimensionality up to \$4,225\$ where most other uncertainty quantification methods fail. Uncertainty propagation tasks are considered and the predictive output Bayesian statistics are compared to those obtained with Monte Carlo estimates.},
	urldate = {2024-04-04},
	journal = {Journal of Computational Physics},
	author = {Zhu, Yinhao and Zabaras, Nicholas},
	month = aug,
	year = {2018},
	note = {arXiv:1801.06879 [physics, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Physics - Computational Physics, Statistics - Machine Learning},
	pages = {415--447},
}

@misc{houlsby_bayesian_2011,
	title = {Bayesian {Active} {Learning} for {Classification} and {Preference} {Learning}},
	url = {http://arxiv.org/abs/1112.5745},
	doi = {10.48550/arXiv.1112.5745},
	abstract = {Information theoretic active learning has been widely studied for probabilistic models. For simple regression an optimal myopic policy is easily tractable. However, for other tasks and with more complex models, such as classification with nonparametric models, the optimal solution is harder to compute. Current approaches make approximations to achieve tractability. We propose an approach that expresses information gain in terms of predictive entropies, and apply this method to the Gaussian Process Classifier (GPC). Our approach makes minimal approximations to the full information theoretic objective. Our experimental performance compares favourably to many popular active learning algorithms, and has equal or lower computational complexity. We compare well to decision theoretic approaches also, which are privy to more information and require much more computational time. Secondly, by developing further a reformulation of binary preference learning to a classification problem, we extend our algorithm to Gaussian Process preference learning.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Houlsby, Neil and Huszár, Ferenc and Ghahramani, Zoubin and Lengyel, Máté},
	month = dec,
	year = {2011},
	note = {arXiv:1112.5745 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{rainforth2023,
	title = {Modern {Bayesian} {Experimental} {Design}},
	url = {http://arxiv.org/abs/2302.14545},
	doi = {10.48550/arXiv.2302.14545},
	abstract = {Bayesian experimental design (BED) provides a powerful and general framework for optimizing the design of experiments. However, its deployment often poses substantial computational challenges that can undermine its practical use. In this review, we outline how recent advances have transformed our ability to overcome these challenges and thus utilize BED effectively, before discussing some key areas for future development in the field.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Rainforth, Tom and Foster, Adam and Ivanova, Desi R. and Smith, Freddie Bickford},
	month = nov,
	year = {2023},
	note = {arXiv:2302.14545 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
}

@misc{kossen_active_2022,
	title = {Active {Surrogate} {Estimators}: {An} {Active} {Learning} {Approach} to {Label}-{Efficient} {Model} {Evaluation}},
	shorttitle = {Active {Surrogate} {Estimators}},
	url = {http://arxiv.org/abs/2202.06881},
	doi = {10.48550/arXiv.2202.06881},
	abstract = {We propose Active Surrogate Estimators (ASEs), a new method for label-efficient model evaluation. Evaluating model performance is a challenging and important problem when labels are expensive. ASEs address this active testing problem using a surrogate-based estimation approach that interpolates the errors of points with unknown labels, rather than forming a Monte Carlo estimator. ASEs actively learn the underlying surrogate, and we propose a novel acquisition strategy, XWED, that tailors this learning to the final estimation task. We find that ASEs offer greater label-efficiency than the current state-of-the-art when applied to challenging model evaluation problems for deep neural networks.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Kossen, Jannik and Farquhar, Sebastian and Gal, Yarin and Rainforth, Tom},
	month = oct,
	year = {2022},
	note = {arXiv:2202.06881 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{smith_prediction-oriented_2023,
	title = {Prediction-{Oriented} {Bayesian} {Active} {Learning}},
	url = {https://proceedings.mlr.press/v206/bickfordsmith23a.html},
	abstract = {Information-theoretic approaches to active learning have traditionally focused on maximising the information gathered about the model parameters, most commonly by optimising the BALD score. We highlight that this can be suboptimal from the perspective of predictive performance. For example, BALD lacks a notion of an input distribution and so is prone to prioritise data of limited relevance. To address this we propose the expected predictive information gain (EPIG), an acquisition function that measures information gain in the space of predictions rather than parameters. We find that using EPIG leads to stronger predictive performance compared with BALD across a range of datasets and models, and thus provides an appealing drop-in replacement.},
	language = {en},
	urldate = {2024-04-04},
	booktitle = {Proceedings of {The} 26th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Smith, Freddie Bickford and Kirsch, Andreas and Farquhar, Sebastian and Gal, Yarin and Foster, Adam and Rainforth, Tom},
	month = apr,
	year = {2023},
	pages = {7331--7348},
}

@misc{hauth_uncertainty_2024,
	title = {Uncertainty {Quantification} of {Graph} {Convolution} {Neural} {Network} {Models} of {Evolving} {Processes}},
	url = {http://arxiv.org/abs/2402.11179},
	doi = {10.48550/arXiv.2402.11179},
	abstract = {The application of neural network models to scientific machine learning tasks has proliferated in recent years. In particular, neural network models have proved to be adept at modeling processes with spatial-temporal complexity. Nevertheless, these highly parameterized models have garnered skepticism in their ability to produce outputs with quantified error bounds over the regimes of interest. Hence there is a need to find uncertainty quantification methods that are suitable for neural networks. In this work we present comparisons of the parametric uncertainty quantification of neural networks modeling complex spatial-temporal processes with Hamiltonian Monte Carlo and Stein variational gradient descent and its projected variant. Specifically we apply these methods to graph convolutional neural network models of evolving systems modeled with recurrent neural network and neural ordinary differential equations architectures. We show that Stein variational inference is a viable alternative to Monte Carlo methods with some clear advantages for complex neural network models. For our exemplars, Stein variational interference gave similar uncertainty profiles through time compared to Hamiltonian Monte Carlo, albeit with generally more generous variance.Projected Stein variational gradient descent also produced similar uncertainty profiles to the non-projected counterpart, but large reductions in the active weight space were confounded by the stability of the neural network predictions and the convoluted likelihood landscape.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Hauth, Jeremiah and Safta, Cosmin and Huan, Xun and Patel, Ravi G. and Jones, Reese E.},
	month = feb,
	year = {2024},
	note = {arXiv:2402.11179 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Physics - Computational Physics},
}

@misc{hauth_uncertainty_2024-1,
	title = {Uncertainty {Quantification} of {Graph} {Convolution} {Neural} {Network} {Models} of {Evolving} {Processes}},
	url = {http://arxiv.org/abs/2402.11179},
	abstract = {The application of neural network models to scientific machine learning tasks has proliferated in recent years. In particular, neural network models have proved to be adept at modeling processes with spatial-temporal complexity. Nevertheless, these highly parameterized models have garnered skepticism in their ability to produce outputs with quantified error bounds over the regimes of interest. Hence there is a need to find uncertainty quantification methods that are suitable for neural networks. In this work we present comparisons of the parametric uncertainty quantification of neural networks modeling complex spatial-temporal processes with Hamiltonian Monte Carlo and Stein variational gradient descent and its projected variant. Specifically we apply these methods to graph convolutional neural network models of evolving systems modeled with recurrent neural network and neural ordinary differential equations architectures. We show that Stein variational inference is a viable alternative to Monte Carlo methods with some clear advantages for complex neural network models. For our exemplars, Stein variational interference gave similar uncertainty profiles through time compared to Hamiltonian Monte Carlo, albeit with generally more generous variance.Projected Stein variational gradient descent also produced similar uncertainty profiles to the non-projected counterpart, but large reductions in the active weight space were confounded by the stability of the neural network predictions and the convoluted likelihood landscape.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Hauth, Jeremiah and Safta, Cosmin and Huan, Xun and Patel, Ravi G. and Jones, Reese E.},
	month = feb,
	year = {2024},
	note = {arXiv:2402.11179 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Physics - Computational Physics},
}

@article{strutz_variational_2024,
	title = {Variational {Bayesian} experimental design for geophysical applications: seismic source location, amplitude versus offset inversion, and estimating {CO2} saturations in a subsurface reservoir},
	volume = {236},
	issn = {0956-540X},
	shorttitle = {Variational {Bayesian} experimental design for geophysical applications},
	url = {https://doi.org/10.1093/gji/ggad492},
	doi = {10.1093/gji/ggad492},
	abstract = {In geophysical surveys or experiments, recorded data are used to constrain properties of the planetary subsurface, oceans, atmosphere or cryosphere. How the experimental data are collected significantly influences which parameters can be resolved and how much confidence can be placed in the results. Bayesian experimental design methods characterize, quantify and maximize expected information post-experiment—an optimization problem. Typical design parameters that can be optimized are source and/or sensor types and locations, and the choice of modelling or data processing methods to be applied to the data. These may all be optimized subject to various physical and cost constraints. This paper introduces variational design methods, and discusses their benefits and limitations in the context of geophysical applications. Variational methods have recently come to prominence due to their importance in machine-learning applications. They can be used to design experiments that best resolve either all model parameters, or the answer to specific questions about the system to be interrogated. The methods are tested in three schematic geophysical applications: (i) estimating a source location given arrival times of radiating energy at sensor locations, (ii) estimating the contrast in seismic velocity across a stratal interface given measurements of the amplitudes of seismic wavefield reflections from that interface, and (iii) designing a survey to best constrain CO2 saturation in a subsurface storage scenario. Variational methods allow the value of expected information from an experiment to be calculated and optimized simultaneously, which results in substantial savings in computational cost. In the context of designing a survey to best constrain CO2 saturation in a subsurface storage scenario, we show that optimal designs may change substantially depending on the particular questions of interest. We also show that one method, so-called DN design, can be effective at substantially lower computational cost than other methods. Overall, this work demonstrates that optimal design methods could be used more widely in Geophysics, as they are in other scientifically advanced fields.},
	number = {3},
	urldate = {2024-04-04},
	journal = {Geophysical Journal International},
	author = {Strutz, Dominik and Curtis, Andrew},
	month = mar,
	year = {2024},
	pages = {1309--1331},
}

@article{alexanderian_optimal_2021,
	title = {Optimal experimental design for infinite-dimensional {Bayesian} inverse problems governed by {PDEs}: a review},
	volume = {37},
	issn = {0266-5611},
	shorttitle = {Optimal experimental design for infinite-dimensional {Bayesian} inverse problems governed by {PDEs}},
	url = {https://dx.doi.org/10.1088/1361-6420/abe10c},
	doi = {10.1088/1361-6420/abe10c},
	abstract = {We present a review of methods for optimal experimental design (OED) for Bayesian inverse problems governed by partial differential equations with infinite-dimensional parameters. The focus is on problems where one seeks to optimize the placement of measurement points, at which data are collected, such that the uncertainty in the estimated parameters is minimized. We present the mathematical foundations of OED in this context and survey the computational methods for the class of OED problems under study. We also outline some directions for future research in this area.},
	language = {en},
	number = {4},
	urldate = {2024-04-04},
	journal = {Inverse Problems},
	author = {Alexanderian, Alen},
	month = mar,
	year = {2021},
	pages = {043001},
}

@misc{zhong_goal-oriented_2024,
	title = {Goal-{Oriented} {Bayesian} {Optimal} {Experimental} {Design} for {Nonlinear} {Models} using {Markov} {Chain} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/2403.18072},
	doi = {10.48550/arXiv.2403.18072},
	abstract = {Optimal experimental design (OED) provides a systematic approach to quantify and maximize the value of experimental data. Under a Bayesian approach, conventional OED maximizes the expected information gain (EIG) on model parameters. However, we are often interested in not the parameters themselves, but predictive quantities of interest (QoIs) that depend on the parameters in a nonlinear manner. We present a computational framework of predictive goal-oriented OED (GO-OED) suitable for nonlinear observation and prediction models, which seeks the experimental design providing the greatest EIG on the QoIs. In particular, we propose a nested Monte Carlo estimator for the QoI EIG, featuring Markov chain Monte Carlo for posterior sampling and kernel density estimation for evaluating the posterior-predictive density and its Kullback-Leibler divergence from the prior-predictive. The GO-OED design is then found by maximizing the EIG over the design space using Bayesian optimization. We demonstrate the effectiveness of the overall nonlinear GO-OED method, and illustrate its differences versus conventional non-GO-OED, through various test problems and an application of sensor placement for source inversion in a convection-diffusion field.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Zhong, Shijie and Shen, Wanggang and Catanach, Tommie and Huan, Xun},
	month = mar,
	year = {2024},
	note = {arXiv:2403.18072 [cs, stat]},
	keywords = {62K05, 62F15, 62B15, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
}

@article{howard_interplanetary_2009,
	title = {Interplanetary {Coronal} {Mass} {Ejections} {Observed} in the {Heliosphere}: 1. {Review} of {Theory}},
	volume = {147},
	issn = {1572-9672},
	url = {https://doi.org/10.1007/s11214-009-9542-5},
	doi = {10.1007/s11214-009-9542-5},
	abstract = {With the recent advancements in interplanetary coronal mass ejection (ICME) imaging it is necessary to understand how heliospheric images may be interpreted, particularly at large elongation angles. Of crucial importance is how the current methods used for coronal mass ejection measurement in coronagraph images must be changed to account for the large elongations involved in the heliosphere. In this review of theory we build up a picture of ICME appearance and evolution at large elongations in terms of how it would appear to an observer near 1 AU from the Sun. We begin by revisiting the basics of Thomson scattering describing how ICMEs are detected, in this we attempt to clarify a number of common misconceptions. We then build up from a single electron to an integrated line of sight, consider the ICME as a collection of lines of sight and describe how a map of ICME appearance may be developed based on its appearance relative to each line of sight. Finally, we discuss how the topology of the ICME affects its observed geometry and kinematic properties, particularly at large elongations. This review is the first of a three-part series of papers, where a review of theory is presented here and a model is developed and used in subsequent papers.},
	number = {1},
	journal = {Space Science Reviews},
	author = {Howard, Timothy A. and Tappin, S. James},
	month = oct,
	year = {2009},
	pages = {31--54},
}

@misc{leger_parametrization_2023,
	title = {Parametrization {Cookbook}: {A} set of {Bijective} {Parametrizations} for using {Machine} {Learning} methods in {Statistical} {Inference}},
	shorttitle = {Parametrization {Cookbook}},
	url = {http://arxiv.org/abs/2301.08297},
	doi = {10.48550/arXiv.2301.08297},
	abstract = {We present in this paper a way to transform a constrained statistical inference problem into an unconstrained one in order to be able to use modern computational methods, such as those based on automatic differentiation, GPU computing, stochastic gradients with mini-batch. Unlike the parametrizations classically used in Machine Learning, the parametrizations introduced here are all bijective and are even diffeomorphisms, thus allowing to keep the important properties from a statistical inference point of view, first of all identifiability. This cookbook presents a set of recipes to use to transform a constrained problem into a unconstrained one. For an easy use of parametrizations, this paper is at the same time a cookbook, and a Python package allowing the use of parametrizations with numpy, but also JAX and PyTorch, as well as a high level and expressive interface allowing to easily describe a parametrization to transform a difficult problem of statistical inference into an easier problem addressable with modern optimization tools.},
	urldate = {2024-03-26},
	publisher = {arXiv},
	author = {Leger, Jean-Benoist},
	month = jan,
	year = {2023},
	note = {arXiv:2301.08297 [stat]},
	keywords = {Statistics - Computation, Statistics - Machine Learning},
}

@article{alam_winograd_2022,
	title = {Winograd {Convolution} for {Deep} {Neural} {Networks}: {Efficient} {Point} {Selection}},
	volume = {21},
	issn = {1539-9087},
	shorttitle = {Winograd {Convolution} for {Deep} {Neural} {Networks}},
	url = {https://dl.acm.org/doi/10.1145/3524069},
	doi = {10.1145/3524069},
	abstract = {Convolutional neural networks (CNNs) have dramatically improved the accuracy of image, video, and audio processing for tasks such as object recognition, image segmentation, and interactive speech systems. CNNs require large amounts of computing resources for both training and inference, primarily because the convolution layers are computationally intensive. Fast convolution algorithms such as Winograd convolution can greatly reduce the computational cost of these layers. However, Winograd convolution has poor numeric properties, such that greater savings in computation cause exponentially increasing floating point errors. A defining feature of each Winograd convolution algorithm is a set of real-value points where polynomials are sampled. The choice of points impacts the numeric accuracy of the algorithm, but the optimal set of points for small convolutions remains unknown. Existing work considers only small integers and simple fractions as candidate points. In this work, we propose a novel approach to point selection using points of the form \{−1𝑐,−𝑐,𝑐,1𝑐\}\{−1c,−c,c,1c\}{\textbackslash}lbrace -{\textbackslash}frac\{1\}\{c\},-c,c,{\textbackslash}frac\{1\}\{c\}{\textbackslash}rbrace using the full range of real-valued numbers for c. We show that groups of this form cause cancellations in the Winograd transform matrices that reduce numeric error. We find empirically that the error for different values of c forms a rough curve across the range of real-value numbers. It is therefore possible to localize the values of c that lead to lower error. We show that it is not necessary to choose integers or simple fractions as evaluation points, and that lower errors can be achieved with non-obvious real-valued points. We study a range of sizes for small convolutions and achieve reduction in error ranging from 2\% to around 59\% for both 1D and 2D convolution, when compared to state of the art. Furthermore, we identify patterns in cases when we select a subset of our proposed points that will always lead to a lower error. Finally, we implement a complete Winograd convolution layer and use it to run state-of-the-art deep convolution neural networks on real datasets and show that our proposed points achieve reduction in error, ranging from 22\% to 63\%, while also showing how an increased Winograd output size can result in execution speed-up for some cases.},
	number = {6},
	urldate = {2024-03-23},
	journal = {ACM Transactions on Embedded Computing Systems},
	author = {Alam, Syed Asad and Anderson, Andrew and Barabasz, Barbara and Gregg, David},
	month = dec,
	year = {2022},
	keywords = {Toom-Cook algorithm, Winograd convolution, deep neural networks},
	pages = {80:1--80:28},
}

@misc{higham_diffusion_2023,
	title = {Diffusion {Models} for {Generative} {Artificial} {Intelligence}: {An} {Introduction} for {Applied} {Mathematicians}},
	shorttitle = {Diffusion {Models} for {Generative} {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2312.14977},
	doi = {10.48550/arXiv.2312.14977},
	abstract = {Generative artificial intelligence (AI) refers to algorithms that create synthetic but realistic output. Diffusion models currently offer state of the art performance in generative AI for images. They also form a key component in more general tools, including text-to-image generators and large language models. Diffusion models work by adding noise to the available training data and then learning how to reverse the process. The reverse operation may then be applied to new random data in order to produce new outputs. We provide a brief introduction to diffusion models for applied mathematicians and statisticians. Our key aims are (a) to present illustrative computational examples, (b) to give a careful derivation of the underlying mathematical formulas involved, and (c) to draw a connection with partial differential equation (PDE) diffusion models. We provide code for the computational experiments. We hope that this topic will be of interest to advanced undergraduate students and postgraduate students. Portions of the material may also provide useful motivational examples for those who teach courses in stochastic processes, inference, machine learning, PDEs or scientific computing.},
	urldate = {2024-02-24},
	publisher = {arXiv},
	author = {Higham, Catherine F. and Higham, Desmond J. and Grindrod, Peter},
	month = dec,
	year = {2023},
	note = {arXiv:2312.14977 [cs]},
	keywords = {68T07, 60J60, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2, I.2.6},
}

@misc{noack_unifying_2023-1,
	title = {A {Unifying} {Perspective} on {Non}-{Stationary} {Kernels} for {Deeper} {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/2309.10068},
	doi = {10.48550/arXiv.2309.10068},
	abstract = {The Gaussian process (GP) is a popular statistical technique for stochastic function approximation and uncertainty quantification from data. GPs have been adopted into the realm of machine learning in the last two decades because of their superior prediction abilities, especially in data-sparse scenarios, and their inherent ability to provide robust uncertainty estimates. Even so, their performance highly depends on intricate customizations of the core methodology, which often leads to dissatisfaction among practitioners when standard setups and off-the-shelf software tools are being deployed. Arguably the most important building block of a GP is the kernel function which assumes the role of a covariance operator. Stationary kernels of the Mat{\textbackslash}'ern class are used in the vast majority of applied studies; poor prediction performance and unrealistic uncertainty quantification are often the consequences. Non-stationary kernels show improved performance but are rarely used due to their more complicated functional form and the associated effort and expertise needed to define and tune them optimally. In this perspective, we want to help ML practitioners make sense of some of the most common forms of non-stationarity for Gaussian processes. We show a variety of kernels in action using representative datasets, carefully study their properties, and compare their performances. Based on our findings, we propose a new kernel that combines some of the identified advantages of existing kernels.},
	urldate = {2024-02-23},
	publisher = {arXiv},
	author = {Noack, Marcus M. and Luo, Hengrui and Risser, Mark D.},
	month = sep,
	year = {2023},
	note = {arXiv:2309.10068 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{garnelo_neural_2018,
	title = {Neural {Processes}},
	url = {http://arxiv.org/abs/1807.01622},
	doi = {10.48550/arXiv.1807.01622},
	abstract = {A neural network (NN) is a parameterised function that can be tuned via gradient descent to approximate a labelled collection of data with high precision. A Gaussian process (GP), on the other hand, is a probabilistic model that defines a distribution over possible functions, and is updated in light of data via the rules of probabilistic inference. GPs are probabilistic, data-efficient and flexible, however they are also computationally intensive and thus limited in their applicability. We introduce a class of neural latent variable models which we call Neural Processes (NPs), combining the best of both worlds. Like GPs, NPs define distributions over functions, are capable of rapid adaptation to new observations, and can estimate the uncertainty in their predictions. Like NNs, NPs are computationally efficient during training and evaluation but also learn to adapt their priors to data. We demonstrate the performance of NPs on a range of learning tasks, including regression and optimisation, and compare and contrast with related models in the literature.},
	urldate = {2024-02-15},
	publisher = {arXiv},
	author = {Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J. and Eslami, S. M. Ali and Teh, Yee Whye},
	month = jul,
	year = {2018},
	note = {arXiv:1807.01622 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{shchur_neural_2021,
	title = {Neural {Temporal} {Point} {Processes}: {A} {Review}},
	shorttitle = {Neural {Temporal} {Point} {Processes}},
	url = {http://arxiv.org/abs/2104.03528},
	doi = {10.48550/arXiv.2104.03528},
	abstract = {Temporal point processes (TPP) are probabilistic generative models for continuous-time event sequences. Neural TPPs combine the fundamental ideas from point process literature with deep learning approaches, thus enabling construction of flexible and efficient models. The topic of neural TPPs has attracted significant attention in the recent years, leading to the development of numerous new architectures and applications for this class of models. In this review paper we aim to consolidate the existing body of knowledge on neural TPPs. Specifically, we focus on important design choices and general principles for defining neural TPP models. Next, we provide an overview of application areas commonly considered in the literature. We conclude this survey with the list of open challenges and important directions for future work in the field of neural TPPs.},
	urldate = {2024-02-15},
	publisher = {arXiv},
	author = {Shchur, Oleksandr and Türkmen, Ali Caner and Januschowski, Tim and Günnemann, Stephan},
	month = aug,
	year = {2021},
	note = {arXiv:2104.03528 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{westny_stability-informed_2023,
	title = {Stability-{Informed} {Initialization} of {Neural} {Ordinary} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2311.15890},
	doi = {10.48550/arXiv.2311.15890},
	abstract = {This paper addresses the training of Neural Ordinary Differential Equations (neural ODEs), and in particular explores the interplay between numerical integration techniques, stability regions, step size, and initialization techniques. It is shown how the choice of integration technique implicitly regularizes the learned model, and how the solver's corresponding stability region affects training and prediction performance. From this analysis, a stability-informed parameter initialization technique is introduced. The effectiveness of the initialization method is displayed across several learning benchmarks and industrial applications.},
	urldate = {2024-02-06},
	publisher = {arXiv},
	author = {Westny, Theodor and Mohammadi, Arman and Jung, Daniel and Frisk, Erik},
	month = dec,
	year = {2023},
	note = {arXiv:2311.15890 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{gao_generalized_2023,
	title = {Generalized {Bayesian} {Inference} for {Scientific} {Simulators} via {Amortized} {Cost} {Estimation}},
	url = {http://arxiv.org/abs/2305.15208},
	doi = {10.48550/arXiv.2305.15208},
	abstract = {Simulation-based inference (SBI) enables amortized Bayesian inference for simulators with implicit likelihoods. But when we are primarily interested in the quality of predictive simulations, or when the model cannot exactly reproduce the observed data (i.e., is misspecified), targeting the Bayesian posterior may be overly restrictive. Generalized Bayesian Inference (GBI) aims to robustify inference for (misspecified) simulator models, replacing the likelihood-function with a cost function that evaluates the goodness of parameters relative to data. However, GBI methods generally require running multiple simulations to estimate the cost function at each parameter value during inference, making the approach computationally infeasible for even moderately complex simulators. Here, we propose amortized cost estimation (ACE) for GBI to address this challenge: We train a neural network to approximate the cost function, which we define as the expected distance between simulations produced by a parameter and observed data. The trained network can then be used with MCMC to infer GBI posteriors for any observation without running additional simulations. We show that, on several benchmark tasks, ACE accurately predicts cost and provides predictive simulations that are closer to synthetic observations than other SBI methods, especially for misspecified simulators. Finally, we apply ACE to infer parameters of the Hodgkin-Huxley model given real intracellular recordings from the Allen Cell Types Database. ACE identifies better data-matching parameters while being an order of magnitude more simulation-efficient than a standard SBI method. In summary, ACE combines the strengths of SBI methods and GBI to perform robust and simulation-amortized inference for scientific simulators.},
	urldate = {2024-01-31},
	publisher = {arXiv},
	author = {Gao, Richard and Deistler, Michael and Macke, Jakob H.},
	month = nov,
	year = {2023},
	note = {arXiv:2305.15208 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{churchill_flow_2023,
	title = {{FLOW} {MAP} {LEARNING} {FOR} {UNKNOWN} {DYNAMICAL} {SYSTEMS}: {OVERVIEW}, {IMPLEMENTATION}, {AND} {BENCHMARKS}},
	volume = {4},
	issn = {2689-3967, 2689-3975},
	shorttitle = {{FLOW} {MAP} {LEARNING} {FOR} {UNKNOWN} {DYNAMICAL} {SYSTEMS}},
	url = {https://www.dl.begellhouse.com/journals/558048804a15188a,498820861ef102d2,3ff0a7594fae0a96.html},
	doi = {10.1615/JMachLearnModelComput.2023049717},
	abstract = {Flow map learning (FML), in conjunction with deep neural networks (DNNs), has shown promise for data driven modeling of unknown dynamical systems. A remarkable ...},
	language = {English},
	number = {2},
	urldate = {2024-01-30},
	journal = {Journal of Machine Learning for Modeling and Computing},
	author = {Churchill, Victor and Xiu, Dongbin},
	year = {2023},
}

@article{roos_sensitivity_2015,
	title = {Sensitivity {Analysis} for {Bayesian} {Hierarchical} {Models}},
	volume = {10},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-10/issue-2/Sensitivity-Analysis-for-Bayesian-Hierarchical-Models/10.1214/14-BA909.full},
	doi = {10.1214/14-BA909},
	abstract = {Prior sensitivity examination plays an important role in applied Bayesian analyses. This is especially true for Bayesian hierarchical models, where interpretability of the parameters within deeper layers in the hierarchy becomes challenging. In addition, lack of information together with identifiability issues may imply that the prior distributions for such models have an undesired influence on the posterior inference. Despite its importance, informal approaches to prior sensitivity analysis are currently used. They require repetitive re-fits of the model with ad-hoc modified base prior parameter values. Other formal approaches to prior sensitivity analysis suffer from a lack of popularity in practice, mainly due to their high computational cost and absence of software implementation. We propose a novel formal approach to prior sensitivity analysis, which is fast and accurate. It quantifies sensitivity without the need for a model re-fit. Through a series of examples we show how our approach can be used to detect high prior sensitivities of some parameters as well as identifiability issues in possibly over-parametrized Bayesian hierarchical models.},
	number = {2},
	urldate = {2024-01-26},
	journal = {Bayesian Analysis},
	author = {Roos, Małgorzata and Martins, Thiago G. and Held, Leonhard and Rue, Håvard},
	month = jun,
	year = {2015},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {Base prior, Bayesian hierarchical models, Bayesian robustness, Calibration, Hellinger distance, Identifiability, formal local sensitivity measure, overparametrisation},
	pages = {321--349},
}

@article{thomas_likelihood-free_2022,
	title = {Likelihood-{Free} {Inference} by {Ratio} {Estimation}},
	volume = {17},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-17/issue-1/Likelihood-Free-Inference-by-Ratio-Estimation/10.1214/20-BA1238.full},
	doi = {10.1214/20-BA1238},
	abstract = {We consider the problem of parametric statistical inference when likelihood computations are prohibitively expensive but sampling from the model is possible. Several so-called likelihood-free methods have been developed to perform inference in the absence of a likelihood function. The popular synthetic likelihood approach infers the parameters by modelling summary statistics of the data by a Gaussian probability distribution. In another popular approach called approximate Bayesian computation, the inference is performed by identifying parameter values for which the summary statistics of the simulated data are close to those of the observed data. Synthetic likelihood is easier to use as no measure of “closeness” is required but the Gaussianity assumption is often limiting. Moreover, both approaches require judiciously chosen summary statistics. We here present an alternative inference approach that is as easy to use as synthetic likelihood but not as restricted in its assumptions, and that, in a natural way, enables automatic selection of relevant summary statistic from a large set of candidates. The basic idea is to frame the problem of estimating the posterior as a problem of estimating the ratio between the data generating distribution and the marginal distribution. This problem can be solved by logistic regression, and including regularising penalty terms enables automatic selection of the summary statistics relevant to the inference task. We illustrate the general theory on canonical examples and employ it to perform inference for challenging stochastic nonlinear dynamical systems and high-dimensional summary statistics.},
	number = {1},
	urldate = {2024-01-25},
	journal = {Bayesian Analysis},
	author = {Thomas, Owen and Dutta, Ritabrata and Corander, Jukka and Kaski, Samuel and Gutmann, Michael U.},
	month = mar,
	year = {2022},
	keywords = {Approximate Bayesian Computation, density-ratio estimation, likelihood-free inference, logistic regression, probabilistic classification, stochastic dynamical systems, summary statistics selection, synthetic likelihood},
	pages = {1--31},
}

@article{papapicco_neural_2022,
	title = {The {Neural} {Network} shifted-proper orthogonal decomposition: {A} machine learning approach for non-linear reduction of hyperbolic equations},
	volume = {392},
	issn = {0045-7825},
	shorttitle = {The {Neural} {Network} shifted-proper orthogonal decomposition},
	url = {https://www.sciencedirect.com/science/article/pii/S004578252200069X},
	doi = {10.1016/j.cma.2022.114687},
	abstract = {Models with dominant advection always posed a difficult challenge for projection-based reduced order modelling. Many methodologies that have recently been proposed are based on the pre-processing of the full-order solutions to accelerate the Kolmogorov N−width decay thereby obtaining smaller linear subspaces with improved accuracy. These methods however must rely on the knowledge of the characteristic speeds in phase space of the solution, limiting their range of applicability to problems with explicit functional form for the advection field. In this work we approach the problem of automatically detecting the correct pre-processing transformation in a statistical learning framework by implementing a deep-learning architecture. The purely data-driven method allowed us to generalise the existing approaches of linear subspace manipulation to non-linear hyperbolic problems with unknown advection fields. The proposed algorithm has been validated against simple test cases to benchmark its performances and later successfully applied to a multiphase simulation.},
	urldate = {2023-12-25},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Papapicco, Davide and Demo, Nicola and Girfoglio, Michele and Stabile, Giovanni and Rozza, Gianluigi},
	month = mar,
	year = {2022},
	keywords = {Deep Neural Networks (DNNs), Multiphase simulation, Non-linear hyperbolic equations, Reduced order modelling (ROM), Shifted-POD (sPOD)},
	pages = {114687},
}

@misc{hess_data-driven_2022,
	title = {Data-{Driven} {Enhanced} {Model} {Reduction} for {Bifurcating} {Models} in {Computational} {Fluid} {Dynamics}},
	url = {http://arxiv.org/abs/2202.09250},
	doi = {10.48550/arXiv.2202.09250},
	abstract = {We investigate various data-driven methods to enhance projection-based model reduction techniques with the aim of capturing bifurcating solutions. To show the effectiveness of the data-driven enhancements, we focus on the incompressible Navier-Stokes equations and different types of bifurcations. To recover solutions past a Hopf bifurcation, we propose an approach that combines proper orthogonal decomposition with Hankel dynamic mode decomposition. To approximate solutions close to a pitchfork bifurcation, we combine localized reduced models with artificial neural networks. Several numerical examples are shown to demonstrate the feasibility of the presented approaches.},
	urldate = {2023-12-25},
	publisher = {arXiv},
	author = {Hess, Martin W. and Quaini, Annalisa and Rozza, Gianluigi},
	month = jul,
	year = {2022},
	note = {arXiv:2202.09250 [cs, math]},
	keywords = {Mathematics - Numerical Analysis},
}

@article{gilpin_model_2023,
	title = {Model scale versus domain knowledge in statistical forecasting of chaotic systems},
	volume = {5},
	url = {https://link.aps.org/doi/10.1103/PhysRevResearch.5.043252},
	doi = {10.1103/PhysRevResearch.5.043252},
	abstract = {Chaos and unpredictability are traditionally synonymous, yet large-scale machine-learning methods recently have demonstrated a surprising ability to forecast chaotic systems well beyond typical predictability horizons. However, recent works disagree on whether specialized methods grounded in dynamical systems theory, such as reservoir computers or neural ordinary differential equations, outperform general-purpose large-scale learning methods such as transformers or recurrent neural networks. These prior studies perform comparisons on few individually chosen chaotic systems, thereby precluding robust quantification of how statistical modeling choices and dynamical invariants of different chaotic systems jointly determine empirical predictability. Here, we perform the largest to-date comparative study of forecasting methods on the classical problem of forecasting chaos: we benchmark 24 state-of-the-art forecasting methods on a crowdsourced database of 135 low-dimensional systems with 17 forecast metrics. We find that large-scale, domain-agnostic forecasting methods consistently produce predictions that remain accurate up to two dozen Lyapunov times, thereby accessing a long-horizon forecasting regime well beyond classical methods. We find that, in this regime, accuracy decorrelates with classical invariant measures of predictability like the Lyapunov exponent. However, in data-limited settings outside the long-horizon regime, we find that physics-based hybrid methods retain a comparative advantage due to their strong inductive biases.},
	number = {4},
	urldate = {2023-12-22},
	journal = {Physical Review Research},
	author = {Gilpin, William},
	month = dec,
	year = {2023},
	pages = {043252},
}

@misc{salih_burgers_2016,
	title = {Burgers' {Equation}},
	url = {https://www.iist.ac.in/sites/default/files/people/IN08026/Burgers_equation_viscous.pdf},
	urldate = {2023-12-18},
	publisher = {Department of Aerospace Engineering, Indian Institute of Space Science and Technology},
	author = {Salih, A.},
	month = feb,
	year = {2016},
}

@article{gao_analytical_2017,
	title = {An analytical solution for two and three dimensional nonlinear {Burgers}' equation},
	volume = {45},
	issn = {0307-904X},
	url = {https://www.sciencedirect.com/science/article/pii/S0307904X16306710},
	doi = {10.1016/j.apm.2016.12.018},
	abstract = {This paper derives analytical solutions for the two dimensional and the three dimensional Burgers' equation. The two-dimensional and three-dimensional Burgers' equation are defined in a square and a cubic space domain, respectively, and a particular set of boundary and initial conditions is considered. The analytical solution for the two dimensional Burgers' equation is given by the quotient of two infinite series which involve Bessel, exponential, and trigonometric functions. The analytical solution for the three dimensional Burgers' equation is given by the quotient of two infinite series which involve hypergeometric, exponential, trigonometric and power functions. For both cases, the solutions can describe shock wave phenomena for large Reynolds numbers (Re ≥ 100), which is useful for testing numerical methods.},
	urldate = {2023-12-18},
	journal = {Applied Mathematical Modelling},
	author = {Gao, Q. and Zou, M. Y.},
	month = may,
	year = {2017},
	keywords = {Analytical solution, Burgers’ equation, Hopf–Cole transformation, Shock wave},
	pages = {255--270},
}

@article{lang_query_1992,
	title = {Query learning can work poorly when a human oracle is used},
	url = {https://www.academia.edu/6168656/Query_learning_can_work_poorly_when_a_human_oracle_is_used},
	abstract = {Query learning can work poorly when a human oracle is used},
	language = {en},
	urldate = {2023-12-15},
	author = {Lang, Kevin and Baum, Eric},
	month = jan,
	year = {1992},
}

@book{settles_active_2012,
	address = {Cham},
	series = {Synthesis {Lectures} on {Artificial} {Intelligence} and {Machine} {Learning}},
	title = {Active {Learning}},
	isbn = {9783031004322 9783031015601},
	url = {https://link.springer.com/10.1007/978-3-031-01560-1},
	language = {en},
	urldate = {2023-12-07},
	publisher = {Springer International Publishing},
	author = {Settles, Burr},
	year = {2012},
	doi = {10.1007/978-3-031-01560-1},
}

@book{noauthor_statistical_2009,
	edition = {1},
	title = {Statistical {Tolerance} {Regions}: {Theory}, {Applications}, and {Computation}},
	shorttitle = {Statistical {Tolerance} {Regions}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9780470473900},
	urldate = {2023-12-04},
	publisher = {John Wiley \& Sons, Ltd},
	year = {2009},
	doi = {10.1002/9780470473900},
}

@article{ngom_fourier_2021,
	title = {Fourier neural networks as function approximators and differential equation solvers},
	volume = {14},
	copyright = {© 2021 UChicago Argonne, LLC. Statistical Analysis and Data Mining published by Wiley Periodicals LLC.},
	issn = {1932-1872},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sam.11531},
	doi = {10.1002/sam.11531},
	abstract = {We present a Fourier neural network (FNN) that can be mapped directly to the Fourier decomposition. The choice of activation and loss function yields results that replicate a Fourier series expansion closely while preserving a straightforward architecture with a single hidden layer. The simplicity of this network architecture facilitates the integration with any other higher-complexity networks, at a data pre- or postprocessing stage. We validate this FNN on naturally periodic smooth functions and on piecewise continuous periodic functions. We showcase the use of this FNN for modeling or solving partial differential equations with periodic boundary conditions. The main advantages of the current approach are the validity of the solution outside the training region, interpretability of the trained model, and simplicity of use.},
	language = {en},
	number = {6},
	urldate = {2023-11-29},
	journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
	author = {Ngom, Marieme and Marin, Oana},
	year = {2021},
	keywords = {Fourier decomposition, differential equations, neural networks},
	pages = {647--661},
}

@article{bolibar_universal_2023,
	title = {Universal differential equations for glacier ice flow modelling},
	volume = {16},
	issn = {1991-959X},
	url = {https://gmd.copernicus.org/articles/16/6671/2023/},
	doi = {10.5194/gmd-16-6671-2023},
	abstract = {Geoscientific models are facing increasing challenges to exploit growing datasets coming from remote sensing. Universal differential equations (UDEs), aided by differentiable programming, provide a new scientific modelling paradigm enabling both complex functional inversions to potentially discover new physical laws and data assimilation from heterogeneous and sparse observations. We demonstrate an application of UDEs as a proof of concept to learn the creep component of ice flow, i.e. a nonlinear diffusivity differential equation, of a glacier evolution model. By combining a mechanistic model based on a two-dimensional shallow-ice approximation partial differential equation with an embedded neural network, i.e. a UDE, we can learn parts of an equation as nonlinear functions that then can be translated into mathematical expressions. We implemented this modelling framework as ODINN.jl, a package in the Julia programming language, providing high performance, source-to-source automatic differentiation (AD) and seamless integration with tools and global datasets from the Open Global Glacier Model in Python. We demonstrate this concept for 17 different glaciers around the world, for which we successfully recover a prescribed artificial law describing ice creep variability by solving ∼ 500 000 ordinary differential equations in parallel. Furthermore, we investigate which are the best tools in the scientific machine learning ecosystem in Julia to differentiate and optimize large nonlinear diffusivity UDEs. This study represents a proof of concept for a new modelling framework aiming at discovering empirical laws for large-scale glacier processes, such as the variability in ice creep and basal sliding for ice flow, and new hybrid surface mass balance models.},
	language = {English},
	number = {22},
	urldate = {2023-11-28},
	journal = {Geoscientific Model Development},
	author = {Bolibar, Jordi and Sapienza, Facundo and Maussion, Fabien and Lguensat, Redouane and Wouters, Bert and Pérez, Fernando},
	month = nov,
	year = {2023},
	pages = {6671--6687},
}

@incollection{macqueen_methods_1967,
	title = {Some methods for classification and analysis of multivariate observations},
	volume = {5.1},
	url = {https://projecteuclid.org/ebooks/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Fifth-Berkeley-Symposium-on-Mathematical-Statistics-and/chapter/Some-methods-for-classification-and-analysis-of-multivariate-observations/bsmsp/1200512992},
	urldate = {2023-11-27},
	booktitle = {Proceedings of the {Fifth} {Berkeley} {Symposium} on {Mathematical} {Statistics} and {Probability}, {Volume} 1: {Statistics}},
	publisher = {University of California Press},
	author = {MacQueen, J.},
	month = jan,
	year = {1967},
	pages = {281--298},
}

@article{bates_distribution-free_2021,
	title = {Distribution-free, {Risk}-controlling {Prediction} {Sets}},
	volume = {68},
	issn = {0004-5411},
	url = {https://dl.acm.org/doi/10.1145/3478535},
	doi = {10.1145/3478535},
	abstract = {While improving prediction accuracy has been the focus of machine learning in recent years, this alone does not suffice for reliable decision-making. Deploying learning systems in consequential settings also requires calibrating and communicating the uncertainty of predictions. To convey instance-wise uncertainty for prediction tasks, we show how to generate set-valued predictions from a black-box predictor that controls the expected loss on future test points at a user-specified level. Our approach provides explicit finite-sample guarantees for any dataset by using a holdout set to calibrate the size of the prediction sets. This framework enables simple, distribution-free, rigorous error control for many tasks, and we demonstrate it in five large-scale machine learning problems: (1) classification problems where some mistakes are more costly than others; (2) multi-label classification, where each observation has multiple associated labels; (3) classification problems where the labels have a hierarchical structure; (4) image segmentation, where we wish to predict a set of pixels containing an object of interest; and (5) protein structure prediction. Last, we discuss extensions to uncertainty quantification for ranking, metric learning, and distributionally robust learning.},
	number = {6},
	urldate = {2023-11-27},
	journal = {Journal of the ACM},
	author = {Bates, Stephen and Angelopoulos, Anastasios and Lei, Lihua and Malik, Jitendra and Jordan, Michael},
	month = sep,
	year = {2021},
	keywords = {Uncertainty quantification, conformal prediction, predictive uncertainty, set-valued prediction},
	pages = {43:1--43:34},
}

@misc{angelopoulos_image--image_2022,
	title = {Image-to-{Image} {Regression} with {Distribution}-{Free} {Uncertainty} {Quantification} and {Applications} in {Imaging}},
	url = {http://arxiv.org/abs/2202.05265},
	doi = {10.48550/arXiv.2202.05265},
	abstract = {Image-to-image regression is an important learning task, used frequently in biological imaging. Current algorithms, however, do not generally offer statistical guarantees that protect against a model's mistakes and hallucinations. To address this, we develop uncertainty quantification techniques with rigorous statistical guarantees for image-to-image regression problems. In particular, we show how to derive uncertainty intervals around each pixel that are guaranteed to contain the true value with a user-specified confidence probability. Our methods work in conjunction with any base machine learning model, such as a neural network, and endow it with formal mathematical guarantees -- regardless of the true unknown data distribution or choice of model. Furthermore, they are simple to implement and computationally inexpensive. We evaluate our procedure on three image-to-image regression tasks: quantitative phase microscopy, accelerated magnetic resonance imaging, and super-resolution transmission electron microscopy of a Drosophila melanogaster brain.},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Angelopoulos, Anastasios N. and Kohli, Amit P. and Bates, Stephen and Jordan, Michael I. and Malik, Jitendra and Alshaabi, Thayer and Upadhyayula, Srigokul and Romano, Yaniv},
	month = feb,
	year = {2022},
	note = {arXiv:2202.05265 [cs, eess, q-bio, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Quantitative Biology - Quantitative Methods, Statistics - Machine Learning},
}

@misc{takamoto_pdebench_2023,
	title = {{PDEBENCH}: {An} {Extensive} {Benchmark} for {Scientific} {Machine} {Learning}},
	shorttitle = {{PDEBENCH}},
	url = {http://arxiv.org/abs/2210.07182},
	doi = {10.48550/arXiv.2210.07182},
	abstract = {Machine learning-based modeling of physical systems has experienced increased interest in recent years. Despite some impressive progress, there is still a lack of benchmarks for Scientific ML that are easy to use but still challenging and representative of a wide range of problems. We introduce PDEBench, a benchmark suite of time-dependent simulation tasks based on Partial Differential Equations (PDEs). PDEBench comprises both code and data to benchmark the performance of novel machine learning models against both classical numerical simulations and machine learning baselines. Our proposed set of benchmark problems contribute the following unique features: (1) A much wider range of PDEs compared to existing benchmarks, ranging from relatively common examples to more realistic and difficult problems; (2) much larger ready-to-use datasets compared to prior work, comprising multiple simulation runs across a larger number of initial and boundary conditions and PDE parameters; (3) more extensible source codes with user-friendly APIs for data generation and baseline results with popular machine learning models (FNO, U-Net, PINN, Gradient-Based Inverse Method). PDEBench allows researchers to extend the benchmark freely for their own purposes using a standardized API and to compare the performance of new models to existing baseline methods. We also propose new evaluation metrics with the aim to provide a more holistic understanding of learning methods in the context of Scientific ML. With those metrics we identify tasks which are challenging for recent ML methods and propose these tasks as future challenges for the community. The code is available at https://github.com/pdebench/PDEBench.},
	urldate = {2023-11-23},
	publisher = {arXiv},
	author = {Takamoto, Makoto and Praditia, Timothy and Leiteritz, Raphael and MacKinlay, Dan and Alesiani, Francesco and Pflüger, Dirk and Niepert, Mathias},
	month = mar,
	year = {2023},
	note = {arXiv:2210.07182 [physics]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Physics - Fluid Dynamics, Physics - Geophysics},
}

@incollection{kolen_gradient_2009,
	title = {Gradient {Flow} in {Recurrent} {Nets}: {The} {Difficulty} of {Learning} {LongTerm} {Dependencies}},
	isbn = {9780470544037},
	shorttitle = {Gradient {Flow} in {Recurrent} {Nets}},
	url = {https://ieeexplore.ieee.org/document/5264952},
	language = {en},
	urldate = {2023-11-17},
	booktitle = {A {Field} {Guide} to {Dynamical} {Recurrent} {Networks}},
	publisher = {IEEE},
	collaborator = {Kolen, John F. and Kremer, Stefan C.},
	year = {2009},
	doi = {10.1109/9780470544037.ch14},
}

@misc{glusenkamp_unifying_2023,
	title = {Unifying supervised learning and {VAEs} -- coverage, systematics and goodness-of-fit in normalizing-flow based neural network models for astro-particle reconstructions},
	url = {http://arxiv.org/abs/2008.05825},
	doi = {10.48550/arXiv.2008.05825},
	abstract = {Neural-network based predictions of event properties in astro-particle physics are getting more and more common. However, in many cases the result is just utilized as a point prediction. Statistical uncertainties and coverage (1), systematic uncertainties (2) or a goodness-of-fit measure (3) are often not calculated. Here we describe a certain choice of training and network architecture that allows to incorporate all these properties into a single network model. We show that a KL-divergence objective of the joint distribution of data and labels allows to unify supervised learning and variational autoencoders (VAEs) under one umbrella of stochastic variational inference. The unification motivates an extended supervised learning scheme which allows to calculate a goodness-of-fit p-value for the neural network model. Conditional normalizing flows amortized with a neural network are crucial in this construction. We discuss how they allow to rigorously define coverage for posteriors defined jointly on a product space, e.g. \${\textbackslash}mathbb\{R\}{\textasciicircum}n {\textbackslash}times {\textbackslash}mathcal\{S\}{\textasciicircum}m\$, which encompasses posteriors over directions. Finally, systematic uncertainties are naturally included in the variational viewpoint. The proposed extended supervised training with amortized normalizing flows incorporates (1) coverage calculation, (2) systematics and (3) a goodness-of-fit measure in a single machine-learning model. There are no constraints on the shape of the involved distributions (e.g. Gaussianity) for these properties to hold, in fact it works with complex multi-modal distributions defined on product spaces like \${\textbackslash}mathbb\{R\}{\textasciicircum}n {\textbackslash}times {\textbackslash}mathcal\{S\}{\textasciicircum}m\$. We see great potential for exploiting this per-event information in event selections or for fast astronomical alerts which require uncertainty guarantees.},
	urldate = {2023-11-17},
	publisher = {arXiv},
	author = {Glüsenkamp, Thorsten},
	month = oct,
	year = {2023},
	note = {arXiv:2008.05825 [astro-ph, physics:hep-ex, stat]},
	keywords = {Astrophysics - High Energy Astrophysical Phenomena, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, High Energy Physics - Experiment, Statistics - Machine Learning},
}

@article{lam_learning_2023,
	title = {Learning skillful medium-range global weather forecasting},
	volume = {0},
	url = {https://www.science.org/doi/10.1126/science.adi2336},
	doi = {10.1126/science.adi2336},
	abstract = {Global medium-range weather forecasting is critical to decision-making across many social and economic domains. Traditional numerical weather prediction uses increased compute resources to improve forecast accuracy, but does not directly use historical weather data to improve the underlying model. Here, we introduce “GraphCast,” a machine learning-based method trained directly from reanalysis data. It predicts hundreds of weather variables, over 10 days at 0.25° resolution globally, in under one minute. GraphCast significantly outperforms the most accurate operational deterministic systems on 90\% of 1380 verification targets, and its forecasts support better severe event prediction, including tropical cyclones tracking, atmospheric rivers, and extreme temperatures. GraphCast is a key advance in accurate and efficient weather forecasting, and helps realize the promise of machine learning for modeling complex dynamical systems.},
	number = {0},
	urldate = {2023-11-16},
	journal = {Science},
	author = {Lam, Remi and Sanchez-Gonzalez, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Alet, Ferran and Ravuri, Suman and Ewalds, Timo and Eaton-Rosen, Zach and Hu, Weihua and Merose, Alexander and Hoyer, Stephan and Holland, George and Vinyals, Oriol and Stott, Jacklynn and Pritzel, Alexander and Mohamed, Shakir and Battaglia, Peter},
	month = nov,
	year = {2023},
	pages = {eadi2336},
}

@inproceedings{lakshminarayanan_simple_2017,
	title = {Simple and {Scalable} {Predictive} {Uncertainty} {Estimation} using {Deep} {Ensembles}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@inproceedings{pearce_high-quality_2018,
	title = {High-{Quality} {Prediction} {Intervals} for {Deep} {Learning}: {A} {Distribution}-{Free}, {Ensembled} {Approach}},
	shorttitle = {High-{Quality} {Prediction} {Intervals} for {Deep} {Learning}},
	url = {https://proceedings.mlr.press/v80/pearce18a.html},
	abstract = {This paper considers the generation of prediction intervals (PIs) by neural networks for quantifying uncertainty in regression tasks. It is axiomatic that high-quality PIs should be as narrow as possible, whilst capturing a specified portion of data. We derive a loss function directly from this axiom that requires no distributional assumption. We show how its form derives from a likelihood principle, that it can be used with gradient descent, and that model uncertainty is accounted for in ensembled form. Benchmark experiments show the method outperforms current state-of-the-art uncertainty quantification methods, reducing average PI width by over 10\%.},
	language = {en},
	urldate = {2023-11-15},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Pearce, Tim and Brintrup, Alexandra and Zaki, Mohamed and Neely, Andy},
	month = jul,
	year = {2018},
	pages = {4075--4084},
}

@misc{sohl-dickstein_deep_2015,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	url = {http://arxiv.org/abs/1503.03585},
	doi = {10.48550/arXiv.1503.03585},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
	month = nov,
	year = {2015},
	note = {arXiv:1503.03585 [cond-mat, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{Jia2019,
	title = {Neural jump stochastic differential equations},
	volume = {32},
	issn = {10495258},
	abstract = {Many time series are effectively generated by a combination of deterministic continuous flows along with discrete jumps sparked by stochastic events. However, we usually do not have the equation of motion describing the flows, or how they are affected by jumps. To this end, we introduce Neural Jump Stochastic Differential Equations that provide a data-driven approach to learn continuous and discrete dynamic behavior, i.e., hybrid systems that both flow and jump. Our approach extends the framework of Neural Ordinary Differential Equations with a stochastic process term that models discrete events. We then model temporal point processes with a piecewise-continuous latent trajectory, where the discontinuities are caused by stochastic events whose conditional intensity depends on the latent state. We demonstrate the predictive capabilities of our model on a range of synthetic and real-world marked point process datasets, including classical point processes (such as Hawkes processes), awards on Stack Overflow, medical records, and earthquake monitoring.},
	number = {2018},
	journal = {Advances in Neural Information Processing Systems},
	author = {Jia, Junteng and Benson, Austin R.},
	year = {2019},
	note = {arXiv: 1905.10403},
	keywords = {★},
}

@article{reinhart_review_2018,
	title = {A {Review} of {Self}-{Exciting} {Spatio}-{Temporal} {Point} {Processes} and {Their} {Applications}},
	volume = {33},
	issn = {0883-4237},
	url = {https://projecteuclid.org/journals/statistical-science/volume-33/issue-3/A-Review-of-Self-Exciting-Spatio-Temporal-Point-Processes-and/10.1214/17-STS629.full},
	doi = {10.1214/17-STS629},
	abstract = {Self-exciting spatio-temporal point process models predict the rate of events as a function of space, time, and the previous history of events. These models naturally capture triggering and clustering behavior, and have been widely used in fields where spatio-temporal clustering of events is observed, such as earthquake modeling, infectious disease, and crime. In the past several decades, advances have been made in estimation, inference, simulation, and diagnostic tools for self-exciting point process models. In this review, I describe the basic theory, survey related estimation and inference techniques from each field, highlight several key applications, and suggest directions for future research.},
	number = {3},
	urldate = {2023-09-08},
	journal = {https://doi.org/10.1214/17-STS629},
	author = {Reinhart, Alex},
	month = aug,
	year = {2018},
	note = {arXiv: 1708.02647
Publisher: Institute of Mathematical Statistics},
	keywords = {Conditional intensity, Hawkes process, epidemic-type aftershock sequence, stochastic declustering},
	pages = {299--318},
}

@article{Lee2021,
	title = {Parameterized neural ordinary differential equations: {Applications} to computational physics problems},
	volume = {477},
	issn = {14712946},
	doi = {10.1098/rspa.2021.0162},
	abstract = {This work proposes an extension of neural ordinary differential equations (NODEs) by introducing an additional set of ODE input parameters to NODEs. This extension allows NODEs to learn multiple dynamics specified by the input parameter instances. Our extension is inspired by the concept of parameterized ODEs, which are widely investigated in computational science and engineering contexts, where characteristics of the governing equations vary over the input parameters. We apply the proposed parameterized NODEs (PNODEs) for learning latent dynamics of complex dynamical processes that arise in computational physics, which is an essential component for enabling rapid numerical simulations for time-critical physics applications. For this, we propose an encoder-decoder-type framework, which models latent dynamics as PNODEs. We demonstrate the effectiveness of PNODEs on benchmark problems from computational physics.},
	number = {2253},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Lee, Kookjin and Parish, Eric J.},
	year = {2021},
	note = {arXiv: 2010.14685},
	keywords = {autoencoders, deep learning, latent-dynamics learning, model reduction, neural ordinary differential equations, nonlinear manifolds, ★},
}

@article{grathwohl_ffjord_2018,
	title = {{FFJORD}: {Free}-form {Continuous} {Dynamics} for {Scalable} {Reversible} {Generative} {Models}},
	url = {https://arxiv.org/abs/1810.01367v3},
	abstract = {A promising class of generative models maps points from a simple distribution
to a complex distribution through an invertible neural network.
Likelihood-based training of these models requires restricting their
architectures to allow cheap computation of Jacobian determinants.
Alternatively, the Jacobian trace can be used if the transformation is
specified by an ordinary differential equation. In this paper, we use
Hutchinson's trace estimator to give a scalable unbiased estimate of the
log-density. The result is a continuous-time invertible generative model with
unbiased density estimation and one-pass sampling, while allowing unrestricted
neural network architectures. We demonstrate our approach on high-dimensional
density estimation, image generation, and variational inference, achieving the
state-of-the-art among exact likelihood methods with efficient sampling.},
	urldate = {2023-08-11},
	journal = {7th International Conference on Learning Representations, ICLR 2019},
	author = {Grathwohl, Will and Chen, Ricky T.Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.01367
Publisher: International Conference on Learning Representations, ICLR},
}

@article{finlay_how_2020,
	title = {How to {Train} {Your} {Neural} {ODE}: the {World} of {Jacobian} and {Kinetic} {Regularization}},
	url = {https://dl.acm.org/doi/10.5555/3524938.3525234},
	doi = {10.5555/3524938.3525234},
	abstract = {Training neural ODEs on large datasets has not been tractable due to the necessity of allowing the adaptive numerical ODE solver to refine its step size to very small values. In practice this leads to dynamics equivalent to many hundreds or even thousands of layers. In this paper, we overcome this apparent difficulty by introducing a theoretically-grounded combination of both optimal transport and stability regularizations which encourage neural ODEs to prefer simpler dynamics out of all the dynamics that solve a problem well. Simpler dynamics lead to faster convergence and to fewer discretizations of the solver, considerably decreasing wall-clock time without loss in performance. Our approach allows us to train neural ODE-based generative models to the same performance as the unregularized dynamics, with significant reductions in training time. This brings neural ODEs closer to practical relevance in large-scale applications.},
	urldate = {2023-08-05},
	author = {Finlay, Chris and Jacobsen, Jörn-Henrik and Nurbekyan, Levon and Oberman, Adam M},
	year = {2020},
}

@article{scolini_observation-based_2019,
	title = {Observation-based modelling of magnetised coronal mass ejections with {EUHFORIA}},
	volume = {626},
	issn = {14320746},
	doi = {10.1051/0004-6361/201935053},
	abstract = {Context. Coronal mass ejections (CMEs) are the primary source of strong space weather disturbances at Earth. Their geo-effectiveness is largely determined by their dynamic pressure and internal magnetic fields, for which reliable predictions at Earth are not possible with traditional cone CME models. Aims. We study two well-observed Earth-directed CMEs using the EUropean Heliospheric FORecasting Information Asset (EUHFORIA) model, testing for the first time the predictive capabilities of a linear force-free spheromak CME model initialised using parameters derived from remote-sensing observations. Methods. Using observation-based CME input parameters, we performed magnetohydrodynamic simulations of the events with EUHFORIA, using the cone and spheromak CME models. Results. Simulations show that spheromak CMEs propagate faster than cone CMEs when initialised with the same kinematic parameters. We interpret these differences as the result of different Lorentz forces acting within cone and spheromak CMEs, which lead to different CME expansions in the heliosphere. Such discrepancies can be mitigated by initialising spheromak CMEs with a reduced speed corresponding to the radial speed only. Results at Earth provide evidence that the spheromak model improves the predictions of B (Bz) by up to 12-60 (22-40) percentage points compared to a cone model. Considering virtual spacecraft located within ±10° around Earth, B (Bz) predictions reach 45-70\% (58-78\%) of the observed peak values. The spheromak model shows inaccurate predictions of the magnetic field parameters at Earth for CMEs propagating away from the Sun-Earth line. Conclusions. The spheromak model successfully predicts the CME properties and arrival time in the case of strictly Earth-directed events, while modelling CMEs propagating away from the Sun-Earth line requires extra care due to limitations related to the assumed spherical shape. The spatial variability of modelling results and the typical uncertainties in the reconstructed CME direction advocate the need to consider predictions at Earth and at virtual spacecraft located around it.},
	urldate = {2023-06-27},
	journal = {Astronomy and Astrophysics},
	author = {Scolini, C. and Rodriguez, L. and Mierla, M. and Pomoell, J. and Poedts, S.},
	month = jun,
	year = {2019},
	note = {arXiv: 1904.07059
Publisher: EDP Sciences},
	keywords = {Magnetohydrodynamics (MHD), Solar wind, Solar-Terrestrial relations, Sun: coronal mass ejections (CMEs), Sun: heliosphere, Sun: magnetic fields},
}

@article{kidger_neural_2022,
	title = {On {Neural} {Differential} {Equations}},
	url = {https://arxiv.org/abs/2202.02435v1},
	abstract = {The conjoining of dynamical systems and deep learning has become a topic of
great interest. In particular, neural differential equations (NDEs) demonstrate
that neural networks and differential equation are two sides of the same coin.
Traditional parameterised differential equations are a special case. Many
popular neural network architectures, such as residual networks and recurrent
networks, are discretisations. NDEs are suitable for tackling generative problems, dynamical systems, and
time series (particularly in physics, finance, ...) and are thus of interest to
both modern machine learning and traditional mathematical modelling. NDEs offer
high-capacity function approximation, strong priors on model space, the ability
to handle irregular data, memory efficiency, and a wealth of available theory
on both sides. This doctoral thesis provides an in-depth survey of the field. Topics include: neural ordinary differential equations (e.g. for hybrid
neural/mechanistic modelling of physical systems); neural controlled
differential equations (e.g. for learning functions of irregular time series);
and neural stochastic differential equations (e.g. to produce generative models
capable of representing complex stochastic dynamics, or sampling from complex
high-dimensional distributions). Further topics include: numerical methods for NDEs (e.g. reversible
differential equations solvers, backpropagation through differential equations,
Brownian reconstruction); symbolic regression for dynamical systems (e.g. via
regularised evolution); and deep implicit models (e.g. deep equilibrium models,
differentiable optimisation). We anticipate this thesis will be of interest to anyone interested in the
marriage of deep learning with dynamical systems, and hope it will provide a
useful reference for the current state of the art.},
	urldate = {2023-06-25},
	author = {Kidger, Patrick},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.02435},
}

@article{gabriel_stpp_2013,
	title = {stpp: {An} {R} {Package} for {Plotting}, {Simulating} and {Analyzing} {Spatio}-{Temporal} {Point} {Patterns}},
	volume = {53},
	issn = {1548-7660},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v053i02},
	doi = {10.18637/JSS.V053.I02},
	abstract = {stpp is an R package for analyzing, simulating and displaying space-time point patterns. It covers many of the models encountered in applications of point process methods to the study of spatio-temporal phenomena. The package also includes estimators of the space-time inhomogeneous K-function and pair correlation function. stpp is the first dedicated unified computational environment in the area of spatio-temporal point processes. In this paper we describe space-time point processes and introduce the package stpp to new users.},
	number = {2},
	urldate = {2023-06-19},
	journal = {Journal of Statistical Software},
	author = {Gabriel, Edith and Rowlingson, Barry and Diggle, Peter J.},
	month = apr,
	year = {2013},
	note = {Publisher: American Statistical Association},
	keywords = {Epidemiology, Inhomogeneous point patterns, Space-time point processes, Spatial statistics},
	pages = {1--29},
}

@article{barnard_sir-huxtparticle_2023,
	title = {{SIR}-{HUXt}—{A} {Particle} {Filter} {Data} {Assimilation} {Scheme} for {CME} {Time}-{Elongation} {Profiles}},
	volume = {21},
	issn = {1542-7390},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1029/2023SW003487},
	doi = {10.1029/2023SW003487},
	abstract = {2 of 16 numerical models. The objective of DA is to combine information from simulations and observations to provide an optimal estimate of the state of a dynamical system. Heliospheric DA is still a relatively new research topic, but progress is beginning to be made. Lang et al. (2017) explored how the Local Ensemble Transform Kalman filter could be used to assimilate in situ observations of solar wind plasma properties into the ENLIL magnetohydrodynamic (MHD) solar wind model, which demonstrated clear improvements in the representivity of the ENLIL simulations. The Burger Radial Variational Data Assimilation (BRaVDA) scheme was developed in Lang and Owens (2019), in which a variational DA scheme was coupled to the hydrodynamic (HD) HUX solar wind model (Riley \& Lionello, 2011), for the assimilation of observations of the solar wind speed. Experiments with synthetic observations and solar wind speed observations from the STEREO spacecraft showed that BRaVDA reduced the errors in the solar wind speed predictions at Earth. This work was extended by Lang et al. (2021) to the HUXt model, a HD solar wind model with explicit time-dependence (Barnard \& Owens, 2022; M. J. Owens et al., 2020), in which it was shown that over the period 2007-2014, BRaVDA returned a 31\% reduction of the root mean square error in hindcasts of the solar wind speed at Earth. These works have so far focused on the assimilation of in situ observations of solar wind plasma properties, but progress has also been made on the assimilation of remote sensing observations, such as those provided by HIs (Eyles et al., 2008; Howard et al., 2008) and interplanetary scintillation (IPS) (Fallows et al., 2022). For example, Barnard et al. (2020) showed that an ensemble of solar-wind-CME simulations with the HUXt model could be weighted by the time-elongation profiles of CMEs derived from the STEREO HI data. This weighting prioritized ensemble members that more closely matched the observed time-elongation profile, and led to up to 20\% improvements in hindcasts of the CMEs arrival time at Earth. Similarly, Iwai et al. (2021) demonstrated how assimilating IPS observations of 12 halo CMEs into the SUSANOO-CME MHD model led to improvements in the predicted Earth arrival times of these CMEs. Although Barnard et al. (2020) demonstrated that HI data contains useful information on CMEs that can be used to constrain the HUXt solar wind simulations, they did not use formal DA methods. In this work, we present the development of SIR-HUXt, which couples a sequential importance resampling (SIR) particle filter DA scheme with the HUXt solar wind model. SIR-HUXt is constructed to assimilate time-elongation profiles of a CMEs flank, such as those typically extracted from the STEREO-HI data (Barnard et al., 2015, 2017; Davies et al., 2009). This is an important milestone toward the development of DA schemes that can directly assimilate the HI intensity data into solar wind numerical models. We present a first test of SIR-HUXt by using Observing System Simulation Experiments (OSSEs) to investigate the performance of SIR-HUXt for a simple synthetic CME scenario and for a range of observer locations relative to Earth. This article proceeds with Section 2 describing the models and methods we use, including the HUXt numerical model, the background to the SIR algorithm, and on OSSEs. Section 3 presents the results of the OSSEs, and our conclusions are presented in Section 4. 2. Methods and Data 2.1. HUXt HUXt is an open source numerical model of the solar wind, developed in Python (Barnard \& Owens, 2022; M. J. Owens et al., 2020). It is a 1D radial model that uses a reduced-physics approach to produce solar wind simulations that emulate the solar wind flows produced by 3-D MHD models, but at a small fraction of the computational cost. The motivation for developing HUXt is that the models simplicity and computational expense permits the development of certain experiments and techniques that would typically be too expensive with 3-D MHD models. For example, the particle filter DA experiments in this study require ≈10 6 5-day simulations of the inner heliosphere, which is currently an impractical demand of 3-D MHD solar wind models with widely available computing resources. Being based on incompressible hydrodynamics, HUXt solves only for the solar wind flow speed. Consequently, the only boundary condition required is the flow speed at the inner boundary. These boundary conditions can be computed from a wide range of coronal models, including but not limited to; potential field source surface based},
	number = {6},
	urldate = {2023-06-11},
	journal = {Space Weather},
	author = {Barnard, Luke and Owens, Mathew and Scott, Chris and Lang, Matthew and Lockwood, Mike},
	month = jun,
	year = {2023},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {CME forecasting, HUXt, data assimilation, heliospheric imaging},
	pages = {e2023SW003487},
}

@article{Barnard2023,
	title = {{SIR}-{HUXt}—{A} {Particle} {Filter} {Data} {Assimilation} {Scheme} for {CME} {Time}-{Elongation} {Profiles}},
	volume = {21},
	issn = {1542-7390},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1029/2023SW003487},
	doi = {10.1029/2023SW003487},
	abstract = {2 of 16 numerical models. The objective of DA is to combine information from simulations and observations to provide an optimal estimate of the state of a dynamical system. Heliospheric DA is still a relatively new research topic, but progress is beginning to be made. Lang et al. (2017) explored how the Local Ensemble Transform Kalman filter could be used to assimilate in situ observations of solar wind plasma properties into the ENLIL magnetohydrodynamic (MHD) solar wind model, which demonstrated clear improvements in the representivity of the ENLIL simulations. The Burger Radial Variational Data Assimilation (BRaVDA) scheme was developed in Lang and Owens (2019), in which a variational DA scheme was coupled to the hydrodynamic (HD) HUX solar wind model (Riley \& Lionello, 2011), for the assimilation of observations of the solar wind speed. Experiments with synthetic observations and solar wind speed observations from the STEREO spacecraft showed that BRaVDA reduced the errors in the solar wind speed predictions at Earth. This work was extended by Lang et al. (2021) to the HUXt model, a HD solar wind model with explicit time-dependence (Barnard \& Owens, 2022; M. J. Owens et al., 2020), in which it was shown that over the period 2007-2014, BRaVDA returned a 31\% reduction of the root mean square error in hindcasts of the solar wind speed at Earth. These works have so far focused on the assimilation of in situ observations of solar wind plasma properties, but progress has also been made on the assimilation of remote sensing observations, such as those provided by HIs (Eyles et al., 2008; Howard et al., 2008) and interplanetary scintillation (IPS) (Fallows et al., 2022). For example, Barnard et al. (2020) showed that an ensemble of solar-wind-CME simulations with the HUXt model could be weighted by the time-elongation profiles of CMEs derived from the STEREO HI data. This weighting prioritized ensemble members that more closely matched the observed time-elongation profile, and led to up to 20\% improvements in hindcasts of the CMEs arrival time at Earth. Similarly, Iwai et al. (2021) demonstrated how assimilating IPS observations of 12 halo CMEs into the SUSANOO-CME MHD model led to improvements in the predicted Earth arrival times of these CMEs. Although Barnard et al. (2020) demonstrated that HI data contains useful information on CMEs that can be used to constrain the HUXt solar wind simulations, they did not use formal DA methods. In this work, we present the development of SIR-HUXt, which couples a sequential importance resampling (SIR) particle filter DA scheme with the HUXt solar wind model. SIR-HUXt is constructed to assimilate time-elongation profiles of a CMEs flank, such as those typically extracted from the STEREO-HI data (Barnard et al., 2015, 2017; Davies et al., 2009). This is an important milestone toward the development of DA schemes that can directly assimilate the HI intensity data into solar wind numerical models. We present a first test of SIR-HUXt by using Observing System Simulation Experiments (OSSEs) to investigate the performance of SIR-HUXt for a simple synthetic CME scenario and for a range of observer locations relative to Earth. This article proceeds with Section 2 describing the models and methods we use, including the HUXt numerical model, the background to the SIR algorithm, and on OSSEs. Section 3 presents the results of the OSSEs, and our conclusions are presented in Section 4. 2. Methods and Data 2.1. HUXt HUXt is an open source numerical model of the solar wind, developed in Python (Barnard \& Owens, 2022; M. J. Owens et al., 2020). It is a 1D radial model that uses a reduced-physics approach to produce solar wind simulations that emulate the solar wind flows produced by 3-D MHD models, but at a small fraction of the computational cost. The motivation for developing HUXt is that the models simplicity and computational expense permits the development of certain experiments and techniques that would typically be too expensive with 3-D MHD models. For example, the particle filter DA experiments in this study require ≈10 6 5-day simulations of the inner heliosphere, which is currently an impractical demand of 3-D MHD solar wind models with widely available computing resources. Being based on incompressible hydrodynamics, HUXt solves only for the solar wind flow speed. Consequently, the only boundary condition required is the flow speed at the inner boundary. These boundary conditions can be computed from a wide range of coronal models, including but not limited to; potential field source surface based},
	number = {6},
	urldate = {2023-06-14},
	journal = {Space Weather},
	author = {Barnard, Luke and Owens, Mathew and Scott, Chris and Lang, Matthew and Lockwood, Mike},
	month = jun,
	year = {2023},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {CME forecasting, HUXt, data assimilation, heliospheric imaging},
	pages = {e2023SW003487},
}

@article{reinhart_review_2018-1,
	title = {A {Review} of {Self}-{Exciting} {Spatio}-{Temporal} {Point} {Processes} and {Their} {Applications}},
	volume = {33},
	issn = {0883-4237},
	url = {https://projecteuclid.org/journals/statistical-science/volume-33/issue-3/A-Review-of-Self-Exciting-Spatio-Temporal-Point-Processes-and/10.1214/17-STS629.full},
	doi = {10.1214/17-STS629},
	abstract = {Self-exciting spatio-temporal point process models predict the rate of events as a function of space, time, and the previous history of events. These models naturally capture triggering and clustering behavior, and have been widely used in fields where spatio-temporal clustering of events is observed, such as earthquake modeling, infectious disease, and crime. In the past several decades, advances have been made in estimation, inference, simulation, and diagnostic tools for self-exciting point process models. In this review, I describe the basic theory, survey related estimation and inference techniques from each field, highlight several key applications, and suggest directions for future research.},
	number = {3},
	urldate = {2023-06-15},
	journal = {https://doi.org/10.1214/17-STS629},
	author = {Reinhart, Alex},
	month = aug,
	year = {2018},
	note = {arXiv: 1708.02647
Publisher: Institute of Mathematical Statistics},
	keywords = {Conditional intensity, Hawkes process, epidemic-type aftershock sequence, stochastic declustering},
	pages = {299--318},
}

@article{Camporeale2021,
	title = {Accrue: {Accurate} and reliable uncertainty estimate in deterministic models},
	volume = {11},
	issn = {21525099},
	doi = {10.1615/Int.J.UncertaintyQuantification.2021034623},
	abstract = {In this paper we focus on the problem of assigning uncertainties to single-point predictions generated by a deterministic model that outputs a continuous variable. This problem applies to any state-of-the-art physics or engineering models that have a computational cost that does not readily allow running ensembles and estimating the uncertainty associated to single-point predictions. Essentially, we devise a method to easily transform a deterministic prediction into a probabilistic one. We show that for doing so, one has to compromise between the accuracy and the reliability (calibration) of such a probabilistic model. Hence, we introduce a cost function that encodes their trade-off, and we call this new method ACCRUE (ACCurate and Reliable Uncertainty Estimate). We use the continuous rank probability score to measure accuracy and we derive an analytic formula for the reliability, in the case of forecasts of continuous scalar variables expressed in terms of Gaussian distributions. The new ACCRUE cost function is then used to estimate the input-dependent variance, given a black-box “oracle” mean function, by solving a two-objective optimization problem. The simple philosophy behind this strategy is that predictions based on the estimated variances should not only be accurate, but also reliable (i.e., statistically consistent with observations). Conversely, early works based on the minimization of classical cost functions, such as the negative log probability density, cannot simultaneously enforce both accuracy and reliability. We show several examples both with synthetic data, where the underlying hidden noise can accurately be recovered, and with large real-world datasets.},
	number = {4},
	journal = {International Journal for Uncertainty Quantification},
	author = {Camporeale, Enrico and Carè, Algo},
	year = {2021},
	keywords = {Calibration, Deterministic models, Machine learning},
	pages = {81--94},
}

@article{sauer_active_2022,
	title = {Active {Learning} for {Deep} {Gaussian} {Process} {Surrogates}},
	volume = {65},
	issn = {15372723},
	url = {https://doi.org/10.1080/00401706.2021.2008505},
	doi = {10.1080/00401706.2021.2008505},
	abstract = {Deep Gaussian processes (DGPs) are increasingly popular as predictive models in machine learning for their nonstationary flexibility and ability to cope with abrupt regime changes in training data. Here, we explore DGPs as surrogates for computer simulation experiments whose response surfaces exhibit similar characteristics. In particular, we transport a DGP’s automatic warping of the input space and full uncertainty quantification, via a novel elliptical slice sampling Bayesian posterior inferential scheme, through to active learning strategies that distribute runs nonuniformly in the input space—something an ordinary (stationary) GP could not do. Building up the design sequentially in this way allows smaller training sets, limiting both expensive evaluation of the simulator code and mitigating cubic costs of DGP inference. When training data sizes are kept small through careful acquisition, and with parsimonious layout of latent layers, the framework can be both effective and computationally tractable. Our methods are illustrated on simulation data and two real computer experiments of varying input dimensionality. We provide an open source implementation in the deepgp package on CRAN.},
	number = {1},
	urldate = {2023-05-01},
	journal = {Technometrics},
	author = {Sauer, Annie and Gramacy, Robert B. and Higdon, David},
	year = {2022},
	note = {arXiv: 2012.08015
Publisher: American Statistical Association},
	keywords = {Computer model, Elliptical slice sampling, Emulator, Kriging, Sequential design},
	pages = {4--18},
}

@article{sun_improved_2021,
	title = {Improved and {Interpretable} {Solar} {Flare} {Predictions} {With} {Spatial} and {Topological} {Features} of the {Polarity} {Inversion} {Line} {Masked} {Magnetograms}},
	volume = {19},
	issn = {1542-7390},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1029/2021SW002837},
	doi = {10.1029/2021SW002837},
	abstract = {Many current research efforts undertake the solar flare classification task using the Space-weather HMI Active Region Patch (SHARP) parameters as the predictors. The SHARP parameters are scalar quantities based on spatial average or integration of physical quantities derived from the vector magnetic field, which loses information of the two-dimensional spatial distribution of the field and related quantities. In this paper, we construct two new sets of spatial features to expand the feature set used for the flare classification task. The first set uses the idea of topological data analysis to summarize the geometric information of the distributions of various SHARP quantities across active regions. The second set utilizes tools coming from spatial statistics to analyze the vertical magnetic field component Br and summarize its spatial variations and clustering patterns. All features are constructed within regions near the polarity inversion lines (PILs) and classification performances using the new features are compared against those using SHARP parameters (also along the PIL). We found that using the new features can improve the skill scores of the flare classification model and new features tend to have higher feature importance, especially the spatial statistics features. This potentially suggests that even using a single magnetic field component, Br, instead of all SHARP parameters, one can still derive strongly predictive features for flare classification.},
	number = {12},
	urldate = {2023-04-12},
	journal = {Space Weather},
	author = {Sun, Hu and Manchester, Ward and Chen, Yang},
	month = dec,
	year = {2021},
	note = {Publisher: John Wiley \& Sons, Ltd
ISBN: 10.1029/2021},
	pages = {e2021SW002837},
}

@article{Yuan2022,
	title = {Towards out of distribution generalization for problems in mechanics},
	volume = {400},
	issn = {00457825},
	url = {https://doi.org/10.1016/j.cma.2022.115569},
	doi = {10.1016/j.cma.2022.115569},
	abstract = {There has been a massive increase in research interest towards applying data driven methods to problems in mechanics, with a particular emphasis on using data driven methods for predictive modeling and design of materials with novel functionality. While traditional machine learning (ML) methods have enabled many breakthroughs, they rely on the assumption that the training (observed) data and testing (unseen) data are independent and identically distributed (i.i.d). However, when these standard ML approaches are applied to real world mechanics problems with unknown test environments, they can be very sensitive to data distribution shifts, and can break down when evaluated on test datasets that violate the i.i.d. assumption. In contrast, out-of-distribution (OOD) generalization approaches assume that the data contained in test environments are allowed to shift (i.e., violate the i.i.d. assumption). To date, multiple methods have been proposed to improve the OOD generalization of ML methods. However, most of these OOD generalization methods have been focused on classification problems, driven in part by the lack of benchmark datasets available for OOD regression problems. Thus, the efficiency of these OOD generalization methods on regression problems, which are typically more relevant to mechanics research than classification problems, is unknown. To address this, we perform a fundamental study of OOD generalization methods for regression problems in mechanics. Specifically, we identify three OOD generalization problems: covariate shift, mechanism shift, and sampling bias. For each problem, we create two benchmark examples that extend the Mechanical MNIST dataset collection, and we investigate the performance of popular OOD generalization methods on these mechanics-specific regression problems. Our numerical experiments show that in most cases, while the OOD algorithms perform better compared to traditional ML methods on these OOD generalization problems, there is a compelling need to develop more robust OOD methods that can generalize the notion of invariance across multiple OOD scenarios. Overall, we expect that this study, as well as the associated open access benchmark datasets, will enable further development of OOD methods for mechanics specific regression problems.},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Yuan, Lingxiao and Park, Harold S. and Lejeune, Emma},
	year = {2022},
	note = {arXiv: 2206.14917
Publisher: Elsevier B.V.},
	keywords = {Covariate shift, Machine Learning, Mechanism shift, Out of distribution, Regression, Sampling bias},
	pages = {115569},
}

@article{Vaswani2017,
	title = {Attention {Is} {All} {You} {Need}},
	volume = {2017-Decem},
	issn = {10495258},
	url = {https://arxiv.org/abs/1706.03762v5},
	doi = {10.48550/arxiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks in an encoder-decoder configuration. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer, based
solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to be
superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014
English-to-German translation task, improving over the existing best results,
including ensembles by over 2 BLEU. On the WMT 2014 English-to-French
translation task, our model establishes a new single-model state-of-the-art
BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction
of the training costs of the best models from the literature. We show that the
Transformer generalizes well to other tasks by applying it successfully to
English constituency parsing both with large and limited training data.},
	urldate = {2023-01-10},
	journal = {Advances in Neural Information Processing Systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.03762
Publisher: Neural information processing systems foundation},
	keywords = {★},
	pages = {5999--6009},
}

@book{pgm2009,
	title = {Probabilistic {Graphical} {Models}: {Principles} and {Techniques}},
	isbn = {978-0-262-01319-2},
	author = {Koller, Daphne and Friedman, Nir},
	year = {2009},
}

@article{Guo2022,
	title = {Bayesian operator inference for data-driven reduced-order modeling},
	volume = {402},
	issn = {00457825},
	url = {https://doi.org/10.1016/j.cma.2022.115336},
	doi = {10.1016/j.cma.2022.115336},
	abstract = {This work proposes a Bayesian inference method for the reduced-order modeling of time-dependent systems. Informed by the structure of the governing equations, the task of learning a reduced-order model from data is posed as a Bayesian inverse problem with Gaussian prior and likelihood. The resulting posterior distribution characterizes the operators defining the reduced-order model, hence the predictions subsequently issued by the reduced-order model are endowed with uncertainty. The statistical moments of these predictions are estimated via a Monte Carlo sampling of the posterior distribution. Since the reduced models are fast to solve, this sampling is computationally efficient. Furthermore, the proposed Bayesian framework provides a statistical interpretation of the regularization term that is present in the deterministic operator inference problem, and the empirical Bayes approach of maximum marginal likelihood suggests a selection algorithm for the regularization hyperparameters. The proposed method is demonstrated on two examples: the compressible Euler equations with noise-corrupted observations, and a single-injector combustion process.},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Guo, Mengwu and McQuarrie, Shane A. and Willcox, Karen E.},
	year = {2022},
	note = {arXiv: 2204.10829
Publisher: Elsevier B.V.},
	keywords = {Bayesian inversion, Data-driven reduced-order modeling, Operator inference, Single-injector combustion, Tikhonov regularization, Uncertainty quantification, ★},
	pages = {115336},
}

@article{Towne2018,
	title = {Spectral proper orthogonal decomposition and its relationship to dynamic mode decomposition and resolvent analysis},
	volume = {847},
	issn = {14697645},
	doi = {10.1017/jfm.2018.283},
	abstract = {We consider the frequency domain form of proper orthogonal decomposition (POD), called spectral proper orthogonal decomposition (SPOD). Spectral POD is derived from a space-time POD problem for statistically stationary flows and leads to modes that each oscillate at a single frequency. This form of POD goes back to the original work of Lumley (Stochastic Tools in Turbulence, Academic Press, 1970), but has been overshadowed by a space-only form of POD since the 1990s. We clarify the relationship between these two forms of POD and show that SPOD modes represent structures that evolve coherently in space and time, while space-only POD modes in general do not. We also establish a relationship between SPOD and dynamic mode decomposition (DMD); we show that SPOD modes are in fact optimally averaged DMD modes obtained from an ensemble DMD problem for stationary flows. Accordingly, SPOD modes represent structures that are dynamic in the same sense as DMD modes but also optimally account for the statistical variability of turbulent flows. Finally, we establish a connection between SPOD and resolvent analysis. The key observation is that the resolvent-mode expansion coefficients must be regarded as statistical quantities to ensure convergent approximations of the flow statistics. When the expansion coefficients are uncorrelated, we show that SPOD and resolvent modes are identical. Our theoretical results and the overall utility of SPOD are demonstrated using two example problems: the complex Ginzburg-Landau equation and a turbulent jet.},
	journal = {Journal of Fluid Mechanics},
	author = {Towne, Aaron and Schmidt, Oliver T. and Colonius, Tim},
	year = {2018},
	note = {arXiv: 1708.04393},
	keywords = {computational methods, low-dimensional models, turbulent flows, ★},
	pages = {821--867},
}

@book{Howard2013,
	title = {Coronal {Mass} {Ejections}: {An} {Introduction}},
	volume = {53},
	isbn = {978-85-7811-079-6},
	abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-α-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 Å for the interface backbone atoms) increased from 21\% with default Glide SP settings to 58\% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63\% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40\% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
	author = {Howard, Timothy A.},
	year = {2013},
	pmid = {25246403},
	note = {arXiv: 1011.1669v3
Publication Title: Journal of Chemical Information and Modeling
Issue: 9
ISSN: 1098-6596},
	keywords = {icle, ★},
}

@article{Love2020,
	title = {Analyzing {AIA} {Flare} {Observations} {Using} {Convolutional} {Neural} {Networks}},
	volume = {7},
	issn = {2296987X},
	doi = {10.3389/fspas.2020.00034},
	abstract = {In order to efficiently analyse the vast amount of data generated by solar space missions and ground-based instruments, modern machine learning techniques such as decision trees, support vector machines (SVMs) and neural networks can be very useful. In this paper we present initial results from using a convolutional neural network (CNN) to analyse observations from the Atmospheric Imaging Assembly (AIA) in the 1,600Å wavelength. The data is pre-processed to locate flaring regions where flare ribbons are visible in the observations. The CNN is created and trained to automatically analyse the shape and position of the flare ribbons, by identifying whether each image belongs into one of four classes: two-ribbon flare, compact/circular ribbon flare, limb flare, or quiet Sun, with the final class acting as a control for any data included in the training or test sets where flaring regions are not present. The network created can classify flare ribbon observations into any of the four classes with a final accuracy of 94\%. Initial results show that most of the images are correctly classified with the compact flare class being the only class where accuracy drops below 90\% and some observations are wrongly classified as belonging to the limb class.},
	number = {June},
	journal = {Frontiers in Astronomy and Space Sciences},
	author = {Love, Teri and Neukirch, Thomas and Parnell, Clare E.},
	year = {2020},
	keywords = {CNNs, classification, machine learning, ribbons, solar flares},
	pages = {1--8},
}

@article{Bobra2015,
	title = {Solar flare prediction using {SDO}/{HMI} vector magnetic field data with a machine-learning algorithm},
	volume = {798},
	issn = {15384357},
	doi = {10.1088/0004-637X/798/2/135},
	abstract = {We attempt to forecast M- and X-class solar flares using a machine-learning algorithm, called support vector machine (SVM), and four years of data from the Solar Dynamics Observatory's Helioseismic and Magnetic Imager, the first instrument to continuously map the full-disk photospheric vector magnetic field from space. Most flare forecasting efforts described in the literature use either line-of-sight magnetograms or a relatively small number of ground-based vector magnetograms. This is the first time a large data set of vector magnetograms has been used to forecast solar flares. We build a catalog of flaring and non-flaring active regions sampled from a database of 2071 active regions, comprised of 1.5 million active region patches of vector magnetic field data, and characterize each active region by 25 parameters. We then train and test the machine-learning algorithm and we estimate its performances using forecast verification metrics with an emphasis on the true skill statistic (TSS). We obtain relatively high TSS scores and overall predictive abilities. We surmise that this is partly due to fine-tuning the SVM for this purpose and also to an advantageous set of features that can only be calculated from vector magnetic field data. We also apply a feature selection algorithm to determine which of our 25 features are useful for discriminating between flaring and non-flaring active regions and conclude that only a handful are needed for good predictive abilities.},
	number = {2},
	journal = {Astrophysical Journal},
	author = {Bobra, M. G. and Couvidat, S.},
	year = {2015},
	note = {arXiv: 1411.1405},
	keywords = {Sun: activity, Sun: flares, ★},
}

@article{Kraaikamp2015,
	title = {Solar {Demon} - {An} approach to detecting flares, dimmings, and {EUV} waves on {SDO}/{AIA} images},
	volume = {5},
	issn = {21157251},
	doi = {10.1051/swsc/2015019},
	abstract = {Flares, dimmings, and extreme ultraviolet (EUV) waves are three types of eruptive phenomena on the Sun, which are main drivers of space weather. Fast and reliable detection of these phenomena helps augment space weather predictions. In the current paper, we introduce Solar Demon, the first software that detects all three phenomena, using a modular design to exploit synergies. While Solar Demon runs in near real-time on SDO/AIA synoptic quick-look images to provide fast detections of flares, dimmings, and EUV waves for space weather purposes, it also processes new Atmospheric Imaging Assembly (AIA) synoptic science images on a regular basis to build dedicated science quality catalogs. An overview of Solar Demon is given, with a focus on the algorithms for EUV wave detection and characterization. Several first results, such as flare and dimming butterfly diagrams for the rising part of Solar Cycle 24, are presented. The main advantages, challenges, and future prospects for Solar Demon are outlined in the Section 5.},
	journal = {Journal of Space Weather and Space Climate},
	author = {Kraaikamp, Emil and Verbeeck, Cis},
	year = {2015},
	keywords = {Dimmings, EUV waves, Event detection, Flares},
}

@article{Inceoglu2022,
	title = {Identification of {Coronal} {Holes} on {AIA}/{SDO} {Images} {Using} {Unsupervised} {Machine} {Learning}},
	volume = {930},
	issn = {0004-637X},
	url = {http://dx.doi.org/10.3847/1538-4357/ac5f43},
	doi = {10.3847/1538-4357/ac5f43},
	abstract = {Through its magnetic activity, the Sun governs the conditions in Earth’s vicinity, creating space weather events, which have drastic effects on our space- and ground-based technology. One of the most important solar magnetic features creating the space weather is the solar wind that originates from the coronal holes (CHs). The identification of the CHs on the Sun as one of the source regions of the solar wind is therefore crucial to achieve predictive capabilities. In this study, we used an unsupervised machine-learning method, k -means, to pixel-wise cluster the passband images of the Sun taken by the Atmospheric Imaging Assembly on the Solar Dynamics Observatory in 171, 193, and 211 Å in different combinations. Our results show that the pixel-wise k -means clustering together with systematic pre- and postprocessing steps provides compatible results with those from complex methods, such as convolutional neural networks. More importantly, our study shows that there is a need for a CH database where a consensus about the CH boundaries is reached by observers independently. This database then can be used as the “ground truth,” when using a supervised method or just to evaluate the goodness of the models.},
	number = {2},
	journal = {The Astrophysical Journal},
	author = {Inceoglu, Fadil and Shprits, Yuri Y. and Heinemann, Stephan G. and Bianco, Stefano},
	year = {2022},
	note = {arXiv: 2203.10491
Publisher: IOP Publishing},
	keywords = {Detection, Solar coronal holes, Space weather, ★},
	pages = {118},
}

@article{Reiss2015,
	title = {Improvements on coronal hole detection in {SDO}/{AIA} images using supervised classification},
	volume = {5},
	issn = {21157251},
	doi = {10.1051/swsc/2015025},
	abstract = {We demonstrate the use of machine learning algorithms in combination with segmentation techniques in order to distinguish coronal holes and filaments in SDO/AIA EUV images of the Sun. Based on two coronal hole detection techniques (intensity-based thresholding, SPoCA), we prepared datasets of manually labeled coronal hole and filament channel regions present on the Sun during the time range 2011-2013. By mapping the extracted regions from EUV observations onto HMI line-of-sight magnetograms we also include their magnetic characteristics. We computed shape measures from the segmented binary maps as well as first order and second order texture statistics from the segmented regions in the EUV images and magnetograms. These attributes were used for data mining investigations to identify the most performant rule to differentiate between coronal holes and filament channels. We applied several classifiers, namely Support Vector Machine (SVM), Linear Support Vector Machine, Decision Tree, and Random Forest, and found that all classification rules achieve good results in general, with linear SVM providing the best performances (with a true skill statistic of ≈ 0.90). Additional information from magnetic field data systematically improves the performance across all four classifiers for the SPoCA detection. Since the calculation is inexpensive in computing time, this approach is well suited for applications on real-time data. This study demonstrates how a machine learning approach may help improve upon an unsupervised feature extraction method.},
	journal = {Journal of Space Weather and Space Climate},
	author = {Reiss, Martin A. and Hofmeister, Stefan J. and De Visscher, Ruben and Temmer, Manuela and Veronig, Astrid M. and Delouille, Veronique and Mampaey, Benjamin and Ahammer, Helmut},
	year = {2015},
	note = {arXiv: 1506.06623},
	keywords = {Coronal holes, Feature extraction, Filament channels, Solar wind, Supervised Classification, Textural features, ★},
	pages = {1--12},
}

@article{Issan2023,
	title = {Predicting solar wind streams from the inner-heliosphere to {Earth} via shifted operator inference},
	volume = {473},
	issn = {10902716},
	url = {https://doi.org/10.1016/j.jcp.2022.111689},
	doi = {10.1016/j.jcp.2022.111689},
	abstract = {Solar wind conditions are predominantly predicted via three-dimensional numerical magnetohydrodynamic (MHD) models. Despite their ability to produce highly accurate predictions, MHD models require computationally intensive high-dimensional simulations. This renders them inadequate for making time-sensitive predictions and for large-ensemble analysis required in uncertainty quantification. This paper presents a new data-driven reduced-order model (ROM) capability for forecasting heliospheric solar wind speeds. Traditional model reduction methods based on Galerkin projection have difficulties with advection-dominated systems—such as solar winds—since they require a large number of basis functions and can become unstable. A core contribution of this work addresses this challenge by extending the non-intrusive operator inference ROM framework to exploit the translational symmetries present in the solar wind caused by the Sun's rotation. The numerical results show that our method can adequately emulate the MHD simulations and is more accurate than a reduced-physics surrogate model, the Heliospheric Upwind Extrapolation model.},
	journal = {Journal of Computational Physics},
	author = {Issan, Opal and Kramer, Boris},
	year = {2023},
	note = {arXiv: 2203.13372
Publisher: Elsevier Inc.},
	keywords = {Data-driven model reduction, Magnetohydrodynamics, Operator inference, Scientific machine learning, Solar wind modeling, Space weather prediction},
	pages = {111689},
}

@article{Edeling2022,
	title = {On the deep active-subspace method},
	volume = {11},
	number = {November},
	journal = {SIAM Journal on Uncertainty Quantification},
	author = {Edeling, Wouter},
	year = {2023},
	keywords = {46n30, 65j99, 65q30, ams subject classifications, analysis, deep active subspaces, epidemiology, gram-schmidt derivative, high-dimensional uncertainty quantification, sensitivity, ★},
	pages = {62--90},
}

@article{verbeke_quantifying_2022,
	title = {Quantifying errors in {3D} {CME} parameters derived from synthetic data using white-light reconstruction techniques},
	issn = {0273-1177},
	doi = {10.1016/J.ASR.2022.08.056},
	abstract = {Current efforts in space weather forecasting of CMEs have been focused on predicting their arrival time and magnetic structure. To make these predictions, methods have been developed to derive the true CME speed, size, position, and mass, among others. Difficulties in determining the input parameters for CME forecasting models arise from the lack of direct measurements of the coronal magnetic fields and uncertainties in estimating the CME 3D geometric and kinematic parameters after eruption. White-light coronagraph images are usually employed by a variety of CME reconstruction techniques that assume more or less complex geometries. This is the first study from our International Space Science Institute (ISSI) team “Understanding Our Capabilities in Observing and Modeling Coronal Mass Ejections”, in which we explore how subjectivity affects the 3D CME parameters that are obtained from the Graduated Cylindrical Shell (GCS) reconstruction technique, which is widely used in CME research. To be able to quantify such uncertainties, the “true” values that are being fitted should be known, which are impossible to derive from observational data. We have designed two different synthetic scenarios where the “true” geometric parameters are known in order to quantify such uncertainties for the first time. We explore this by using two sets of synthetic data: 1) Using the ray-tracing option from the GCS model software itself, and 2) Using 3D magnetohydrodynamic (MHD) simulation data from the Magnetohydrodynamic Algorithm outside a Sphere code. Our experiment includes different viewing configurations using single and multiple viewpoints. CME reconstructions using a single viewpoint had the largest errors and error ranges overall for both synthetic GCS and simulated MHD white-light data. As the number of viewpoints increased from one to two, the errors decreased by approximately 4° in latitude, 22° in longitude, 14° in tilt, and 10° in half-angle. Our results quantitatively show the critical need for at least two viewpoints to be able to reduce the uncertainty in deriving CME parameters. We did not find a significant decrease in errors when going from two to three viewpoints for our specific hypothetical three spacecraft scenario using synthetic GCS white-light data. As we expected, considering all configurations and numbers of viewpoints, the mean absolute errors in the measured CME parameters are generally significantly higher in the case of the simulated MHD white-light data compared to those from the synthetic white-light images generated by the GCS model. We found the following CME parameter error bars as a starting point for quantifying the minimum error in CME parameters from white-light reconstructions: Δθ (latitude)=6°-3°+2°, Δϕ (longitude)=11°-6°+18°, Δγ (tilt)=25°-7°+8°, Δα(half-angle)=10°-6°+12°, Δh (height)=0.6-0.4+1.2 R⊙, and Δκ (ratio)=0.1-0.02+0.03.},
	urldate = {2023-02-20},
	journal = {Advances in Space Research},
	author = {Verbeke, Christine and Mays, M. Leila and Kay, Christina and Riley, Pete and Palmerio, Erika and Dumbović, Mateja and Mierla, Marilena and Scolini, Camilla and Temmer, Manuela and Paouris, Evangelos and Balmaceda, Laura A. and Cremades, Hebe and Hinterreiter, Jürgen},
	month = aug,
	year = {2022},
	note = {Publisher: Pergamon},
	keywords = {Coronal mass ejections, Remote-sensing observations, Solar corona},
}

@article{chattopadhyay_long-term_2022,
	title = {Long-term stability and generalization of observationally-constrained stochastic data-driven models for geophysical turbulence},
	url = {https://arxiv.org/abs/2205.04601v1},
	doi = {10.48550/arxiv.2205.04601},
	abstract = {Recent years have seen a surge in interest in building deep learning-based
fully data-driven models for weather prediction. Such deep learning models if
trained on observations can mitigate certain biases in current state-of-the-art
weather models, some of which stem from inaccurate representation of
subgrid-scale processes. However, these data-driven models, being
over-parameterized, require a lot of training data which may not be available
from reanalysis (observational data) products. Moreover, an accurate,
noise-free, initial condition to start forecasting with a data-driven weather
model is not available in realistic scenarios. Finally, deterministic
data-driven forecasting models suffer from issues with long-term stability and
unphysical climate drift, which makes these data-driven models unsuitable for
computing climate statistics. Given these challenges, previous studies have
tried to pre-train deep learning-based weather forecasting models on a large
amount of imperfect long-term climate model simulations and then re-train them
on available observational data. In this paper, we propose a convolutional
variational autoencoder-based stochastic data-driven model that is pre-trained
on an imperfect climate model simulation from a 2-layer quasi-geostrophic flow
and re-trained, using transfer learning, on a small number of noisy
observations from a perfect simulation. This re-trained model then performs
stochastic forecasting with a noisy initial condition sampled from the perfect
simulation. We show that our ensemble-based stochastic data-driven model
outperforms a baseline deterministic encoder-decoder-based convolutional model
in terms of short-term skills while remaining stable for long-term climate
simulations yielding accurate climatology.},
	urldate = {2023-01-11},
	author = {Chattopadhyay, Ashesh and Pathak, Jaideep and Nabizadeh, Ebrahim and Bhimji, Wahid and Hassanzadeh, Pedram},
	month = may,
	year = {2022},
	note = {arXiv: 2205.04601},
}

@book{Camporeale2018,
	title = {Machine learning techniques for space weather},
	isbn = {978-0-12-811788-0},
	abstract = {Machine Learning Techniques for Space Weather provides a thorough and accessible presentation of machine learning techniques that can be employed by space weather professionals. Additionally, it presents an overview of real-world applications in space science to the machine learning community, offering a bridge between the fields. As this volume demonstrates, real advances in space weather can be gained using nontraditional approaches that take into account nonlinear and complex dynamics, including information theory, nonlinear auto-regression models, neural networks and clustering algorithms. Offering practical techniques for translating the huge amount of information hidden in data into useful knowledge that allows for better prediction, this book is a unique and important resource for space physicists, space weather professionals and computer scientists in related fields.},
	author = {Camporeale, Enrico and Wing, Simon and Johnson, Jay R.},
	year = {2018},
	doi = {10.1016/C2016-0-01976-9},
	note = {Publication Title: Machine Learning Techniques for Space Weather},
	keywords = {★},
}

@article{kay_osprei_2022,
	title = {{OSPREI}: {A} {Coupled} {Approach} to {Modeling} {CME}-{Driven} {Space} {Weather} {With} {Automatically} {Generated}, {User}-{Friendly} {Outputs}},
	volume = {20},
	issn = {15427390},
	doi = {10.1029/2021SW002914},
	abstract = {Coronal mass ejections (CMEs) drive space weather activity at Earth and throughout the solar system. Current CME-related space weather predictions rely on information reconstructed from coronagraphs, sometimes from only a single viewpoint, to drive a simple interplanetary propagation model, which only gives the arrival time or limited additional information. We present the coupling of three established models into OSPREI (Open Solar Physics Rapid Ensemble Information), a new tool that describes Sun-to-Earth CME behavior, including the location, orientation, size, shape, speed, arrival time, and internal thermal and magnetic properties, on the timescale needed for forecasts. First, Forecasting a CME's Altered Trajectory (ForeCAT) describes the trajectory that a CME takes through the solar corona. Second, ANother Type of Ensemble Arrival Time Results simulates the propagation, including expansion and deformation, of a CME in interplanetary space and determines the evolution of internal properties via conservation laws. Finally, ForeCAT In situ Data Observer produces in situ profiles for a CME's interaction with a synthetic spacecraft. OSPREI includes ensemble modeling by varying each input parameter to probe any uncertainty in their values, yielding probabilities for all outputs. Standardized visualizations are automatically generated, providing easily accessible, essential information for space weather forecasting. We show OSPREI results for a CMEs observed in the corona on 22 April and 09 May 2021. We approach these CME as a forecasting proof-of-concept, using information analogous to what would be available in real time rather than fine-tuning input parameters to achieve a best fit for a detailed scientific study. The OSPREI “prediction” shows good agreement with the arrival time and in situ properties.},
	number = {4},
	urldate = {2022-12-08},
	journal = {Space Weather},
	author = {Kay, C. and Mays, M. L. and Collado-Vega, Y. M.},
	month = apr,
	year = {2022},
	note = {arXiv: 2109.06960
Publisher: John Wiley and Sons Inc},
}

@article{kay_osprei_2022-1,
	title = {{OSPREI}: {A} {Coupled} {Approach} to {Modeling} {CME}-{Driven} {Space} {Weather} {With} {Automatically} {Generated}, {User}-{Friendly} {Outputs}},
	volume = {20},
	issn = {1542-7390},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1029/2021SW002914},
	doi = {10.1029/2021SW002914},
	abstract = {Coronal mass ejections (CMEs) drive space weather activity at Earth and throughout the solar system. Current CME-related space weather predictions rely on information reconstructed from coronagraphs, sometimes from only a single viewpoint, to drive a simple interplanetary propagation model, which only gives the arrival time or limited additional information. We present the coupling of three established models into OSPREI (Open Solar Physics Rapid Ensemble Information), a new tool that describes Sun-to-Earth CME behavior, including the location, orientation, size, shape, speed, arrival time, and internal thermal and magnetic properties, on the timescale needed for forecasts. First, Forecasting a CME's Altered Trajectory (ForeCAT) describes the trajectory that a CME takes through the solar corona. Second, ANother Type of Ensemble Arrival Time Results simulates the propagation, including expansion and deformation, of a CME in interplanetary space and determines the evolution of internal properties via conservation laws. Finally, ForeCAT In situ Data Observer produces in situ profiles for a CME's interaction with a synthetic spacecraft. OSPREI includes ensemble modeling by varying each input parameter to probe any uncertainty in their values, yielding probabilities for all outputs. Standardized visualizations are automatically generated, providing easily accessible, essential information for space weather forecasting. We show OSPREI results for a CMEs observed in the corona on 22 April and 09 May 2021. We approach these CME as a forecasting proof-of-concept, using information analogous to what would be available in real time rather than fine-tuning input parameters to achieve a best fit for a detailed scientific study. The OSPREI “prediction” shows good agreement with the arrival time and in situ properties.},
	number = {4},
	urldate = {2022-12-08},
	journal = {Space Weather},
	author = {Kay, C. and Mays, M. L. and Collado-Vega, Y. M.},
	month = apr,
	year = {2022},
	note = {arXiv: 2109.06960
Publisher: John Wiley \& Sons, Ltd
ISBN: 10.1029/2021},
	pages = {e2021SW002914},
}

@article{huang_global_2006,
	title = {Global {Optimization} of {Stochastic} {Black}-{Box} {Systems} via {Sequential} {Kriging} {Meta}-{Models}},
	volume = {34},
	issn = {1573-2916},
	url = {https://link.springer.com/article/10.1007/s10898-005-2454-3},
	doi = {10.1007/S10898-005-2454-3},
	abstract = {This paper proposes a new method that extends the efficient global optimization to address stochastic black-box systems. The method is based on a kriging meta-model that provides a global prediction of the objective values and a measure of prediction uncertainty at every point. The criterion for the infill sample selection is an augmented expected improvement function with desirable properties for stochastic responses. The method is empirically compared with the revised simplex search, the simultaneous perturbation stochastic approximation, and the DIRECT methods using six test problems from the literature. An application case study on an inventory system is also documented. The results suggest that the proposed method has excellent consistency and efficiency in finding global optimal solutions, and is particularly useful for expensive systems.},
	number = {3},
	urldate = {2022-11-21},
	journal = {Journal of Global Optimization 2006 34:3},
	author = {Huang, D. and Allen, T. T. and Notz, W. I. and Zeng, N.},
	month = mar,
	year = {2006},
	note = {Publisher: Springer},
	keywords = {Computer Science, Operations Research/Decision Theory, Optimization, Real Functions, general},
	pages = {441--466},
}

@article{Meng2015,
	title = {Alfvén wave solar model ({AWSoM}): {Proton} temperature anisotropy and solar wind acceleration},
	volume = {454},
	issn = {13652966},
	doi = {10.1093/mnras/stv2249},
	abstract = {Temperature anisotropy has been frequently observed in the solar corona and the solar wind, yet poorly represented in computational models of the solar wind. Therefore, we have included proton temperature anisotropy in our Alfvén wave solar model (AWSoM). This model solves the magnetohydrodynamic equations augmented with low-frequency Alfvén wave turbulence. The wave reflection due to Alfvén speed gradient and field-aligned vorticity results in turbulent cascade. At the gyroradius scales, the apportioning of the turbulence dissipation into coronal heating of the protons and electrons is through stochastic heating. This paper focuses on the impacts of the proton temperature anisotropy on the solar wind.We apply AWSoM to simulate the steady solar wind from the corona to 1AU using synoptic magnetograms. The Alfvén wave energy density at the inner boundary is prescribed with a uniform Poynting flux per field strength. We present the proton temperature anisotropy distribution, and investigate the firehose instability in the heliosphere from our simulations. In particular, the comparisons between the simulated and observed solar wind properties at 1AU during the ramping-up phase and the maximum of solar cycle 24 imply the importance of addressing the proton temperature anisotropy in solar wind modelling to capture the fast solar wind speed.},
	number = {4},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Meng, X. and Van der Holst, B. and Tóth, G. and Gombosi, T. I.},
	year = {2015},
	keywords = {Methods: Numerical, Solar wind, ★},
	pages = {3697--3709},
}

@article{Hastie2008_StatLearnin,
	title = {Elements of {Statistical} {Learning}},
	issn = {00349437},
	journal = {Springer Series in Statistics},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2008},
}

@article{Sokolov2013,
	title = {Magnetohydrodynamic waves and coronal heating: {Unifying} empirical and mhd turbulence models},
	volume = {764},
	issn = {15384357},
	doi = {10.1088/0004-637X/764/1/23},
	abstract = {We present a new global model of the solar corona, including the low corona, the transition region, and the top of the chromosphere. The realistic three-dimensional magnetic field is simulated using the data from the photospheric magnetic field measurements. The distinctive feature of the new model is incorporating MHD Alfvén wave turbulence. We assume this turbulence and its nonlinear dissipation to be the only momentum and energy source for heating the coronal plasma and driving the solar wind. The difference between the turbulence dissipation efficiency in coronal holes and that in closed field regions is because the nonlinear cascade rate degrades in strongly anisotropic (imbalanced) turbulence in coronal holes (no inward propagating wave), thus resulting in colder coronal holes, from which the fast solar wind originates. The detailed presentation of the theoretical model is illustrated with the synthetic images for multi-wavelength EUV emission compared with the observations from SDO AIA and STEREO EUVI instruments for the Carrington rotation 2107. © 2013. The American Astronomical Society. All rights reserved.},
	number = {1},
	journal = {Astrophysical Journal},
	author = {Sokolov, Igor V. and Van Der Holst, Bart and Oran, Rona and Downs, Cooper and Roussev, Ilia I. and Jin, Meng and Manchester, Ward B. and Evans, Rebekah M. and Gombosi, Tamas I.},
	year = {2013},
	keywords = {Sun: UV radiation, Sun: corona, Sun: transition region, solar wind, ★},
}

@techreport{Ng2012a,
	title = {Multifidelity {Uncertainty} {Quantification} {Using} {Non}-{Intrusive} {Polynomial} {Chaos} and {Stochastic} {Collocation}},
	abstract = {This paper explores the extension of multifidelity modeling concepts to the field of uncertainty quantification. Motivated by local correction functions that enable the provable convergence of a multifidelity optimization approach to an optimal high-fidelity point solution , we extend these ideas to global discrepancy modeling within a stochastic domain and seek convergence of a multifidelity uncertainty quantification process to globally integrated high-fidelity statistics. For constructing stochastic models of both the low fidelity model and the model discrepancy, we employ stochastic expansion methods (nonintrusive polynomial chaos and stochastic collocation) computed from sparse grids, where we seek to employ a coarsely resolved grid for the discrepancy in combination with a more finely resolved grid for the low fidelity model. The resolutions of these grids may be statically defined or determined through uniform and adaptive refinement processes. Adaptive refinement is particularly attractive, as it has the ability to preferentially target stochastic regions where the model discrepancy becomes more complex; i.e., where the predictive capabilities of the low-fidelity model start to break down and greater reliance on the high fidelity model (via the discrepancy) is necessary. These adaptive refinement processes can either be performed separately for the different sparse grids or within a unified multifi-delity algorithm. In particular, we propose an adaptive greedy multifidelity approach in which we extend the generalized sparse grid concept to consider candidate index set refinements drawn from multiple sparse grids. We demonstrate that the multifidelity UQ process converges more rapidly than a single-fidelity UQ in cases where the variance of the discrepancy is reduced relative to the variance of the high fidelity model (resulting in reductions in initial stochastic error) and/or where the spectrum of the expansion coefficients of the model discrepancy decays more rapidly than that of the high-fidelity model (resulting in accelerated convergence rates).},
	author = {Ng, L W T and Eldred, M S},
	year = {2012},
	keywords = {★},
}

@article{Huan2018,
	title = {Compressive sensing with cross-validation and stop-sampling for sparse polynomial chaos expansions},
	volume = {6},
	issn = {21662525},
	doi = {10.1137/17M1141096},
	abstract = {Compressive sensing is a powerful technique for recovering sparse solutions of underdetermined linear systems, which is often encountered in uncertainty quantification analysis of expensive and high-dimensional physical models. We perform numerical investigations employing several compressive sensing solvers that target the unconstrained LASSO formulation, with a focus on linear systems that arise in the construction of polynomial chaos expansions. With core solvers of l1\_ls, SpaRSA, CGIST, FPC\_AS, and ADMM, we develop techniques to mitigate overfitting through an automated selection of regularization constant based on cross-validation, and a heuristic strategy to guide the stop-sampling decision. Practical recommendations on parameter settings for these techniques are provided and discussed. The overall method is applied to a series of numerical examples of increasing complexity, including large eddy simulations of supersonic turbulent jet-in-crossflow involving a 24-dimensional input. Through empirical phase-transition diagrams and convergence plots, we illustrate sparse recovery performance under structures induced by polynomial chaos, accuracy and computational tradeoffs between polynomial bases of different degrees, and practicability of conducting compressive sensing for a realistic, high-dimensional physical application. Across test cases studied in this paper, we find ADMM to have demonstrated empirical advantages through consistent lower errors and faster computational times.},
	number = {2},
	journal = {SIAM-ASA Journal on Uncertainty Quantification},
	author = {Huan, Xun and Safta, Cosmin and Sargsyan, Khachik and Vane, Zachary P. and Lacaze, Guilhem and Oefelein, Joseph C. and Najm, Habib N.},
	year = {2018},
	note = {Publisher: Society for Industrial and Applied Mathematics Publications},
	keywords = {1-regularization, Compressed sensing, LASSO, Sequential compressive sensing, Sparse reconstruction, Sparse regression, Uncertainty quantification, ★},
	pages = {907--936},
}

@book{Constantine2015,
	title = {Active {Subspaces}: {Emerging} {Ideas} for {Dimension} {Reduction} in {Parameter} {Studies}},
	volume = {15},
	isbn = {978-1-61197-385-3},
	abstract = {Scientists and engineers use computer simulations to study relationships between a model's input parameters and its outputs. However, thorough parameter studies are challenging, if not impossible, when the simulation is expensive and the model has several inputs. To enable studies in these instances, the engineer may attempt to reduce the dimension of the model's input parameter space. Active subspaces are an emerging set of dimension reduction tools that identify important directions in the parameter space. This book describes techniques for discovering a model's active subspace and proposes methods for exploiting the reduced dimension to enable otherwise infeasible parameter studies. Readers will find new ideas for dimension reduction, easy-to-implement algorithms, and several examples of active subspaces in action.},
	author = {Constantine, Paul G.},
	year = {2015},
	note = {Issue: 2},
}

@article{xu_bandit-learning_nodate,
	title = {A bandit-learning approach to multifidelity approximation ∗},
	author = {Xu, Yiming and Keshavarzzadeh, Vahid and Kirby, Robert M},
	note = {arXiv: 2103.15342v3},
	keywords = {62-08, 62j05, 65c05, 65n30, ams subject classifications, bandit learning, consistency, linear regression, monte carlo method, multifidelity},
	pages = {1--41},
}

@article{park_deepsdf_2019,
	title = {{DeepSDF}: {Learning} {Continuous} {Signed} {Distance} {Functions} for {Shape} {Representation}},
	url = {https://arxiv.org/abs/1901.05103},
	doi = {10.48550/arxiv.1901.05103},
	abstract = {Computer graphics, 3D computer vision and robotics communities have produced
multiple approaches to representing 3D geometry for rendering and
reconstruction. These provide trade-offs across fidelity, efficiency and
compression capabilities. In this work, we introduce DeepSDF, a learned
continuous Signed Distance Function (SDF) representation of a class of shapes
that enables high quality shape representation, interpolation and completion
from partial and noisy 3D input data. DeepSDF, like its classical counterpart,
represents a shape's surface by a continuous volumetric field: the magnitude of
a point in the field represents the distance to the surface boundary and the
sign indicates whether the region is inside (-) or outside (+) of the shape,
hence our representation implicitly encodes a shape's boundary as the
zero-level-set of the learned function while explicitly representing the
classification of space as being part of the shapes interior or not. While
classical SDF's both in analytical or discretized voxel form typically
represent the surface of a single shape, DeepSDF can represent an entire class
of shapes. Furthermore, we show state-of-the-art performance for learned 3D
shape representation and completion while reducing the model size by an order
of magnitude compared with previous work.},
	urldate = {2022-08-12},
	author = {Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.05103},
}

@article{Osher1988,
	title = {Fronts propagating with curvature-dependent speed: {Algorithms} based on {Hamilton}-{Jacobi} formulations},
	volume = {79},
	issn = {10902716},
	doi = {10.1016/0021-9991(88)90002-2},
	abstract = {We devise new numerical algorithms, called PSC algorithms, for following fronts propagating with curvature-dependent speed. The speed may be an arbitrary function of curvature, and the front also can be passively advected by an underlying flow. These algorithms approximate the equations of motion, which resemble Hamilton-Jacobi equations with parabolic right-hand sides, by using techniques from hyperbolic conservation laws. Non-oscillatory schemes of various orders of accuracy are used to solve the equations, providing methods that accurately capture the formation of sharp gradients and cusps in the moving fronts. The algorithms handle topological merging and breaking naturally, work in any number of space dimensions, and do not require that the moving surface be written as a function. The methods can be also used for more general Hamilton-Jacobi-type problems. We demonstrate our algorithms by computing the solution to a variety of surface motion problems. © 1988.},
	number = {1},
	journal = {Journal of Computational Physics},
	author = {Osher, Stanley and Sethian, James A.},
	year = {1988},
	pages = {12--49},
}

@book{2006,
	title = {Past, present and future of statistical science},
	volume = {1999},
	isbn = {978-1-4822-0498-8},
	abstract = {Past, Present, and Future of Statistical Science was commissioned in 2013 by the Committee of Presidents of Statistical Societies (COPSS) to celebrate its 50th anniversary and the International Year of Statistics. COPSS consists of five charter member statistical societies in North America and is best known for sponsoring prestigious awards in statistics, such as the COPSS Presidents’ award. Through the contributions of a distinguished group of 50 statisticians who are past winners of at least one of the five awards sponsored by COPSS, this volume showcases the breadth and vibrancy of statistics, describes current challenges and new opportunities, highlights the exciting future of statistical science, and provides guidance to future generations of statisticians. The book is not only about statistics and science but also about people and their passion for discovery. Distinguished authors present expository articles on a broad spectrum of topics in statistical education, research, and applications. Topics covered include reminiscences and personal reflections on statistical careers, perspectives on the field and profession, thoughts on the discipline and the future of statistical science, and advice for young statisticians. Many of the articles are accessible not only to professional statisticians and graduate students but also to undergraduate students interested in pursuing statistics as a career and to all those who use statistics in solving real-world problems. A consistent theme of all the articles is the passion for statistics enthusiastically shared by the authors. Their success stories inspire, give a sense of statistics as a discipline, and provide a taste of the exhilaration of discovery, success, and professional accomplishment.},
	author = {Lin, Xihong},
	year = {2006},
	note = {Publication Title: منشورات جامعة دمشق
Issue: December},
	keywords = {★},
}

@article{Xiu,
	title = {The {Wiener}-{Askey} {Polynomial} {Chaos} for {Stochastic} {Differential} {Equations}},
	volume = {24},
	issn = {1064-8275},
	url = {http://epubs.siam.org/doi/10.1137/S1064827501387826},
	doi = {10.1137/S1064827501387826},
	number = {2},
	journal = {SIAM Journal on Scientific Computing},
	author = {Xiu, Dongbin and Karniadakis, George Em},
	month = jan,
	year = {2002},
	pages = {619--644},
}

@article{reiss_forecasting_2020,
	title = {Forecasting the {Ambient} {Solar} {Wind} with {Numerical} {Models}. {II}. {An} {Adaptive} {Prediction} {System} for {Specifying} {Solar} {Wind} {Speed} near the {Sun}},
	volume = {891},
	issn = {1538-4357},
	url = {https://iopscience.iop.org/article/10.3847/1538-4357/ab78a0},
	doi = {10.3847/1538-4357/ab78a0},
	number = {2},
	journal = {The Astrophysical Journal},
	author = {Reiss, Martin A. and MacNeice, Peter J. and Muglach, Karin and Arge, Charles N. and Möstl, Christian and Riley, Pete and Hinterreiter, Jürgen and Bailey, Rachel L. and Weiss, Andreas J. and Owens, Mathew J. and Amerstorfer, Tanja and Amerstorfer, Ute},
	month = mar,
	year = {2020},
	pages = {165},
}

@article{Auder2009,
	title = {Global sensitivity analysis based on entropy},
	volume = {3},
	doi = {10.1201/9781482266481-304},
	abstract = {This paper deals with the sensitivity analysis of model output, using entropy. By the past, variance has been used, and some measures directly related to data distributions have been performed. We expect entropy to give another point of view, refining the other sensitivity analysis, and may be to solve some situations where the other methods were useless. Two different indices based on entropy are presented in this work: those defined by Krzykacz-Hausmann 2001 based on conditional entropy which use directly the definition of Shannon's entropy, and those later suggested by Liu, Chen, and Sudjianto 2006, based on the Kullbak-Leibler entropy, which measure the difference between two probability distributions. After a brief presentation of the two entropy-based indices, two application cases are studied, one analytical function and one industrial computer code. Our goal is to show that some indices are better suited for some specific cases, and that they all enable different but relevant sensitivity analyses to be performed. © 2009 Taylor \& Francis Group.},
	journal = {Safety, Reliability and Risk Analysis: Theory, Methods and Applications - Proceedings of the Joint ESREL and SRA-Europe Conference},
	author = {Auder, Benjamin and Iooss, Bertrand},
	year = {2009},
	note = {ISBN: 9780415485135},
	keywords = {★},
	pages = {2107--2115},
}

@article{Middleton2008,
	title = {Use of information theory with discrete models of continuous systems},
	volume = {37},
	issn = {03081079},
	doi = {10.1080/03081070701250937},
	abstract = {A technique is presented whereby the Shannon entropy can be used to define two parametersthe effective range and the uncertainty indexwhich can be used to quantify consistently the uncertainty in realistic continuous systems by comparing the probability distribution associated with the actual system to a uniform distribution having the same entropy value. Knowing the entropy value of a distribution which describes a system can allow decision-makers to choose more easily between systems in order to achieve a given objective. It also provides a metric for ranking the importance of input parameters for a given system based upon their relative impact upon the total uncertainty of the output of the system. Since the data collected from most engineering systems are actually a set of discrete samples, a method is introduced to calculate the Shannon entropy using spreadsheet software and then calculate the effective range and uncertainty index. The technique is used to choose between two policies for an example construction project and to rank input variables in order of decreasing effect upon the output uncertainty of the project.},
	number = {3},
	journal = {International Journal of General Systems},
	author = {Middleton, Bobby D. and Golay, Michael W.},
	year = {2008},
	note = {ISBN: 0308107070125},
	keywords = {Entropy, Information theory, System dynamics, Systems, Uncertainty},
	pages = {347--371},
}

@article{Gupta2018,
	title = {Revisiting the {Basis} of {Sensitivity} {Analysis} for {Dynamical} {Earth} {System} {Models}},
	volume = {54},
	issn = {19447973},
	doi = {10.1029/2018WR022668},
	abstract = {This paper investigates the problem of global sensitivity analysis (GSA) of Dynamical Earth System Models and proposes a basis for how such analyses should be performed. We argue that (a) performance metric-based approaches to parameter GSA are actually identifiability analyses, (b) the use of a performance metric to assess sensitivity unavoidably distorts the information provided by the model about relative parameter importance, and (c) it is a serious conceptual flaw to interpret the results of such an analysis as being consistent and accurate indications of the sensitivity of the model response to parameter perturbations. Further, because such approaches depend on availability of system state/output observational data, the analysis they provide is necessarily incomplete. Here we frame the GSA problem from first principles, using trajectories of the partial derivatives of model outputs with respect to controlling factors as the theoretical basis for sensitivity, and construct a global sensitivity matrix from which statistical indices of total period time-aggregate parameter importance, and time series of time-varying parameter importance, can be inferred. We demonstrate this framework using the HBV-SASK conceptual hydrologic model applied to the Oldman basin in Canada and show that it disagrees with performance metric-based methods regarding which parameters exert the strongest controls on model behavior. Further, it is highly efficient, requiring less than 1,000 base samples to obtain stable and robust parameter importance assessments for our 10-parameter example.},
	number = {11},
	journal = {Water Resources Research},
	author = {Gupta, Hoshin V. and Razavi, Saman},
	year = {2018},
	keywords = {Parameter importance analysis, dynamical systems, efficiency and robustness, global sensitivity analysis, global sensitivity matrix, time-varying sensitivity, ★},
	pages = {8692--8717},
}

@article{Lemos2022,
	title = {Rediscovering orbital mechanics with machine learning},
	url = {http://arxiv.org/abs/2202.02306},
	abstract = {We present an approach for using machine learning to automatically discover the governing equations and hidden properties of real physical systems from observations. We train a "graph neural network" to simulate the dynamics of our solar system's Sun, planets, and large moons from 30 years of trajectory data. We then use symbolic regression to discover an analytical expression for the force law implicitly learned by the neural network, which our results showed is equivalent to Newton's law of gravitation. The key assumptions that were required were translational and rotational equivariance, and Newton's second and third laws of motion. Our approach correctly discovered the form of the symbolic force law. Furthermore, our approach did not require any assumptions about the masses of planets and moons or physical constants. They, too, were accurately inferred through our methods. Though, of course, the classical law of gravitation has been known since Isaac Newton, our result serves as a validation that our method can discover unknown laws and hidden properties from observed data. More broadly this work represents a key step toward realizing the potential of machine learning for accelerating scientific discovery.},
	author = {Lemos, Pablo and Jeffrey, Niall and Cranmer, Miles and Ho, Shirley and Battaglia, Peter},
	year = {2022},
	note = {arXiv: 2202.02306},
}

@article{Reich2013,
	title = {Downloaded 09 / 03 / 14 to 128 . 117 . 88 . 64 . {Redistribution} subject to {SIAM} license or copyright ; see http://www.siam.org/journals/ojsa.php {A} {NONPARAMETRIC} {ENSEMBLE} {TRANSFORM} {METHOD} {FOR} {Copyright} © by {SIAM} . {Unauthorized} reproduction of this article},
	volume = {35},
	number = {4},
	author = {Reich, Sebastian},
	year = {2013},
	keywords = {10, 1137, 130907367, 62f15, 62m20, 65c05, 86a22, 93e11, ams subject classifications, bayesian inference, doi, linear, monte carlo method, programming, resampling, sequential data assimilation},
	pages = {2013--2024},
}

@article{Dubreuil2014a,
	title = {Construction of bootstrap confidence intervals on sensitivity indices computed by polynomial chaos expansion},
	volume = {121},
	issn = {09518320},
	url = {http://dx.doi.org/10.1016/j.ress.2013.09.011},
	doi = {10.1016/j.ress.2013.09.011},
	abstract = {Sensitivity analysis aims at quantifying influence of input parameters dispersion on the output dispersion of a numerical model. When the model evaluation is time consuming, the computation of Sobol' indices based on Monte Carlo method is not applicable and a surrogate model has to be used. Among all approximation methods, polynomial chaos expansion is one of the most efficient to calculate variance-based sensitivity indices. Indeed, their computation is analytically derived from the expansion coefficients but without error estimators of the meta-model approximation. In order to evaluate the reliability of these indices, we propose to build confidence intervals by bootstrap re-sampling on the experimental design used to estimate the polynomial chaos approximation. Since the evaluation of the sensitivity indices is obtained with confidence intervals, it is possible to find a design of experiments allowing the computation of sensitivity indices with a given accuracy. © 2013 Elsevier Ltd.},
	journal = {Reliability Engineering and System Safety},
	author = {Dubreuil, S. and Berveiller, M. and Petitjean, F. and Salaün, M.},
	year = {2014},
	note = {Publisher: Elsevier},
	keywords = {Bootstrap re-sampling, Polynomial chaos expansion, Sensitivity analysis, ★},
	pages = {263--275},
}

@article{Sachdeva2021,
	title = {Simulating {Solar} {Maximum} {Conditions} {Using} the {Alfvén} {Wave} {Solar} {Atmosphere} {Model} ({AWSoM})},
	volume = {923},
	issn = {0004-637X},
	url = {http://dx.doi.org/10.3847/1538-4357/ac307c},
	doi = {10.3847/1538-4357/ac307c},
	abstract = {To simulate solar coronal mass ejections (CMEs) and predict their time of arrival and geomagnetic impact, it is important to accurately model the background solar wind conditions in which CMEs propagate. We use the Alfvén Wave Solar atmosphere Model (AWSoM) within the the Space Weather Modeling Framework to simulate solar maximum conditions during two Carrington rotations and produce solar wind background conditions comparable to the observations. We describe the inner boundary conditions for AWSoM using the ADAPT global magnetic maps and validate the simulated results with EUV observations in the low corona and measured plasma parameters at L1 as well as at the position of the Solar Terrestrial Relations Observatory spacecraft. This work complements our prior AWSoM validation study for solar minimum conditions and shows that during periods of higher magnetic activity, AWSoM can reproduce the solar plasma conditions (using properly adjusted photospheric Poynting flux) suitable for providing proper initial conditions for launching CMEs.},
	number = {2},
	journal = {The Astrophysical Journal},
	author = {Sachdeva, Nishtha and Tóth, Gábor and Manchester, Ward B. and van der Holst, Bart and Huang, Zhenguang and Sokolov, Igor V. and Zhao, Lulu and Shidi, Qusai Al and Chen, Yuxi and Gombosi, Tamas I. and Henney, Carl J. and Lloveras, Diego G. and Vásquez, Alberto M.},
	year = {2021},
	note = {Publisher: IOP Publishing},
	keywords = {Heliosphere, Solar corona, Solar wind, Space weather},
	pages = {176},
}

@article{Storlie2009,
	title = {Implementation and evaluation of nonparametric regression procedures for sensitivity analysis of computationally demanding models},
	volume = {94},
	issn = {09518320},
	url = {http://dx.doi.org/10.1016/j.ress.2009.05.007},
	doi = {10.1016/j.ress.2009.05.007},
	abstract = {The analysis of many physical and engineering problems involves running complex computational models (simulation models, computer codes). With problems of this type, it is important to understand the relationships between the input variables (whose values are often imprecisely known) and the output. The goal of sensitivity analysis (SA) is to study this relationship and identify the most significant factors or variables affecting the results of the model. In this presentation, an improvement on existing methods for SA of complex computer models is described for use when the model is too computationally expensive for a standard Monte-Carlo analysis. In these situations, a meta-model or surrogate model can be used to estimate the necessary sensitivity index for each input. A sensitivity index is a measure of the variance in the response that is due to the uncertainty in an input. Most existing approaches to this problem either do not work well with a large number of input variables and/or they ignore the error involved in estimating a sensitivity index. Here, a new approach to sensitivity index estimation using meta-models and bootstrap confidence intervals is described that provides solutions to these drawbacks. Further, an efficient yet effective approach to incorporate this methodology into an actual SA is presented. Several simulated and real examples illustrate the utility of this approach. This framework can be extended to uncertainty analysis as well. © 2009 Elsevier Ltd.},
	number = {11},
	journal = {Reliability Engineering and System Safety},
	author = {Storlie, Curtis B. and Swiler, Laura P. and Helton, Jon C. and Sallaberry, Cedric J.},
	year = {2009},
	note = {Publisher: Elsevier},
	keywords = {Bootstrap, Confidence intervals, Meta-model, Nonparametric regression, Sensitivity analysis, Surrogate model, Uncertainty analysis, Variance decomposition, ★},
	pages = {1735--1763},
}

@article{Blatman2010a,
	title = {Efficient computation of global sensitivity indices using sparse polynomial chaos expansions},
	volume = {95},
	issn = {09518320},
	doi = {10.1016/j.ress.2010.06.015},
	abstract = {Global sensitivity analysis aims at quantifying the relative importance of uncertain input variables onto the response of a mathematical model of a physical system. ANOVA-based indices such as the Sobol' indices are well-known in this context. These indices are usually computed by direct Monte Carlo or quasi-Monte Carlo simulation, which may reveal hardly applicable for computationally demanding industrial models. In the present paper, sparse polynomial chaos (PC) expansions are introduced in order to compute sensitivity indices. An adaptive algorithm allows the analyst to build up a PC-based metamodel that only contains the significant terms whereas the PC coefficients are computed by least-square regression using a computer experimental design. The accuracy of the metamodel is assessed by leave-one-out cross validation. Due to the genuine orthogonality properties of the PC basis, ANOVA-based sensitivity indices are post-processed analytically. This paper also develops a bootstrap technique which eventually yields confidence intervals on the results. The approach is illustrated on various application examples up to 21 stochastic dimensions. Accurate results are obtained at a computational cost 23 orders of magnitude smaller than that associated with Monte Carlo simulation. © 2010 Elsevier Ltd. All rights reserved.},
	number = {11},
	journal = {Reliability Engineering and System Safety},
	author = {Blatman, Graud and Sudret, Bruno},
	year = {2010},
	keywords = {ANOVA, Global sensitivity analysis, Sequential experimental design, Sobol' indices, Sparse polynomial chaos, Stepwise regression},
	pages = {1216--1229},
}

@article{Gombosi2021,
	title = {What sustained multi-disciplinary research can achieve: {The} space weather modeling framework},
	volume = {11},
	issn = {21157251},
	doi = {10.1051/swsc/2021020},
	abstract = {Magnetohydrodynamics (MHD)-based global space weather models have mostly been developed and maintained at academic institutions. While the "free spirit"approach of academia enables the rapid emergence and testing of new ideas and methods, the lack of long-Term stability and support makes this arrangement very challenging. This paper describes a successful example of a university-based group, the Center of Space Environment Modeling (CSEM) at the University of Michigan, that developed and maintained the Space Weather Modeling Framework (SWMF) and its core element, the BATS-R-US extended MHD code. It took a quarter of a century to develop this capability and reach its present level of maturity that makes it suitable for research use by the space physics community through the Community Coordinated Modeling Center (CCMC) as well as operational use by the NOAA Space Weather Prediction Center (SWPC).},
	journal = {Journal of Space Weather and Space Climate},
	author = {Gombosi, Tamas I. and Chen, Yuxi and Glocer, Alex and Huang, Zhenguang and Jia, Xianzhe and Liemohn, Michael W. and Manchester, Ward B. and Pulkkinen, Tuija and Sachdeva, Nishtha and Al Shidi, Qusai and Sokolov, Igor V. and Szente, Judit and Tenishev, Valeriy and Toth, Gabor and Van Der Holst, Bart and Welling, Daniel T. and Zhao, Lulu and Zou, Shasha and Gombosi, Tamas I.},
	year = {2021},
	note = {arXiv: 2105.13227},
	keywords = {MHD, Scientific computing, Solar flares and CMEs, Space plasma physics, Space weather},
}

@article{SkamarockW.C.KlempJ.B.DudhiaJ.GillD.O.BarkerD.M.WangW.Powers2005,
	title = {A description of the advanced research {WRF} version 2},
	abstract = {The development of the Weather Research and Forecasting (WRF) modeling system is a multiagency effort intended to provide a next-generation mesoscale forecast model and data assimilation system that will advance both the understanding and prediction of mesoscale weather and accelerate the transfer of research advances into operations. The model is being developed as a collaborative effort ort among the NCAR Mesoscale and Microscale Meteorology (MMM) Division, the National Oceanic and Atmospheric Administration's (NOAA) National Centers for Environmental Prediction (NCEP) and Forecast System Laboratory (FSL), the Department of Defense's Air Force Weather Agency (AFWA) and Naval Research Laboratory (NRL), the Center for Analysis and Prediction of Storms (CAPS) at the University of Oklahoma, and the Federal Aviation Administration (FAA), along with the participation of a number of university scientists. The WRF model is designed to be a flexible, state-of-the-art, portable code that is an efficient in a massively parallel computing environment. A modular single-source code is maintained that can be configured for both research and operations. It offers numerous physics options, thus tapping into the experience of the broad modeling community. Advanced data assimilation systems are being developed and tested in tandem with the model. WRF is maintained and supported as a community model to facilitate wide use, particularly for research and teaching, in the university community. It is suitable for use in a broad spectrum of applications across scales ranging from meters to thousands of kilometers. Such applications include research and operational numerical weather prediction (NWP), data assimilation and parameterized-physics research, downscaling climate simulations, driving air quality models, atmosphere-ocean coupling, and idealized simulations (e.g boundary-layer eddies, convection, baroclinic waves).},
	number = {June},
	author = {Skamarock, W. C., Klemp, J. B., Dudhia, J., Gill, D. O., Barker, D. M., Wang, W., Powers, J. G.},
	year = {2005},
	pages = {100},
}

@article{Coen2013,
	title = {Wrf-fire: {Coupled} weather-wildland fire modeling with the weather research and forecasting model},
	volume = {52},
	issn = {15588424},
	doi = {10.1175/JAMC-D-12-023.1},
	abstract = {A wildland fire-behavior module, named WRF-Fire, was integrated into the Weather Research and Forecasting (WRF) public domain numerical weather prediction model. The fire module is a surface firebehavior model that is two-way coupled with the atmospheric model. Near-surface winds from the atmospheric model are interpolated to a finer fire grid and are used, with fuel properties and local terrain gradients, to determine the fire's spread rate and direction. Fuel consumption releases sensible and latent heat fluxes into the atmospheric model's lowest layers, driving boundary layer circulations. The atmospheric model, configured in turbulence-resolving large-eddy-simulation mode, was used to explore the sensitivity of simulated fire characteristics such as perimeter shape, fire intensity, and spread rate to external factors known to influence fires, such as fuel characteristics and wind speed, and to explain how these external parameters affect the overall fire properties. Through the use of theoretical environmental vertical profiles, a suite of experiments using conditions typical of the daytime convective boundary layer was conducted in which these external parameters were varied around a control experiment. Results showed that simulated fires evolved into the expected bowed shape because of fire-atmosphere feedbacks that control airflow in and near fires. The coupled model reproduced expected differences in fire shapes and heading-region fire intensity among grass, shrub, and forest-litter fuel types; reproduced the expected narrow, rapid spread in higher wind speeds; and reproduced the moderate inhibition of fire spread in higher fuel moistures. The effects of fuel load were more complex: higher fuel loads increased the heat flux and fire-plume strength and thus the inferred fire effects but had limited impact on spread rate. © 2013 American Meteorological Society.},
	number = {1},
	journal = {Journal of Applied Meteorology and Climatology},
	author = {Coen, Janice L. and Cameron, Marques and Michalakes, John and Patton, Edward G. and Riggan, Philip J. and Yedinak, Kara M.},
	year = {2013},
	keywords = {★},
	pages = {16--38},
}

@article{Rackauckas2020,
	title = {Universal {Differential} {Equations} for {Scientific} {Machine} {Learning}},
	issn = {2331-8422},
	url = {http://arxiv.org/abs/2001.04385},
	abstract = {In the context of science, the well-known adage "a picture is worth a thousand words" might well be "a model is worth a thousand datasets." In this manuscript we introduce the SciML software ecosystem as a tool for mixing the information of physical laws and scientific models with data-driven machine learning approaches. We describe a mathematical object, which we denote universal differential equations (UDEs), as the unifying framework connecting the ecosystem. We show how a wide variety of applications, from automatically discovering biological mechanisms to solving high-dimensional Hamilton-Jacobi-Bellman equations, can be phrased and efficiently handled through the UDE formalism and its tooling. We demonstrate the generality of the software tooling to handle stochasticity, delays, and implicit constraints. This funnels the wide variety of SciML applications into a core set of training mechanisms which are highly optimized, stabilized for stiff equations, and compatible with distributed parallelism and GPU accelerators.},
	author = {Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali and Edelman, Alan},
	year = {2020},
	note = {arXiv: 2001.04385},
	pages = {1--55},
}

@article{Gao2021,
	title = {Physics-informed graph neural {Galerkin} networks: {A} unified framework for solving {PDE}-governed forward and inverse problems},
	url = {http://arxiv.org/abs/2107.12146},
	abstract = {Despite the great promise of the physics-informed neural networks (PINNs) in solving forward and inverse problems, several technical challenges are present as roadblocks for more complex and realistic applications. First, most existing PINNs are based on point-wise formulation with fully-connected networks to learn continuous functions, which suffer from poor scalability and hard boundary enforcement. Second, the infinite search space over-complicates the non-convex optimization for network training. Third, although the convolutional neural network (CNN)-based discrete learning can significantly improve training efficiency, CNNs struggle to handle irregular geometries with unstructured meshes. To properly address these challenges, we present a novel discrete PINN framework based on graph convolutional network (GCN) and variational structure of PDE to solve forward and inverse partial differential equations (PDEs) in a unified manner. The use of a piecewise polynomial basis can reduce the dimension of search space and facilitate training and convergence. Without the need of tuning penalty parameters in classic PINNs, the proposed method can strictly impose boundary conditions and assimilate sparse data in both forward and inverse settings. The flexibility of GCNs is leveraged for irregular geometries with unstructured meshes. The effectiveness and merit of the proposed method are demonstrated over a variety of forward and inverse computational mechanics problems governed by both linear and nonlinear PDEs.},
	author = {Gao, Han and Zahr, Matthew J. and Wang, Jian-Xun},
	year = {2021},
	note = {arXiv: 2107.12146},
	keywords = {graph convolutional neural networks, inverse problem, learning, mechanics, partial differential equations, physics-informed machine},
}

@article{Rossi2020,
	title = {Temporal {Graph} {Networks} for {Deep} {Learning} on {Dynamic} {Graphs}},
	url = {http://arxiv.org/abs/2006.10637},
	abstract = {Graph Neural Networks (GNNs) have recently become increasingly popular due to their ability to learn complex systems of relations or interactions arising in a broad spectrum of problems ranging from biology and particle physics to social networks and recommendation systems. Despite the plethora of different models for deep learning on graphs, few approaches have been proposed thus far for dealing with graphs that present some sort of dynamic nature (e.g. evolving features or connectivity over time). In this paper, we present Temporal Graph Networks (TGNs), a generic, efficient framework for deep learning on dynamic graphs represented as sequences of timed events. Thanks to a novel combination of memory modules and graph-based operators, TGNs are able to significantly outperform previous approaches being at the same time more computationally efficient. We furthermore show that several previous models for learning on dynamic graphs can be cast as specific instances of our framework. We perform a detailed ablation study of different components of our framework and devise the best configuration that achieves state-of-the-art performance on several transductive and inductive prediction tasks for dynamic graphs.},
	author = {Rossi, Emanuele and Chamberlain, Ben and Frasca, Fabrizio and Eynard, Davide and Monti, Federico and Bronstein, Michael},
	year = {2020},
	note = {arXiv: 2006.10637},
	pages = {1--16},
}

@article{Perumal2020,
	title = {Comparison of {Recurrent} {Neural} {Network} {Architectures} for {Wildfire} {Spread} {Modelling}},
	volume = {2020-Janua},
	doi = {10.1109/SAUPEC/RobMech/PRASA48453.2020.9078028},
	abstract = {Wildfire modelling is an attempt to reproduce fire behaviour. Through active fire analysis, it is possible to reproduce a dynamical process, such as wildfires, with limited duration time series data. Recurrent neural networks (RNNs) can model dynamic temporal behaviour due to their ability to remember their internal input. In this paper, we compare the Gated Recurrent Unit (GRU) and the Long Short-Term Memory (LSTM) network. We try to determine whether a wildfire continues to burn and given that it does, we aim to predict which one of the 8 cardinal directions the wildfire will spread in. Overall the GRU performs better for longer time series than the LSTM. We have shown that although we are reasonable at predicting the direction in which the wildfire will spread, we are not able to asses if the wildfire continues to burn due to the lack of auxiliary data.},
	number = {May 2020},
	journal = {2020 International SAUPEC/RobMech/PRASA Conference, SAUPEC/RobMech/PRASA 2020},
	author = {Perumal, Rylan and Zyl, Terence L.Van},
	year = {2020},
	note = {arXiv: 2005.13040
ISBN: 9781728141626},
	keywords = {machine learning, recurrent neural networks, wildfire spread modelling},
}

@article{Rozen2021,
	title = {Moser {Flow}: {Divergence}-based {Generative} {Modeling} on {Manifolds}},
	url = {http://arxiv.org/abs/2108.08052},
	abstract = {We are interested in learning generative models for complex geometries described via manifolds, such as spheres, tori, and other implicit surfaces. Current extensions of existing (Euclidean) generative models are restricted to specific geometries and typically suffer from high computational costs. We introduce Moser Flow (MF), a new class of generative models within the family of continuous normalizing flows (CNF). MF also produces a CNF via a solution to the change-of-variable formula, however differently from other CNF methods, its model (learned) density is parameterized as the source (prior) density minus the divergence of a neural network (NN). The divergence is a local, linear differential operator, easy to approximate and calculate on manifolds. Therefore, unlike other CNFs, MF does not require invoking or backpropagating through an ODE solver during training. Furthermore, representing the model density explicitly as the divergence of a NN rather than as a solution of an ODE facilitates learning high fidelity densities. Theoretically, we prove that MF constitutes a universal density approximator under suitable assumptions. Empirically, we demonstrate for the first time the use of flow models for sampling from general curved surfaces and achieve significant improvements in density estimation, sample quality, and training complexity over existing CNFs on challenging synthetic geometries and real-world benchmarks from the earth and climate sciences.},
	number = {2020},
	author = {Rozen, Noam and Grover, Aditya and Nickel, Maximilian and Lipman, Yaron},
	year = {2021},
	note = {arXiv: 2108.08052},
	pages = {1--16},
}

@article{Rajaram2020,
	title = {Non-intrusive parametric reduced order modeling using randomized algorithms},
	volume = {1 PartF},
	doi = {10.2514/6.2020-0417},
	abstract = {This paper demonstrates the creation of purely data-driven, non-intrusive parametric reduced order models (ROMs) for emulation of high-dimensional field outputs using randomized linear algebra techniques. Typically, low-dimensional representations are built using the Proper Orthogonal Decomposition (POD) combined with interpolation/regression in the latent space via machine learning. However, even moderately large simulations can lead to data sets on which the cost of computing the POD becomes intractable due to storage and computational complexity of the numerical procedure. In an attempt to reduce the offline cost, the proposed method demonstrates the application of randomized singular value decomposition (SVD) and sketching-based randomized SVD to compute the POD basis. The predictive capability of ROMs resulting from regular SVD and randomized/sketching-based algorithms are compared with each other to ensure that the decrease in computational cost does not result in a loss in accuracy. Demonstrations on canonical and practical fluid flow problems show that the ROMs resulting from randomized methods are competitive with ROMs that employ the conventional deterministic method. Through this new method, it is hoped that truly large-scale parametric ROMs can be constructed under a significantly limited computational budget.},
	number = {January},
	journal = {AIAA Scitech 2020 Forum},
	author = {Rajaram, Dushhyanth and Puranik, Tejas G. and Perron, Christian and Mavris, Dimitri N.},
	year = {2020},
	note = {ISBN: 9781624105951},
	pages = {1--24},
}

@article{Liu2021,
	title = {Algorithms for {Verifying} {Deep} {Neural} {Networks}},
	volume = {4},
	issn = {2167-3888},
	doi = {10.1561/2400000035},
	abstract = {Deep neural networks are widely used for nonlinear function approximation with applications ranging from computer vision to control. Although these networks involve the composition of simple arithmetic operations, it can be very challenging to verify whether a particular network satisfies certain input-output properties. This article surveys methods that have emerged recently for soundly verifying such properties. These methods borrow insights from reachability analysis, optimization, and search. We discuss fundamental differences and connections between existing algorithms. In addition, we provide pedagogical implementations of existing methods and compare them on a set of benchmark problems.},
	number = {3-4},
	journal = {Foundations and Trends® in Optimization},
	author = {Liu, Changliu and Arnon, Tomer and Lazarus, Christopher and Strong, Christopher and Barrett, Clark and Kochenderfer, Mykel J.},
	year = {2021},
	note = {arXiv: 1903.06758},
	keywords = {★},
	pages = {244--404},
}

@article{Tropp2019,
	title = {Streaming low-rank matrix approximation with an application to scientific simulation},
	volume = {41},
	issn = {10957197},
	doi = {10.1137/18M1201068},
	abstract = {This paper argues that randomized linear sketching is a natural tool for on-the-fly compression of data matrices that arise from large-scale scientific simulations and data collection. The technical contribution consists in a new algorithm for constructing an accurate low-rank approximation of a matrix from streaming data. This method is accompanied by an a priori analysis that allows the user to set algorithm parameters with confidence and an a posteriori error estimator that allows the user to validate the quality of the reconstructed matrix. In comparison to previous techniques, the new method achieves smaller relative approximation errors and is less sensitive to parameter choices. As concrete applications, the paper outlines how the algorithm can be used to compress a Navier-Stokes simulation and a sea surface temperature dataset.},
	number = {4},
	journal = {SIAM Journal on Scientific Computing},
	author = {Tropp, Joel A. and Yurtsever, Alp and Udell, Madeleine and Cevher, Volkan},
	year = {2019},
	note = {arXiv: 1902.08651},
	keywords = {Dimension reduction, Matrix approximation, Numerical linear algebra, Singular value decomposition, Sketching, Streaming, ★},
	pages = {A2430--A2463},
}

@article{Williams2018,
	title = {Unsupervised {Discovery} of {Demixed}, {Low}-{Dimensional} {Neural} {Dynamics} across {Multiple} {Timescales} through {Tensor} {Component} {Analysis}},
	volume = {98},
	issn = {10974199},
	url = {https://doi.org/10.1016/j.neuron.2018.05.015},
	doi = {10.1016/j.neuron.2018.05.015},
	abstract = {Perceptions, thoughts, and actions unfold over millisecond timescales, while learned behaviors can require many days to mature. While recent experimental advances enable large-scale and long-term neural recordings with high temporal fidelity, it remains a formidable challenge to extract unbiased and interpretable descriptions of how rapid single-trial circuit dynamics change slowly over many trials to mediate learning. We demonstrate a simple tensor component analysis (TCA) can meet this challenge by extracting three interconnected, low-dimensional descriptions of neural data: neuron factors, reflecting cell assemblies; temporal factors, reflecting rapid circuit dynamics mediating perceptions, thoughts, and actions within each trial; and trial factors, describing both long-term learning and trial-to-trial changes in cognitive state. We demonstrate the broad applicability of TCA by revealing insights into diverse datasets derived from artificial neural networks, large-scale calcium imaging of rodent prefrontal cortex during maze navigation, and multielectrode recordings of macaque motor cortex during brain machine interface learning. Williams et al. describe an unsupervised method to uncover simple structure in large-scale recordings by extracting distinct cell assemblies with rapid within-trial dynamics, reflecting interpretable aspects of perceptions, actions, and thoughts, and slower across-trial dynamics reflecting learning and internal state changes.},
	number = {6},
	journal = {Neuron},
	author = {Williams, Alex H. and Kim, Tony Hyun and Wang, Forea and Vyas, Saurabh and Ryu, Stephen I. and Shenoy, Krishna V. and Schnitzer, Mark and Kolda, Tamara G. and Ganguli, Surya},
	year = {2018},
	pmid = {29887338},
	note = {Publisher: Elsevier Inc.},
	keywords = {brain machine interfaces, dimensionality reduction, gain modulation, large-scale recordings, learning, motor control, navigation, neural data analysis, recurrent neural networks, single-trial analysis},
	pages = {1099--1115.e8},
}

@article{Kobyzev2020,
	title = {Normalizing {Flows}: {An} {Introduction} and {Review} of {Current} {Methods}},
	issn = {0162-8828},
	doi = {10.1109/tpami.2020.2992934},
	abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kobyzev, Ivan and Prince, Simon and Brubaker, Marcus},
	year = {2020},
	note = {arXiv: 1908.09257},
	pages = {1--1},
}

@article{Gamboa2014,
	title = {Sensitivity analysis for multidimensional and functional outputs},
	volume = {8},
	issn = {19357524},
	doi = {10.1214/14-EJS895},
	abstract = {Let X:=(X{\textless}inf{\textgreater}1{\textless}/inf{\textgreater},..., X{\textless}inf{\textgreater}p{\textless}/inf{\textgreater}) be random objects (the inputs), defined on some probability space (Ω, F, P) and valued in some measurable space E = E{\textless}inf{\textgreater}1{\textless}/inf{\textgreater}×...×E{\textless}inf{\textgreater}p{\textless}/inf{\textgreater}. Further, let Y:= Y =f(X{\textless}inf{\textgreater}1{\textless}/inf{\textgreater},..., X{\textless}inf{\textgreater}p{\textless}/inf{\textgreater}) be the output. Here, f is a measurable function from E to some Hilbert space H (H could be either of finite or infinite dimension). In this work, we give a natural generalization of the Sobol indices (that are classically defined when Y∈R), when the output belongs to H. These indices have very nice properties. First, they are invariant under isometry and scaling. Further they can be, as in dimension 1, easily estimated by using the so-called Pick and Freeze method. We investigate the asymptotic behaviour of such an estimation scheme.},
	number = {1},
	journal = {Electronic Journal of Statistics},
	author = {Gamboa, Fabrice and Janon, Alexandre and Klein, Thierry and Lagnoux, Agnès},
	year = {2014},
	note = {arXiv: 1311.1797},
	keywords = {Concentration inequalities, Quadratic functionals, Semi-parametric efficient estimation, Sensitiv- ity analysis, Sobol indices, Temporal output, Vector output, ★},
	pages = {575--603},
}

@techreport{MunSiew,
	title = {{DCAE}\_DMD model for temporal forecasting in space weather},
	abstract = {Based off of SIAM UQ 2022 presentation - this work focuses on using deep autoencoders in conjunction with basic DMD with control in order to make forecasts of thermospheric density ?? (double check).},
	author = {Mun Siew, Peng and Linares, Richard},
}

@article{Dubreuil2014,
	title = {Construction of bootstrap confidence intervals on sensitivity indices computed by polynomial chaos expansion},
	volume = {121},
	issn = {09518320},
	url = {http://dx.doi.org/10.1016/j.ress.2013.09.011},
	doi = {10.1016/j.ress.2013.09.011},
	abstract = {Sensitivity analysis aims at quantifying influence of input parameters dispersion on the output dispersion of a numerical model. When the model evaluation is time consuming, the computation of Sobol' indices based on Monte Carlo method is not applicable and a surrogate model has to be used. Among all approximation methods, polynomial chaos expansion is one of the most efficient to calculate variance-based sensitivity indices. Indeed, their computation is analytically derived from the expansion coefficients but without error estimators of the meta-model approximation. In order to evaluate the reliability of these indices, we propose to build confidence intervals by bootstrap re-sampling on the experimental design used to estimate the polynomial chaos approximation. Since the evaluation of the sensitivity indices is obtained with confidence intervals, it is possible to find a design of experiments allowing the computation of sensitivity indices with a given accuracy. © 2013 Elsevier Ltd.},
	journal = {Reliability Engineering and System Safety},
	author = {Dubreuil, S. and Berveiller, M. and Petitjean, F. and Salaün, M.},
	year = {2014},
	note = {Publisher: Elsevier},
	keywords = {Bootstrap re-sampling, Polynomial chaos expansion, Sensitivity analysis},
	pages = {263--275},
}

@article{Safta2021,
	title = {Low-{Rank} {Tensor} {Network} {Approximations} for {Earth} {System} {Model}.},
	doi = {10.2172/1854317},
	author = {Safta, Cosmin and Sargsyan, Khachik and Jakeman, John and Gorodetsky, Alex},
	year = {2021},
}

@article{Chami2021,
	title = {{HoroPCA}: {Hyperbolic} {Dimensionality} {Reduction} via {Horospherical} {Projections}},
	url = {http://arxiv.org/abs/2106.03306},
	abstract = {This paper studies Principal Component Analysis (PCA) for data lying in hyperbolic spaces. Given directions, PCA relies on: (1) a parameterization of subspaces spanned by these directions, (2) a method of projection onto subspaces that preserves information in these directions, and (3) an objective to optimize, namely the variance explained by projections. We generalize each of these concepts to the hyperbolic space and propose HoroPCA, a method for hyperbolic dimensionality reduction. By focusing on the core problem of extracting principal directions, HoroPCA theoretically better preserves information in the original data such as distances, compared to previous generalizations of PCA. Empirically, we validate that HoroPCA outperforms existing dimensionality reduction methods, significantly reducing error in distance preservation. As a data whitening method, it improves downstream classification by up to 3.9\% compared to methods that don't use whitening. Finally, we show that HoroPCA can be used to visualize hyperbolic data in two dimensions.},
	author = {Chami, Ines and Gu, Albert and Nguyen, Dat and Ré, Christopher},
	year = {2021},
	note = {arXiv: 2106.03306},
	keywords = {★},
	pages = {1--31},
}

@article{Dunlavy2011,
	title = {Temporal link prediction using matrix and tensor factorizations},
	volume = {5},
	issn = {15564681},
	doi = {10.1145/1921632.1921636},
	abstract = {The data in many disciplines such as social networks, Web analysis, etc. is link-based, and the link structure can be exploited for many different data mining tasks. In this article, we consider the problem of temporal link prediction: Given link data for times 1 through T, can we predict the links at time T +1? If our data has underlying periodic structure, can we predict out even further in time, i.e., links at time T + 2, T + 3, etc.? In this article, we consider bipartite graphs that evolve over time and consider matrix-and tensor-based methods for predicting future links. We present a weight-based method for collapsing multiyear data into a single matrix. We show how the well-known Katz method for link prediction can be extended to bipartite graphs and, moreover, approximated in a scalable way using a truncated singular value decomposition. Using a CANDECOMP/PARAFAC tensor decomposition of the data, we illustrate the usefulness of exploiting the natural three-dimensional structure of temporal link data. Through several numerical experiments, we demonstrate that both matrix-and tensor-based techniques are effective for temporal link prediction despite the inherent difficulty of the problem. Additionally, we show that tensor-based techniques are particularly effective for temporal data with varying periodic patterns. © 2011 ACM.},
	number = {2},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Dunlavy, Daniel M. and Kolda, Tamara G. and Acar, Evrim},
	year = {2011},
	note = {arXiv: 1005.4006},
	keywords = {CANDECOMP, Evolution, Link mining, Link prediction, PARAFAC, Tensor factorization},
	pages = {1--27},
}

@article{Balch2020,
	title = {Fired ({Fire} events delineation): {An} open, flexible algorithm and database of us fire events derived from the modis burned area product (2001–2019)},
	volume = {12},
	issn = {20724292},
	doi = {10.3390/rs12213498},
	abstract = {Harnessing the fire data revolution, i.e., the abundance of information from satellites, government records, social media, and human health sources, now requires complex and challenging data integration approaches. Defining fire events is key to that effort. In order to understand the spatial and temporal characteristics of fire, or the classic fire regime concept, we need to critically define fire events from remote sensing data. Events, fundamentally a geographic concept with delineated spatial and temporal boundaries around a specific phenomenon that is homogenous in some property, are key to understanding fire regimes and more importantly how they are changing. Here, we describe Fire Events Delineation (FIRED), an event-delineation algorithm, that has been used to derive fire events (N = 51,871) from the MODIS MCD64 burned area product for the coterminous US (CONUS) from January 2001 to May 2019. The optimized spatial and temporal parameters to cluster burned area pixels into events were an 11-day window and a 5-pixel (2315 m) distance, when optimized against 13,741 wildfire perimeters in the CONUS from the Monitoring Trends in Burn Severity record. The linear relationship between the size of individual FIRED and Monitoring Trends in Burn Severity (MTBS) events for the CONUS was strong (R2 = 0.92 for all events). Importantly, this algorithm is open-source and flexible, allowing the end user to modify the spatio-temporal threshold or even the underlying algorithm approach as they see fit. We expect the optimized criteria to vary across regions, based on regional distributions of fire event size and rate of spread. We describe the derived metrics provided in a new national database and how they can be used to better understand US fire regimes. The open, flexible FIRED algorithm could be utilized to derive events in any satellite product. We hope that this open science effort will help catalyze a community-driven, data-integration effort (termed OneFire) to build a more complete picture of fire.},
	number = {21},
	journal = {Remote Sensing},
	author = {Balch, Jennifer K. and St. Denis, Lise A. and Mahood, Adam L. and Mietkiewicz, Nathan P. and Williams, Travis M. and McGlinchy, Joe and Cook, Maxwell C.},
	year = {2020},
	keywords = {Data harmonization, Event-builder algorithm, Fire regimes, Open fire science, Satellite fire detections},
	pages = {1--18},
}

@article{Saltelli2010,
	title = {Variance based sensitivity analysis of model output. {Design} and estimator for the total sensitivity index},
	volume = {181},
	issn = {00104655},
	url = {http://dx.doi.org/10.1016/j.cpc.2009.09.018},
	doi = {10.1016/j.cpc.2009.09.018},
	abstract = {Variance based methods have assessed themselves as versatile and effective among the various available techniques for sensitivity analysis of model output. Practitioners can in principle describe the sensitivity pattern of a model Y = f (X1, X2, ..., Xk) with k uncertain input factors via a full decomposition of the variance V of Y into terms depending on the factors and their interactions. More often practitioners are satisfied with computing just k first order effects and k total effects, the latter describing synthetically interactions among input factors. In sensitivity analysis a key concern is the computational cost of the analysis, defined in terms of number of evaluations of f (X1, X2, ..., Xk) needed to complete the analysis, as f (X1, X2, ..., Xk) is often in the form of a numerical model which may take long processing time. While the computational cost is relatively cheap and weakly dependent on k for estimating first order effects, it remains expensive and strictly k-dependent for total effect indices. In the present note we compare existing and new practices for this index and offer recommendations on which to use. © 2009 Elsevier B.V. All rights reserved.},
	number = {2},
	journal = {Computer Physics Communications},
	author = {Saltelli, Andrea and Annoni, Paola and Azzini, Ivano and Campolongo, Francesca and Ratto, Marco and Tarantola, Stefano},
	year = {2010},
	note = {Publisher: Elsevier B.V.},
	keywords = {★},
	pages = {259--270},
}

@article{Ba2018,
	title = {A {Sequential} {Maximum} {Projection} {Design} {Framework} for {Computer} {Experiments}},
	volume = {28},
	url = {https://www.jstor.org/stable/44841929},
	doi = {10.5705/ss.202016.0165},
	abstract = {Many computer experiments involve a large number of input factors, but many of them are inert and only a subset are important. This paper develops a new sequential design framework that can accommodate multiple responses and quickly screen out inert factors so that the final design is space-filling with respect to the active factors. By folding over Latin hypercube designs with sliced structure, this sequential design can have flexible sample size in each stage and also ensure that each stage, as well as the whole combined design, are all approximately Latin hypercube designs. The sequential framework does not require prescribing the total sample size and, under the presence of inert factors, can lead to substantial savings in simulation resources. Even if all factors are important, the proposed sequential design can still achieve a similar overall space-filling property compared to a maximin Latin hypercube design optimized in a single stage.},
	number = {2},
	journal = {Statistica Sinica},
	author = {Ba, Shan and Myers, William and Wang, Dianpeng},
	year = {2018},
}

@article{Blatman2010,
	title = {Efficient computation of global sensitivity indices using sparse polynomial chaos expansions},
	volume = {95},
	issn = {09518320},
	doi = {10.1016/j.ress.2010.06.015},
	abstract = {Global sensitivity analysis aims at quantifying the relative importance of uncertain input variables onto the response of a mathematical model of a physical system. ANOVA-based indices such as the Sobol' indices are well-known in this context. These indices are usually computed by direct Monte Carlo or quasi-Monte Carlo simulation, which may reveal hardly applicable for computationally demanding industrial models. In the present paper, sparse polynomial chaos (PC) expansions are introduced in order to compute sensitivity indices. An adaptive algorithm allows the analyst to build up a PC-based metamodel that only contains the significant terms whereas the PC coefficients are computed by least-square regression using a computer experimental design. The accuracy of the metamodel is assessed by leave-one-out cross validation. Due to the genuine orthogonality properties of the PC basis, ANOVA-based sensitivity indices are post-processed analytically. This paper also develops a bootstrap technique which eventually yields confidence intervals on the results. The approach is illustrated on various application examples up to 21 stochastic dimensions. Accurate results are obtained at a computational cost 23 orders of magnitude smaller than that associated with Monte Carlo simulation. © 2010 Elsevier Ltd. All rights reserved.},
	number = {11},
	journal = {Reliability Engineering and System Safety},
	author = {Blatman, Graud and Sudret, Bruno},
	year = {2010},
	keywords = {ANOVA, Global sensitivity analysis, Sequential experimental design, Sobol' indices, Sparse polynomial chaos, Stepwise regression, ★},
	pages = {1216--1229},
}

@article{Fleeter2020,
	title = {Multilevel and multifidelity uncertainty quantification for cardiovascular hemodynamics},
	volume = {365},
	issn = {00457825},
	url = {https://doi.org/10.1016/j.cma.2020.113030},
	doi = {10.1016/j.cma.2020.113030},
	abstract = {Standard approaches for uncertainty quantification in cardiovascular modeling pose challenges due to the large number of uncertain inputs and the significant computational cost of realistic three-dimensional simulations. We propose an efficient uncertainty quantification framework utilizing a multilevel multifidelity Monte Carlo (MLMF) estimator to improve the accuracy of hemodynamic quantities of interest while maintaining reasonable computational cost. This is achieved by leveraging three cardiovascular model fidelities, each with varying spatial resolution to rigorously quantify the variability in hemodynamic outputs. We employ two low-fidelity models (zero- and one-dimensional) to construct several different estimators. Our goal is to investigate and compare the efficiency of estimators built from combinations of these two low-fidelity model alternatives and our high-fidelity three-dimensional models. We demonstrate this framework on healthy and diseased models of aortic and coronary anatomy, including uncertainties in material property and boundary condition parameters. Our goal is to demonstrate that for this application it is possible to accelerate the convergence of the estimators by utilizing a MLMF paradigm. Therefore, we compare our approach to single fidelity Monte Carlo estimators and to a multilevel Monte Carlo approach based only on three-dimensional simulations, but leveraging multiple spatial resolutions. We demonstrate significant, on the order of 10 to 100 times, reduction in total computational cost with the MLMF estimators. We also examine the differing properties of the MLMF estimators in healthy versus diseased models, as well as global versus local quantities of interest. As expected, global quantities such as outlet pressure and flow show larger reductions than local quantities, such as those relating to wall shear stress, as the latter rely more heavily on the highest fidelity model evaluations. Similarly, healthy models show larger reductions than diseased models. In all cases, our workflow coupling Dakota's MLMF estimators with the SimVascular cardiovascular modeling framework makes uncertainty quantification feasible for constrained computational budgets.},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Fleeter, Casey M. and Geraci, Gianluca and Schiavazzi, Daniele E. and Kahn, Andrew M. and Marsden, Alison L.},
	year = {2020},
	note = {arXiv: 1908.04875
Publisher: Elsevier B.V.},
	keywords = {Cardiovascular modeling, Multifidelity Monte Carlo, Multilevel Monte Carlo, Multilevel multifidelity Monte Carlo, Uncertainty quantification, ★},
	pages = {113030},
}

@article{Cui2016,
	title = {Scalable posterior approximations for large-scale {Bayesian} inverse problems via likelihood-informed parameter and state reduction},
	volume = {315},
	issn = {10902716},
	doi = {10.1016/j.jcp.2016.03.055},
	abstract = {Two major bottlenecks to the solution of large-scale Bayesian inverse problems are the scaling of posterior sampling algorithms to high-dimensional parameter spaces and the computational cost of forward model evaluations. Yet incomplete or noisy data, the state variation and parameter dependence of the forward model, and correlations in the prior collectively provide useful structure that can be exploited for dimension reduction in this setting-both in the parameter space of the inverse problem and in the state space of the forward model. To this end, we show how to jointly construct low-dimensional subspaces of the parameter space and the state space in order to accelerate the Bayesian solution of the inverse problem. As a byproduct of state dimension reduction, we also show how to identify low-dimensional subspaces of the data in problems with high-dimensional observations. These subspaces enable approximation of the posterior as a product of two factors: (i) a projection of the posterior onto a low-dimensional parameter subspace, wherein the original likelihood is replaced by an approximation involving a reduced model; and (ii) the marginal prior distribution on the high-dimensional complement of the parameter subspace. We present and compare several strategies for constructing these subspaces using only a limited number of forward and adjoint model simulations. The resulting posterior approximations can rapidly be characterized using standard sampling techniques, e.g., Markov chain Monte Carlo. Two numerical examples demonstrate the accuracy and efficiency of our approach: inversion of an integral equation in atmospheric remote sensing, where the data dimension is very high; and the inference of a heterogeneous transmissivity field in a groundwater system, which involves a partial differential equation forward model with high dimensional state and parameters.},
	journal = {Journal of Computational Physics},
	author = {Cui, Tiangang and Marzouk, Youssef and Willcox, Karen},
	year = {2016},
	note = {arXiv: 1510.06053},
	keywords = {Bayesian inference, Dimension reduction, Inverse problems, Low-rank approximation, Markov chain Monte Carlo, Model reduction},
	pages = {363--387},
}

@book{Saltelli2008,
	title = {Global {Sensitivity} {Analysis}: {The} {Primer} by {Andrea} {Saltelli}, {Marco} {Ratto}, {Terry} {Andres}, {Francesca} {Campolongo}, {Jessica} {Cariboni}, {Debora} {Gatelli}, {Michaela} {Saisana}, {Stefano} {Tarantola}},
	volume = {76},
	isbn = {978-0-470-05997-5},
	abstract = {Complex mathematical and computational models are used in all areas of society and technology and yet model based science is increasingly contested or refuted, especially when models are applied to controversial themes in domains such as health, the environment or the economy. More stringent standards of proofs are demanded from model-based numbers, especially when these numbers represent potential financial losses, threats to human health or the state of the environment. Quantitative sensitivity analysis is generally agreed to be one such standard. Mathematical models are good at mapping assumptions into inferences. A modeller makes assumptions about laws pertaining to the system, about its status and a plethora of other, often arcane, system variables and internal model settings. To what extent can we rely on the model-based inference when most of these assumptions are fraught with uncertainties? Global Sensitivity Analysis offers an accessible treatment of such problems via quantitative sensitivity analysis, beginning with the first principles and guiding the reader through the full range of recommended practices with a rich set of solved exercises. The text explains the motivation for sensitivity analysis, reviews the required statistical concepts, and provides a guide to potential applications. The book: Provides a self-contained treatment of the subject, allowing readers to learn and practice global sensitivity analysis without further materials. Presents ways to frame the analysis, interpret its results, and avoid potential pitfalls. Features numerous exercises and solved problems to help illustrate the applications. Is authored by leading sensitivity analysis practitioners, combining a range of disciplinary backgrounds. Postgraduate students and practitioners in a wide range of subjects, including statistics, mathematics, engineering, physics, chemistry, environmental sciences, biology, toxicology, actuarial sciences, and econometrics will find much of use here. This book will prove equally valuable to engineers working on risk analysis and to financial analysts concerned with pricing and hedging.},
	author = {Saltelli, Andrea},
	year = {2008},
	doi = {10.1111/j.1751-5823.2008.00062_17.x},
	note = {Publication Title: International Statistical Review
Issue: 3
ISSN: 03067734},
}

@misc{Abramowitz1970,
	title = {Abramowitz\_and\_Stegun.{Pdf}},
	author = {Abramowitz, Milton and Irene, Stegun},
	year = {1970},
	note = {Pages: 470},
	keywords = {★},
}

@article{Foster2021,
	title = {Deep {Adaptive} {Design}: {Amortizing} {Sequential} {Bayesian} {Experimental} {Design}},
	url = {http://arxiv.org/abs/2103.02438},
	abstract = {We introduce Deep Adaptive Design (DAD), a general method for amortizing the cost of performing sequential adaptive experiments using the framework of Bayesian optimal experimental design (BOED). Traditional sequential BOED approaches require substantial computational time at each stage of the experiment. This makes them unsuitable for most real-world applications, where decisions must typically be made quickly. DAD addresses this restriction by learning an amortized design network upfront and then using this to rapidly run (multiple) adaptive experiments at deployment time. This network takes as input the data from previous steps, and outputs the next design using a single forward pass; these design decisions can be made in milliseconds during the live experiment. To train the network, we introduce contrastive information bounds that are suitable objectives for the sequential setting, and propose a customized network architecture that exploits key symmetries. We demonstrate that DAD successfully amortizes the process of experimental design, outperforming alternative strategies on a number of problems.},
	author = {Foster, Adam and Ivanova, Desi R. and Malik, Ilyas and Rainforth, Tom},
	year = {2021},
	note = {arXiv: 2103.02438},
	keywords = {★},
}

@article{Kossen2021,
	title = {Self-{Attention} {Between} {Datapoints}: {Going} {Beyond} {Individual} {Input}-{Output} {Pairs} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2106.02584},
	abstract = {We challenge a common assumption underlying most supervised deep learning: that a model makes a prediction depending only on its parameters and the features of a single input. To this end, we introduce a general-purpose deep learning architecture that takes as input the entire dataset instead of processing one datapoint at a time. Our approach uses self-attention to reason about relationships between datapoints explicitly, which can be seen as realizing non-parametric models using parametric attention mechanisms. However, unlike conventional non-parametric models, we let the model learn end-to-end from the data how to make use of other datapoints for prediction. Empirically, our models solve cross-datapoint lookup and complex reasoning tasks unsolvable by traditional deep learning models. We show highly competitive results on tabular data, early results on CIFAR-10, and give insight into how the model makes use of the interactions between points.},
	author = {Kossen, Jannik and Band, Neil and Lyle, Clare and Gomez, Aidan N. and Rainforth, Tom and Gal, Yarin},
	year = {2021},
	note = {arXiv: 2106.02584},
}

@article{OLeary-Roseberry2020,
	title = {Derivative-{Informed} {Projected} {Neural} {Networks} for {High}-{Dimensional} {Parametric} {Maps} {Governed} by {PDEs}},
	url = {http://arxiv.org/abs/2011.15110},
	abstract = {Many-query problems, arising from uncertainty quantification, Bayesian inversion, Bayesian optimal experimental design, and optimization under uncertainty-require numerous evaluations of a parameter-to-output map. These evaluations become prohibitive if this parametric map is high-dimensional and involves expensive solution of partial differential equations (PDEs). To tackle this challenge, we propose to construct surrogates for high-dimensional PDE-governed parametric maps in the form of projected neural networks that parsimoniously capture the geometry and intrinsic low-dimensionality of these maps. Specifically, we compute Jacobians of these PDE-based maps, and project the high-dimensional parameters onto a low-dimensional derivative-informed active subspace; we also project the possibly high-dimensional outputs onto their principal subspace. This exploits the fact that many high-dimensional PDE-governed parametric maps can be well-approximated in low-dimensional parameter and output subspace. We use the projection basis vectors in the active subspace as well as the principal output subspace to construct the weights for the first and last layers of the neural network, respectively. This frees us to train the weights in only the low-dimensional layers of the neural network. The architecture of the resulting neural network captures to first order, the low-dimensional structure and geometry of the parametric map. We demonstrate that the proposed projected neural network achieves greater generalization accuracy than a full neural network, especially in the limited training data regime afforded by expensive PDE-based parametric maps. Moreover, we show that the number of degrees of freedom of the inner layers of the projected network is independent of the parameter and output dimensions, and high accuracy can be achieved with weight dimension independent of the discretization dimension.},
	author = {O'Leary-Roseberry, Thomas and Villa, Umberto and Chen, Peng and Ghattas, Omar},
	year = {2020},
	note = {arXiv: 2011.15110},
	keywords = {49m41, 65c20, 93e20, 93e35, active subspace, adjoint-based, ams subject classifications, deep learning, derivative-informed dimen-, hessian, neural networks, parametrized pdes, proper orthogonal decomposition, sensitivity, sion reduction, surrogate modeling, uncertainty quantification},
}

@article{Buluc2021,
	title = {Randomized {Algorithms} for {Scientific} {Computing} ({RASC})},
	url = {http://arxiv.org/abs/2104.11079},
	abstract = {Randomized algorithms have propelled advances in artificial intelligence and represent a foundational research area in advancing AI for Science. Future advancements in DOE Office of Science priority areas such as climate science, astrophysics, fusion, advanced materials, combustion, and quantum computing all require randomized algorithms for surmounting challenges of complexity, robustness, and scalability. This report summarizes the outcomes of that workshop, "Randomized Algorithms for Scientific Computing (RASC)," held virtually across four days in December 2020 and January 2021.},
	author = {Buluc, Aydin and Kolda, Tamara G. and Wild, Stefan M. and Anitescu, Mihai and DeGennaro, Anthony and Jakeman, John and Kamath, Chandrika and {Ramakrishnan} and {Kannan} and Lopes, Miles E. and Martinsson, Per-Gunnar and Myers, Kary and Nelson, Jelani and Restrepo, Juan M. and Seshadhri, C. and Vrabie, Draguna and Wohlberg, Brendt and Wright, Stephen J. and Yang, Chao and Zwart, Peter},
	year = {2021},
	note = {arXiv: 2104.11079},
	keywords = {★},
}

@article{Pai2018,
	title = {Patient {Similarity} {Networks} for {Precision} {Medicine}},
	volume = {430},
	issn = {10898638},
	url = {https://doi.org/10.1016/j.jmb.2018.05.037},
	doi = {10.1016/j.jmb.2018.05.037},
	abstract = {Clinical research and practice in the 21st century is poised to be transformed by analysis of computable electronic medical records and population-level genome-scale patient profiles. Genomic data capture genetic and environmental state, providing information on heterogeneity in disease and treatment outcome, but genomic-based clinical risk scores are limited. Achieving the goal of routine precision medicine that takes advantage of these rich genomics data will require computational methods that support heterogeneous data, have excellent predictive performance, and ideally, provide biologically interpretable results. Traditional machine-learning approaches excel at performance, but often have limited interpretability. Patient similarity networks are an emerging paradigm for precision medicine, in which patients are clustered or classified based on their similarities in various features, including genomic profiles. This strategy is analogous to standard medical diagnosis, has excellent performance, is interpretable, and can preserve patient privacy. We review new methods based on patient similarity networks, including Similarity Network Fusion for patient clustering and netDx for patient classification. While these methods are already useful, much work is required to improve their scalability for contemporary genetic cohorts, optimize parameters, and incorporate a wide range of genomics and clinical data. The coming 5 years will provide an opportunity to assess the utility of network-based algorithms for precision medicine.},
	number = {18},
	journal = {Journal of Molecular Biology},
	author = {Pai, Shraddha and Bader, Gary D.},
	year = {2018},
	pmid = {29860027},
	note = {Publisher: The Authors},
	keywords = {genomics, machine learning, networks, patient classifier, precision medicine},
	pages = {2924--2938},
}

@article{Romano2019,
	title = {Conformalized quantile regression},
	issn = {23318422},
	abstract = {Conformal prediction is a technique for constructing prediction intervals that attain valid coverage in finite samples, without making distributional assumptions. Despite this appeal, existing conformal methods can be unnecessarily conservative because they form intervals of constant or weakly varying length across the input space. In this paper we propose a new method that is fully adaptive to heteroscedasticity. It combines conformal prediction with classical quantile regression, inheriting the advantages of both. We establish a theoretical guarantee of valid coverage, supplemented by extensive experiments on popular regression datasets. We compare the efficiency of conformalized quantile regression to other conformal methods, showing that our method tends to produce shorter intervals.},
	journal = {arXiv},
	author = {Romano, Yaniv and Patterson, Evan and Candès, Emmanuel J.},
	year = {2019},
	note = {arXiv: 1905.03222},
}

@article{Ruby2021,
	title = {High-energy-density-physics measurements in implosions using {Bayesian} inference},
	volume = {28},
	issn = {10897674},
	doi = {10.1063/5.0040616},
	abstract = {Convergent high-energy-density (HED) experimental platforms are used to study matter under some of the most extreme conditions that can be produced on Earth, comparable to the interior of stars. There are many challenges in using these systems for fundamental measurements currently being addressed by new analysis methods, such as the combination of a reduced physics model and Bayesian inference, allowing a self-consistent inference of physical quantities with a robust error analysis. These methods in combination with simple (as compared to inertial confinement fusion implosions) implosion platforms, which can be modified to show sensitivity to different physical mechanisms of interest, are used to study the physical properties of matter under extreme conditions. This work discusses a subset of implosion targets for studying opacity effects, electron-ion equilibration, and thermal conductivity and, as an example, a system consisting of a thick-shelled, gas-filled laser-direct-drive implosion is used to show how a reduced model and Bayesian inference can help inform experimental design decisions such as diagnostic choice. It is shown that for this system that a combination of neutron and X-ray self-emission diagnostics is critical for constraining the details of the thermodynamic states in the system and that the conductivity exponent in a Spitzer like framework can be constrained to the 30\% level in deuterium at gigabar conditions. This process can be applied to many HED systems to make underlying model assumptions explicit and facilitate experimental design and analysis.},
	number = {3},
	journal = {Physics of Plasmas},
	author = {Ruby, J. J. and Gaffney, J. A. and Rygg, J. R. and Ping, Y. and Collins, G. W.},
	year = {2021},
	note = {Publisher: AIP Publishing LLC},
}

@article{Hamilton2020,
	title = {Graph {Representation} {Learning} {Hamilton}},
	volume = {14},
	issn = {19394616},
	doi = {10.2200/S01045ED1V01Y202009AIM046},
	abstract = {Graph-structured data is ubiquitous throughout the natural and social sciences, from telecommunication networks to quantum chemistry. Building relational inductive biases into deep learning architectures is crucial for creating systems that can learn, reason, and generalize from this kind of data. Recent years have seen a surge in research on graph representation learning, including techniques for deep graph embeddings, generalizations of convolutional neural networks to graph-structured data, and neural message-passing approaches inspired by belief propagation. These advances in graph representation learning have led to new state-of-the-art results in numerous domains, including chemical synthesis, 3D vision, recommender systems, question answering, and social network analysis. This book provides a synthesis and overview of graph representation learning. It begins with a discussion of the goals of graph representation learning as well as key methodological foundations in graph theory and network analysis. Following this, the book introduces and reviews methods for learning node embeddings, including random-walk-based methods and applications to knowledge graphs. It then provides a technical synthesis and introduction to the highly successful graph neural network (GNN) formalism, which has become a dominant and fast-growing paradigm for deep learning with graph data. The book concludes with a synthesis of recent advancements in deep generative models for graphs a nascent but quickly growing subset of graph representation learning.},
	number = {3},
	journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	author = {Hamilton, William L.},
	year = {2020},
	keywords = {deep learning, geometric deep learning, graph convolutions, graph embeddings, graph neural networks, graph signal processing, knowledge graphs, network analysis, node embeddings, relational data, social networks, spectral graph theory},
	pages = {1--159},
}

@article{Aliabouzar2020,
	title = {Standing wave-assisted acoustic droplet vaporization for single and dual payload release in acoustically-responsive scaffolds},
	volume = {66},
	issn = {18732828},
	doi = {10.1016/j.ultsonch.2020.105109},
	abstract = {© 2020 Elsevier B.V. An ultrasound standing wave field (SWF) has been utilized in many biomedical applications. Here, we demonstrate how a SWF can enhance drug release using acoustic droplet vaporization (ADV) in an acoustically-responsive scaffold (ARS). ARSs are composite fibrin hydrogels containing payload-carrying, monodispersed perfluorocarbon (PFC) emulsions and have been used to stimulate regenerative processes such as angiogenesis. Elevated amplitudes in the SWF significantly enhanced payload release from ARSs containing dextran-loaded emulsions (nominal diameter: 6 μm) compared to the -SWF condition, both at sub- and suprathreshold excitation pressures. At 2.5 MHz and 4 MPa peak rarefactional pressure, the cumulative percentage of payload released from ARSs reached 84.1 ± 5.4\% and 66.1 ± 4.4\% under + SWF and -SWF conditions, respectively, on day 10. A strategy for generating a SWF for an in situ ARS is also presented. For dual-payload release studies, bi-layer ARSs containing a different payload within each layer were exposed to temporally staggered ADV at 3.25 MHz (day 0) and 8.6 MHz (day 4). Sequential payload release was demonstrated using dextran payloads as well as two growth factors relevant to angiogenesis: basic fibroblast growth factor (bFGF) and platelet-derived growth factor BB (PDGF-BB). In addition, bubble growth and fibrin degradation were characterized in the ARSs under +SWF and -SWF conditions. These results highlight the utility of a SWF for modulating single and dual payload release from an ARS and can be used in future therapeutic studies.},
	journal = {Ultrasonics Sonochemistry},
	author = {Aliabouzar, M. and Jivani, A. and Lu, X. and Kripfgans, O.D. and Fowlkes, J.B. and Fabiilli, M.L.},
	year = {2020},
	keywords = {Acoustic droplet vaporization, Controlled release, Fibrin, Growth factors, Perfluorocarbon, Standing waves, Ultrasound},
}

@article{Economon2016a,
	title = {{SU2}: {An} open-source suite for multiphysics simulation and design},
	volume = {54},
	issn = {00011452},
	doi = {10.2514/1.J053813},
	abstract = {This paper presents the main objectives and a description of the SU2 suite, including the novel software architecture and open-source software engineering strategy. SU2 is a computational analysis and design package that has been developed to solve multiphysics analysis and optimization tasks using unstructured mesh topologies. Its unique architecture iswell suited for extensibility to treat partial-differential-equation-based problems not initially envisioned. The common framework adopted enables the rapid implementation of newphysics packages that can be tightly coupled to form a powerful ensemble of analysis tools to address complex problems facing many engineering communities. The framework is demonstrated on a number, solving both the flow and adjoint systems of equations to provide a highfidelity predictive capability and sensitivity information that can be used for optimal shape design using a gradientbased framework, goal-oriented adaptive mesh refinement, or uncertainty quantification.},
	number = {3},
	journal = {AIAA Journal},
	author = {Economon, Thomas D. and Palacios, Francisco and Copeland, Sean R. and Lukaczyk, Trent W. and Alonso, Juan J.},
	year = {2016},
	pages = {828--846},
}

@article{Debusschere2005a,
	title = {Numerical challenges in the use of polynomial chaos representations for stochastic processes},
	volume = {26},
	issn = {10648275},
	doi = {10.1137/S1064827503427741},
	abstract = {This paper gives an overview of the use of polynomial chaos (PC) expansions to represent stochastic processes in numerical simulations. Several methods are presented for performing arithmetic on, as well as for evaluating polynomial and nonpolynomial functions of variables represented by PC expansions. These methods include Taylor series, a newly developed integration method, as well as a sampling-based spectral projection method for nonpolynomial function evaluations. A detailed analysis of the accuracy of the PC representations, and of the different methods for nonpolynomial function evaluations, is performed. It is found that the integration method offers a robust and accurate approach for evaluating nonpolynomial functions, even when very high-order information is present in the PC expansions. © 2004 Society for Industrial and Applied Mathematics.},
	number = {2},
	journal = {SIAM Journal on Scientific Computing},
	author = {Debusschere, Bert J. and Najm, Habib N. and Pébayt, Philippe P. and Knio, Omar M. and Ghanem, Roger G. and Le Maître, Olivier P.},
	year = {2005},
	keywords = {Polynomial chaos, Spectral uncertainty quantification, Stochastic},
	pages = {698--719},
}

@article{Radonic2000,
	title = {Popliteal artery entrapment syndrome: {Diagnosis} and management, with report of three cases},
	volume = {27},
	issn = {07302347},
	abstract = {Popliteal artery entrapment syndrome is an important albeit infrequent cause of serious disability among young adults and athletes with anomalous anatomic relationships between the popliteal artery and surrounding musculotendinous structures. We report our experience with 3 patients, in whom we used duplex ultrasonography, computed tomography, digital subtraction angiography, and conventional arteriography to diagnose popliteal artery entrapment and to grade the severity of dynamic circulatory insufficiency and arterial damage. We used a posterior surgical approach to give the best view of the anatomic structures compressing the popliteal artery. In 2 patients, in whom compression had not yet damaged the arterial wall, operative decompression of the artery by resection of the aberrant muscle was su fficient. In the 3rd patient, operative reconstruction of an occluded segment with autologous vein graft was necessary, in addition to decompression of the vessel and resection of aberrant muscle. The result in each case was complete recovery, with absence of symptoms and with patency verified by Doppler examination. We conclude that clinicians who encounter young patients with progressive lowerlimb arterial insufficiency should be aware of the possibility of popliteal artery entrapment. Early diagnosis through a combined approach (careful physical examination and history-taking, duplex ultrasonography, computerized tomography, and angiography) is necessary for exact diagnosis. The treatment of choice is the surgical creation of normal anatomy within the popliteal fossa.},
	number = {1},
	journal = {Texas Heart Institute Journal},
	author = {Radonić, Vedran and Koplić, Stevan and Giunio, Lovel and Božić, Ivo and Mašković, Josip and Buća, Ante},
	year = {2000},
	keywords = {Angiography, digital subtraction, Popliteal artery entrapment syndrome, Popliteal artery/surgery, Tomography, x-ray computed, Ultrasonography, Doppler, duplex, ★},
	pages = {3--13},
}

@article{Borgonovo2016,
	title = {Sensitivity analysis: {A} review of recent advances},
	volume = {248},
	issn = {03772217},
	url = {http://dx.doi.org/10.1016/j.ejor.2015.06.032},
	doi = {10.1016/j.ejor.2015.06.032},
	abstract = {The solution of several operations research problems requires the creation of a quantitative model. Sensitivity analysis is a crucial step in the model building and result communication process. Through sensitivity analysis we gain essential insights on model behavior, on its structure and on its response to changes in the model inputs. Several interrogations are possible and several sensitivity analysis methods have been developed, giving rise to a vast and growing literature. We present an overview of available methods, structuring them into local and global methods. For local methods, we discuss Tornado diagrams, one way sensitivity functions, differentiation-based methods and scenario decomposition through finite change sensitivity indices, providing a unified view of the associated sensitivity measures. We then analyze global sensitivity methods, first discussing screening methods such as sequential bifurcation and the Morris method. We then address variance-based, moment-independent and value of information-based sensitivity methods. We discuss their formalization in a common rationale and present recent results that permit the estimation of global sensitivity measures by post-processing the sample generated by a traditional Monte Carlo simulation. We then investigate in detail the methodological issues concerning the crucial step of correctly interpreting the results of a sensitivity analysis. A classical example is worked out to illustrate some of the approaches.},
	number = {3},
	journal = {European Journal of Operational Research},
	author = {Borgonovo, Emanuele and Plischke, Elmar},
	year = {2016},
	note = {Publisher: Elsevier Ltd.},
	keywords = {Computer experiments, Sensitivity analysis, Simulation},
	pages = {869--887},
}

@article{Sobol2001,
	title = {Global sensitivity indices for nonlinear mathematical models and their {Monte} {Carlo} estimates},
	volume = {55},
	issn = {03784754},
	doi = {10.1016/S0378-4754(00)00270-6},
	number = {1-3},
	journal = {Mathematics and Computers in Simulation},
	author = {Sobol, I. M.},
	year = {2001},
	keywords = {Mathematical modelling, Monte Carlo method, Quasi-Monte Carlo method, Sensitivity analysis, ★},
	pages = {271--280},
}

@article{Sobol2009,
	title = {Derivative based global sensitivity measures and their link with global sensitivity indices},
	volume = {79},
	issn = {03784754},
	doi = {10.1016/j.matcom.2009.01.023},
	abstract = {A model function f(x1,...,xn) defined in the unit hypercube Hn with Lebesque measure dx = dx1...dxn is considered. If the function is square integrable, global sensitivity indices provide adequate estimates for the influence of individual factors xi or groups of such factors. Alternative estimators that require less computer time can also be used. If the function f is differentiable, functionals depending on ∂f/∂xi have been suggested as estimators for the influence of xi. The Morris importance measure modified by Campolongo, Cariboni and Saltelli μ* is an approximation of the functional μi = ∫Hn fenced(∂ f / ∂ xi) d x. In this paper a similar functional is studiedνi = ∫Hn fenced(frac(∂ f, ∂ xi))2 d xEvidently, μi ≤ sqrt(νi), and νi ≤ C μi if fenced(∂ f / ∂ xi) ≤ C. A link between νi and the sensitivity index Sit o t is established:Sit o t ≤ frac(νi, π2 D)where D is the total variance of f(x1,...,xn). Thus small νi imply small Sit o t, and unessential factors xi (that is xi corresponding to a very small Sit o t) can be detected analyzing computed values ν1,...,νn. However, ranking influential factors xi using these values can give false conclusions. Generalized Sit o t and νi can be applied in situations where the factors x1,...,xn are independent random variables. If xi is a normal random variable with variance σi2, then Sit o t ≤ νi σi2 / D. © 2009 IMACS.},
	number = {10},
	journal = {Mathematics and Computers in Simulation},
	author = {Sobol', I. M. and Kucherenko, S.},
	year = {2009},
	keywords = {Derivative based global sensitivity measure, Global sensitivity index, Morris method, Quasi Monte Carlo method},
	pages = {3009--3017},
}

@article{Shibeshi2005,
	title = {The {Rheology} of {Blood} {Flow} in a {Branched} {Arterial} {System}.},
	volume = {15},
	issn = {1617-8106 (Print)},
	doi = {10.1901/jaba.2005.15-398},
	abstract = {Blood flow rheology is a complex phenomenon. Presently there is no universally agreed upon model to represent the viscous property of blood. However, under the general classification of non-Newtonian models that simulate blood behavior to different degrees of accuracy, there are many variants. The power law, Casson and Carreau models are popular non-Newtonian models and affect hemodynamics quantities under many conditions. In this study, the finite volume method is used to investigate hemodynamics predictions of each of the models. To implement the finite volume method, the computational fluid dynamics software Fluent 6.1 is used. In this numerical study the different hemorheological models are found to predict different results of hemodynamics variables which are known to impact the genesis of atherosclerosis and formation of thrombosis. The axial velocity magnitude percentage difference of up to 2 \% and radial velocity difference up to 90 \% is found at different sections of the T-junction geometry. The size of flow recirculation zones and their associated separation and reattachment point's locations differ for each model. The wall shear stress also experiences up to 12 \% shift in the main tube. A velocity magnitude distribution of the grid cells shows that the Newtonian model is close dynamically to the Casson model while the power law model resembles the Carreau model. ZUSAMMENFASSUNG: Die Rheologie von Blutströmungen ist ein komplexes Phänomen. Gegenwärtig existiert kein allgemein akzeptiertes Modell, um die viskosen Eigenschaften von Blut wiederzugeben. Jedoch gibt es mehrere Varianten unter der allgemeinen Klassifikation von nicht-Newtonschen Modellen, die das Verhalten von Blut mit unterschiedlicher Genauigkeit simulieren. Die Potenzgesetz-, Casson und Carreau-Modelle sind beliebte nicht-New-tonsche Modelle und beeinflussen die hämodynamischen Eigenschaften in vielen Situationen. In dieser Studie wurde die finite Volumenmethode angewandt, um die hämodynamischen Vorhersagen dieser Modelle zu untersuchen. Um die finite Volumenmethode zu implementieren, wurde die Fluiddynamiksoftware Fluent 6.1 verwendet. In dieser numerischen Studie wurde gefunden, dass die unterschiedlichen hämorheologischen Modelle unterschiedliche Resultate für die hämodynamischen Grössen vorhersagen, von denen bekannt ist, dass sie die Entstehung von Arteriosklerose und die Bildung von Thrombose beeinflussen. Es wurde gefunden, dass die relative Differenz der axialen Geschwindigkeit bis zu 2\% und die der radialen Geschwindigkeit bis zu 90\% in unterschiedlichen Abschnitten der T-Verbindung beträgt. Die Grösse der Strömungszirkulationszonen und ihrer dazugehörigen Trennungs- und Vereinigungspunkte differieren für jedes Modell. Die Scherspannung an der Wand erfährt ebenfalls eine Verschiebung im Hauptrohr von bis zu 12\%. Der Verlauf der Geschwindigkeit auf den Gitterzellen zeigt, dass das Newtonsche Modell mit Bezug auf die Dynamik dem Casson-Modell nahe ist, während das Potenzgesetzmodell dem Carreau-Modell ähnlich ist. R\#ENTITYSTARTX000E9;SUM\#ENTITYSTARTX000E9;: La rhéologie de l'écoulement sanguin est un phénomène complexe. Présentement, il n'y a pas de consensus universel sur le modèle qui représente la propriété visqueuse du sang. Cependant, parmi la classification générale des modèles non-Newtoniens qui simulent le comportement du sang avec différents degrés de précision, il y a plusieurs différences. Les lois de puissance, les modèles de Casson et Carreau sont des modèles non-Newtoniens populaires et ont un effet sur les quantités hémodynamiques sous plusieurs conditions. Dans cette étude, la méthode de volume fini est utilisée pour explorer les prédictions hémodynamiques de chacun de ces modèles. Pour implémenter la méthode de volume fini, le logiciel de calcul de dynamique des fluides Fluent 6.1 a été utilisé. Dans cette étude numérique, les différents modèles hémorhéologiques tendent à prédire des résultats différents pour les variables hémodynamiques qui sont reconnues comme ayant un impact sur la genèse de l'artériosclérose et de la thrombose. Une différence jusqu'à 2\% dans l'amplitude de la vélocité axiale et une différence jusqu'à 90\% dans la vélocité radiale sont découverts dans différentes sections d'une géométrie de type jonction en T. La taille des zones de re-circulation d'écoulement et les localisations des points de séparation et de rattachement qui leur sont associées, diffèrent pour chacun des modèles. La contrainte de cisaillement aux parois présente également un déplacement de 12\% dans le tube principal. La distribution de l'amplitude de vitesse dans les cellules du maillage montre que le modèle Newtonien est dynamiquement proche du modèle de Casson tandis que le modèle en loi de puissance ressemble au modèle de Carreau.},
	language = {eng},
	number = {6},
	journal = {Applied rheology (Lappersdorf, Germany : Online)},
	author = {Shibeshi, Shewaferaw S and Collins, William E},
	year = {2005},
	pmid = {16932804},
	keywords = {ShibeshiCollins},
	pages = {398--405},
}

@article{UrregoBlanco2016,
	title = {Journal of geophysical research},
	volume = {175},
	issn = {00280836},
	doi = {10.1002/2015JC011558},
	number = {4449},
	journal = {Journal of Geophysical Research: Oceans},
	author = {Urrego-Blanco, Jorge and Urban, Nathan and Hunke, Elizabeth and Turner, Adrian and Jeffery, Nicole},
	year = {2016},
	pages = {238},
}

@article{Zhang2020a,
	title = {Modern {Monte} {Carlo} methods for efficient uncertainty quantification and propagation: {A} survey},
	issn = {19390068},
	doi = {10.1002/wics.1539},
	abstract = {Uncertainty quantification (UQ) includes the characterization, integration, and propagation of uncertainties that result from stochastic variations and a lack of knowledge or data in the natural world. Monte Carlo (MC) method is a sampling-based approach that has widely used for quantification and propagation of uncertainties. However, the standard MC method is often time-consuming if the simulation-based model is computationally intensive. This article gives an overview of modern MC methods to address the existing challenges of the standard MC in the context of UQ. Specifically, multilevel Monte Carlo (MLMC) extending the concept of control variates achieves a significant reduction of the computational cost by performing most evaluations with low accuracy and corresponding low cost, and relatively few evaluations at high accuracy and corresponding high cost. Multifidelity Monte Carlo (MFMC) accelerates the convergence of standard Monte Carlo by generalizing the control variates with different models having varying fidelities and varying computational costs. Multimodel Monte Carlo method (MMMC), having a different setting of MLMC and MFMC, aims to address the issue of UQ and propagation when data for characterizing probability distributions are limited. Multimodel inference combined with importance sampling is proposed for quantifying and efficiently propagating the uncertainties resulting from small data sets. All of these three modern MC methods achieve a significant improvement of computational efficiency for probabilistic UQ, particularly uncertainty propagation. An algorithm summary and the corresponding code implementation are provided for each of the modern MC methods. The extension and application of these methods are discussed in detail. This article is categorized under: Statistical and Graphical Methods of Data Analysis {\textgreater} Monte Carlo Methods Statistical and Graphical Methods of Data Analysis {\textgreater} Sampling.},
	number = {November},
	journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
	author = {Zhang, Jiaxin},
	year = {2020},
	note = {arXiv: 2011.00680},
	keywords = {Monte Carlo methods, multifidelity Monte Carlo, multilevel Monte Carlo, uncertainty propagation, uncertainty quantification, ★},
}

@article{Meng,
	title = {Multi-fidelity {Bayesian} {Neural} {Networks} : {Algorithms} and},
	author = {Meng, Xuhui and Babaee, Hessam and Karniadakis, George Em},
	note = {arXiv: 2012.13294v1},
	pages = {1--31},
}

@article{Meng2020,
	title = {A composite neural network that learns from multi-fidelity data: {Application} to function approximation and inverse {PDE} problems},
	volume = {401},
	issn = {10902716},
	doi = {10.1016/j.jcp.2019.109020},
	abstract = {Currently the training of neural networks relies on data of comparable accuracy but in real applications only a very small set of high-fidelity data is available while inexpensive lower fidelity data may be plentiful. We propose a new composite neural network (NN) that can be trained based on multi-fidelity data. It is comprised of three NNs, with the first NN trained using the low-fidelity data and coupled to two high-fidelity NNs, one with activation functions and another one without, in order to discover and exploit nonlinear and linear correlations, respectively, between the low-fidelity and the high-fidelity data. We first demonstrate the accuracy of the new multi-fidelity NN for approximating some standard benchmark functions but also a 20-dimensional function that is not easy to approximate with other methods, e.g. Gaussian process regression. Subsequently, we extend the recently developed physics-informed neural networks (PINNs) to be trained with multi-fidelity data sets (MPINNs). MPINNs contain four fully-connected neural networks, where the first one approximates the low-fidelity data, while the second and third construct the correlation between the low- and high-fidelity data and produce the multi-fidelity approximation, which is then used in the last NN that encodes the partial differential equations (PDEs). Specifically, by decomposing the correlation into a linear and nonlinear part, the present model is capable of learning both the linear and complex nonlinear correlations between the low- and high-fidelity data adaptively. By training the MPINNs, we can: (1) obtain the correlation between the low- and high-fidelity data, (2) infer the quantities of interest based on a few scattered data, and (3) identify the unknown parameters in the PDEs. In particular, we employ the MPINNs to learn the hydraulic conductivity field for unsaturated flows as well as the reactive models for reactive transport. The results demonstrate that MPINNs can achieve relatively high accuracy based on a very small set of high-fidelity data. Despite the relatively low dimension and limited number of fidelities (two-fidelity levels) for the benchmark problems in the present study, the proposed model can be readily extended to very high-dimensional regression and classification problems involving multi-fidelity data.},
	number = {1},
	journal = {Journal of Computational Physics},
	author = {Meng, Xuhui and Karniadakis, George Em},
	year = {2020},
	note = {arXiv: 1903.00104},
	keywords = {Adversarial data, Multi-fidelity, Physics-informed neural networks, Porous media, Reactive transport},
	pages = {1--29},
}

@article{mckay_comparison_1979,
	title = {A {Comparison} of {Three} {Methods} for {Selecting} {Values} of {Input} {Variables} in the {Analysis} of {Output} from a {Computer} {Code}},
	volume = {21},
	issn = {0040-1706},
	url = {https://www.jstor.org/stable/1268522},
	doi = {10.2307/1268522},
	abstract = {Two types of sampling plans are examined as alternatives to simple random sampling in Monte Carlo studies. These plans are shown to be improvements over simple random sampling with respect to variance for a class of estimators which includes the sample mean and the empirical distribution function.},
	number = {2},
	urldate = {2022-02-04},
	journal = {Technometrics},
	author = {McKay, M. D. and Beckman, R. J. and Conover, W. J.},
	year = {1979},
	note = {Publisher: [Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]},
	pages = {239--245},
}

@article{comon_downloaded_2008,
	title = {Downloaded 09 / 29 / 21 to 35 . 3 . 222 . 225 {Redistribution} subject to {SIAM} license or copyright ; see https://epubs.siam.org/page/terms {Downloaded} 09 / 29 / 21 to 35 . 3 . 222 . 225 {Redistribution} subject to {SIAM} license or copyright ; see https://epubs},
	volume = {30},
	number = {3},
	author = {Comon, Pierre and Golub, Gene and Lim, Lek-heng and Mourrain, Bernard},
	year = {2008},
	keywords = {15a03, 15a18, 15a21, 15a69, 15a72, ams subject classifications, candecomp, decomposition, generic, maximal symmetric rank, multiway arrays, outer product decomposition, parafac, quantics, symmetric outer product, symmetric rank, symmetric tensor rank, tensor rank, tensors},
	pages = {1254--1279},
}

@article{Surana2016,
	title = {Dynamic tensor time series modeling and analysis},
	doi = {10.1109/CDC.2016.7798500},
	abstract = {In this paper we propose a model reduction and identification approach for multilinear dynamical system (MLDS) driven by noise. Compared to standard linear dynamical system based approaches which fit vector or matrix models to tensor time series, MLDS provides more natural, compact and accurate representation of tensorial data with fewer model parameters. The proposed algorithm for identifying MLDS parameters employs techniques from multilinear subspace learning: mulilinear Principal Component Analysis and multilinear regression. In addition compact array normal distribution is used to represent and estimate model error and output noise.We illustrate the benefits of the proposed approach on some real world datasets.},
	number = {Cdc},
	journal = {2016 IEEE 55th Conference on Decision and Control, CDC 2016},
	author = {Surana, Amit and Patterson, Geoff and Rajapakse, Indika},
	year = {2016},
	note = {Publisher: IEEE
ISBN: 9781509018376},
	keywords = {★},
	pages = {1637--1642},
}

@article{Sidiropoulos2017,
	title = {Tensor {Decomposition} for {Signal} {Processing} and {Machine} {Learning}},
	volume = {65},
	issn = {1053587X},
	doi = {10.1109/TSP.2017.2690524},
	abstract = {Tensors or multiway arrays are functions of three or more indices (i,j,k,⋯)-similar to matrices (two-way arrays), which are functions of two indices (r,c) for (row, column). Tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining, and machine learning. This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. As such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth and depth that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. Some background in applied optimization is useful but not strictly required. The material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning.},
	number = {13},
	journal = {IEEE Transactions on Signal Processing},
	author = {Sidiropoulos, Nicholas D. and De Lathauwer, Lieven and Fu, Xiao and Huang, Kejun and Papalexakis, Evangelos E. and Faloutsos, Christos},
	year = {2017},
	note = {arXiv: 1607.01668
Publisher: IEEE},
	keywords = {Cramér-Rao bound, Gauss-Newton, NP-hard problems, Tensor decomposition, Tucker model, alternating direction method of multipliers, alternating optimization, canonical polyadic decomposition (CPD), classification, collaborative filtering, communications, gradient descent, harmonic retrieval, higher-order singular value decomposition (HOSVD), mixture modeling, multilinear singular value decomposition (MLSVD), parallel factor analysis (PARAFAC), rank, source separation, speech separation, stochastic gradient, subspace learning, tensor factorization, topic modeling, uniqueness},
	pages = {3551--3582},
}

@article{Kolda2009,
	title = {Tensor decompositions and applications},
	volume = {51},
	issn = {00361445},
	doi = {10.1137/07070111X},
	abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N-way array. Decompositions of higher-order tensors (i.e., N-way arrays with N ≥ 3) have applications in psychometrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors. © 2009 Society for Industrial and Applied Mathematics.},
	number = {3},
	journal = {SIAM Review},
	author = {Kolda, Tamara G. and Bader, Brett W.},
	year = {2009},
	keywords = {Canonical decomposition (CANDECOMP), Higher-order principal components analysis (Tucker, Higher-order singular value decomposition (HOSVD), Multilinear algebra, Multiway arrays, Parallel factors (PARAFAC), Tensor decompositions, ★},
	pages = {455--500},
}

@inproceedings{Jivani2021,
	title = {Uncertainty quantification for a turbulent round jet using multifidelity karhunen-loève expansions},
	isbn = {978-1-62410-609-5},
	abstract = {© 2021, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved. Understanding the behavior of turbulent jets under variable environment and uncertain conditions is critical for predicting and mitigating aircraft jet noise. However, uncertainty quantification (UQ) for jet noise, which requires repeated expensive eddy-resolving simulations, is often computationally prohibitive. We thus build surrogate models, in particular Karhunen-Loève expansions (KLEs) for field quantities of interest in three-dimensional turbulent round jets. We build them in a multifidelity manner by combining simulation data from high-fidelity enhanced delayed detached-eddy simulation (EDDES) and low-fidelity Reynolds-averaged Navier-Stokes (RANS), generated under uncertain nozzle exit stagnation pressure and inlet eddy viscosity ratio. Furthermore, we form the KLEs in conjunction with polynomial chaos expansions in order to explicitly associate their randomness to each physical source of uncertainty, and so justifying the combining procedure in the multifidelity construct. We illustrate advantages of the new multifidelity KLE against single-fidelity KLEs, with the former achieving more accurate predictions at locations away from existing high-fidelity training data. With the KLE surrogate, we conduct UQ inexpensively.},
	booktitle = {{AIAA} {Scitech} 2021 {Forum}},
	author = {Jivani, A. and Huan, X. and Safta, C. and Zhou, B.Y. and Gauger, N.R.},
	year = {2021},
}

@article{chen_projected_nodate,
	title = {Projected {Stein} {Variational} {Newton} : {A} {Fast} and {Scalable} {Bayesian} {Inference} {Method} in {High} {Dimensions} {arXiv} : 1901 . 08659v2 [ math . {OC} ] 9 {Feb} 2020},
	author = {Chen, Peng and Wu, Keyi and Chen, Joshua and Leary-roseberry, Thomas O and Ghattas, Omar},
	note = {arXiv: 1901.08659v2},
}

@article{hawkins_bayesian_nodate,
	title = {Bayesian {Tensorized} {Neural} {Networks} with {Automatic} {Rank} {Selection}},
	author = {Hawkins, Cole},
	note = {arXiv: 1905.10478v1},
}

@misc{SMITH2014,
	title = {({Computational} {Science} and {Engineering}) {Ralph} {C}. {Smith} - {Uncertainty} {Quantification}\_ {Theory}, {Implementation}, and {Applications}-{SIAM}-{Society} for {Industrial} and {Applied} {Mathematics} (2013).pdf},
	author = {SMITH, RALPH C.},
	year = {2014},
	note = {Pages: 401},
}

@techreport{Oberkampf2002,
	title = {Verification and validation in computational fluid dynamics},
	abstract = {Verification and validation (V\&V) are the primary means to assess accuracy and reliability in computational simulations. This paper presents an extensive review of the literature in V\&V in computational fluid dynamics (CFD), discusses methods and procedures for assessing V\&V, and develops a number of extensions to existing ideas. The review of the development of V\&V terminology and methodology points out the contributions from members of the operations research, statistics, and CFD communities. Fundamental issues in V\&V are addressed, such as code verification versus solution verification, model validation versus solution validation, the distinction between error and uncertainty, conceptual sources of error and uncertainty, and the relationship between validation and prediction. The fundamental strategy of verification is the identification and quantification of errors in the computational model and its solution. In verification activities, the accuracy of a computational solution is primarily measured relative to two types of highly accurate solutions: analytical solutions and highly accurate numerical solutions. Methods for determining the accuracy of numerical solutions are presented and the importance of software testing during verification activities is emphasized. The fundamental strategy of validation is to assess how accurately the computational results compare with the experimental data, with quantified error and uncertainty estimates for both. This strategy employs a hierarchical methodology that segregates and simplifies the physical and coupling phenomena involved in the complex engineering system of interest. A hypersonic cruise missile is used as an example of how this hierarchical structure is formulated. The discussion of validation assessment also encompasses a number of other important topics. A set of guidelines is proposed for designing and conducting validation experiments, supported by an explanation of how validation experiments are different from traditional experiments and testing. A description is given of a relatively new procedure for estimating experimental uncertainty that has proven more effective at estimating random and correlated bias errors in wind-tunnel experiments than traditional methods. Consistent with the authors' contention that nondeterministic simulations are needed in many validation comparisons, a three-step statistical approach is offered for incorporating experimental uncertainties into the computational analysis. The discussion of validation assessment ends with the topic of validation metrics, where two sample problems are used to demonstrate how such metrics should be constructed. In the spirit of advancing the state of the art in V\&V, the paper concludes with recommendations of topics for future research and with suggestions for needed changes in the implementation of V\&V in production and commercial software. r 2002 Published by Elsevier Science Ltd.},
	author = {Oberkampf, William L and Trucano, Timothy G},
	year = {2002},
	note = {Publication Title: Progress in Aerospace Sciences
Volume: 38},
	pages = {209--272},
}

@article{Vickers2019,
	title = {A simple, step-by-step guide to interpreting decision curve analysis},
	volume = {3},
	issn = {2397-7523},
	doi = {10.1186/s41512-019-0064-7},
	abstract = {Background Decision curve analysis is a method to evaluate prediction models and diagnostic tests that was introduced in a 2006 publication. Decision curves are now commonly reported in the literature, but there remains widespread misunderstanding of and confusion about what they mean. Summary of commentary In this paper, we present a didactic, step-by-step introduction to interpreting a decision curve analysis and answer some common questions about the method. We argue that many of the difficulties with interpreting decision curves can be solved by relabeling the y-axis as "benefit" and the x-axis as "preference." A model or test can be recommended for clinical use if it has the highest level of benefit across a range of clinically reasonable preferences. Conclusion Decision curves are readily interpretable if readers and authors follow a few simple guidelines.},
	number = {1},
	journal = {Diagnostic and Prognostic Research},
	author = {Vickers, Andrew J. and van Calster, Ben and Steyerberg, Ewout W.},
	year = {2019},
	pmid = {31592444},
	note = {Publisher: Diagnostic and Prognostic Research
ISBN: 4151201900647},
	keywords = {Net benefit,Decision curve analysis,Educational pa, decision curve analysis, educational paper, net benefit, ★},
	pages = {1--8},
}

@article{DElia2013,
	title = {Coarse-{Grid} {Sampling} {Interpolatory} {Methods} for {Approximating} {Gaussian} {Random} {Fields}},
	volume = {1},
	issn = {2166-2525},
	doi = {10.1137/120883311},
	number = {1},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {D'Elia, Marta and Gunzburger, Max},
	year = {2013},
	keywords = {60g60, 60g99, 62d05, 62j10, 65d05, 65f15, 65n50, ams subject classifications, cholesky factorization, covariance functions, eve expansion, grid-based sampling, interpolation, karhunen, lo, low-rank approximations, piecewise polynomial ap-, proximation, random fields},
	pages = {270--296},
}

@article{Nazarathy2020,
	title = {Statistics with {Julia}: {Fundamentals} for {Data} {Science}, {Machine} {Learning} and {Artificial} {Intelligence}.},
	author = {Nazarathy, Yoni and Klok, Hayden},
	year = {2020},
	keywords = {★},
}

@book{GhanemHandbookUQ2017,
	title = {Handbook of {Uncertainty} {Quantification}},
	isbn = {978-3-319-12384-4},
	author = {Ghanem, Roger and Higdon, David and Owhadi, Houman},
	year = {2017},
	doi = {10.1007/978-3-319-12385-1},
}

@article{Ramadhan2020a,
	title = {Oceananigans . jl : {Fast} and friendly geophysical fluid dynamics on {GPUs}},
	volume = {5},
	doi = {10.1137/141000671},
	author = {Ramadhan, Ali and Wagner, Gregory Leclaire and Hill, Chris and Churavy, Valentin and Besard, Tim and Souza, Andre and Ferrari, Raffaele and Marshall, John},
	year = {2020},
	pages = {2019--2021},
}

@article{Lacaze2017,
	title = {Large {Eddy} {Simulation} of the {HIFiRE} {Direct} {Connect} {Rig} {Scramjet} {Combustor}},
	doi = {10.2514/6.2017-0142},
	number = {January},
	author = {Lacaze, Guilhem and Vane, Zachary P and Oefelein, Joseph C},
	year = {2017},
}

@book{Santner2018,
	title = {The {Design} and {Analysis} of {Computer} {Experiments}},
	author = {Santner, Thomas and Williams, Brian and Notz, William},
	year = {2018},
	note = {Publication Title: Springer},
}

@article{Queipo2005,
	title = {Surrogate-based analysis and optimization},
	volume = {41},
	issn = {03760421},
	doi = {10.1016/j.paerosci.2005.02.001},
	abstract = {A major challenge to the successful full-scale development of modern aerospace systems is to address competing objectives such as improved performance, reduced costs, and enhanced safety. Accurate, high-fidelity models are typically time consuming and computationally expensive. Furthermore, informed decisions should be made with an understanding of the impact (global sensitivity) of the design variables on the different objectives. In this context, the so-called surrogate-based approach for analysis and optimization can play a very valuable role. The surrogates are constructed using data drawn from high-fidelity models, and provide fast approximations of the objectives and constraints at new design points, thereby making sensitivity and optimization studies feasible. This paper provides a comprehensive discussion of the fundamental issues that arise in surrogate-based analysis and optimization (SBAO), highlighting concepts, methods, techniques, as well as practical implications. The issues addressed include the selection of the loss function and regularization criteria for constructing the surrogates, design of experiments, surrogate selection and construction, sensitivity analysis, convergence, and optimization. The multi-objective optimal design of a liquid rocket injector is presented to highlight the state of the art and to help guide future efforts. © 2005 Elsevier Ltd. All rights reserved.},
	number = {1},
	journal = {Progress in Aerospace Sciences},
	author = {Queipo, Nestor V. and Haftka, Raphael T. and Shyy, Wei and Goel, Tushar and Vaidyanathan, Rajkumar and Kevin Tucker, P.},
	year = {2005},
	keywords = {★},
	pages = {1--28},
}

@article{Ng2016,
	title = {Monte {Carlo} information-reuse approach to aircraft conceptual design optimization under uncertainty},
	volume = {53},
	issn = {15333868},
	doi = {10.2514/1.C033352},
	abstract = {This paper develops a multi-information source formulation for aerospace design under uncertainty problems. As a specific demonstration of the approach, it presents the optimization under uncertainty of an advanced subsonic transport aircraft developed to meet the NASA N + 3 goals and shows how the multi-information source approach enables practical turnaround time for this conceptual aircraft optimization under uncertainty problem. In the conceptual design phase, there are often uncertainties about future developments of the underlying technologies. An aircraft design that is robust to uncertainty is more likely to meet performance requirements as the technologies mature in the intermediate and detailed design phases, reducing the need for expensive redesigns. In the particular example selected here to present the new approach, the multi-information source approach uses an information-reuse estimator that takes advantage of the correlation of the aircraft model in the design space to reduce the number of model evaluations needed to achieve a given standard error in the Monte Carlo estimates of the relevant design statistics (mean and variance). Another contribution of the paper is to extend the approach to reuse information during trade studies that involve solving multiple optimization under uncertainty problems, enabling the analysis of the risk-performance tradeoff in optimal aircraft designs.},
	number = {2},
	journal = {Journal of Aircraft},
	author = {Ng, Leo W.T. and Willcox, Karen E.},
	year = {2016},
	pages = {427--438},
}

@article{Gianluca2017,
	title = {A multifidelity multilevel {Monte} {Carlo} method for uncertainty propagation in aerospace applications},
	doi = {10.2514/6.2017-1951},
	abstract = {The accurate evaluation of the performance of complex engineering devices needs to rely on high-fidelity numerical simulations and the systematic characterization and propagation of uncertainties. Several sources of uncertainty may impact the performance of an engineering device through operative conditions, manufacturing tolerances, and even physical models. In the presence of multiphysics systems the number of the uncertain parameters can be fairly large and their propagation through the numerical codes still remains prohibitive because the overall computational budget often allows for only an handful of such high-fidelity realizations. On the other side, common engineering practice can take advantage from a solid history of development and assessment of so called low-fidelity models which albeit less accurate are often capable to at least capture overall trends and parameter dependencies of the system. In this contribution we address the forward propagation of uncertainty parameters relying on statistical estimators built on sequences of numerical and physical discretizations which are provably convergent to the high-fidelity statistics, while exploiting low-fidelity computational models to increase the reliability and confidence in the numerical predictions. The performances of the approaches are presented by means of two fairly complicated aerospace problems, namely the aero-thermo-structural analysis of a turbofan engine nozzle and a flow through a scramjet-like device.},
	number = {January},
	journal = {19th AIAA Non-Deterministic Approaches Conference, 2017},
	author = {Gianluca, Geraci and Eldred, Michael S. and Iaccarino, Gianluca},
	year = {2017},
	note = {ISBN: 9781624104527},
	pages = {1--24},
}

@article{Peherstorfer2016,
	title = {Optimal {Model} {Management} for {Multifidelity} {Monte} {Carlo} {Estimation}},
	volume = {38},
	number = {5},
	author = {Peherstorfer, Benjamin and Willcox, Karen and Gunzburger, M A X},
	year = {2016},
	keywords = {1, 10, 1137, 15m1046472, 65m22, 65n22, ams subject classifications, doi, in, introduction, long and successful history, model reduction, monte carlo simulation, multifidelity, multilevel techniques have a, surrogate modeling},
}

@article{KennedyOHagan2001,
	title = {Bayesian {Calibration} of {Computer} {Models}},
	volume = {63},
	issn = {1369-7412},
	number = {3},
	journal = {Journal of the Royal Statistical Society},
	author = {Kennedy, Marc and O'Hagan, Anthony},
	year = {2001},
	keywords = {★},
	pages = {425--464},
}

@article{Griffiths2003,
	title = {Determining the optimal cross section of beams},
	volume = {78},
	issn = {17593433},
	doi = {10.4203/ccp.78.36},
	abstract = {Constrained shape discovery and optimisation is a difficult engineering problem. In this paper the problem of finding the optimum cross section of a beam is solved using a suitably crafted Genetic Algorithm (GA). Previous work using GAs for this problem has only managed to evolve satisfactory solutions through applying heuristics that operate directly on the genotype. Such heavy guidance of the GA potentially stifles innovation, can only be applied to situations where the correct answer is known and limits the applicability of the search. This research demonstrates the ability of the GA, in its purest form (unbiased and unguided search) to evolve good, near optimal results. Performing an unbiased search, using only the evolutionary process to search for good solutions, allows the GA to be applied where no heuristic knowledge is available. Advanced 2-dimensional genetic operators, in conjunction with a suitably designed fitness function, allow a productive evolutionary search. The initial test case is the evolution of an optimal beam cross-section, subject to several load cases. It is shown that the methods developed lead to consistently good solutions, despite the complexity of the process. © 2003, Civil-Comp Ltd.},
	journal = {Civil-Comp Proceedings},
	author = {Griffiths, D. R. and Miles, J. C.},
	year = {2003},
	note = {ISBN: 094874992X},
	keywords = {Domain knowledge, Genetic algorithms, Heuristics, Optimisation, Shape discovery, Unguided search},
}

@article{Doost2016,
	title = {Heart blood flow simulation: {A} perspective review},
	volume = {15},
	issn = {1475925X},
	doi = {10.1186/s12938-016-0224-8},
	abstract = {Cardiovascular disease (CVD), the leading cause of death today, incorporates a wide range of cardiovascular system malfunctions that affect heart functionality. It is believed that the hemodynamic loads exerted on the cardiovascular system, the left ventricle (LV) in particular, are the leading cause of CVD initiation and propagation. Moreover, it is believed that the diagnosis and prognosis of CVD at an early stage could reduce its high mortality and morbidity rate. Therefore, a set of robust clinical cardiovascular assessment tools has been introduced to compute the cardiovascular hemodynamics in order to provide useful insights to physicians to recognize indicators leading to CVD and also to aid the diagnosis of CVD. Recently, a combination of computational fluid dynamics (CFD) and different medical imaging tools, image-based CFD (IB-CFD), has been widely employed for cardiovascular functional assessment by providing reliable hemodynamic parameters. Even though the capability of CFD to provide reliable flow dynamics in general fluid mechanics problems has been widely demonstrated for many years, up to now, the clinical implications of the IB-CFD patient-specific LVs have not been applicable due to its limitations and complications. In this paper, we review investigations conducted to numerically simulate patient-specific human LV over the past 15 years using IB-CFD methods. Firstly, we divide different studies according to the different LV types (physiological and different pathological conditions) that have been chosen to reconstruct the geometry, and then discuss their contributions, methodologies, limitations, and findings. In this regard, we have studied CFD simulations of intraventricular flows and related cardiology insights, for (i) Physiological patient-specific LV models, (ii) Pathological heart patient-specific models, including myocardial infarction, dilated cardiomyopathy, hypertrophic cardiomyopathy and hypoplastic left heart syndrome. Finally, we discuss the current stage of the IB-CFD LV simulations in order to mimic realistic hemodynamics of patient-specific LVs. We can conclude that heart flow simulation is on the right track for developing into a useful clinical tool for heart function assessment, by (i) incorporating most of heart structures' (such as heart valves) operations, and (ii) providing useful diagnostic indices based hemodynamic parameters, for routine adoption in clinical usage.},
	number = {1},
	journal = {BioMedical Engineering Online},
	author = {Doost, Siamak N. and Ghista, Dhanjoo and Su, Boyang and Zhong, Liang and Morsi, Yosry S.},
	year = {2016},
	pmid = {27562639},
	note = {Publisher: BioMed Central
ISBN: 1293801602248},
	keywords = {Cardiovascular diseases (CVDs), Computational fluid dynamics (CFD), Fluid structure interaction (FSI), Hemodynamics, Left ventricle (LV)},
	pages = {1--28},
}

@article{Hausfather2020,
	title = {Evaluating the {Performance} of {Past} {Climate} {Model} {Projections}},
	volume = {47},
	issn = {19448007},
	doi = {10.1029/2019GL085378},
	abstract = {Retrospectively comparing future model projections to observations provides a robust and independent test of model skill. Here we analyze the performance of climate models published between 1970 and 2007 in projecting future global mean surface temperature (GMST) changes. Models are compared to observations based on both the change in GMST over time and the change in GMST over the change in external forcing. The latter approach accounts for mismatches in model forcings, a potential source of error in model projections independent of the accuracy of model physics. We find that climate models published over the past five decades were skillful in predicting subsequent GMST changes, with most models examined showing warming consistent with observations, particularly when mismatches between model-projected and observationally estimated forcings were taken into account.},
	number = {1},
	journal = {Geophysical Research Letters},
	author = {Hausfather, Zeke and Drake, Henri F. and Abbott, Tristan and Schmidt, Gavin A.},
	year = {2020},
	pages = {1--10},
}

@article{Ramadhan2020,
	title = {Capturing missing physics in climate model parameterizations using neural differential equations},
	url = {http://arxiv.org/abs/2010.12559},
	abstract = {Even with today's immense computational resources, climate models cannot resolve every cloud in the atmosphere or eddying swirl in the ocean. However, collectively these small-scale turbulent processes play a key role in setting Earth's climate. Climate models attempt to represent unresolved scales via surrogate models known as parameterizations. These have limited fidelity and can exhibit structural deficiencies. Here we demonstrate that neural differential equations (NDEs) may be trained by highly resolved fluid-dynamical models of the scales to be parameterized and those NDEs embedded in an ocean model. They can incorporate conservation laws and are stable in time. We argue that NDEs provide a new route forward to the development of surrogate models for climate science, opening up exciting new opportunities.},
	author = {Ramadhan, Ali and Marshall, John and Souza, Andre and Wagner, Gregory LeClaire and Ponnapati, Manvitha and Rackauckas, Christopher},
	year = {2020},
	note = {arXiv: 2010.12559},
}

@article{Marzouk2009,
	title = {A stochastic collocation approach to {Bayesian} inference in inverse problems},
	volume = {6},
	issn = {19917120},
	doi = {10.4208/cicp.2009.v6.p826},
	abstract = {We present an efficient numerical strategy for the Bayesian solution of inverse problems. Stochastic collocation methods, based on generalized polynomial chaos (gPC), are used to construct a polynomial approximation of the forward solution over the support of the prior distribution. This approximation then defines a surrogate posterior probability density that can be evaluated repeatedly at minimal computational cost. The ability to simulate a large number of samples from the posterior distribution results in very accurate estimates of the inverse solution and its associated uncertainty. Combined with high accuracy of the gPC-based forward solver, the new algorithm can provide great efficiency in practical applications. A rigorous error analysis of the algorithm is conducted, where we establish convergence of the approximate posterior to the true posterior and obtain an estimate of the convergence rate. It is proved that fast (exponential) convergence of the gPC forward solution yields similarly fast (exponential) convergence of the posterior. The numerical strategy and the predicted convergence rates are then demonstrated on nonlinear inverse problems of varying smoothness and dimension. © 2009 Global-Science Press.},
	number = {4},
	journal = {Communications in Computational Physics},
	author = {Marzouk, Youssef and Xiu, Dongbin},
	year = {2009},
	keywords = {Bayesian inference, Generalized polynomial chaos, Inverse problems, Stochastic collocation, Uncertainty quantification},
	pages = {826--847},
}

@article{Gramacy2015,
	title = {Local {Gaussian} {Process} {Approximation} for {Large} {Computer} {Experiments}},
	volume = {24},
	issn = {15372715},
	doi = {10.1080/10618600.2014.914442},
	abstract = {We provide a new approach to approximate emulation of large computer experiments. By focusing expressly on desirable properties of the predictive equations, we derive a family of local sequential design schemes that dynamically define the support of a Gaussian process predictor based on a local subset of the data. We further derive expressions for fast sequential updating of all needed quantities as the local designs are built up iteratively. Then we show how independent application of our local design strategy across the elements of a vast predictive grid facilitates a trivially parallel implementation. The end result is a global predictor able to take advantage of modern multicore architectures, providing a nonstationary modeling feature as a bonus. We demonstrate our method on two examples using designs with thousands of data points, and compare to the method of compactly supported covariances. Supplementary materials for this article are available online.},
	number = {2},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Gramacy, Robert B. and Apley, Daniel W.},
	year = {2015},
	note = {arXiv: 1303.0383},
	keywords = {Active learning, Compactly supported, Covariance, Emulator, Local kriging neighborhoods, Sequential design, Sequential updating, Surrogate model},
	pages = {561--578},
}

@article{Ghanem2019,
	title = {Design optimization of a scramjet under uncertainty using probabilistic learning on manifolds},
	volume = {399},
	issn = {10902716},
	url = {https://doi.org/10.1016/j.jcp.2019.108930},
	doi = {10.1016/j.jcp.2019.108930},
	abstract = {We demonstrate, on a scramjet combustion problem, a constrained probabilistic learning approach that augments physics-based datasets with realizations that adhere to underlying constraints and scatter. The constraints are captured and delineated through diffusion maps, while the scatter is captured and sampled through a projected stochastic differential equation. The objective function and constraints of the optimization problem are then efficiently framed as non-parametric conditional expectations. Different spatial resolutions of a large-eddy simulation filter are used to explore the robustness of the model to the training dataset and to gain insight into the significance of spatial resolution on optimal design.},
	journal = {Journal of Computational Physics},
	author = {Ghanem, R. G. and Soize, C. and Safta, C. and Huan, X. and Lacaze, G. and Oefelein, J. C. and Najm, H. N.},
	year = {2019},
	note = {Publisher: Elsevier Inc.},
	keywords = {Diffusion maps, Machine learning, Optimization under uncertainty, Sampling on manifolds, Scramjet simulations, Uncertainty quantification},
	pages = {108930},
}

@article{Zhou2019a,
	title = {Towards a real-time in-flight ice detection system via computational aeroacoustics and bayesian neural networks},
	abstract = {In-flight rotor icing presents a serious problem in the operation of rotorcraft in cold climates, as complex ice shapes can significantly degrade the aerodynamic performance and handling characteristics of rotorcraft. Reliable real-time detection of ice formation is a critical enabling technology in improving rotorcraft safety. In this work, we propose a novel approach towards developing a real-time in-flight ice detection system using computational aeroacoustics and Bayesian neural networks. In particular, an icing simulation code based on a fully unsteady collection efficiency approach is coupled with the aeroacoustic solver available in the open-source software suite SU2, in order to compute far-field noise signatures corresponding to discrete iced rotor blades in various icing conditions. Additionally, Bayesian neural networks are constructed from the dataset thus generated to enable rapid predictions together with uncertainty information, of aerodynamic performance indicators from acoustic signal, as a first step in developing an in-flight ice detection and warning system.},
	journal = {AIAA Aviation 2019 Forum},
	author = {Zhou, Beckett Y. and Gauger, Nicolas R. and Morelli, Myles and Guardone, Alberto and Hauth, Jeremiah and Huan, Xun},
	year = {2019},
	note = {ISBN: 9781624105890},
	pages = {1--15},
}

@article{Wang2020b,
	title = {A perspective on regression and {Bayesian} approaches for system identification of pattern formation dynamics},
	volume = {10},
	issn = {20950349},
	url = {http://dx.doi.org/10.1016/j.taml.2020.01.028},
	doi = {10.1016/j.taml.2020.01.028},
	abstract = {We present two approaches to system identification, i.e. the identification of partial differential equations (PDEs) from measurement data. The first is a regression-based variational system identification procedure that is advantageous in not requiring repeated forward model solves and has good scalability to large number of differential operators. However it has strict data type requirements needing the ability to directly represent the operators through the available data. The second is a Bayesian inference framework highly valuable for providing uncertainty quantification, and flexible for accommodating sparse and noisy data that may also be indirect quantities of interest. However, it also requires repeated forward solutions of the PDE models which is expensive and hinders scalability. We provide illustrations of results on a model problem for pattern formation dynamics, and discuss merits of the presented methods.},
	number = {3},
	journal = {Theoretical and Applied Mechanics Letters},
	author = {Wang, Zhenlin and Wu, Bowei and Garikipati, Krishna and Huan, Xun},
	year = {2020},
	note = {arXiv: 2001.05646
Publisher: The Authors. Published by Elsevier Ltd on behalf of The Chinese Society of Theoretical and Applied Mechanics},
	keywords = {Bayesian inference, Computational mechanics, Inverse problem, Materials physics, Pattern formation},
	pages = {188--194},
}

@article{Denamiel2019,
	title = {Stochastic {Surrogate} {Model} for {Meteotsunami} {Early} {Warning} {System} in the {Eastern} {Adriatic} {Sea}},
	volume = {124},
	issn = {21699291},
	doi = {10.1029/2019JC015574},
	abstract = {The meteotsunami early warning system prototype using stochastic surrogate approach and running operationally in the eastern Adriatic Sea is presented. First, the atmospheric internal gravity waves (IGWs) driving the meteotsunamis are either forecasted with state-of-the-art deterministic models at least a day in advance or detected through measurements at least 2 hr before the meteotsunami reaches sensitive locations. The extreme sea-level hazard forecast at endangered locations is then derived with an innovative stochastic surrogate model—implemented with generalized polynomial chaos expansion (gPCE) method and synthetic IGWs forcing a barotropic ocean model—used with the input parameters extracted from deterministic model results and/or measurements. The evaluation of the system, both against five historical events and for all the detected potential meteotsunamis since late 2018 when the early warning system prototype became operational, reveals that the meteotsunami hazard is conservatively assessed but often overestimated at some locations. Despite some needed improvements and developments, this study demonstrates that gPCE-based methods can be used for atmospherically driven extreme sea-level hazard assessment and in geosciences in wide.},
	number = {11},
	journal = {Journal of Geophysical Research: Oceans},
	author = {Denamiel, Cléa and Šepić, Jadranka and Huan, Xun and Bolzer, Célia and Vilibić, Ivica},
	year = {2019},
	keywords = {eastern Adriatic, extreme sea-level hazard assessment, meteotsunami early warning system},
	pages = {8485--8499},
}

@article{Bayarri2015,
	title = {Probabilistic quantification of hazards: {A} methodology using small ensembles of physics-based simulations and statistical surrogates},
	volume = {5},
	issn = {21525099},
	doi = {10.1615/Int.J.UncertaintyQuantification.2015011451},
	abstract = {This paper presents a novel approach to assessing the hazard threat to a locale due to a large volcanic avalanche. The methodology combines: (i) mathematical modeling of volcanic mass flows; (ii) field data of avalanche frequency, volume, and runout; (iii) large-scale numerical simulations of flow events; (iv) use of statistical methods to minimize computational costs, and to capture unlikely events; (v) calculation of the probability of a catastrophic flow event over the next T years at a location of interest; and (vi) innovative computational methodology to implement these methods. This unified presentation collects elements that have been separately developed, and incorporates new contributions to the process. The field data and numerical simulations used here are subject to uncertainty from many sources, uncertainties that must be properly accounted for in assessing the hazard. The methodology presented here will be demonstrated with data from the Soufrière Hills Volcano on the island of Montserrat, where there is a relatively complete record of volcanic mass flows from the past 15 years. This methodology can be transferred to other volcanic sites with similar characteristics and where sparse historical data have prevented such high-quality analysis. More generally, the core of this methodology is widely applicable and can be used for other hazard scenarios, such as floods or ash plumes.},
	number = {4},
	journal = {International Journal for Uncertainty Quantification},
	author = {Bayarri, M. J. and Berger, J. O. and Calder, E. S. and Patra, A. K. and Pitman, E. B. and Spiller, E. T. and Wolpert, Robert L.},
	year = {2015},
	keywords = {Computer modeling, Hazard assessment, Statistics, Uncertainty, Volcanic hazards},
	pages = {297--325},
}

@article{Debusschere2005,
	title = {Numerical challenges in the use of polynomial chaos representations for stochastic processes},
	volume = {26},
	issn = {10648275},
	doi = {10.1137/S1064827503427741},
	abstract = {This paper gives an overview of the use of polynomial chaos (PC) expansions to represent stochastic processes in numerical simulations. Several methods are presented for performing arithmetic on, as well as for evaluating polynomial and nonpolynomial functions of variables represented by PC expansions. These methods include Taylor series, a newly developed integration method, as well as a sampling-based spectral projection method for nonpolynomial function evaluations. A detailed analysis of the accuracy of the PC representations, and of the different methods for nonpolynomial function evaluations, is performed. It is found that the integration method offers a robust and accurate approach for evaluating nonpolynomial functions, even when very high-order information is present in the PC expansions. © 2004 Society for Industrial and Applied Mathematics.},
	number = {2},
	journal = {SIAM Journal on Scientific Computing},
	author = {Debusschere, Bert J. and Najm, Habib N. and Pébayt, Philippe P. and Knio, Omar M. and Ghanem, Roger G. and Le Maître, Olivier P.},
	year = {2005},
	keywords = {Polynomial chaos, Spectral uncertainty quantification, Stochastic, ★},
	pages = {698--719},
}

@article{Giles2008,
	title = {Multilevel {Monte} {Carlo} path simulation},
	volume = {56},
	issn = {0030364X},
	doi = {10.1287/opre.1070.0496},
	abstract = {We show that multigrid ideas can be used to reduce the computational complexity of estimating an expected value arising from a stochastic differential equation using Monte Carlo path simulations. In the simplest case of a Lipschitz payoff and a Euler discretisation, the computational cost to achieve an accuracy of O(ε) is reduced from O(ε-3) to O(ε-2(logε)2). The analysis is supported, by numerical results showing significant computational savings. © 2008 INFORMS.},
	number = {3},
	journal = {Operations Research},
	author = {Giles, Michael B.},
	year = {2008},
	keywords = {Analysis of algorithms: Computational complexity, Finance, Simulation: Efficiency. Area of review : Financial, ★},
	pages = {607--617},
}

@article{Economon2016,
	title = {{SU2}: {An} open-source suite for multiphysics simulation and design},
	volume = {54},
	issn = {00011452},
	doi = {10.2514/1.J053813},
	abstract = {This paper presents the main objectives and a description of the SU2 suite, including the novel software architecture and open-source software engineering strategy. SU2 is a computational analysis and design package that has been developed to solve multiphysics analysis and optimization tasks using unstructured mesh topologies. Its unique architecture iswell suited for extensibility to treat partial-differential-equation-based problems not initially envisioned. The common framework adopted enables the rapid implementation of newphysics packages that can be tightly coupled to form a powerful ensemble of analysis tools to address complex problems facing many engineering communities. The framework is demonstrated on a number, solving both the flow and adjoint systems of equations to provide a highfidelity predictive capability and sensitivity information that can be used for optimal shape design using a gradientbased framework, goal-oriented adaptive mesh refinement, or uncertainty quantification.},
	number = {3},
	journal = {AIAA Journal},
	author = {Economon, Thomas D. and Palacios, Francisco and Copeland, Sean R. and Lukaczyk, Trent W. and Alonso, Juan J.},
	year = {2016},
	keywords = {★},
	pages = {828--846},
}

@article{Gorodetsky2020,
	title = {A generalized approximate control variate framework for multifidelity uncertainty quantification},
	volume = {408},
	issn = {10902716},
	url = {https://doi.org/10.1016/j.jcp.2020.109257},
	doi = {10.1016/j.jcp.2020.109257},
	abstract = {We describe and analyze a variance reduction approach for Monte Carlo (MC) sampling that accelerates the estimation of statistics of computationally expensive simulation models using an ensemble of models with lower cost. These lower cost models — which are typically lower fidelity with unknown statistics — are used to reduce the variance in statistical estimators relative to a MC estimator with equivalent cost. We derive the conditions under which our proposed approximate control variate framework recovers existing multifidelity variance reduction schemes as special cases. We demonstrate that existing recursive/nested strategies are suboptimal because they use the additional low-fidelity models only to efficiently estimate the unknown mean of the first low-fidelity model. As a result, they cannot achieve variance reduction beyond that of a control variate estimator that uses a single low-fidelity model with known mean. However, there often exists about an order-of-magnitude gap between the maximum achievable variance reduction using all low-fidelity models and that achieved by a single low-fidelity model with known mean. We show that our proposed approach can exploit this gap to achieve greater variance reduction by using non-recursive sampling schemes. The proposed strategy reduces the total cost of accurately estimating statistics, especially in cases where only low-fidelity simulation models are accessible for additional evaluations. Several analytic examples and an example with a hyperbolic PDE describing elastic wave propagation in heterogeneous media are used to illustrate the main features of the methodology.},
	journal = {Journal of Computational Physics},
	author = {Gorodetsky, Alex A. and Geraci, Gianluca and Eldred, Michael S. and Jakeman, John D.},
	year = {2020},
	note = {arXiv: 1811.04988
Publisher: Elsevier Inc.},
	keywords = {Control variates, Monte Carlo, Multifidelity modeling, Variance reduction},
	pages = {109257},
}

@article{Peherstorfer2018,
	title = {Survey of multifidelity methods in uncertainty propagation, inference, and optimization},
	volume = {60},
	issn = {00361445},
	doi = {10.1137/16M1082469},
	abstract = {In many situations across computational science and engineering, multiple computational models are available that describe a system of interest. These different models have varying evaluation costs and varying fidelities. Typically, a computationally expensive high-fidelity model describes the system with the accuracy required by the current application at hand, while lower-fidelity models are less accurate but computationally cheaper than the high-fidelity model. Outer-loop applications, such as optimization, inference, and uncertainty quantification, require multiple model evaluations at many different inputs, which often leads to computational demands that exceed available resources if only the high-fidelity model is used. This work surveys multifidelity methods that accelerate the solution of outer-loop applications by combining high-fidelity and low-fidelity model evaluations, where the low-fidelity evaluations arise from an explicit low-fidelity model (e.g., a simplified physics approximation, a reduced model, a data-fit surrogate) that approximates the same output quantity as the high-fidelity model. The overall premise of these multifidelity methods is that low-fidelity models are leveraged for speedup while the high-fidelity model is kept in the loop to establish accuracy and/or convergence guarantees. We categorize multifidelity methods according to three classes of strategies: adaptation, fusion, and filtering. The paper reviews multifidelity methods in the outer-loop contexts of uncertainty propagation, inference, and optimization.},
	number = {3},
	journal = {SIAM Review},
	author = {Peherstorfer, Benjamin and Willcox, Karen and Gunzburger, Max},
	year = {2018},
	note = {arXiv: 1806.10761},
	keywords = {Model reduction, Multifidelity, Multifidelity optimization, Multifidelity statistical inference, Multifidelity uncertainty propagation, Multifidelity uncertainty quantification, Surrogate models, ★},
	pages = {550--591},
}

@article{Sun2019,
	title = {Synthesizing simulation and field data of solar irradiance},
	volume = {12},
	issn = {19321872},
	doi = {10.1002/sam.11414},
	abstract = {Predicting the intensity and amount of sunlight as a function of location and time is an essential component in identifying promising locations for economical solar farming. Although weather models and irradiance data are relatively abundant, these have yet, to our knowledge, been hybridized on a continental scale. Rather, much of the emphasis in the literature has been on short-term localized forecasting. This is probably because the amount of data involved in a more global analysis is prohibitive with the canonical toolkit, via the Gaussian process (GP). Here we show how GP surrogate and discrepancy models can be combined to tractably and accurately predict solar irradiance on time-aggregated and daily scales with measurements at thousands of sites across the continental United States. Our results establish short-term accuracy of bias-corrected weather-based simulation of irradiance, when realizations are available in real space-time (eg, in future days), and provide accurate surrogates for smoothing in the more common situation where reliable weather data is not available (eg, in future years).},
	number = {4},
	journal = {Statistical Analysis and Data Mining},
	author = {Sun, Furong and Gramacy, Robert B. and Haaland, Benjamin and Lu, Siyuan and Hwang, Youngdeok},
	year = {2019},
	note = {arXiv: 1806.05131},
	keywords = {approximate kriging, calibration, inverse-variance weighting, nonparametric regression, space-filling design, surrogate modeling, ★},
	pages = {311--324},
}

@article{Renganathan2019,
	title = {Aerodynamic data fusion towards the digital twin paradigm},
	volume = {58},
	issn = {23318422},
	abstract = {We consider the fusion of two aerodynamic data sets originating from differing fidelity physical or computer experiments. We specifically address the fusion of: 1) noisy and incomplete fields from wind tunnel measurements and 2) deterministic but biased fields from numerical simulations. These two data sources are fused in order to estimate the true field that best matches measured quantities that serves as the ground truth. For example, two sources of pressure fields about an aircraft are fused based on measured forces and moments from a wind-tunnel experiment. A fundamental challenge in this problem is that the true field is unknown and can not be estimated with 100\% certainty. We employ a Bayesian framework to infer the true fields conditioned on measured quantities of interest; essentially we perform a statistical correction to the data. The fused data may then be used to construct more accurate surrogate models suitable for early stages of aerospace design. We also introduce an extension of the Proper Orthogonal Decomposition with constraints to solve the same problem. Both methods are demonstrated on fusing the pressure distributions for flow past the RAE2822 airfoil and the Common Research Model wing at transonic conditions. Comparison of both methods reveal that the Bayesian method is more robust when data is scarce while capable of also accounting for uncertainties in the data. Furthermore, given adequate data, the POD based and Bayesian approaches lead to similar results.},
	number = {9},
	journal = {arXiv},
	author = {Renganathan, S. Ashwin and Harada, Kohei and Mavris, Dimitri N.},
	year = {2019},
	keywords = {★},
}

@article{Granados-Ortiz2019,
	title = {On the influence of uncertainty in computational simulations of a high-speed jet flow from an aircraft exhaust},
	volume = {180},
	issn = {00457930},
	url = {https://doi.org/10.1016/j.compfluid.2018.12.003},
	doi = {10.1016/j.compfluid.2018.12.003},
	abstract = {A classic approach to Computational Fluid Dynamics (CFD) is to perform simulations with a fixed set of variables in order to account for parameters and boundary conditions. However, experiments and real-life performance are subject to variability in their conditions. In recent years, the interest of performing simulations under uncertainty is increasing, but this is not yet a common rule, and simulations with lack of information are still taking place. This procedure could be missing details such as whether sources of uncertainty affect dramatic parts in the simulation of the flow. One of the reasons of avoiding to quantify uncertainties is that they usually require to run an unaffordable number of CFD simulations to develop the study. To face this problem, Non-Intrusive Uncertainty Quantification (UQ) has been applied to 3D Reynolds-Averaged Navier-Stokes simulations of an under-expanded jet from an aircraft exhaust with the Spalart-Allmaras turbulent model, in order to assess the impact of inaccuracies and quality in the simulation. To save a large number of computations, sparse grids are used to compute the integrals and built surrogates for UQ. Results show that some regions of the jet plume can be more sensitive than others to variance in both physical and turbulence model parameters. The Spalart-Allmaras turbulent model is demonstrated to have an accurate performance with respect to other turbulent models in RANS, LES and experimental data, and the contribution of a large variance in its parameter is analysed. This investigation explicitly outlines, exhibits and proves the details of the relationship between diverse sources of input uncertainty, the sensitivity of different quantities of interest to said uncertainties and the spatial distribution arising due to their propagation in the simulation of the high-speed jet flow. This analysis represents first numerical study that provides evidence for this heuristic observation.},
	journal = {Computers and Fluids},
	author = {Granados-Ortiz, Francisco Javier and Arroyo, Carlos Pérez and Puigt, Guillaume and Lai, Choi Hong and Airiau, Christophe},
	year = {2019},
	note = {Publisher: Elsevier Ltd},
	keywords = {CFD, Jets, Kriging, Polynomial chaos, RANS, Uncertainty quantification},
	pages = {139--158},
}

@article{Geraci2014,
	title = {Decomposing high-order statistics for sensitivity analysis},
	number = {April 2015},
	journal = {Center for Turbulence Research annual Briefs 2014},
	author = {Geraci, Gianluca and Congedo, Pietro Marco and Iaccarino, Gianluca},
	year = {2014},
	keywords = {★},
}

@article{Wang2020a,
	title = {System inference for the spatio-temporal evolution of infectious diseases: {Michigan} in the time of {COVID}-19},
	volume = {66},
	issn = {14320924},
	url = {https://doi.org/10.1007/s00466-020-01894-2},
	doi = {10.1007/s00466-020-01894-2},
	abstract = {We extend the classical SIR model of infectious disease spread to account for time dependence in the parameters, which also include diffusivities. The temporal dependence accounts for the changing characteristics of testing, quarantine and treatment protocols, while diffusivity incorporates a mobile population. This model has been applied to data on the evolution of the COVID-19 pandemic in the US state of Michigan. For system inference, we use recent advances; specifically our framework for Variational System Identification (Wang et al. in Comput Methods Appl Mech Eng 356:44–74, 2019; arXiv:2001.04816 [cs.CE]) as well as Bayesian machine learning methods.},
	number = {5},
	journal = {Computational Mechanics},
	author = {Wang, Z. and Zhang, X. and Teichert, G. H. and Carrasco-Teja, M. and Garikipati, K.},
	year = {2020},
	note = {arXiv: 2007.00865
Publisher: Springer Berlin Heidelberg},
	keywords = {Compartmental models, Epidemiology, Inverse problems, Neural networks, Optimization, ★},
	pages = {1153--1176},
}

@incollection{doi:10.2514/6.2008-4746,
	title = {Modeling {Swirling} {Jet} {Flows} {Using} a {Hybrid} {RANS}/{LES} {Methodology}},
	url = {https://arc.aiaa.org/doi/abs/10.2514/6.2008-4746},
	booktitle = {44th {AIAA}/{ASME}/{SAE}/{ASEE} {Joint} {Propulsion} {Conference} \&amp; {Exhibit}},
	author = {Chenoweth, James and Kannepalli, Chandrasekhar and Arunajatesan, Srinivasan and Hosangadi, Ashvin},
	doi = {10.2514/6.2008-4746},
}

@article{Udell2019,
	title = {Why {Are} {Big} {Data} {Matrices} {Approximately} {Low} {Rank}?},
	volume = {1},
	doi = {10.1137/18m1183480},
	abstract = {Matrices of (approximate) low rank are pervasive in data science, appearing in recommender systems, movie preferences, topic models, medical records, and genomics. While there is a vast literature on how to exploit low rank structure in these datasets, there is less attention on explaining why the low rank structure appears in the first place. Here, we explain the effectiveness of low rank models in data science by considering a simple generative model for these matrices: we suppose that each row or column is associated to a (possibly high dimensional) bounded latent variable, and entries of the matrix are generated by applying a piecewise analytic function to these latent variables. These matrices are in general full rank. However, we show that we can approximate every entry of an \$m {\textbackslash}times n\$ matrix drawn from this model to within a fixed absolute error by a low rank matrix whose rank grows as \${\textbackslash}mathcal O({\textbackslash}log(m + n))\$. Hence any sufficiently large matrix from such a latent variable model can be approximated, up to a small entrywise error, by a low rank matrix.},
	number = {1},
	journal = {SIAM Journal on Mathematics of Data Science},
	author = {Udell, Madeleine and Townsend, Alex},
	year = {2019},
	note = {arXiv: 1705.07474},
	keywords = {1, 10, 1137, 15a03, 18m1183480, 68p99, ams subject classifications, big data, doi, introduction, johnson--lindenstrauss lemma, low rank matrices, low rank matrices appear, numerous tech-, throughout the sciences},
	pages = {144--160},
}

@article{Champion2019,
	title = {Data-driven discovery of coordinates and governing equations},
	volume = {116},
	issn = {10916490},
	doi = {10.1073/pnas.1906995116},
	abstract = {The discovery of governing equations from scientific data has the potential to transform data-rich fields that lack well-characterized quantitative descriptions. Advances in sparse regression are currently enabling the tractable identification of both the structure and parameters of a nonlinear dynamical system from data. The resulting models have the fewest terms necessary to describe the dynamics, balancing model complexity with descriptive ability, and thus promoting interpretability and generalizability. This provides an algorithmic approach to Occam’s razor for model discovery. However, this approach fundamentally relies on an effective coordinate system in which the dynamics have a simple representation. In this work, we design a custom deep autoencoder network to discover a coordinate transformation into a reduced space where the dynamics may be sparsely represented. Thus, we simultaneously learn the governing equations and the associated coordinate system. We demonstrate this approach on several example high-dimensional systems with low-dimensional behavior. The resulting modeling framework combines the strengths of deep neural networks for flexible representation and sparse identification of nonlinear dynamics (SINDy) for parsimonious models. This method places the discovery of coordinates and models on an equal footing.},
	number = {45},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Champion, Kathleen and Lusch, Bethany and Nathan Kutz, J. and Brunton, Steven L.},
	year = {2019},
	pmid = {31636218},
	note = {arXiv: 1904.02107},
	keywords = {Deep learning, Dynamical systems, Machine learning, Model discovery},
	pages = {22445--22451},
}

@article{Donoho2000,
	title = {High-dimensional data analysis: {The} curses and blessings of dimensionality},
	abstract = {The coming century is surely the century of data. A combination of blind faith and serious purpose makes our society invest massively in the collection and processing of data of all kinds, on scales unimaginable until recently. Hyperspectral Imagery, Internet Portals, Financial tick-by-tick data, and DNA Microarrays are just a few of the better- known sources, feeding data in torrential streams into scientific and business databases worldwide. In traditional statistical data analysis, we think of observations of instances of par- ticular phenomena (e.g. instance ↔ human being), these observations being a vector of values we measured on several variables (e.g. blood pressure, weight, height, ...). In traditional statistical methodology, we assumed many observations and a few, well- chosen variables. The trend today is towards more observations but even more so, to radically larger numbers of variables – voracious, automatic, systematic collection of hyper-informative detail about each observed instance. We are seeing examples where the observations gathered on individual instances are curves, or spectra, or images, or even movies, so that a single observation has dimensions in the thousands or billions, while there are only tens or hundreds of instances available for study. Classical methods are simply not designed to cope with this kind of explosive growth of dimensionality of the observation vector. We can say with complete confidence that in the coming cen- tury, high-dimensional data analysis will be a very significant activity, and completely new methods of high-dimensional data analysis will be developed; we just don’t know what they are yet. Mathematicians are ideally prepared for appreciating the abstract issues involved in finding patterns in such high-dimensional data. Two of the most influential prin- ciples in the coming century will be principles originally discovered and cultivated by mathematicians: the blessings of dimensionality and the curse of dimensionality. The curse of dimensionality is a phrase used by several subfields in the mathematical sciences; I use it here to refer to the apparent intractability of systematically searching through a high-dimensional space, the apparent intractability of accurately approxi- mating a general high-dimensional function, the apparent intractability of integrating a high-dimensional function. The blessings of dimensionality are less widely noted, but they include the concen- tration of measure phenomenon (so-called in the geometry of Banach spaces), which means that certain random fluctuations are very well controlled in high dimensions and the success of asymptotic methods, used widely in mathematical statistics and statistical physics, which suggest that statements about very high-dimensional settings may be made where moderate dimensions would be too complicated. There is a large body of interesting work going on in the mathematical sciences, both to attack the curse of dimensionality in specific ways, and to extend the benefits of dimensionality. I will mention work in high-dimensional approximation theory, in probability theory, and in mathematical statistics. I expect to see in the coming decades many further mathematical elaborations to our inventory of Blessings and Curses, and I expect such contributions to have a broad impact on society’s ability to extract meaning from the massive datasets it has decided to compile. In my talk, I will also draw on my personal research experiences which suggest to me (1) there are substantial chances that by interpreting ongoing development in high-dimensional data analysis, mathematicians can become aware of new problems in harmonic analysis; and (2) that many of the problems of data analysis even in fairly low dimensions are unsolved and are similar to problems in mathematics which have only recently been attacked, and for which only the merest beginnings have been made. Both fields can progress together.},
	journal = {AMS Math Challenges Lecture},
	author = {Donoho, Dl L and {others}},
	year = {2000},
	keywords = {Data Mining. Multivariate Data Analysis. Principal},
	pages = {1--32},
}

@article{Sesia2020,
	title = {A comparison of some conformal quantile regression methods},
	volume = {9},
	issn = {2049-1573},
	doi = {10.1002/sta4.261},
	abstract = {We compare two recently proposed methods that combine ideas from conformal inference and quantile regression to produce locally adaptive and marginally valid prediction intervals under sample exchangeability (Romano et al., 2019; Kivaranovic et al., 2019). First, we prove that these two approaches are asymptotically efficient in large samples, under some additional assumptions. Then we compare them empirically on simulated and real data. Our results demonstrate that the method in Romano et al. (2019) typically yields tighter prediction intervals in finite samples. Finally, we discuss how to tune these procedures by fixing the relative proportions of observations used for training and conformalization.},
	number = {1},
	journal = {Stat},
	author = {Sesia, Matteo and Candès, Emmanuel J.},
	year = {2020},
	note = {arXiv: 1909.05433},
	pages = {1--20},
}

@article{McInnes2018,
	title = {{UMAP}: {Uniform} manifold approximation and projection for dimension reduction},
	issn = {23318422},
	abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAPis constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
	journal = {arXiv},
	author = {McInnes, Leland and Healy, John and Melville, James},
	year = {2018},
	note = {arXiv: 1802.03426},
}

@article{Joseph2015,
	title = {Maximum projection designs for computer experiments},
	volume = {102},
	issn = {14643510},
	doi = {10.1093/biomet/asv002},
	abstract = {Space-filling properties are important in designing computer experiments. The traditional maximin and minimax distance designs consider only space-filling in the full-dimensional space; this can result in poor projections onto lower-dimensional spaces, which is undesirable when only a few factors are active. Restricting maximin distance design to the class of Latin hypercubes can improve one-dimensional projections but cannot guarantee good space-filling properties in larger subspaces. We propose designs that maximize space-filling properties on projections to all subsets of factors. We call our designs maximum projection designs. Our design criterion can be computed at no more cost than a design criterion that ignores projection properties.},
	number = {2},
	journal = {Biometrika},
	author = {Joseph, V. Roshan and Gul, Evren and Ba, Shan},
	year = {2015},
	keywords = {Experimental design, Gaussian process, Latin hypercube design, Screening design, Space-filling .design},
	pages = {371--380},
}

@article{Jung2018,
	title = {Design {Space} {Exploration} for {Powertrain} {Electrification} using {Gaussian} {Processes}},
	volume = {2018-June},
	issn = {07431619},
	doi = {10.23919/ACC.2018.8430899},
	abstract = {Design space exploration of hybrid electric vehicles is an important multi-objective global optimization problem. One of the main objectives is to minimize fuel consumption while maintaining satisfactory driveability performance and vehicle cost. The design problem often includes multiple design options, including different driveline architectures and component sizes, where different candidates have various trade-offs between different, in many cases contradictory, performance requirements. Thus, there is no global optimum but a set of Pareto-optimal solutions to be explored. The objective functions can be expensive to evaluate, due to time-consuming simulations, which requires careful selection of which candidates to evaluate. A design space exploration algorithm is proposed for finding the set of Pareto-optimal solutions when the design search space includes multiple design options. As a case study, powertrain optimization is performed for a medium-sized series hybrid electric delivery truck.},
	journal = {Proceedings of the American Control Conference},
	author = {Jung, Daniel and Ahmed, Qadeer and Rizzoni, Giorgio},
	year = {2018},
	note = {Publisher: AACC
ISBN: 9781538654286},
	pages = {846--851},
}

@article{Callaham2019,
	title = {Robust flow reconstruction from limited measurements via sparse representation},
	volume = {4},
	issn = {2469990X},
	url = {https://doi.org/10.1103/PhysRevFluids.4.103907},
	doi = {10.1103/PhysRevFluids.4.103907},
	abstract = {In many applications it is important to estimate a fluid flow field from limited and possibly corrupt measurements. Current methods in flow estimation often use least squares regression to reconstruct the flow field, finding the minimum-energy solution that is consistent with the measured data. However, this approach may be prone to overfitting and sensitive to noise. To address these challenges we instead seek a sparse representation of the data in a library of examples. Sparse representation has been widely used for image recognition and reconstruction, and it is well-suited to structured data with limited, corrupt measurements. We explore sparse representation for flow reconstruction on a variety of fluid data sets with a wide range of complexity, including vortex shedding past a cylinder at low Reynolds number, a mixing layer, and two geophysical flows. In addition, we compare several measurement strategies and consider various types of noise and corruption over a range of intensities. We find that sparse representation has considerably improved the estimation accuracy and robustness to noise and corruption compared with least squares methods. We also introduce a sparse estimation procedure on local spatial patches for complex multiscale flows that preclude a global sparse representation. Based on these results, sparse representation is a promising framework for extracting useful information from complex flow fields with realistic measurements.},
	number = {10},
	journal = {Physical Review Fluids},
	author = {Callaham, Jared L. and Maeda, Kazuki and Brunton, Steven L.},
	year = {2019},
	note = {arXiv: 1810.06723
Publisher: American Physical Society},
	keywords = {doi:10.1103/PhysRevFluids.4.103907 url:https://doi, ★},
	pages = {103907},
}

@article{Wang2020,
	title = {Double robust principal component analysis},
	volume = {391},
	issn = {18728286},
	doi = {10.1016/j.neucom.2020.01.097},
	abstract = {Robust Principal Component Analysis (RPCA) aiming to recover underlying clean data with low-rank structure from the corrupted data, is a powerful tool in machine learning and data mining. However, in many real-world applications where new data (i.e., out-of-samples) in the testing phase can be unseen in the training procedure, (1) RPCA which is a transductive method can be naturally incapable of handing out-of-samples, and (2) violently applying RPCA into this applications does not explicitly consider the relationships between reconstruction error and low-rank representation. To tackle these problems, in this paper, we propose a Double Robust Principal Component Analysis to deal with the out-of-sample problems, which is termed as DRPCA. More specifically, we integrate a reconstruction error into the criterion function of RPCA. Our proposed model can then benefit from (1) the robustness of principal components to outliers and missing values, (2) the bridge between reconstruction error and low-rank representation, (3) low-rank clean data extraction from new datum by a linear transform. To this end, extensive experiments on several datasets demonstrate its superiority, when comparing with the state-of-the-art models, in several clustering and low-rank recovery tasks.},
	journal = {Neurocomputing},
	author = {Wang, Qianqian and Gao, Quan Xue and Sun, Gan and Ding, Chris},
	year = {2020},
	note = {arXiv: 0912.3599v1},
	keywords = {Double, Low-rank representation, Robust principal component analysis},
	pages = {119--128},
}

@article{Zhang2020,
	title = {Basic {Framework} and {Main} {Methods} of {Uncertainty} {Quantification}},
	volume = {2020},
	author = {Zhang, Juan and Yin, Junping and Wang, Ruili},
	year = {2020},
	keywords = {★},
}

@article{Scherl2020,
	title = {Robust principal component analysis for modal decomposition of corrupt fluid flows},
	volume = {5},
	issn = {2469990X},
	doi = {10.1103/PhysRevFluids.5.054401},
	abstract = {Modal analysis techniques are used to identify patterns and develop reduced-order models in a variety of fluid applications. However, experimentally acquired flow fields may be corrupted with incorrect and missing entries, which may degrade modal decomposition. Here we use robust principal component analysis (RPCA) to improve the quality of flow-field data by leveraging global coherent structures to identify and replace spurious data points. RPCA is a robust variant of principal component analysis, also known as proper orthogonal decomposition in fluids, that decomposes a data matrix into the sum of a low-rank matrix containing coherent structures and a sparse matrix of outliers and corrupt entries. We apply RPCA filtering to a range of fluid simulations and experiments of varying complexities and assess the accuracy of low-rank structure recovery. First, we analyze direct numerical simulations of flow past a circular cylinder at Reynolds number 100 with artificial outliers, alongside similar particle image velocimetry (PIV) measurements at Reynolds number 413. Next, we apply RPCA filtering to a turbulent channel flow simulation from the Johns Hopkins Turbulence database, demonstrating that dominant coherent structures are preserved in the low-rank matrix. Finally, we investigate PIV measurements behind a two-bladed cross-flow turbine that exhibits both broadband and coherent phenomena. In all cases, we find that RPCA filtering extracts dominant coherent structures and identifies and fills in incorrect or missing measurements. The performance is particularly striking when flow fields are analyzed using dynamic mode decomposition, which is sensitive to noise and outliers.},
	number = {5},
	journal = {Physical Review Fluids},
	author = {Scherl, Isabel and Strom, Benjamin and Shang, Jessica K. and Williams, Owen and Polagye, Brian L. and Brunton, Steven L.},
	year = {2020},
	note = {arXiv: 1905.07062},
	keywords = {★},
}

@article{Schulz2018,
	title = {A tutorial on {Gaussian} process regression: {Modelling}, exploring, and exploiting functions},
	volume = {85},
	issn = {10960880},
	url = {https://doi.org/10.1016/j.jmp.2018.03.001},
	doi = {10.1016/j.jmp.2018.03.001},
	abstract = {This tutorial introduces the reader to Gaussian process regression as an expressive tool to model, actively explore and exploit unknown functions. Gaussian process regression is a powerful, non-parametric Bayesian approach towards regression problems that can be utilized in exploration and exploitation scenarios. This tutorial aims to provide an accessible introduction to these techniques. We will introduce Gaussian processes which generate distributions over functions used for Bayesian non-parametric regression, and demonstrate their use in applications and didactic examples including simple regression problems, a demonstration of kernel-encoded prior assumptions and compositions, a pure exploration scenario within an optimal design framework, and a bandit-like exploration–exploitation scenario where the goal is to recommend movies. Beyond that, we describe a situation modelling risk-averse exploration in which an additional constraint (not to sample below a certain threshold) needs to be accounted for. Lastly, we summarize recent psychological experiments utilizing Gaussian processes. Software and literature pointers are also provided.},
	journal = {Journal of Mathematical Psychology},
	author = {Schulz, Eric and Speekenbrink, Maarten and Krause, Andreas},
	year = {2018},
	note = {Publisher: Elsevier Inc.},
	keywords = {Active learning, Bandit problems, Exploration–exploitation, Gaussian process regression, ★},
	pages = {1--16},
}

@article{Urquhart2020,
	title = {Surrogate-based optimisation using adaptively scaled radial basis functions},
	volume = {88},
	issn = {15684946},
	url = {https://doi.org/10.1016/j.asoc.2019.106050},
	doi = {10.1016/j.asoc.2019.106050},
	abstract = {Aerodynamic shape optimisation is widely used in several applications, such as road vehicles, aircraft and trains. This paper investigates the performance of two surrogate-based optimisation methods; a Proper Orthogonal Decomposition-based method and a force-based surrogate model. The generic passenger vehicle DrivAer is used as a test case where the predictive capability of the surrogate in terms of aerodynamic drag is presented. The Proper Orthogonal Decomposition-based method uses simulation results from topologically different meshes by interpolating all solutions to a common mesh for which the decomposition is calculated. Both the Proper Orthogonal Decomposition- and force-based approaches make use of Radial Basis Function interpolation. The Radial Basis Function hyperparameters are optimised using differential evolution. Additionally, the axis scaling is treated as a hyperparameter, which reduces the interpolation error by more than 50\% for the investigated test case. It is shown that the force-based approach performs better than the Proper Orthogonal Decomposition method, especially at low sample counts, both with and without adaptive scaling. The sample points, from which the surrogate model is built, are determined using an optimised Latin Hypercube sampling plan. The Latin Hypercube sampling plan is extended to include both continuous and categorical values, which further improve the surrogate's predictive capability when categorical design parameters, such as on/off parameters, are included in the design space. The performance of the force-based surrogate model is compared with four other gradient-free optimisation techniques: Random Sample, Differential Evolution, Nelder–Mead and Bayesian Optimisation. The surrogate model performed as good as, or better than these algorithms, for 17 out of the 18 investigated benchmark problems.},
	journal = {Applied Soft Computing Journal},
	author = {Urquhart, Magnus and Ljungskog, Emil and Sebben, Simone},
	year = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {Aerodynamics, Bayesian optimisation, Benchmarking, Black box optimisation, Evolutionary algorithm, Global optimisation, Gradient-free, Latin Hypercube Sampling, Optimisation, Proper Orthogonal Decomposition, Radial Basis Function interpolation, Surrogate model},
	pages = {106050},
}

@article{Machine2019,
	title = {I {Mproving} {Machine} {Classification} {Using} {Human}},
	author = {Machine, Mproving and Using, Classification and Uncertainty, Human},
	year = {2019},
	pages = {1--10},
}

@article{Li2018,
	title = {Visualizing the loss landscape of neural nets},
	volume = {2018-Decem},
	issn = {10495258},
	abstract = {Neural network training relies on our ability to find “good” minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple “filter normalization” method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
	number = {Nips 2018},
	journal = {Advances in Neural Information Processing Systems},
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	year = {2018},
	note = {arXiv: 1712.09913},
	pages = {6389--6399},
}

@article{Passi2019,
	title = {Problem formulation and fairness},
	doi = {10.1145/3287560.3287567},
	abstract = {Formulating data science problems is an uncertain and difficult process. It requires various forms of discretionary work to translate high-level objectives or strategic goals into tractable problems, necessitating, among other things, the identification of appropriate target variables and proxies. While these choices are rarely self-evident, normative assessments of data science projects often take them for granted, even though different translations can raise profoundly different ethical concerns. Whether we consider a data science project fair often has as much to do with the formulation of the problem as any property of the resulting model. Building on six months of ethnographic fieldwork with a corporate data science team-and channeling ideas from sociology and history of science, critical data studies, and early writing on knowledge discovery in databases-we describe the complex set of actors and activities involved in problem formulation. Our research demonstrates that the specification and operationalization of the problem are always negotiated and elastic, and rarely worked out with explicit normative considerations in mind. In so doing, we show that careful accounts of everyday data science work can help us better understand how and why data science problems are posed in certain ways-and why specific formulations prevail in practice, even in the face of what might seem like normatively preferable alternatives. We conclude by discussing the implications of our findings, arguing that effective normative interventions will require attending to the practical work of problem formulation.},
	journal = {FAT* 2019 - Proceedings of the 2019 Conference on Fairness, Accountability, and Transparency},
	author = {Passi, Samir and Barocas, Solon},
	year = {2019},
	note = {ISBN: 9781450361255},
	keywords = {Data Science, Fairness, Machine Learning, Problem Formulation, Target Variable},
	pages = {39--48},
}

@article{Olver2020,
	title = {Fast {Algorithms} using {Orthogonal} {Polynomials}},
	issn = {22965017},
	doi = {10.1007/978-3-030-47048-7_4},
	abstract = {We review recent advances in algorithms for quadrature, transforms, differen- tial equations and singular integral equations using orthogonal polynomials. Quadrature based on asymptotics has facilitated optimal complexity quadrat- ure rules, allowing for efficient computation of quadrature rules with millions of nodes. Transforms based on rank structures in change-of-basis operators allow for quasi-optimal complexity, including in multivariate settings such as on triangles and for spherical harmonics. Ordinary and partial differential equations can be solved via sparse linear algebra when set up using ortho- gonal polynomials as a basis, provided that care is taken with the weights of orthogonality. A similar idea, together with low-rank approximation, gives an efficient method for solving singular integral equations. These techniques can be combined to produce high-performance codes for a wide range of problems that appear in applications. Downloaded},
	journal = {Applied and Numerical Harmonic Analysis},
	author = {Olver, Sheehan and Slevinsky, Richard and Townsend, Alex},
	year = {2020},
	pages = {121--191},
}

@article{Martinsson2020,
	title = {Randomized numerical linear algebra: {Foundations} \& {Algorithms}},
	doi = {10.1017/S0962492920000021},
	abstract = {This survey describes probabilistic algorithms for linear algebra computations, such as factorizing matrices and solving linear systems. It focuses on techniques that have a proven track record for real-world problem instances. The paper treats both the theoretical foundations of the subject and the practical computational issues. Topics covered include norm estimation; matrix approximation by sampling; structured and unstructured random embeddings; linear regression problems; low-rank approximation; subspace iteration and Krylov methods; error estimation and adaptivity; interpolatory and CUR factorizations; Nyström approximation of positive-semidefinite matrices; single view (“streaming”) algorithms; full rank-revealing factorizations; solvers for linear systems; and approximation of kernel matrices that arise in machine learning and in scientific computing.},
	journal = {arXiv},
	author = {Martinsson, Per Gunnar and Tropp, Joel A.},
	year = {2020},
	note = {arXiv: 2002.01387},
	pages = {403--572},
}

@article{Xiu2002,
	title = {{THE} {WIENER} – {ASKEY} {POLYNOMIAL} {CHAOS} {FOR} {STOCHASTIC} orthogonal polynomials},
	volume = {24},
	number = {2},
	journal = {Society},
	author = {Xiu, Dongbin and Karniadakis, George E M},
	year = {2002},
	keywords = {65c20, 65c30, ams subject classifications, askey scheme, equations, galerkin projection, orthogonal polynomials, pii, polynomial chaos, s1064827501387826, spectral methods, stochastic differential},
	pages = {619--644},
}

@article{Xiao2019,
	title = {Quantification of model uncertainty in {RANS} simulations: {A} review},
	volume = {108},
	issn = {03760421},
	url = {https://doi.org/10.1016/j.paerosci.2018.10.001},
	doi = {10.1016/j.paerosci.2018.10.001},
	abstract = {In computational fluid dynamics simulations of industrial flows, models based on the Reynolds-averaged Navier–Stokes (RANS) equations are expected to play an important role in decades to come. However, model uncertainties are still a major obstacle for the predictive capability of RANS simulations. This review examines both the parametric and structural uncertainties in turbulence models. We review recent literature on data-free (uncertainty propagation) and data-driven (statistical inference) approaches for quantifying and reducing model uncertainties in RANS simulations. Moreover, the fundamentals of uncertainty propagation and Bayesian inference are introduced in the context of RANS model uncertainty quantification. Finally, the literature on uncertainties in scale-resolving simulations is briefly reviewed with particular emphasis on large eddy simulations.},
	number = {October 2018},
	journal = {Progress in Aerospace Sciences},
	author = {Xiao, Heng and Cinnella, Paola},
	year = {2019},
	note = {arXiv: 1806.10434
Publisher: Elsevier Ltd},
	keywords = {Bayesian inference, Machine learning, Model-form uncertainty, Reynolds-averaged Navier–Stokes equations, Turbulence modeling},
	pages = {1--31},
}

@article{Rumpfkeil2020,
	title = {Multi-fidelity, gradient-enhanced, and locally optimized sparse polynomial chaos and kriging surrogate models applied to benchmark problems},
	volume = {1 PartF},
	doi = {10.2514/6.2020-0677},
	abstract = {In this paper, multi-fidelity, gradient-enhanced, and locally optimized sparse polynomial chaos expansion (PCE) and kriging surrogate models are constructed in lieu of solely using computationally expensive high-fidelity engineering analyses. Once an accurate surrogate model is built, it can be used for evaluating a large number of designs for design space exploration or for uncertainty quantification. To demonstrate that accurate multi-fidelity, gradient-enhanced, and locally optimized surrogate models can be obtained at lower computational cost than basic high-fidelity ones, a number of benchmark problems are employed. These include polynomial and non-polynomial analytical functions of arbitrary dimension, a heterogeneous non-polynomial analytical function in one, two, and three dimensions as well as a coupled spring-mass-system leading to a linear system of ODEs of arbitrary dimension.},
	number = {January},
	journal = {AIAA Scitech 2020 Forum},
	author = {Rumpfkeil, Markus P. and Beran, Philip S.},
	year = {2020},
	note = {ISBN: 9781624105951},
	keywords = {★},
	pages = {1--23},
}

@article{Shahane2019,
	title = {Uncertainty quantification in three dimensional natural convection using polynomial chaos expansion and deep neural networks},
	volume = {139},
	issn = {00179310},
	doi = {10.1016/j.ijheatmasstransfer.2019.05.014},
	abstract = {This paper analyzes the effects of input uncertainties on the outputs of a three dimensional natural convection problem in a differentially heated cubical enclosure. Two different cases are considered for parameter uncertainty propagation and global sensitivity analysis. In case A, stochastic variation is introduced in the two non-dimensional parameters (Rayleigh and Prandtl numbers) with an assumption that the boundary temperature is uniform. Being a two dimensional stochastic problem, the polynomial chaos expansion (PCE) method is used as a surrogate model. Case B deals with non-uniform stochasticity in the boundary temperature. Instead of the traditional Gaussian process model with the Karhunen-Loève expansion, a novel approach is successfully implemented to model uncertainty in the boundary condition. The boundary is divided into multiple domains and the temperature imposed on each domain is assumed to be an independent and identically distributed (i.i.d) random variable. Deep neural networks are trained with the boundary temperatures as inputs and Nusselt number, internal temperature or velocities as outputs. The number of domains which is essentially the stochastic dimension is 4, 8, 16 or 32. Rigorous training and testing process shows that the neural network is able to approximate the outputs to a reasonable accuracy. For a high stochastic dimension such as 32, it is computationally expensive to fit the PCE. This paper demonstrates a novel way of using the deep neural network as a surrogate modeling method for uncertainty quantification with the number of simulations much fewer than that required for fitting the PCE, thus, saving the computational cost.},
	journal = {International Journal of Heat and Mass Transfer},
	author = {Shahane, Shantanu and Aluru, Narayana R. and Vanka, Surya Pratap},
	year = {2019},
	note = {arXiv: 1810.11934},
	keywords = {Deep neural networks, Natural convection, Polynomial chaos expansion, Uncertainty quantification},
	pages = {613--631},
}

@article{Moreland2005,
	title = {Diverging {Color} {Maps} for {Scientific} {Visualization} ( {Expanded} )},
	author = {Moreland, Kenneth},
	year = {2005},
}

@article{Li2020,
	title = {Fourier {Neural} {Operator} for {Parametric} {Partial} {Differential} {Equations}},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	year = {2020},
	note = {arXiv: 2010.08895v1},
}

@article{carlberg_network_nodate,
	title = {The network uncertainty quantification method for propagating uncertainties in component-based systems},
	author = {Carlberg, Kevin and Guzzetti, Sofia and Khalil, Mohammad and Sargsyan, Khachik},
	note = {arXiv: 1908.11476v1},
	keywords = {1, 35r60, 49m20, 49m27, 60h15, 60h35, 65c20, 65n55, ams subject classifications, and engineering, anderson acceleration, domain decomposition, introduction, many systems in science, network, ranging from power grids, relaxation methods, to gas, uncertainty propagation, uncertainty quantification, ★},
	pages = {1--34},
}

@misc{karhunen_representationofsignalsnonlinearpca_karhunenpdf_nodate,
	title = {{RepresentationOfSignalsNonLinearPCA}\_Karhunen.pdf},
	author = {Karhunen, J},
	keywords = {★},
}

@article{scholz_gene_2005,
	title = {Gene expression {Non}-linear {PCA} : a missing data approach},
	volume = {21},
	doi = {10.1093/bioinformatics/bti634},
	number = {20},
	author = {Scholz, Matthias and Kaplan, Fatma and Guy, Charles L and Kopka, Joachim and Selbig, Joachim},
	year = {2005},
	keywords = {★},
	pages = {3887--3895},
}

@article{hasegawa_machine-learning-based_2020,
	title = {Machine-learning-based reduced-order modeling for unsteady flows around bluff bodies of various shapes},
	volume = {34},
	issn = {1432-2250},
	url = {https://doi.org/10.1007/s00162-020-00528-w},
	doi = {10.1007/s00162-020-00528-w},
	number = {4},
	journal = {Theoretical and Computational Fluid Dynamics},
	author = {Hasegawa, Kazuto and Fukami, Kai and Murata, Takaaki and Fukagata, Koji},
	year = {2020},
	note = {Publisher: Springer Berlin Heidelberg},
	keywords = {Reduced-order modeling,Machine learning,Unsteady w, machine learning, reduced-order modeling, unsteady wake},
	pages = {367--383},
}

@article{bates_experiments_2011,
	title = {Experiments {Using} a {Permutation} {Genetic} {Algorithm}},
	number = {April 2004},
	author = {Bates, Stuart J and Sienz, Johann and Toropov, Vassili V and Latin, Eglais and Doe, Hypercube and Experiments, Design Of and Doe, Latin Hypercube and Latin, Optimal and Doe, Hypercube and Lecturer, Senior and Aerospace, Programme and Design, Principal and Specialist, Optimization},
	year = {2011},
	pages = {1--7},
}

@article{urquhart_surrogate-based_2020,
	title = {Surrogate-based optimisation using adaptively scaled radial basis functions},
	volume = {88},
	issn = {1568-4946},
	url = {https://doi.org/10.1016/j.asoc.2019.106050},
	doi = {10.1016/j.asoc.2019.106050},
	journal = {Applied Soft Computing Journal},
	author = {Urquhart, Magnus and Ljungskog, Emil and Sebben, Simone},
	year = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {bayesian optimisation, black box optimisation, evolutionary algorithm, ★},
	pages = {106050},
}

@article{Mckay1979,
	title = {A {Comparison} of {Three} {Methods} for {Selecting} {Values} of {Input} {Variables} in the {Analysis} of {Output} from a {Computer} {Code}},
	volume = {21},
	doi = {10.1080/00401706.1979.10489755},
	number = {2},
	author = {Mckay, M D and Beckman, R J and Conover, W J},
	year = {1979},
	pages = {239--245},
}

@article{gerritsma_time-dependent_2010,
	title = {Time-dependent generalized polynomial chaos},
	volume = {229},
	issn = {0021-9991},
	url = {http://dx.doi.org/10.1016/j.jcp.2010.07.020},
	doi = {10.1016/j.jcp.2010.07.020},
	number = {22},
	journal = {Journal of Computational Physics},
	author = {Gerritsma, Marc and Steen, Jan-bart Van Der and Vos, Peter and Karniadakis, George},
	year = {2010},
	note = {Publisher: Elsevier Inc.},
	pages = {8333--8363},
}

@article{Huan2018b,
	title = {Global {Sensitivity} {Analysis} and {Estimation} of {Model} {Error} , {Toward} {Uncertainty} {Quantification} in {Scramjet} {Computations}},
	volume = {56},
	doi = {10.2514/1.J056278},
	number = {3},
	author = {Huan, Xun and Safta, Cosmin and Sargsyan, Khachik},
	year = {2018},
}

@article{Chastaing2012,
	title = {Generalized {Hoeffding}-{Sobol} decomposition for dependent variables - application to sensitivity analysis},
	volume = {6},
	issn = {19357524},
	doi = {10.1214/12-EJS749},
	abstract = {In this paper, we consider a regression model built on dependent variables. This regression modelizes an input output relationship. Under boundedness type assumptions on the joint density function of the input variables, we show that a generalized Hoeffding-Sobol decomposition is available. This leads to new indices measuring the sensitivity of the output with respect to the input variables. We also study and discuss the estimation of these new indices.},
	journal = {Electronic Journal of Statistics},
	author = {Chastaing, Gaelle and Gamboa, Fabrice and Prieur, Clémentine},
	year = {2012},
	note = {arXiv: 1112.1788},
	keywords = {Dependent variables, Hoeffding decomposition, Sensitivity index, Sobol decomposition, ★},
	pages = {2420--2448},
}

@article{Cousin2019,
	title = {On the consistency of {Sobol} indices with respect to stochastic ordering of model parameters},
	volume = {23},
	issn = {12623318},
	doi = {10.1051/ps/2018001},
	abstract = {In the past decade, Sobol's variance decomposition has been used as a tool to assess how the output of a model is affected by the uncertainty on its input parameters. We show some links between global sensitivity analysis and stochastic ordering theory. More specifically, we study the influence of inputs' distributions on Sobol indices in relation with stochastic orders. This gives an argument in favor of using Sobol's indices in uncertainty quantification, as one indicator among others.},
	journal = {ESAIM - Probability and Statistics},
	author = {Cousin, A. and Janon, A. and Maume-Deschamps, V. and Niang, I.},
	year = {2019},
	note = {arXiv: 1407.5565},
	keywords = {Sensitivity analysis, Sobol indices, Stochastic orders, ★},
	pages = {387--408},
}

@article{Johnson2012,
	title = {Hybrid space-filling designs for computer experiments},
	doi = {10.1007/978-3-7908-2846-7-19},
	abstract = {Computer models play an increasingly important role in engineering design and in the study of complex systems, where physical experiments on the real system or even a prototype are prohibitively expensive. Both deterministic and stochastic computer models are used in these situations. A deterministic computer model is a set of complex equations whose solution depends on the input conditions and the levels of design factors or parameters but not on random elements. Examples include finite element models and computational fluid dynamics models. Spacefilling designs are usually employed to study these deterministic computer models and often the modeling strategy involves fitting a spatial correlation or Kriging model (the Gaussian stochastic process model) to the data, because this model interpolates the experimental data exactly.We provide a survey of these designs and the modeling strategy, and propose a new type of hybrid space-filling design. The new design is a hybrid consisting of design points from a traditional space-filling design augmented by runs from a near saturated I-optimal design for a polynomial. We illustrate the construction of these designs with examples, and demonstrate their performance in response prediction for several situations. A comparison with standard space-filling designs is provided. © Springer-Verlag Berlin Heidelberg 2012.},
	journal = {Frontiers in Statistical Quality Control 10},
	author = {Johnson, Rachel T. and Montgomery, Douglas C. and Kennedy, Kathryn S.},
	year = {2012},
	keywords = {Gaussian process model, Linear regression, Optimal design, Response surface},
	pages = {287--301},
}

@inproceedings{Sanchez2015,
	title = {Work {Smarter}, {Not} {Harder}: {A} {Tutorial} {On} {Designing} and {Conducting} {Simulation} {Experiments}},
	isbn = {978-1-4673-9743-8},
	abstract = {Simulation models are integral to modern scientific research, national defense, industry and manufacturing, and in public policy debates. These models tend to be extremely complex, often with thousands of factors and many sources of uncertainty. To understand the impact of these factors and their interactions on model outcomes requires efficient, high-dimensional design of experiments. Unfortunately, all too often, many large-scale simulation models continue to be explored in ad hoc ways. This suggests that more simulation researchers and practitioners need to be aware of the power of experimental design in order to get the most from their simulation studies. In this tutorial, we demonstrate the basic concepts important for design and conducting simulation experiments, and provide references to other resources for those wishing to learn more. This tutorial (an update of previous WSC tutorials) will prepare you to make your next simulation study a simulation experiment.},
	booktitle = {2015 {Winter} {Simulation} {Conference} ({WSC})., 2015},
	author = {Sanchez, Susan and Wan, Hong},
	year = {2015},
	pages = {779--790},
}

@article{Joseph2016,
	title = {Space-filling designs for computer experiments: {A} review},
	volume = {28},
	issn = {15324222},
	doi = {10.1080/08982112.2015.1100447},
	abstract = {Improving the quality of a product/process using a computer simulator is a much less expensive option than the real physical testing. However, simulation using computationally intensive computer models can be time consuming and, therefore, directly doing the optimization on the computer simulator can be infeasible. Experimental design and statistical modeling techniques can be used to overcome this problem. This article reviews experimental designs known as space-filling designs that are suitable for computer simulations. In the article, a special emphasis is given for a recently developed space-filling design called maximum projection design. Its advantages are illustrated using a simulation conducted for optimizing a milling process.},
	number = {1},
	journal = {Quality Engineering},
	author = {Joseph, V. Roshan},
	year = {2016},
	keywords = {design of experiments, kriging, optimization, simulation},
	pages = {28--35},
}

@article{Guo2014,
	title = {Time-dependent global sensitivity analysis for long-term degeneracy model using polynomial chaos},
	volume = {2014},
	issn = {16878140},
	doi = {10.1155/2014/719825},
	abstract = {Global sensitivity is used to quantify the influence of uncertain model inputs on the output variability of static models in general. However, very few approaches can be applied for the sensitivity analysis of long-term degeneracy models, as far as time-dependent reliability is concerned. The reason is that the static sensitivity may not reflect the completed sensitivity during the entire life circle. This paper presents time-dependent global sensitivity analysis for long-term degeneracy models based on polynomial chaos expansion (PCE). Sobol' indices are employed as the time-dependent global sensitivity since they provide accurate information on the selected uncertain inputs. In order to compute Sobol' indices more efficiently, this paper proposes a moving least squares (MLS) method to obtain the time-dependent PCE coefficients with acceptable simulation effort. Then Sobol' indices can be calculated analytically as a postprocessing of the time-dependent PCE coefficients with almost no additional cost. A test case is used to show how to conduct the proposed method, then this approach is applied to an engineering case, and the time-dependent global sensitivity is obtained for the long-term degeneracy mechanism model. © 2014 Jianbin Guo et al.},
	journal = {Advances in Mechanical Engineering},
	author = {Guo, Jianbin and Du, Shaohua and Wang, Yao and Zeng, Shengkui},
	year = {2014},
}

@article{arge_air_2013,
	title = {Air {Force} {Data} {Assimilative} {Photospheric} {Flux} {Transport} ( {ADAPT} ) {Model} {Air} {Force} {Data} {Assimilative} {Photospheric} {Flux} {Transport} ( {ADAPT} ) {Model}},
	volume = {343},
	number = {April 2010},
	author = {Arge, C Nick and Henney, Carl J and Koller, Josef and Compeau, C Rich and Young, Shawn and Mackenzie, David and Fay, Alex and John, W and Arge, C Nick and Henney, Carl J and Koller, Josef and Compeau, C Rich and Mackenzie, David and Fay, Alex and Harvey, John W},
	year = {2013},
	note = {ISBN: 9780735407596},
	keywords = {50, 60, 95, 96, bh, ci, corona, data assimilation, heliosphere and by extension, mz, pacs, pc, q, solar flux transport, solar magnetic fields, solar photosphere, the solar wind-magnetosphere interaction, the state of the},
}

@article{Lincoln1967,
	title = {Geomagnetic indices},
	volume = {11},
	issn = {00746142},
	doi = {10.1016/B978-0-12-480301-5.50009-4},
	number = {P1},
	journal = {International Geophysics},
	author = {Menvielle, Michel and Iyemori, Toshihiko},
	year = {1967},
	pages = {67--100},
}

@article{ManchesterIV2004,
	title = {Modeling a space weather event from the {Sun} to the {Earth}: {CME} generation and interplanetary propagation},
	volume = {109},
	issn = {21699402},
	doi = {10.1029/2003JA010150},
	abstract = {We present a three-dimensional (3-D) numerical ideal magnetohydrodynamics (MHD) model describing the time-dependent expulsion of a coronal mass ejection (CME) from the solar corona propagating to 1 astronomical unit (AU). The simulations are performed using the Block Adaptive Tree Solar-Wind Roe Upwind Scheme (BATS-R-US) code. We begin by developing a global steady-state model of the corona that possesses high-latitude coronal holes and a helmet streamer structure with a current sheet at the equator. The Archimedean spiral topology of the interplanetary magnetic field is reproduced along with fast and slow speed solar wind. Within this model system, we drive a CME to erupt by the introduction of a Gibson-Low magnetic flux rope that is anchored at both ends in the photosphere and embedded in the helmet streamer in an initial state of force imbalance. The flux rope rapidly expands and is ejected from the corona with maximum speeds in excess of 1000 km/s. Physics-based adaptive mesh refinement (AMR) allows us to capture the structure of the CME focused on a particular Sun-Earth line with high spatial resolution given to the bow shock ahead of the flux rope as well as to the current sheet behind. The CME produces a large magnetic cloud at 1 AU ({\textgreater}100 R⊙) in which Bz undergoes a full rotation from north to south with an amplitude of 20 nT. In a companion paper, we find that the CME is very effective in generating strong geomagnetic activity at the Earth in two ways. First, through the strong sustained southward Bz (lasting more than 10 hours) and, second, by a pressure increase associated with the CME-driven shock that compresses the magnetosphere. Copyright 2004 by the American Geophysical Union.},
	number = {A2},
	journal = {Journal of Geophysical Research: Space Physics},
	author = {Manchester IV, Ward B. and Gombosi, Tamas I. and Roussev, Ilia and Ridley, Aaron and De Zeeuw, Darren L. and Sokolov, I. V. and Powell, Kenneth G. and Tóth, Gábor},
	year = {2004},
	keywords = {Coronal mass ejection, Magnetohydrodynamics, Space weather},
}

@article{Singer2001,
	title = {Space weather forecasting: {A} grand challenge},
	volume = {125},
	issn = {23288779},
	doi = {10.1029/GM125p0023},
	abstract = {Space Environment Center (SEC) is the United States' official source of space weather alerts, warnings, and forecasts. Forecasts are used to support activities that are impacted by space weather such as electric power transmission, satellite operations, humans in space, navigation, and communication. This article presents a brief review of current space weather forecasting capabilities, and then focuses on the science, the models, the data, the new technologies, and the process for transitioning research into operations that is needed to meet the challenge to improve space weather forecasting in the new millennium. Forecasting critical parameters such as the interplanetary magnetic field at the magnetopause, and critical events such as coronal mass ejections are two examples of challenges to the research, observation, and modeling communities. Major improvements in space weather forecasting will be achieved when these, as well as other, challenges are met. The forecasting challenge is also discussed in the context of the goals of the US National Space Weather Program (NSWP) and other international activities.},
	journal = {Geophysical Monograph Series},
	author = {Singer, H. J. and Heckman, G. R. and Hirman, J. W.},
	year = {2001},
	note = {ISBN: 9781118668351},
	pages = {23--29},
}

@article{Sachdeva2019,
	title = {Validation of the {Alfvén} {Wave} {Solar} {Atmosphere} {Model} ({AWSoM}) with {Observations} from the {Low} {Corona} to 1 au},
	volume = {887},
	issn = {0004-637X},
	url = {http://dx.doi.org/10.3847/1538-4357/ab4f5e},
	doi = {10.3847/1538-4357/ab4f5e},
	abstract = {We perform a validation study of the latest version of the Alfv{\textbackslash}'\{e\}n Wave Solar atmosphere Model (AWSoM) within the Space Weather Modeling Framework (SWMF). To do so, we compare the simulation results of the model with a comprehensive suite of observations for Carrington rotations representative of the solar minimum conditions extending from the solar corona to the heliosphere up to the Earth. In the low corona (\$r {\textless} 1.25\$ {\textbackslash}Rs), we compare with EUV images from both STEREO-A/EUVI and SDO/AIA and to three-dimensional (3-D) tomographic reconstructions of the electron temperature and density based on these same data. We also compare the model to tomographic reconstructions of the electron density from SOHO/LASCO observations (\$2.55 {\textless} r {\textless} 6.0\${\textbackslash}Rs). In the heliosphere, we compare model predictions of solar wind speed with velocity reconstructions from InterPlanetary Scintillation (IPS) observations. For comparison with observations near the Earth, we use OMNI data. Our results show that the improved AWSoM model performs well in quantitative agreement with the observations between the inner corona and 1 AU. The model now reproduces the fast solar wind speed in the polar regions. Near the Earth, our model shows good agreement with observations of solar wind velocity, proton temperature and density. AWSoM offers an extensive application to study the solar corona and larger heliosphere in concert with current and future solar missions as well as being well suited for space weather predictions.},
	number = {1},
	journal = {The Astrophysical Journal},
	author = {Sachdeva, Nishtha and Holst, Bart van der and Manchester, Ward B. and Tóth, Gabor and Chen, Yuxi and Lloveras, Diego G. and Vásquez, Alberto M. and Lamy, Philippe and Wojak, Julien and Jackson, Bernard V. and Yu, Hsiu-Shan and Henney, Carl J.},
	year = {2019},
	note = {arXiv: 1910.08110
Publisher: IOP Publishing},
	keywords = {Solar corona,Magnetohydrodynamics,Solar coronal wa},
	pages = {83},
}

@article{VanDerHolst2014,
	title = {Alfvén wave solar model ({AWSoM}): {Coronal} heating},
	volume = {782},
	issn = {15384357},
	doi = {10.1088/0004-637X/782/2/81},
	abstract = {We present a new version of the Alfvén wave solar model, a global model from the upper chromosphere to the corona and the heliosphere. The coronal heating and solar wind acceleration are addressed with low-frequency Alfvén wave turbulence. The injection of Alfvén wave energy at the inner boundary is such that the Poynting flux is proportional to the magnetic field strength. The three-dimensional magnetic field topology is simulated using data from photospheric magnetic field measurements. This model does not impose open-closed magnetic field boundaries; those develop self-consistently. The physics include the following. (1) The model employs three different temperatures, namely the isotropic electron temperature and the parallel and perpendicular ion temperatures. The firehose, mirror, and ion-cyclotron instabilities due to the developing ion temperature anisotropy are accounted for. (2) The Alfvén waves are partially reflected by the Alfvén speed gradient and the vorticity along the field lines. The resulting counter-propagating waves are responsible for the nonlinear turbulent cascade. The balanced turbulence due to uncorrelated waves near the apex of the closed field lines and the resulting elevated temperatures are addressed. (3) To apportion the wave dissipation to the three temperatures, we employ the results of the theories of linear wave damping and nonlinear stochastic heating. (4) We have incorporated the collisional and collisionless electron heat conduction. We compare the simulated multi-wavelength extreme ultraviolet images of CR2107 with the observations from STEREO/EUVI and the Solar Dynamics Observatory/AIA instruments. We demonstrate that the reflection due to strong magnetic fields in the proximity of active regions sufficiently intensifies the dissipation and observable emission. © 2014. The American Astronomical Society. All rights reserved..},
	number = {2},
	journal = {Astrophysical Journal},
	author = {Van Der Holst, B. and Sokolov, I. V. and Meng, X. and Jin, M. and Manchester, W. B. and Tóth, G. and Gombosi, T. I.},
	year = {2014},
	keywords = {interplanetary medium, magnetohydrodynamics, methods, mhd, numerical, solar wind},
}

@article{Marin2012,
	title = {Approximate {Bayesian} computational methods},
	volume = {22},
	issn = {09603174},
	doi = {10.1007/s11222-011-9288-2},
	abstract = {Approximate Bayesian Computation (ABC) methods, also known as likelihood-free techniques, have appeared in the past ten years as the most satisfactory approach to intractable likelihood problems, first in genetics then in a broader spectrum of applications. However, these methods suffer to some degree from calibration difficulties that make them rather volatile in their implementation and thus render them suspicious to the users of more traditional Monte Carlo methods. In this survey, we study the various improvements and extensions brought on the original ABC algorithm in recent years. © 2011 Springer Science+Business Media, LLC.},
	number = {6},
	journal = {Statistics and Computing},
	author = {Marin, Jean Michel and Pudlo, Pierre and Robert, Christian P. and Ryder, Robin J.},
	year = {2012},
	note = {arXiv: 1101.0955
ISBN: 1122201192},
	keywords = {ABC methodology, Bayesian model choice, Bayesian statistics, DIYABC, Likelihood-free methods},
	pages = {1167--1180},
}

@article{Jin2017,
	title = {Data-{Constrained} {Coronal} {Mass} {Ejections} in a {Global} {Magnetohydrodynamics} {Model}},
	volume = {834},
	issn = {0004-637X},
	url = {http://dx.doi.org/10.3847/1538-4357/834/2/173},
	doi = {10.3847/1538-4357/834/2/173},
	abstract = {We present a first-principles-based coronal mass ejection (CME) model suitable for both scientific and operational purposes by combining a global magnetohydrodynamics (MHD) solar wind model with a flux rope-driven CME model. Realistic CME events are simulated self-consistently with high fidelity and forecasting capability by constraining initial flux rope parameters with observational data from GONG, SOHO/LASCO, and STEREO/COR. We automate this process so that minimum manual intervention is required in specifying the CME initial state. With the newly developed data-driven Eruptive Event Generator Gibson-Low (EEGGL), we present a method to derive Gibson-Low (GL) flux rope parameters through a handful of observational quantities so that the modeled CMEs can propagate with the desired CME speeds near the Sun. A test result with CMEs launched with different Carrington rotation magnetograms are shown. Our study shows a promising result for using the first-principles-based MHD global model as a forecasting tool, which is capable of predicting the CME direction of propagation, arrival time, and ICME magnetic field at 1 AU (see companion paper by Jin et al. 2016b).},
	number = {2},
	journal = {The Astrophysical Journal},
	author = {Jin, M. and Manchester, W. B. and van der Holst, B. and Sokolov, I. and Tóth, G. and Mullinix, R. E. and Taktakishvili, A. and Chulaki, A. and Gombosi, T. I.},
	year = {2017},
	note = {Publisher: IOP Publishing},
	keywords = {Sun: corona, Sun: coronal mass ejections (CMEs), cmes, corona, coronal mass ejections, interplanetary medium, magnetohydrodynamics, magnetohydrodynamics (MHD), methods, methods: numerical, mhd, numerical, solar wind, sun},
	pages = {173},
}

@book{OHagan2006,
	title = {Uncertain {Judgements}},
	isbn = {978-0-470-02999-2},
	author = {O'Hagan, Anthony},
	year = {2006},
	doi = {10.1002/9780470683019.scard},
	note = {Publication Title: Comparing Clinical Measurement Methods},
}

@article{Raissi2019,
	title = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {10902716},
	url = {https://doi.org/10.1016/j.jcp.2018.10.045},
	doi = {10.1016/j.jcp.2018.10.045},
	abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.},
	journal = {Journal of Computational Physics},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
	year = {2019},
	note = {Publisher: Elsevier Inc.},
	keywords = {Data-driven scientific computing, Machine learning, Nonlinear dynamics, Predictive modeling, Runge–Kutta methods},
	pages = {686--707},
}

@article{Huan2018a,
	title = {Global sensitivity analysis and estimation of model error, toward uncertainty quantification in scramjet computations},
	volume = {56},
	issn = {00011452},
	doi = {10.2514/1.J056278},
	abstract = {The development of scramjet engines is an important research area for advancing hypersonic and orbital flights. Progress toward optimal engine designs requires accurate flow simulations together with uncertainty quantification. However, performing uncertainty quantification for scramjet simulations is challenging due to the large number of uncertainparameters involvedandthe high computational costofflow simulations. These difficulties are addressedin this paper by developing practical uncertainty quantification algorithms and computational methods, and deploying themin the current studyto large-eddy simulations ofajet incrossflow inside a simplified HIFiRE Direct Connect Rig scramjet combustor. First, global sensitivity analysis is conducted to identify influential uncertain input parameters, which can help reduce the system's stochastic dimension. Second, because models of different fidelity are used in the overall uncertainty quantification assessment, a framework for quantifying and propagating the uncertainty due to model error is presented. These methods are demonstrated on a nonreacting jet-in-crossflow test problem in a simplified scramjet geometry, with parameter space up to 24 dimensions, using static and dynamic treatments of the turbulence subgrid model, and with two-dimensional and three-dimensional geometries.},
	number = {3},
	journal = {AIAA Journal},
	author = {Huan, Xun and Safta, Cosmin and Geraci, Gianluca and Eldred, Michael S. and Vane, Zachary P. and Lacaze, Guilhem and Oefelein, Joseph C. and Najm, Habib N.},
	year = {2018},
	note = {arXiv: 1707.09478},
	pages = {1170--1184},
}

@article{Jiao2020,
	title = {Solar {Flare} {Intensity} {Prediction} {With} {Machine} {Learning} {Models}},
	volume = {18},
	issn = {15427390},
	doi = {10.1029/2020SW002440},
	abstract = {We develop a mixed long short-term memory (LSTM) regression model to predict the maximum solar flare intensity within a 24-hr time window 0–24, 6–30, 12–36, and 24–48 hr ahead of time using 6, 12, 24, and 48 hr of data (predictors) for each Helioseismic and Magnetic Imager (HMI) Active Region Patch (HARP). The model makes use of (1) the Space-Weather HMI Active Region Patch (SHARP) parameters as predictors and (2) the exact flare intensities instead of class labels recorded in the Geostationary Operational Environmental Satellites (GOES) data set, which serves as the source of the response variables. Compared to solar flare classification, the model offers us more detailed information about the exact maximum flux level, that is, intensity, for each occurrence of a flare. We also consider classification models built on top of the regression model and obtain better results in solar flare classifications as compared to Chen et al. (2019, https://doi.org/10.1029/2019SW002214). Our results suggest that the most efficient time period for predicting the solar activity is within 24 hr before the prediction time using the SHARP parameters and the LSTM model.},
	number = {7},
	journal = {Space Weather},
	author = {Jiao, Zhenbang and Sun, Hu and Wang, Xiantong and Manchester, Ward and Gombosi, Tamas and Hero, Alfred and Chen, Yang},
	year = {2020},
	note = {arXiv: 1912.06120},
	pages = {1--28},
}

@article{Toth2012,
	title = {Adaptive numerical algorithms in space weather modeling},
	volume = {231},
	issn = {10902716},
	doi = {10.1016/j.jcp.2011.02.006},
	abstract = {Space weather describes the various processes in the Sun-Earth system that present danger to human health and technology. The goal of space weather forecasting is to provide an opportunity to mitigate these negative effects. Physics-based space weather modeling is characterized by disparate temporal and spatial scales as well as by different relevant physics in different domains. A multi-physics system can be modeled by a software framework comprising several components. Each component corresponds to a physics domain, and each component is represented by one or more numerical models. The publicly available Space Weather Modeling Framework (SWMF) can execute and couple together several components distributed over a parallel machine in a flexible and efficient manner. The framework also allows resolving disparate spatial and temporal scales with independent spatial and temporal discretizations in the various models.Several of the computationally most expensive domains of the framework are modeled by the Block-Adaptive Tree Solarwind Roe-type Upwind Scheme (BATS-R-US) code that can solve various forms of the magnetohydrodynamic (MHD) equations, including Hall, semi-relativistic, multi-species and multi-fluid MHD, anisotropic pressure, radiative transport and heat conduction. Modeling disparate scales within BATS-R-US is achieved by a block-adaptive mesh both in Cartesian and generalized coordinates. Most recently we have created a new core for BATS-R-US: the Block-Adaptive Tree Library (BATL) that provides a general toolkit for creating, load balancing and message passing in a 1, 2 or 3 dimensional block-adaptive grid. We describe the algorithms of BATL and demonstrate its efficiency and scaling properties for various problems.BATS-R-US uses several time-integration schemes to address multiple time-scales: explicit time stepping with fixed or local time steps, partially steady-state evolution, point-implicit, semi-implicit, explicit/implicit, and fully implicit numerical schemes. Depending on the application, we find that different time stepping methods are optimal. Several of the time integration schemes exploit the block-based granularity of the grid structure.The framework and the adaptive algorithms enable physics-based space weather modeling and even short-term forecasting. © 2011 Elsevier Inc.},
	number = {3},
	journal = {Journal of Computational Physics},
	author = {Tóth, Gábor and van der Holst, Bart and Sokolov, Igor V. and De Zeeuw, Darren L. and Gombosi, Tamas I. and Fang, Fang and Manchester, Ward B. and Meng, Xing and Najib, Dalal and Powell, Kenneth G. and Stout, Quentin F. and Glocer, Alex and Ma, Ying Juan and Opher, Merav},
	year = {2012},
	keywords = {65D99 Numerical approximation, 77A05 Magnetohydrodynamics},
	pages = {870--903},
}

@article{Jakeman2020,
	title = {Adaptive multi-index collocation for uncertainty quantification and sensitivity analysis},
	volume = {121},
	issn = {10970207},
	doi = {10.1002/nme.6268},
	abstract = {In this paper, we present an adaptive algorithm to construct response surface approximations of high-fidelity models using a hierarchy of lower fidelity models. Our algorithm is based on multi-index stochastic collocation and automatically balances physical discretization error and response surface error to construct an approximation of model outputs. This surrogate can be used for uncertainty quantification (UQ) and sensitivity analysis (SA) at a fraction of the cost of a purely high-fidelity approach. We demonstrate the effectiveness of our algorithm on a canonical test problem from the UQ literature and a complex multiphysics model that simulates the performance of an integrated nozzle for an unmanned aerospace vehicle. We find that, when the input-output response is sufficiently smooth, our algorithm produces approximations that can be over two orders of magnitude more accurate than single fidelity approximations for a fixed computational budget.},
	number = {6},
	journal = {International Journal for Numerical Methods in Engineering},
	author = {Jakeman, John D. and Eldred, Michael S. and Geraci, Gianluca and Gorodetsky, Alex},
	year = {2020},
	note = {arXiv: 1909.13845},
	keywords = {decision making, modeling, multifidelity, sensitivity analysis, simulation, uncertainty quantification, validation, ★},
	pages = {1314--1343},
}

@article{NasehiTehrani2012,
	title = {L1 regularization method in electrical impedance tomography by using the {L1}-curve ({Pareto} frontier curve)},
	volume = {36},
	issn = {0307904X},
	url = {http://dx.doi.org/10.1016/j.apm.2011.07.055},
	doi = {10.1016/j.apm.2011.07.055},
	abstract = {Electrical impedance tomography (EIT), as an inverse problem, aims to calculate the internal conductivity distribution at the interior of an object from current-voltage measurements on its boundary. Many inverse problems are ill-posed, since the measurement data are limited and imperfect. To overcome ill-posedness in EIT, two main types of regularization techniques are widely used. One is categorized as the projection methods, such as truncated singular value decomposition (SVD or TSVD). The other categorized as penalty methods, such as Tikhonov regularization, and total variation methods. For both of these methods, a good regularization parameter should yield a fair balance between the perturbation error and regularized solution. In this paper a new method combining the least absolute shrinkage and selection operator (LASSO) and the basis pursuit denoising (BPDN) is introduced for EIT. For choosing the optimum regularization we use the L1-curve (Pareto frontier curve) which is similar to the L-curve used in optimising L2-norm problems. In the L1-curve we use the L1-norm of the solution instead of the L2 norm. The results are compared with the TSVD regularization method where the best regularization parameters are selected by observing the Picard condition and minimizing generalized cross validation (GCV) function. We show that this method yields a good regularization parameter corresponding to a regularized solution. Also, in situations where little is known about the noise level σ, it is also useful to visualize the L1-curve in order to understand the trade-offs between the norms of the residual and the solution. This method gives us a means to control the sparsity and filtering of the ill-posed EIT problem. Tracing this curve for the optimum solution can decrease the number of iterations by three times in comparison with using LASSO or BPDN separately. © 2011 Elsevier Inc.},
	number = {3},
	journal = {Applied Mathematical Modelling},
	author = {Nasehi Tehrani, J. and McEwan, A. and Jin, C. and van Schaik, A.},
	year = {2012},
	note = {Publisher: Elsevier Inc.},
	keywords = {Electrical impedance tomography, L1-curve (Pareto frontier curve), Regularization},
	pages = {1095--1105},
}

@article{Peherstorfer2019,
	title = {Multifidelity {Monte} {Carlo} estimation with adaptive low-fidelity models},
	volume = {7},
	issn = {21662525},
	doi = {10.1137/17M1159208},
	abstract = {Multifidelity Monte Carlo (MFMC) estimation combines low- and high-fidelity models to speed up the estimation of statistics of the high-fidelity model outputs. MFMC optimally samples the lowand high-fidelity models such that the MFMC estimator has minimal mean-squared error (MSE) for a given computational budget. In the setup of MFMC, the low-fidelity models are static; i.e., they are given and fixed and cannot be changed and adapted. We introduce the adaptive MFMC (AMFMC) method that splits the computational budget between adapting the low-fidelity models to improve their approximation quality and sampling the low- and high-fidelity models to reduce the MSE of the estimator. Our AMFMC approach derives the quasi-optimal balance between adaptation and sampling in the sense that our approach minimizes an upper bound of the MSE, instead of the error directly. We show that the quasi-optimal number of adaptations of the low-fidelity models is bounded even in the limit of an infinite budget. This shows that adapting low-fidelity models in MFMC beyond a certain approximation accuracy is unnecessary and can even be wasteful. Our AMFMC approach trades off adaptation and sampling and so avoids overadaptation of the low- fidelity models. Besides the costs of adapting low-fidelity models, our AMFMC approach can also take into account the costs of the initial construction of the low-fidelity models ("offline costs"), which is critical if low-fidelity models are computationally expensive to build such as reduced models and data-fit surrogate models. Numerical results demonstrate that our adaptive approach can achieve orders of magnitude speedups compared to MFMC estimators with static low-fidelity models and compared to Monte Carlo estimators that use the high-fidelity model alone.},
	number = {2},
	journal = {SIAM-ASA Journal on Uncertainty Quantification},
	author = {Peherstorfer, Benjamin},
	year = {2019},
	keywords = {Model reduction, Monte Carlo, Multifidelity, Multilevel, Surrogate models, Uncertainty quantification, ★},
	pages = {579--603},
}

@article{Sinsbeck2015,
	title = {Impact of data assimilation on cost-accuracy tradeoff in multifidelity models},
	volume = {3},
	issn = {21662525},
	doi = {10.1137/141001743},
	abstract = {Observable phenomena can often be described by alternative models with different degrees of fidelity. Such models typically contain uncertain parameters and forcings, rendering predictions of the state variables uncertain as well. Within the probabilistic framework, solutions of these models are given in terms of their probability density functions (PDFs). In the presence of data, the latter can be treated as prior distributions. Uncertainty and assimilation of measurements into model predictions, e.g., via Bayesian updating of solution PDFs, pose a question of model selection: Given a significant difference in computational cost, is a lower-fidelity model preferable to its higher-fidelity counterpart? We investigate this question in the context of multiphase flow in heterogeneous porous media whose hydraulic properties are uncertain. While low-fidelity (reduced-complexity) models introduce a model error, their moderate computational cost makes it possible to generate more realizations, which reduces the (e.g., Monte Carlo) sampling error. These two errors determine the model with the smallest total error. Our analysis suggests that assimilation of measurements of a quantity of interest (a medium's saturation, in our example) influences both types of errors, increasing the probability that the predictive accuracy of a reduced-complexity model exceeds that of its higher-fidelity counterpart.},
	number = {1},
	journal = {SIAM-ASA Journal on Uncertainty Quantification},
	author = {Sinsbeck, Michael and Tartakovsky, Daniel M.},
	year = {2015},
	keywords = {Porous media, Reduced complexity, Stochastic, Subsurface, Uncertainty quantification, Unsaturated, ★},
	pages = {954--968},
}

@article{Heas2020,
	title = {Selecting {Reduced} {Models} in the {Cross}-{Entropy} {Method}},
	volume = {8},
	doi = {10.1137/18m1192500},
	abstract = {This paper deals with the estimation of rare event probabilities using importance sampling (IS), where an \{{\textbackslash}it optimal\} proposal distribution is computed with the cross-entropy (CE) method. Although, IS optimised with the CE method leads to an efficient reduction of the estimator variance, this approach remains unaffordable for problems where the repeated evaluation of the score function represents a too intensive computational effort. This is often the case for score functions related to the solution of a partial differential equation (PDE) with random inputs. This work proposes to alleviate computation by adapting a score function approximation along the CE optimisation process. The score function approximation is obtained by selecting the surrogate of lowest dimensionality, whose accuracy guarantees to pass the current CE optimisation stage. The adaptation of the surrogate relies on certified upper bounds on the error norm. An asymptotic analysis provides some theoretical guarantees on the efficiency and convergence of the proposed algorithm. Numerical results demonstrate the gain brought by the adaptive method in the context of pollution alerts and a system modelled by a PDE.},
	number = {2},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Héas, P.},
	year = {2020},
	note = {arXiv: 1806.01916},
	keywords = {10, 1137, 18m1192500, 60-08, 65c05, 65n30, 68q87, ams subject classifications, certified, cross-entropy optimization, doi, error bound, importance sampling, partial differential equation, rare event simulation, reduced basis},
	pages = {511--538},
}

@book{Gorodetsky2020a,
	title = {A generalized approximate control variate framework for multifidelity uncertainty quantification},
	volume = {408},
	isbn = {0-02-199912-0},
	abstract = {We describe and analyze a variance reduction approach for Monte Carlo (MC) sampling that accelerates the estimation of statistics of computationally expensive simulation models using an ensemble of models with lower cost. These lower cost models — which are typically lower fidelity with unknown statistics — are used to reduce the variance in statistical estimators relative to a MC estimator with equivalent cost. We derive the conditions under which our proposed approximate control variate framework recovers existing multifidelity variance reduction schemes as special cases. We demonstrate that existing recursive/nested strategies are suboptimal because they use the additional low-fidelity models only to efficiently estimate the unknown mean of the first low-fidelity model. As a result, they cannot achieve variance reduction beyond that of a control variate estimator that uses a single low-fidelity model with known mean. However, there often exists about an order-of-magnitude gap between the maximum achievable variance reduction using all low-fidelity models and that achieved by a single low-fidelity model with known mean. We show that our proposed approach can exploit this gap to achieve greater variance reduction by using non-recursive sampling schemes. The proposed strategy reduces the total cost of accurately estimating statistics, especially in cases where only low-fidelity simulation models are accessible for additional evaluations. Several analytic examples and an example with a hyperbolic PDE describing elastic wave propagation in heterogeneous media are used to illustrate the main features of the methodology.},
	author = {Gorodetsky, Alex A. and Geraci, Gianluca and Eldred, Michael S. and Jakeman, John D.},
	year = {2020},
	doi = {10.1016/j.jcp.2020.109257},
	note = {arXiv: 1811.04988
Publication Title: Journal of Computational Physics
ISSN: 10902716},
	keywords = {Control variates, Monte Carlo, Multifidelity modeling, Variance reduction, ★},
}

@article{Hui2015,
	title = {Tuning {Parameter} {Selection} for the {Adaptive} {Lasso} {Using} {ERIC}},
	volume = {110},
	issn = {1537274X},
	doi = {10.1080/01621459.2014.951444},
	abstract = {The adaptive Lasso is a commonly applied penalty for variable selection in regression modeling. Like all penalties though, its performance depends critically on the choice of the tuning parameter. One method for choosing the tuning parameter is via information criteria, such as those based on AIC and BIC. However, these criteria were developed for use with unpenalized maximum likelihood estimators, and it is not clear that they take into account the effects of penalization. In this article, we propose the extended regularized information criterion (ERIC) for choosing the tuning parameter in adaptive Lasso regression. ERIC extends the BIC to account for the effect of applying the adaptive Lasso on the bias-variance tradeoff. This leads to a criterion whose penalty for model complexity is itself a function of the tuning parameter. We show the tuning parameter chosen by ERIC is selection consistent when the number of variables grows with sample size, and that this consistency holds in a wider range of contexts compared to using BIC to choose the tuning parameter. Simulation show that ERIC can significantly outperform BIC and other information criteria proposed (for choosing the tuning parameter) in selecting the true model. For ultra high-dimensional data (p {\textgreater} n), we consider a two-stage approach combining sure independence screening with adaptive Lasso regression using ERIC, which is selection consistent and performs strongly in simulation. Supplementary materials for this article are available online.},
	number = {509},
	journal = {Journal of the American Statistical Association},
	author = {Hui, Francis K.C. and Warton, David I. and Foster, Scott D.},
	year = {2015},
	keywords = {BIC, Consistency, High-dimensional data, Information criteria, Penalized likelihood, Regularization parameter, Variable selection},
	pages = {262--269},
}

@article{Sun2013,
	title = {Consistent selection of tuning parameters via variable selection stability},
	volume = {14},
	issn = {15324435},
	abstract = {Penalized regression models are popularly used in high-dimensional data analysis to conduct variable selection and model fitting simultaneously. Whereas success has been widely reported in literature, their performances largely depend on the tuning parameters that balance the trade-off between model fitting and model sparsity. Existing tuning criteria mainly follow the route of minimizing the estimated prediction error or maximizing the posterior model probability, such as cross validation, AIC and BIC. This article introduces a general tuning parameter selection criterion based on variable selection stability. The key idea is to select the tuning parameters so that the resultant penalized regression model is stable in variable selection. The asymptotic selection consistency is established for both fixed and diverging dimensions. Its effectiveness is also demonstrated in a variety of simulated examples as well as an application to the prostate cancer data. © 2013 Wei Sun, Junhui Wang and Yixin Fang.},
	journal = {Journal of Machine Learning Research},
	author = {Sun, Wei and Wang, Junhui and Fang, Yixin},
	year = {2013},
	note = {arXiv: 1208.3380},
	keywords = {Kappa coefficient, Penalized regression, Selection consistency, Stability, Tuning},
	pages = {3419--3440},
}

@article{Battles2004,
	title = {An extension of {MATLAB} to continuous functions and operators},
	volume = {25},
	issn = {10648275},
	doi = {10.1137/S1064827503430126},
	abstract = {An object-oriented MATLAB system is described for performing numerical linear algebra on continuous functions and operators rather than the usual discrete vectors and matrices. About eighty MATLAB functions from plot and sum to svd and cond have been overloaded so that one can work with our \& quot;chebfun" objects using almost exactly the usual MATLAB syntax. All functions live on [-1, 1] and are represented by values at sufficiently many Chebyshev points for the polynomial interpolant to be accurate to close to machine precision. Each of our overloaded operations raises questions about the proper generalization of familiar notions to the continuous context and about appropriate methods of interpolation, differentiation, integration, zerofinding, or transforms. Applications in approximation theory and numerical analysis are explored, and possible extensions for more substantial problems of scientific computing are mentioned.},
	number = {5},
	journal = {SIAM Journal on Scientific Computing},
	author = {Battles, Zachary and Trefethen, Lloyd N.},
	year = {2004},
	keywords = {Barycentric formula, Chebyshev points, FFT, Interpolation, MATLAB, Spectral methods, ★},
	pages = {1743--1770},
}

@article{Cortiella2020,
	title = {Sparse {Identification} of {Nonlinear} {Dynamical} {Systems} via {Reweighted} \${\textbackslash}ell\_1\$-regularized {Least} {Squares}},
	url = {http://arxiv.org/abs/2005.13232},
	abstract = {This work proposes an iterative sparse-regularized regression method to recover governing equations of nonlinear dynamical systems from noisy state measurements. The method is inspired by the Sparse Identification of Nonlinear Dynamics (SINDy) approach of \{{\textbackslash}it [Brunton et al., PNAS, 113 (15) (2016) 3932-3937]\}, which relies on two main assumptions: the state variables are known \{{\textbackslash}it a priori\} and the governing equations lend themselves to sparse, linear expansions in a (nonlinear) basis of the state variables. The aim of this work is to improve the accuracy and robustness of SINDy in the presence of state measurement noise. To this end, a reweighted \${\textbackslash}ell\_1\$-regularized least squares solver is developed, wherein the regularization parameter is selected from the corner point of a Pareto curve. The idea behind using weighted \${\textbackslash}ell\_1\$-norm for regularization -- instead of the standard \${\textbackslash}ell\_1\$-norm -- is to better promote sparsity in the recovery of the governing equations and, in turn, mitigate the effect of noise in the state variables. We also present a method to recover single physical constraints from state measurements. Through several examples of well-known nonlinear dynamical systems, we demonstrate empirically the accuracy and robustness of the reweighted \${\textbackslash}ell\_1\$-regularized least squares strategy with respect to state measurement noise, thus illustrating its viability for a wide range of potential applications.},
	author = {Cortiella, Alexandre and Park, Kwang-Chun and Doostan, Alireza},
	year = {2020},
	note = {arXiv: 2005.13232},
	keywords = {basis pursuit denoising, bpdn, nonlinear system identification, pareto curve, reweighted, sindy, sparse regression, ℓ 1 -regularization},
	pages = {1--33},
}

@article{Widman2002,
	title = {The {L}-curve and its use in the numerical treatment of inverse problems},
	volume = {35},
	issn = {15320464},
	doi = {10.1016/s1532-0464(02)00008-4},
	abstract = {The L-curve is a log-log plot of the norm of a regularized solution versus the norm of the corresponding residual norm. It is a convenient graphical tool for displaying the trade-off between the size of a regularized solution and its fit to the given data, as the regularization parameter varies. The L-curve thus gives insight into the regularizing properties of the underlying regularization method, and it is an aid in choosing an appropriate regularization parameter for the given data. In this chapter we summarize the main properties of the L-curve, and demonstrate by examples its usefulness and its limitations both as an analysis tool and as a method for choosing the regularization parameter. 1 Introduction Practically all regularization methods for computing stable solutions to inverse problems involve a trade-off between the "size" of the regularized solution and the quality of the fit that it provides to the given data. What distinguishes the various regularization methods is how...},
	number = {1},
	journal = {Journal of Biomedical Informatics},
	author = {Hansen, P C},
	year = {2002},
	keywords = {★},
	pages = {51},
}

@article{Hansen1993,
	title = {The {Use} of the {L}-{Curve} in the {Regularization} of {Discrete} {Ill}-{Posed} {Problems}},
	volume = {14},
	doi = {10.1137/0914086},
	number = {6},
	journal = {SIAM Journal on Scientific Computing},
	author = {Hansen, P C and O'Leary, D P},
	year = {1993},
	keywords = {discrepancy principle, generalized cross validation, ill-posed problems, in many applications such, introduction, l-curve, parameter choice, regularization, seismography},
	pages = {1487--1503},
}

@article{Donoho2015,
	title = {50 {Years} of {Data} {Science}},
	abstract = {More than 50 years ago, John Tukey called for a reformation of academic statistics. In 'The Future of Data Analysis', he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or 'data analysis'. Ten to twenty years ago, John Chambers, Bill Cleveland and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland even suggested the catchy name "Data Science" for his envisioned field.},
	author = {Donoho, David},
	year = {2015},
	pages = {1--41},
}

@article{Ghanem2017a,
	title = {Handbook of uncertainty quantification},
	doi = {10.1007/978-3-319-12385-1},
	abstract = {The topic of Uncertainty Quantification (UQ) has witnessed massive developments in response to the promise of achieving risk mitigation through scientific prediction. It has led to the integration of ideas from mathematics, statistics and engineering being used to lend credence to predictive assessments of risk but also to design actions (by engineers, scientists and investors) that are consistent with risk aversion. The objective of this Handbook is to facilitate the dissemination of the forefront of UQ ideas to their audiences. We recognize that these audiences are varied, with interests ranging from theory to application, and from research to development and even execution.},
	journal = {Handbook of Uncertainty Quantification},
	author = {Ghanem, Roger and Owhadi, Houman and Higdon, David},
	year = {2017},
	note = {ISBN: 9783319123851},
	keywords = {bayesian inference, global sensitivity analysis, markov chain monte carlo, polynomial chaos, quadrature, surrogate modeling, uncertainty propagation, uq toolkit, uqtk},
	pages = {1--2053},
}

@inproceedings{10.1007/978-3-319-52995-0_6,
	address = {Cham},
	title = {Single-{Stream} {Round} {Jet} at {M} = 0.9},
	isbn = {978-3-319-52995-0},
	abstract = {Results from three partners obtained using different non-zonal Grey Area Mitigation (GAM) approaches in different CFD codes are compared for an isothermal, single-stream, static round jet at \$\$ M = 0.9 \$\$and \$\$ Re\_\{D\} = 1.1 {\textbackslash}times 10{\textasciicircum}\{6\} \$\$. A systematic grid resolution study, using up to four different resolutions, shows encouraging convergence with grid refinement of the mean flow and (for one partner) the far-field noise towards experimental data. Excellent results are achieved using the novel methods for an important flow which was previously difficult to handle using standard DES methods.},
	booktitle = {{Go4Hybrid}: {Grey} {Area} {Mitigation} for {Hybrid} {RANS}-{LES} {Methods}},
	publisher = {Springer International Publishing},
	author = {Fuchs, M and Mockett, C and Shur, M and Strelets, M and Kok, J C},
	editor = {Mockett, Charles and Haase, Werner and Schwamborn, Dieter},
	year = {2018},
	pages = {125--137},
}

@incollection{Debusschere2017,
	address = {Cham},
	title = {Uncertainty {Quantification} {Toolkit} ({UQTk})},
	isbn = {978-3-319-12385-1},
	url = {https://doi.org/10.1007/978-3-319-12385-1_56},
	abstract = {The UQ Toolkit (UQTk) is a collection of tools for uncertainty quantification, ranging from intrusive and nonintrusive forward propagation of uncertainty to inverse problems and sensitivity analysis. This chapter first outlines the UQTk design philosophy, followed by an overview of the available methods and the way they are implemented in UQTk. The second part of this chapter is a detailed example that illustrates a UQ workflow from surrogate construction, and calibration, to forward propagation and attribution.},
	booktitle = {Handbook of {Uncertainty} {Quantification}},
	publisher = {Springer International Publishing},
	author = {Debusschere, Bert and Sargsyan, Khachik and Safta, Cosmin and Chowdhary, Kenny},
	editor = {Ghanem, Roger and Higdon, David and Owhadi, Houman},
	year = {2017},
	doi = {10.1007/978-3-319-12385-1_56},
	pages = {1807--1827},
}

@article{Ng2012,
	title = {Multifidelity uncertainty quantification using non-intrusive polynomial chaos and stochastic collocation},
	issn = {02734508},
	doi = {10.2514/6.2012-1852},
	abstract = {This paper explores the extension of multifidelity modeling concepts to the field of uncertainty quantification. Motivated by local correction functions that enable the provable convergence of a multifidelity optimization approach to an optimal high-fidelity point solution, we extend these ideas to global discrepancy modeling within a stochastic domain and seek convergence of a multifidelity uncertainty quantification process to globally integrated high-fidelity statistics. For constructing stochastic models of both the low fidelity model and the model discrepancy, we employ stochastic expansion methods (nonintrusive polynomial chaos and stochastic collocation) computed from sparse grids, where we seek to employ a coarsely resolved grid for the discrepancy in combination with a more finely resolved grid for the low fidelity model. The resolutions of these grids may be statically defined or determined through uniform and adaptive refinement processes. Adaptive refinement is particularly attractive, as it has the ability to preferentially target stochastic regions where the model discrepancy becomes more complex; i.e., where the predictive capabilities of the low-fidelity model start to break down and greater reliance on the high fidelity model (via the discrepancy) is necessary. These adaptive refinement processes can either be performed separately for the different sparse grids or within a unified multifidelity algorithm. In particular, we propose an adaptive greedy multifidelity approach in which we extend the generalized sparse grid concept to consider candidate index set refinements drawn from multiple sparse grids. We demonstrate that the multifidelity UQ process converges more rapidly than a single-fidelity UQ in cases where the variance of the discrepancy is reduced relative to the variance of the high fidelity model (resulting in reductions in initial stochastic error) and/or where the spectrum of the expansion coefficients of the model discrepancy decays more rapidly than that of the high-fidelity model (resulting in accelerated convergence rates). © 2012 AIAA.},
	number = {April},
	journal = {Collection of Technical Papers - AIAA/ASME/ASCE/AHS/ASC Structures, Structural Dynamics and Materials Conference},
	author = {Ng, L. W.T. and Eldred, M. S.},
	year = {2012},
	note = {ISBN: 9781600869372},
	pages = {1--17},
}

@book{Ghanem2017,
	title = {Handbook of uncertainty quantification},
	isbn = {978-3-319-12385-1},
	abstract = {The topic of Uncertainty Quantification (UQ) has witnessed massive developments in response to the promise of achieving risk mitigation through scientific prediction. It has led to the integration of ideas from mathematics, statistics and engineering being used to lend credence to predictive assessments of risk but also to design actions (by engineers, scientists and investors) that are consistent with risk aversion. The objective of this Handbook is to facilitate the dissemination of the forefront of UQ ideas to their audiences. We recognize that these audiences are varied, with interests ranging from theory to application, and from research to development and even execution.},
	author = {Ghanem, Roger and Owhadi, Houman and Higdon, David},
	year = {2017},
	doi = {10.1007/978-3-319-12385-1},
	note = {Publication Title: Handbook of Uncertainty Quantification},
}

@incollection{Eldred2017,
	address = {Cham},
	title = {Multifidelity {Uncertainty} {Quantification} {Using} {Spectral} {Stochastic} {Discrepancy} {Models}},
	isbn = {978-3-319-12385-1},
	url = {https://doi.org/10.1007/978-3-319-12385-1_25},
	abstract = {When faced with a restrictive evaluation budget that is typical of today's high-fidelity simulation models, the effective exploitation of lower-fidelity alternatives within the uncertainty quantification (UQ) process becomes critically important. Herein, we explore the use of multifidelity modeling within UQ, for which we rigorously combine information from multiple simulation-based models within a hierarchy of fidelity, in seeking accurate high-fidelity statistics at lower computational cost. Motivated by correction functions that enable the provable convergence of a multifidelity optimization approach to an optimal high-fidelity point solution, we extend these ideas to discrepancy modeling within a stochastic domain and seek convergence of a multifidelity uncertainty quantification process to globally integrated high-fidelity statistics. For constructing stochastic models of both the low-fidelity model and the model discrepancy, we employ stochastic expansion methods (non-intrusive polynomial chaos and stochastic collocation) computed by integration/interpolation on structured sparse grids or regularized regression on unstructured grids. We seek to employ a coarsely resolved grid for the discrepancy in combination with a more finely resolved grid for the low-fidelity model. The resolutions of these grids may be defined statically or determined through uniform and adaptive refinement processes. Adaptive refinement is particularly attractive, as it has the ability to preferentially target stochastic regions where the model discrepancy becomes more complex, i.e., where the predictive capabilities of the low-fidelity model start to break down and greater reliance on the high-fidelity model (via the discrepancy) is necessary. These adaptive refinement processes can either be performed separately for the different grids or within a coordinated multifidelity algorithm. In particular, we present an adaptive greedy multifidelity approach in which we extend the generalized sparse grid concept to consider candidate index set refinements drawn from multiple sparse grids, as governed by induced changes in the statistical quantities of interest and normalized by relative computational cost. Through a series of numerical experiments using statically defined sparse grids, adaptive multifidelity sparse grids, and multifidelity compressed sensing, we demonstrate that the multifidelity UQ process converges more rapidly than a single-fidelity UQ in cases where the variance of the discrepancy is reduced relative to the variance of the high-fidelity model (resulting in reductions in initial stochastic error), where the spectrum of the expansion coefficients of the model discrepancy decays more rapidly than that of the high-fidelity model (resulting in accelerated convergence rates), and/or where the discrepancy is more sparse than the high-fidelity model (requiring the recovery of fewer significant terms).},
	booktitle = {Handbook of {Uncertainty} {Quantification}},
	publisher = {Springer International Publishing},
	author = {Eldred, Michael S and Ng, Leo W T and Barone, Matthew F and Domino, Stefan P},
	editor = {Ghanem, Roger and Higdon, David and Owhadi, Houman},
	year = {2017},
	doi = {10.1007/978-3-319-12385-1_25},
	pages = {991--1036},
}

@incollection{Sargsyan2017,
	address = {Cham},
	title = {Surrogate {Models} for {Uncertainty} {Propagation} and {Sensitivity} {Analysis}},
	isbn = {978-3-319-12385-1},
	url = {https://doi.org/10.1007/978-3-319-12385-1_22},
	abstract = {For computationally intensive tasks such as design optimization, global sensitivity analysis, or parameter estimation, a model of interest needs to be evaluated multiple times exploring potential parameter ranges or design conditions. If a single simulation of the computational model is expensive, it is common to employ a precomputed surrogate approximation instead. The construction of an appropriate surrogate does still require a number of training evaluations of the original model. Typically, more function evaluations lead to more accurate surrogates, and therefore a careful accuracy-vs-efficiency tradeoff needs to take place for a given computational task. This chapter specifically focuses on polynomial chaos surrogates that are well suited for forward uncertainty propagation tasks, discusses a few construction mechanisms for such surrogates, and demonstrates the computational gain on select test functions.},
	booktitle = {Handbook of {Uncertainty} {Quantification}},
	publisher = {Springer International Publishing},
	author = {Sargsyan, Khachik},
	editor = {Ghanem, Roger and Higdon, David and Owhadi, Houman},
	year = {2017},
	doi = {10.1007/978-3-319-12385-1_22},
	pages = {673--698},
}

@article{Fernandez-Godino2016,
	title = {Review of multi-fidelity models},
	url = {http://arxiv.org/abs/1609.07196},
	abstract = {Simulations are often computationally expensive and the need for multiple realizations, as in uncertainty quantification or optimization, makes surrogate models an attractive option. For expensive high-fidelity models (HFMs), however, even performing the number of simulations needed for fitting a surrogate may be too expensive. Inexpensive but less accurate low-fidelity models (LFMs) are often also available. Multi-fidelity models (MFMs) combine HFMs and LFMs in order to achieve accuracy at a reasonable cost. With the increasing popularity of MFMs in mind, the aim of this paper is to summarize the state-of-the-art of MFM trends. For this purpose, publications in this field are classified based on application, surrogate selection if any, the difference between fidelities, the method used to combine these fidelities, the field of application and the year published. Available methods of combining fidelities are also reviewed, focusing our attention especially on multi-fidelity surrogate models in which fidelities are combined inside a surrogate model. Computation time savings are usually the reason for using MFMs, hence it is important to properly report the achieved savings. Unfortunately, we find that many papers do not present sufficient information to determine these savings. Therefore, the paper also includes guidelines for authors to present their MFM savings in a way that is useful to future MFM users. Based on papers that provided enough information, we find that time savings are highly problem dependent and that MFM methods we surveyed provided time savings up to 90\%. Keywords: Multi-fidelity, Variable-complexity, Variable-fidelity, Surrogate models, Optimization, Uncertainty quantification, Review, Survey},
	author = {Fernández-Godino, M. Giselle and Park, Chanyoung and Kim, Nam-Ho and Haftka, Raphael T.},
	year = {2016},
	note = {arXiv: 1609.07196},
	keywords = {fl 32611, gainesville, models, multi-fidelity, optimization, review, surrogate, survey, uncertainty quantification, university of florida, variable-complexity, variable-fidelity, ★},
}

@book{Maitre2010,
	title = {Specral {Methods} for {Uncertainty} {Quantification}: with {Applications} to {Computational} {Fluid} {Dynamics}},
	isbn = {978-90-481-3519-6},
	publisher = {Springer Netherlands, Houten, Netherlands},
	author = {Le Maitre, O.P. and Knio, O.M.},
	year = {2010},
	doi = {10.1007/978-90-481-3520-2},
	keywords = {★},
}

@article{Nemirovski2006,
	title = {Downloaded 09 / 21 / 14 to 210 . 32 . 178 . 82 . {Redistribution} subject to {SIAM} license or copyright ; see http://www.siam.org/journals/ojsa.php},
	volume = {45},
	number = {2},
	author = {Equation, Space-fractional Diffusion and Ervin, Vincent J and Heuer, Norbert and Roop, John Paul},
	year = {2007},
	keywords = {050642757, 1, 10, 1137, 65n30, ams subject classification, anomalous diffusion, doi, finite element approximation, in this paper we, introduction, nonlinear parabolic equation, study the numerical approximation, to time},
	pages = {572--591},
}

@article{GiselleFernandez-Godino2019,
	title = {Issues in deciding whether to use multifidelity surrogates},
	volume = {57},
	issn = {00011452},
	doi = {10.2514/1.J057750},
	abstract = {Multifidelity surrogates are essential in cases where it is not affordable to have more than a few high-fidelity samples, but it is affordable to have as many low-fidelity samples as needed. In these cases, given a good correlation between the models, the performance of multifidelity models can be outstanding. The first objective of this paper is to discuss progress in creating accurate multifidelity surrogates when they are essential. A more ambiguous situation exists when it may be possible to afford enough high-fidelity samples to construct an accurate surrogate model. In that case, the question is whether a multifidelity surrogate will afford a substantial cost reduction for comparable accuracy. Our the second objective is to see if there are any indications under what circumstances this substantial cost reduction is realized. From the literature, it appears that it is hard to get an idea, in terms of cost savings, of when it is useful to invest the additional effort of creating and using multifidelity surrogates. It is observed that in some cases the inclusion of low-fidelity samples along with the high-fidelity samples in building multifidelity surrogates led to less accurate surrogates than just using the available high-fidelity samples.},
	number = {5},
	journal = {AIAA Journal},
	author = {Giselle Fernández-Godino, M. and Park, Chanyoung and Kim, Nam H. and Haftka, Raphael T.},
	year = {2019},
	pages = {2039--2054},
}

@article{Biehler2019,
	title = {Multifidelity approaches for uncertainty quantification},
	volume = {42},
	issn = {09367195},
	doi = {10.1002/gamm.201900008},
	abstract = {The aim of this paper is to give an overview of different multifidelity uncertainty quantification (UQ) schemes. Therefore, different views on multifidelity UQ approaches from a frequentist, Bayesian, and possibilistic perspective are provided and recent developments are discussed. Differences as well as similarities between the methods are highlighted and strategies to construct low-fidelity models are explained. In addition, two state-of-the-art examples to showcase the capabilities of these methods and the tremendous reduction of computational costs that can be achieved when using these approaches are provided.},
	number = {2},
	journal = {GAMM Mitteilungen},
	author = {Biehler, Jonas and Mäck, Markus and Nitzler, Jonas and Hanss, Michael and Koutsourelakis, Phaedon Stelios and Wall, Wolfgang A.},
	month = may,
	year = {2019},
	note = {Publisher: Wiley-VCH Verlag},
	keywords = {Bayesian, multifidelity, possibilistic, uncertainty quantification},
}

@article{Najm2009,
	title = {Uncertainty {Quantification} and {Polynomial} {Chaos} {Techniques} in {Computational} {Fluid} {Dynamics}},
	volume = {41},
	issn = {0066-4189},
	doi = {10.1146/annurev.fluid.010908.165248},
	abstract = {The quantification of uncertainty in computational fluid dynamics (CFD) predictions is both a significant challenge and an important goal. Probabilistic uncertainty quantification (UQ) methods have been used to propagate uncertainty from model inputs to outputs when input ...},
	number = {1},
	journal = {Annual Review of Fluid Mechanics},
	author = {Najm, Habib N.},
	month = jan,
	year = {2009},
	note = {Publisher: Annual Reviews},
	keywords = {★},
	pages = {35--52},
}

@article{Boyd2010,
	title = {Distributed optimization and statistical learning via the alternating direction method of multipliers},
	volume = {3},
	issn = {19358237},
	doi = {10.1561/2200000016},
	abstract = {printed},
	number = {1},
	journal = {Foundations and Trends in Machine Learning},
	author = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
	year = {2010},
	pages = {1--122},
}

@inproceedings{Huan2019,
	title = {Uncertainty propagation using conditional random fields in large-eddy simulations of scramjet computations},
	isbn = {978-1-62410-578-4},
	doi = {10.2514/6.2019-0724},
	booktitle = {{AIAA} {Scitech} 2019 {Forum}},
	publisher = {American Institute of Aeronautics and Astronautics Inc, AIAA},
	author = {Huan, Xun and Safta, Cosmin and Vane, Zachary P. and Lacaze, Guilhem and Oefelein, Joseph C. and Najm, Habib N.},
	year = {2019},
}

@article{Wilke2019,
	title = {Variable-fidelity methodology for the aerodynamic optimization of helicopter rotors},
	volume = {57},
	issn = {00011452},
	doi = {10.2514/1.J056486},
	abstract = {The design of helicopter rotor blades is a challenging task. On the one hand, there are the demanding simulations, which are a multidisciplinary endeavor. On the other hand, tools for parametric studies or optimizations require many simulations, making the design process even more costly. In the rotorcraft community, two routes for the numerical optimization task are observed. The first route is based upon local gradient search algorithms, which exploit low-fidelity tools or adjoint-based computational fluid dynamics (CFD) simulations. The second route is surrogate-based optimization in combination with high-fidelity CFD simulations. These surrogate-based optimizations can be further accelerated, when knowledge from low-fidelity models is used. This paper presents a framework that is developed for the multi-objective aerodynamic optimization of helicopter rotor blades including surrogate models based on different fidelities. The individual components necessary for performing a variable-fidelity, multi-objective optimization are reviewed before being applied. A novel technique to deal with unsuccessful simulations referred to as a failure map is additionally presented. The gain of the variable-fidelity optimizations in contrast to the single-fidelity optimizations is quantified, and a reduction in computational resources of up to 69\% is observed. The failure map requires 78\% less resources in contrast to the classical failure handling.},
	number = {8},
	journal = {AIAA Journal},
	author = {Wilke, Gunther},
	year = {2019},
	pages = {3145--3158},
}

@article{Lauwers2007,
	title = {One is enough!},
	volume = {28},
	issn = {16107438},
	doi = {10.1177/2158244011428647},
	abstract = {We postulate that multi-wheel statically-stable mobile robots for operation in human environments are an evolutionary dead end. Robots of this class tall enough to interact meaningfully with people must have low centers of gravity, overly wide bases of support, and very low accelerations to avoid tipping over. Accordingly, we are developing an inverse of this type of mobile robot that is the height, width, and weight of a person, having a high center of gravity, that balances dynamically on a single spherical wheel. Unlike balancing 2-wheel platforms which must turn before driving in some direction, the single-wheel robot can move directly in any direction. We present the overall design, actuator mechanism based on an inverse mouse-ball drive, control system, and initial results including dynamic balancing, station keeping, and point-to-point motion.},
	number = {July},
	journal = {Springer Tracts in Advanced Robotics},
	author = {Lauwers, Tom and Kantor, George and Hollis, Ralph},
	year = {2007},
	pages = {114--127},
}

@article{Davenport2016,
	title = {An {Overview} of {Low}-{Rank} {Matrix} {Recovery} from {Incomplete} {Observations}},
	volume = {10},
	issn = {19324553},
	doi = {10.1109/JSTSP.2016.2539100},
	abstract = {Low-rank matrices play a fundamental role in modeling and computational methods for signal processing and machine learning. In many applications where low-rank matrices arise, these matrices cannot be fully sampled or directly observed, and one encounters the problem of recovering the matrix given only incomplete and indirect observations. This paper provides an overview of modern techniques for exploiting low-rank structure to perform matrix recovery in these settings, providing a survey of recent advances in this rapidly-developing field. Specific attention is paid to the algorithms most commonly used in practice, the existing theoretical guarantees for these algorithms, and representative practical applications of these techniques.},
	number = {4},
	journal = {IEEE Journal on Selected Topics in Signal Processing},
	author = {Davenport, Mark A. and Romberg, Justin},
	year = {2016},
	note = {arXiv: 1601.06422
Publisher: IEEE},
	pages = {608--622},
}

@article{Boutsidis2008,
	title = {{SVD} based initialization: {A} head start for nonnegative matrix factorization},
	volume = {41},
	issn = {00313203},
	doi = {10.1016/j.patcog.2007.09.010},
	abstract = {We describe Nonnegative Double Singular Value Decomposition (NNDSVD), a new method designed to enhance the initialization stage of nonnegative matrix factorization (NMF). NNDSVD can readily be combined with existing NMF algorithms. The basic algorithm contains no randomization and is based on two SVD processes, one approximating the data matrix, the other approximating positive sections of the resulting partial SVD factors utilizing an algebraic property of unit rank matrices. Simple practical variants for NMF with dense factors are described. NNDSVD is also well suited to initialize NMF algorithms with sparse factors. Many numerical examples suggest that NNDSVD leads to rapid reduction of the approximation error of many NMF algorithms. © 2007 Elsevier Ltd. All rights reserved.},
	number = {4},
	journal = {Pattern Recognition},
	author = {Boutsidis, C. and Gallopoulos, E.},
	year = {2008},
	keywords = {Low rank, NMF, Nonnegative matrix factorization, Perron-Frobenius, SVD, Singular value decomposition, Sparse NMF, Sparse factorization, Structured initialization},
	pages = {1350--1362},
}

@article{Prasadan2019,
	title = {Time {Series} {Source} {Separation} using {Dynamic} {Mode} {Decomposition}},
	url = {http://arxiv.org/abs/1903.01310%0Ahttp://dx.doi.org/10.5281/zenodo.2656681},
	doi = {10.5281/zenodo.2656681},
	abstract = {The Dynamic Mode Decomposition (DMD) extracted dynamic modes are the non-orthogonal eigenvectors of the matrix that best approximates the one-step temporal evolution of the multivariate samples. In the context of dynamical system analysis, the extracted dynamic modes are a generalization of global stability modes. We apply DMD to a data matrix whose rows are linearly independent, additive mixtures of latent time series. We show that when the latent time series are uncorrelated at a lag of one time-step then, in the large sample limit, the recovered dynamic modes will approximate, up to a column-wise normalization, the columns of the mixing matrix. Thus, DMD is a time series blind source separation algorithm in disguise, but is different from closely related second order algorithms such as the Second-Order Blind Identification (SOBI) method and the Algorithm for Multiple Unknown Signals Extraction (AMUSE). All can unmix mixed stationary, ergodic Gaussian time series in a way that kurtosis-based Independent Components Analysis (ICA) fundamentally cannot. We use our insights on single lag DMD to develop a higher-lag extension, analyze the finite sample performance with and without randomly missing data, and identify settings where the higher lag variant can outperform the conventional single lag variant. We validate our results with numerical simulations, and highlight how DMD can be used in change point detection.},
	author = {Prasadan, Arvind and Nadakuditi, Raj Rao},
	year = {2019},
	note = {arXiv: 1903.01310},
}

@article{Takeishi2017,
	title = {Bayesian dynamic mode decomposition},
	issn = {10450823},
	doi = {10.24963/ijcai.2017/392},
	abstract = {Dynamic mode decomposition (DMD) is a datadriven method for calculating a modal representation of a nonlinear dynamical system, and it has been utilized in various fields of science and engineering. In this paper, we propose Bayesian DMD, which provides a principled way to transfer the advantages of the Bayesian formulation into DMD. To this end, we first develop a probabilistic model corresponding to DMD, and then, provide the Gibbs sampler for the posterior inference in Bayesian DMD. Moreover, as a specific example, we discuss the case of using a sparsity-promoting prior for an automatic determination of the number of dynamic modes. We investigate the empirical performance of Bayesian DMD using synthetic and real-world datasets.},
	journal = {IJCAI International Joint Conference on Artificial Intelligence},
	author = {Takeishi, Naoya and Kawahara, Yoshinobu and Tabei, Yasuo and Yairi, Takehisa},
	year = {2017},
	note = {ISBN: 9780999241103},
	keywords = {Machine Learning: Time-series/Data Streams, Machine Learning: Unsupervised Learning, Uncertainty in AI: Uncertainty in AI},
	pages = {2814--2821},
}

@article{Levy2001,
	title = {Efficient sequential {Karhunen}-{Loeve} basis extraction},
	volume = {2},
	doi = {10.1109/ICCV.2001.937701},
	abstract = {An approach to reduce computational effort, relying on the relatively small dimension of the partial Karhunen-Loeve (KL) basis is suggested. An algorithm that does not require to store the entire set of input images before proceeding to the calculation of the KL basis is proposed. The algorithm is named Sequential Karhunen Loeve algorithm (SKL).},
	number = {8},
	journal = {Proceedings of the IEEE International Conference on Computer Vision},
	author = {Levy, Avraham and Lindenbaum, Michael},
	year = {2001},
	pages = {739},
}

@article{Gavish2014,
	title = {The optimal hard threshold for singular values is 4/√3},
	volume = {60},
	issn = {00189448},
	doi = {10.1109/TIT.2014.2323359},
	abstract = {We consider recovery of low-rank matrices from noisy data by hard thresholding of singular values, in which empirical singular values below a threshold λ are set to 0. We study the asymptotic mean squared error (AMSE) in a framework, where the matrix size is large compared with the rank of the matrix to be recovered, and the signal-to-noise ratio of the low-rank piece stays constant. The AMSE-optimal choice of hard threshold, in the case of n-by- n matrix in white noise of level σ, is simply (4/√3) √nσ ≈ 2.309 √nσ when σ is known, or simply 2.858 · ymed when σ is unknown, where ymed is the median empirical singular value. For nonsquare, m by n matrices with m ≠ n the thresholding coefficients 4/√3 and 2.858 are replaced with different provided constants that depend on m/n. Asymptotically, this thresholding rule adapts to unknown rank and unknown noise level in an optimal manner: it is always better than hard thresholding at any other value, and is always better than ideal truncated singular value decomposition (TSVD), which truncates at the true rank of the low-rank matrix we are trying to recover. Hard thresholding at the recommended value to recover an n-by-n matrix of rank r guarantees an AMSE at most 3 nrσ2. In comparison, the guarantees provided by TSVD, optimally tuned singular value soft thresholding and the best guarantee achievable by any shrinkage of the data singular values are 5 nrσ2, 6 nrσ2, and 2 nrσ2, respectively. The recommended value for hard threshold also offers, among hard thresholds, the best possible AMSE guarantees for recovering matrices with bounded nuclear norm. Empirical evidence suggests that performance improvement over TSVD and other popular shrinkage rules can be substantial, for different noise distributions, even in relatively small n. © 2014 IEEE.},
	number = {8},
	journal = {IEEE Transactions on Information Theory},
	author = {Gavish, Matan and Donoho, David L.},
	year = {2014},
	note = {arXiv: 1305.5870},
	keywords = {Singular values shrinkage, bulk edge, low-rank matrix denoising, optimal threshold, quarter circle law, scree plot elbow truncation, unique admissible},
	pages = {5040--5053},
}

@article{Alexanderian2015,
	title = {A brief note on the {Karhunen}-{Loeve} expansion},
	url = {http://arxiv.org/abs/1509.07526},
	abstract = {We provide a detailed derivation of the Karhunen-Lo{\textbackslash}`eve expansion of a stochastic process. We also discuss briefly Gaussian processes, and provide a simple numerical study for the purpose of illustration.},
	author = {Alexanderian, Alen},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.07526},
}

@techreport{Gerbrands1981,
	title = {{ON} {THE} {RELATIONSHIPS} {BETWEEN} {SVD}, {KLT} {AND} {PCA}},
	abstract = {In recent literature on digital image processing much attention is devoted to the singular value decomposition (SVD) of a matrix. Many authors refer to the Karhunen-Loeve transform (KLT) and principal components analysis (PCA) while treating the SVD. In this paper we give definitions of the three transforms and investigate their relationships. It is shown that in the context of multivariate statistical analysis and statistical pattern recognition the three transforms are very similar if a specific estimate of the column covariance matrix is used. In the context of two-dimensional image processing this similarity still holds if one single matrix is considered. In that approach the use of the names KLT and PCA is rather inappropriate and confusing. If the matrix is considered to be a realization of a two-dimensional random process, the SVD and the two statistically defined transforms differ substantially. Image processing Statistical analysis Statistical pattern recognition Orthogonal image transforms Singular value decomposition Karhunen-Loeve transform Principal components},
	author = {Gerbrands, Jan J},
	year = {1981},
	note = {Volume: 14},
	pages = {375--381},
}

@article{OHagan2011,
	title = {Polynomial {Chaos}: {A} {Tutorial} and {Critique} from a {Statisticianís} {Perspective}},
	abstract = {This article is written in the spirit of helping recent e¤orts to build bridges between the community of researchers in …elds such as applied mathematics and engineering, where the term UQ began, and the commu-nity of statisticians who work on problems of uncertainty in the predictions of mechanistic models. It is addressed to researchers and practitioners in both communities. The …rst purpose of the article is to explain polynomial chaos, one of the key tools of the …rst community, in terms that will be readily understood by a statistician in the second community. The second purpose is to explain to researchers in the …rst community some aspects of PC, both in theory and in practice, that a statistician might regard as de…ciencies or limitations.},
	journal = {Tonyohagan.Co.Uk},
	author = {O'Hagan, Anthony},
	year = {2011},
	keywords = {★},
	pages = {1--16},
}

@article{urzay_supersonic_2018,
	title = {Supersonic {Combustion} in {Air}-{Breathing} {Propulsion} {Systems} for {Hypersonic} {Flight}},
	volume = {50},
	url = {https://doi.org/10.1146/annurev-fluid-122316-},
	doi = {10.1146/annurev-fluid-122316},
	abstract = {Great efforts have been dedicated during the last decades to the research and development of hypersonic aircrafts that can fly at several times the speed of sound. These aerospace vehicles have revolutionary applications in national security as advanced hypersonic weapons, in space exploration as reusable stages for access to low Earth orbit, and in commercial aviation as fast long-range methods for air transportation of passengers around the globe. This review addresses the topic of supersonic combustion, which represents the central physical process that enables scramjet hypersonic propulsion systems to accelerate aircrafts to ultra-high speeds. The description focuses on recent experimental flights and ground-based research programs and highlights associated fundamental flow physics, subgrid-scale model development, and full-system numerical simulations. 593},
	journal = {Annu. Rev. Fluid Mech},
	author = {Urzay, Javier},
	year = {2018},
	keywords = {compressible flows, high-speed chemical propulsion, hypersonics, scramjets, sound barrier, turbulent combustion},
	pages = {593--627},
}

@techreport{Ubaru2016,
	title = {Fast methods for estimating the {Numerical} rank of large matrices},
	abstract = {We present two computationally inexpensive techniques for estimating the numerical rank of a matrix, combining powerful tools from computational linear algebra. These techniques exploit three key ingredients. The first is to approximate the projector on the non-null invariant subspace of the matrix by using a polynomial filter. Two types of filters are discussed, one based on Her-mite interpolation and the other based on Cheby-shev expansions. The second ingredient employs stochastic trace estimators to compute the rank of this wanted eigen-projector, which yields the desired rank of the matrix. In order to obtain a good filter, it is necessary to detect a gap between the eigenvalues that correspond to noise and the relevant eigenvalues that correspond to the non-null invariant subspace. The third ingredient of the proposed approaches exploits the idea of spectral density, popular in physics, and the Lanczos spectroscopic method to locate this gap.},
	author = {Ubaru, Shashanka and Saad, Yousef},
	year = {2016},
	keywords = {★},
}

@article{Yan2018,
	title = {Gaussian processes and polynomial chaos expansion for regression problem: {Linkage} via the {RKHS} and comparison via the {KL} divergence},
	volume = {20},
	issn = {10994300},
	doi = {10.3390/e20030191},
	abstract = {In this paper, we examine two widely-used approaches, the polynomial chaos expansion (PCE) and Gaussian process (GP) regression, for the development of surrogate models. The theoretical differences between the PCE and GP approximations are discussed. A state-of-the-art PCE approach is constructed based on high precision quadrature points; however, the need for truncation may result in potential precision loss; the GP approach performs well on small datasets and allows a fine and precise trade-off between fitting the data and smoothing, but its overall performance depends largely on the training dataset. The reproducing kernel Hilbert space (RKHS) and Mercer's theorem are introduced to form a linkage between the two methods. The theorem has proven that the two surrogates can be embedded in two isomorphic RKHS, by which we propose a novel method named Gaussian process on polynomial chaos basis (GPCB) that incorporates the PCE and GP. A theoretical comparison is made between the PCE and GPCB with the help of the Kullback-Leibler divergence. We present that the GPCB is as stable and accurate as the PCE method. Furthermore, the GPCB is a one-step Bayesian method that chooses the best subset of RKHS in which the true function should lie, while the PCE method requires an adaptive procedure. Simulations of 1D and 2D benchmark functions show that GPCB outperforms both the PCE and classical GP methods. In order to solve high dimensional problems, a random sample scheme with a constructive design (i.e., tensor product of quadrature points) is proposed to generate a valid training dataset for the GPCB method. This approach utilizes the nature of the high numerical accuracy underlying the quadrature points while ensuring the computational feasibility. Finally, the experimental results show that our sample strategy has a higher accuracy than classical experimental designs; meanwhile, it is suitable for solving high dimensional problems.},
	number = {3},
	journal = {Entropy},
	author = {Yan, Liang and Duan, Xiaojun and Liu, Bowen and Xu, Jin},
	year = {2018},
	keywords = {Experimental design, Gaussian process, Kullback-Leibler divergence, Polynomial chaos expansion, Reproducing kernel Hilbert space, ★},
}

@article{Uw2017,
	title = {Spectral {Clustering}},
	doi = {10.1007/978-1-4899-7687-1_100437},
	journal = {Encyclopedia of Machine Learning and Data Mining},
	author = {Uw, S},
	year = {2017},
	pages = {1167--1167},
}

@article{Brunton2016,
	title = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
	volume = {113},
	issn = {10916490},
	doi = {10.1073/pnas.1517384113},
	abstract = {Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.},
	number = {15},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan and Bialek, William},
	year = {2016},
	note = {arXiv: 1509.03580},
	keywords = {Dynamical systems, Machine learning, Optimization, Sparse regression, System identification, ★},
	pages = {3932--3937},
}

@article{VonLuxburg2007,
	title = {A tutorial on spectral clustering},
	volume = {17},
	issn = {09603174},
	doi = {10.1007/s11222-007-9033-z},
	abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed. © 2007 Springer Science+Business Media, LLC.},
	number = {4},
	journal = {Statistics and Computing},
	author = {Von Luxburg, Ulrike},
	year = {2007},
	note = {arXiv: 0711.0189},
	keywords = {Graph Laplacian, Spectral clustering},
	pages = {395--416},
}

@techreport{Srivastava2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Salakhutdinov, Ruslan},
	year = {2014},
	note = {Publication Title: Journal of Machine Learning Research
Volume: 15},
	keywords = {deep learning, model combination, neural networks, regularization, ★},
	pages = {1929--1958},
}

@techreport{Angelil,
	title = {Bubble {Evolution} and {Properties} in {Homogeneous} {Nucleation} {Simulations}},
	abstract = {We analyze the properties of naturally formed nano-bubbles in Lennard-Jones molecular dynamics simulations of liquid-to-vapor nucleation in the boiling and the cavitation regimes. The large computational volumes provide a realistic environment at unchanging average temperature and liquid pressure, which allows us to accurately measure properties of bubbles from their inception as stable, critically sized bubbles, to their continued growth into the constant speed regime. Bubble gas densities are up to 50\% lower than the equilibrium vapor densities at the liquid temperature, yet quite close to the gas equilibrium density at the lower gas temperatures measured in the simulations: The latent heat of transformation results in bubble gas temperatures up to 25\% below those of the surrounding bulk liquid. In the case of rapid bubble growth-typical for the cavitation regime-compression of the liquid outside the bubble leads to local temperature increases of up to 5\%, likely significant enough to alter the surface tension as well as the local viscosity. The liquid-vapor bubble interface is thinner than expected from planar coexistence simulations by up to 50\%. Bubbles near the critical size are extremely non-spherical, yet they quickly become spherical as they grow. The Rayleigh-Plesset description of bubble-growth gives good agreement in the cavitation regime.},
	author = {Angélil, Raymond and Diemand, Jürg and Tanaka, Kyoko K and Tanaka, Hidekazu},
	note = {arXiv: 1411.4782v2
Volume: 36},
}

@article{Zhou2019,
	title = {Towards {Real}-{Time} {In}-{Flight} {Ice} {Detection} {Systems} via {Computational} {Aeroacoustics} and {Machine} {Learning}},
	doi = {10.2514/6.2019-3103},
	number = {June},
	author = {Zhou, Beckett Yx and Gauger, Nicolas R. and Hauth, Jeremiah and Huan, Xun and Morelli, Myles and Guardone, Alberto},
	year = {2019},
	pages = {1--15},
}

@article{Petz2001,
	title = {Entropy, von {Neumann} and the von {Neumann} {Entropy}},
	doi = {10.1007/978-94-017-2012-0_7},
	abstract = {This paper is an introduction to the von Neumann entropy in a historic approach. Von Neumann's gedanken experiment is repeated, which led him to the formula of thermodynamic entropy of a statistical operator. In the analysis of his ideas we stress the role of superselection sectors and summarize von Neumann's knowledge about quantum mechanical entropy. The final part of the paper is devoted to important developments discovered long after von Neumann's work. Subadditivity and the interpretation of the von Neumann entropy as channel capacity are among those.},
	journal = {John von Neumann and the Foundations of Quantum Physics},
	author = {Petz, Dénes},
	year = {2001},
	note = {arXiv: math-ph/0102013},
	pages = {83--96},
}

@article{LeMagoarou2016,
	title = {Flexible {Multilayer} {Sparse} {Approximations} of {Matrices} and {Applications}},
	volume = {10},
	issn = {19324553},
	doi = {10.1109/JSTSP.2016.2543461},
	abstract = {The computational cost of many signal processing and machine learning techniques is often dominated by the cost of applying certain linear operators to high-dimensional vectors. This paper introduces an algorithm aimed at reducing the complexity of applying linear operators in high dimension by approximately factorizing the corresponding matrix into few sparse factors. The approach relies on recent advances in nonconvex optimization. It is first explained and analyzed in details and then demonstrated experimentally on various problems including dictionary learning for image denoising and the approximation of large matrices arising in inverse problems.},
	number = {4},
	journal = {IEEE Journal on Selected Topics in Signal Processing},
	author = {Le Magoarou, Luc and Gribonval, Remi},
	year = {2016},
	note = {arXiv: 1506.07300
ISBN: 2011277906},
	keywords = {Sparse representations, dictionary learning, fast algorithms, image denoising, inverse problems, low complexity, ★},
	pages = {688--700},
}

@article{Tu2014,
	title = {On dynamic mode decomposition: {Theory} and applications},
	volume = {1},
	issn = {21582505},
	doi = {10.3934/jcd.2014.1.391},
	abstract = {Originally introduced in the fluid mechanics community, dynamic mode decomposition (DMD) has emerged as a powerful tool for analyzing the dynamics of nonlinear systems. However, existing DMD theory deals primarily with sequential time series for which the measurement dimension is much larger than the number of measurements taken. We present a theoretical framework in which we define DMD as the eigendecomposition of an approximating linear operator. This generalizes DMD to a larger class of datasets, including nonsequential time series. We demonstrate the utility of this approach by presenting novel sampling strategies that increase computational effciency and mitigate the effects of noise, respectively. We also introduce the concept of linear consistency, which helps explain the potential pitfalls of applying DMD to rank-deficient datasets, illustrating with examples. Such computations are not considered in the existing literature but can be understood using our more general framework. In addition, we show that our theory strengthens the connections between DMD and Koopman operator theory. It also establishes connections between DMD and other techniques, including the eigensystem realization algorithm (ERA), a system identification method, and linear inverse modeling (LIM), a method from climate science. We show that under certain conditions, DMD is equivalent to LIM.},
	number = {2},
	journal = {Journal of Computational Dynamics},
	author = {Tu, Jonathan H. and Rowley, Clarence W. and Luchtenburg, Dirk M. and Brunton, Steven L. and Kutz, J. Nathan},
	year = {2014},
	note = {arXiv: 1312.0041},
	keywords = {Dynamic mode decomposition, Koopman operator, Reduced-order models, Spectral analysis, Time series analysis},
	pages = {391--421},
}

@article{PatricHeas2017,
	title = {{OPTIMAL} {LOW}-{RANK} {DYNAMIC} {MODE} {DECOMPOSITION} {Patrick} {H} ´ eas and {C} ´ edric {Herzet} {INRIA} {Centre} {Rennes} - {Bretagne} {Atlantique} , {Campus} universitaire de {Beaulieu} , 35000 {Rennes} , {France}},
	number = {3},
	journal = {IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2017},
	author = {Patric Heas, Cedric Herzet{\textbackslash}},
	year = {2017},
	note = {ISBN: 9781509041176},
	keywords = {★},
	pages = {4456--4460},
}

@techreport{Gal2016,
	title = {Uncertainty in {Deep} {Learning}},
	author = {Gal, Yarin},
	year = {2016},
}

@techreport{Heckerman1996a,
	title = {A {Tutorial} on {Learning} {With} {Bayesian} {Networks}},
	abstract = {A Bayesian network is a graphical model that encodes probabilistic relationships among variables of interest. When used in conjunction with statistical techniques, the graphical model has several advantages for data analysis. One, because the model encodes dependencies among all variables, it readily handles situations where some data entries are missing. Two, a Bayesian network can be used to learn causal relationships, and hence can be used to gain understanding about a problem domain and to predict the consequences of intervention. Three, because the model has both a causal and probabilistic semantics, it is an ideal representation for combining prior knowledge (which often comes in causal form) and data. Four, Bayesian statistical methods in conjunction with Bayesian networks offer an efficient and principled approach for avoiding the overfitting of data. In this paper, we discuss methods for constructing Bayesian networks from prior knowledge and summarize Bayesian statistical methods for using data to improve these models. With regard to the latter task, we describe methods for learning both the parameters and structure of a Bayesian network, including techniques for learning with incomplete data. In addition, we relate Bayesian-network methods for learning to techniques for supervised and unsupervised learning. We illustrate the graphical-modeling approach using a real-world case study.},
	author = {Heckerman, David},
	year = {1996},
	keywords = {★},
}

@article{Liu2016,
	title = {Stein variational gradient descent: {A} general purpose {Bayesian} inference algorithm},
	issn = {10495258},
	abstract = {We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Liu, Qiang and Wang, Dilin},
	year = {2016},
	note = {arXiv: 1608.04471},
	pages = {2378--2386},
}

@article{Larsen,
	title = {Lanczos {Bidiagonalization} with {Partial} {Reorthogonalization} - svds},
	author = {Larsen, Rasmus},
}

@article{Huang2017,
	title = {Fast {Ambiguity} {Resolution} for {Pulsar}-{Based} {Navigation} by {Means} of {Hypothesis} {Testing}},
	volume = {53},
	issn = {00189251},
	doi = {10.1109/TAES.2017.2649698},
	abstract = {The problem of the global navigation satellite system carrier-phase ambiguity resolution (AR) has drawn a lot of attention and to some extent been well solved in the past. However, the pulse-phase AR in the pulsar-based navigation has not been fully investigated due to its special feature that the source pulsars are far away enough to be considered stationary relative to the observers. In this paper, the indeterminate AR problem is proposed to describe this AR situation by a group of indeterminate measurement equations and a new method is developed that enables faster estimation of pulse-phase ambiguities. This method is based on the hypothesis testing that helps one to construct the ambiguity acceptance space at a certain significance level. The acceptance space is then reformulated as a linear form via singular value decomposition that is much easier to search for. Besides, the search algorithm is redesigned that uses particle swarm optimization to quickly find an initial solution and employs a new parameter to compress the search space. As a result, the ambiguity search can be performed much more efficiently especially at big problem sizes.},
	number = {1},
	journal = {IEEE Transactions on Aerospace and Electronic Systems},
	author = {Huang, Liangwei and Lin, Qingqing and Zhang, Xinyuan and Shuai, Ping},
	year = {2017},
	keywords = {Acceptance space, ambiguity resolution (AR), hypothesis testing, matching search, pulsar-based navigation (PNAV)},
	pages = {137--147},
}

@article{Lin2016,
	title = {Densities of {Large} {Matrices} ∗},
	volume = {58},
	number = {1},
	author = {Lin, Lin},
	year = {2016},
	keywords = {10, 1137, 130934283, 15a18, 65f15, ams subject classifications, approximation of distribu-, density of states, doi, large scale sparse matrix, quantum mechanics, spectral density, tion, ★},
	pages = {34--65},
}

@article{Fleeter2019,
	title = {Multilevel and multifidelity uncertainty quantification for cardiovascular hemodynamics},
	url = {http://arxiv.org/abs/1908.04875},
	abstract = {Standard approaches for uncertainty quantification (UQ) in cardiovascular modeling pose challenges due to the large number of uncertain inputs and the significant computational cost of realistic 3D simulations. We propose an efficient UQ framework utilizing a multilevel multifidelity Monte Carlo (MLMF) estimator to improve the accuracy of hemodynamic quantities of interest while maintaining reasonable computational cost. This is achieved by leveraging three cardiovascular model fidelities, each with varying spatial resolution to rigorously quantify the variability in hemodynamic outputs. Our goal is to investigate and compare the efficiency of estimators built from two low-fidelity model alternatives and our high-fidelity 3D models. We demonstrate this framework on healthy and diseased models of aortic and coronary anatomy, including uncertainties in material property and boundary condition parameters. Our goal is to demonstrate that for this application it is possible to accelerate the convergence of the estimators by utilizing a MLMF paradigm. Therefore, we compare our approach to single fidelity Monte Carlo estimators and to a multilevel Monte Carlo approach based only on 3D simulations, but leveraging multiple spatial resolutions. We demonstrate significant, on the order of 10 to 100 times, reduction in total computational cost with the MLMF estimators. We also examine the differing properties of the MLMF estimators in healthy versus diseased models, as well as global versus local quantities of interest. As expected, healthy models and global quantities show larger reductions than diseased models and local quantities as the latter rely more heavily on the highest fidelity model evaluations. In all cases, our workflow coupling Dakota MLMF estimators with the SimVascular cardiovascular workflow make UQ feasible for constrained computational budgets.},
	author = {Fleeter, Casey M. and Geraci, Gianluca and Schiavazzi, Daniele E. and Kahn, Andrew M. and Marsden, Alison L.},
	year = {2019},
	note = {arXiv: 1908.04875},
	keywords = {cardiovascular modeling, monte carlo, multifidelity, multilevel monte carlo, multilevel multifidelity monte carlo, uncertainty quantification, ★},
}

@article{Quick2019,
	title = {Correction: {Multifidelity} {Uncertainty} {Quantification} with {Applications} in {Wind} {Turbine} {Aerodynamics}},
	doi = {10.2514/6.2019-0542.c1},
	abstract = {The propagation of input uncertainty through engineering models allows designers to better understand the range of possible outcomes resulting from design decisions. This could lead to greater trust between modelers and stakeholders in the wind energy industry. In this study, we apply multilevel-multifidelity Monte Carlo sampling to flow over an airfoil, assuming uncertainty in the inflow conditions, and characterize the associated computational savings compared to standard Monte Carlo approaches. The truth model is provided by an airfoil simulation with a very fine computational time step, and auxiliary lower-level models are provided by simulations with coarser time steps. Reynolds-averaged Navier Stokes and detached eddy simulations are used to obtain two different model fidelities. The primary quantity of interest for this analysis is the lift force, which is examined for a range of angles of attack. We launch an initial set of "trial" samples to determine the optimal allocation of model evaluations, and these trial evaluations are used to inform a larger sampling effort. Using the multilevel-multifidelity approach, we achieve roughly an order of magnitude variance reduction in expected lift as compared to the standard Monte Carlo approach with an equivalent computational cost.},
	number = {January},
	author = {Quick, Julian and Hamlington, Peter E. and King, Ryan and Sprague, Michael A.},
	year = {2019},
}

@article{Sculley2015,
	title = {Hidden technical debt in machine learning systems},
	volume = {2015-Janua},
	issn = {10495258},
	abstract = {Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean François and Dennison, Dan},
	year = {2015},
	pages = {2503--2511},
}

@article{VillanuevaZacarias2018,
	title = {A framework to guide the selection and configuration of machine-learning-based data analytics solutions in manufacturing},
	volume = {72},
	issn = {22128271},
	url = {https://doi.org/10.1016/j.procir.2018.03.215},
	doi = {10.1016/j.procir.2018.03.215},
	abstract = {Users in manufacturing willing to apply machine-learning-based (ML-based) data analytics face challenges related to data quality or to the selection and configuration of proper ML algorithms. Current approaches are either purely empirical or reliant on technical data. This makes understanding and comparing candidate solutions difficult, and also ignores the way it impacts the real application problem. In this paper, we propose a framework to generate analytics solutions based on a systematic profiling of all aspects involved. With it, users can visually and systematically explore relevant alternatives for their specific scenario, and obtain recommendations in terms of costs, productivity, results quality, or execution time.},
	journal = {Procedia CIRP},
	author = {Villanueva Zacarias, Alejandro Gabriel and Reimann, Peter and Mitschang, Bernhard},
	year = {2018},
	note = {Publisher: Elsevier B.V.
ISBN: 4971168578402},
	keywords = {data analytics, generative design, learning algorithms, machine learning},
	pages = {153--158},
}

@article{Blundell2015,
	title = {Weight {Uncertainty} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1505.05424},
	abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	month = may,
	year = {2015},
	note = {arXiv: 1505.05424},
}

@inproceedings{Mohr2004,
	title = {Cell-centred multigrid revisited},
	volume = {7},
	doi = {10.1007/s00791-004-0137-0},
	abstract = {In this paper we treat the cell-centred multigrid approach, which distinguishes itself from the classical vertexcentred multigrid by a non-nested hierarchy of grid nodes and the use of constant, problem-independent transfer operators even in complicated situations. We demonstrate, that the tool of Local Fourier Analysis can also be profitably applied in this setting. We consider in detail the standard transfer operators from literature and their respective polynomial and Fourier orders, paying special attention to the combination of piecewise constant interpolation and its adjoint. Furthermore, we give several numerical examples for model problems and an application from biomedical engineering. © Springer-Verlag 2004.},
	booktitle = {Computing and {Visualization} in {Science}},
	publisher = {Springer Verlag},
	author = {Mohr, Marcus and Wienands, Roman},
	year = {2004},
	note = {Issue: 3-4
ISSN: 14330369},
	pages = {129--140},
}

@article{Edelman2005,
	title = {Random matrix theory},
	volume = {14},
	issn = {0962-4929},
	url = {https://www.cambridge.org/core/product/identifier/S0962492904000236/type/journal_article},
	doi = {10.1017/S0962492904000236},
	abstract = {{\textless}p{\textgreater}Random matrix theory is now a big subject with applications in many disciplines of science, engineering and finance. This article is a survey specifically oriented towards the needs and interests of a numerical analyst. This survey includes some original material not found anywhere else. We include the important mathematics which is a very modern development, as well as the computational software that is transforming the theory into useful practice.{\textless}/p{\textgreater}},
	journal = {Acta Numerica},
	author = {Edelman, Alan and Rao, N. Raj},
	month = may,
	year = {2005},
	pages = {233--297},
}

@article{Carrassi2018,
	title = {Data assimilation in the geosciences: {An} overview of methods, issues, and perspectives},
	volume = {9},
	issn = {17577799},
	doi = {10.1002/wcc.535},
	abstract = {We commonly refer to state-estimation theory in geosciences as data assimilation. This term encompasses the entire sequence of operations that, starting from the observations of a system, and from additional statistical and dynamical information (such as a dynamical evolution model), provides an estimate of its state. Data assimilation is standard practice in numerical weather prediction, but its application is becoming widespread in many other areas of climate, atmosphere, ocean and environment modeling; in all circumstances where one intends to estimate the state of a large dynamical system based on limited information. While the complexity of data assimilation, and of the methods thereof, stands on its interdisciplinary nature across statistics, dynamical systems and numerical optimization, when applied to geosciences an additional difficulty arises by the continually increasing sophistication of the environmental models. Thus, in spite of data assimilation being nowadays ubiquitous in geosciences, it has so far remained a topic mostly reserved to experts. We aim this overview article at geoscientists with a background in mathematical and physical modeling, who are interested in the rapid development of data assimilation and its growing domains of application in environmental science, but so far have not delved into its conceptual and methodological complexities.},
	number = {5},
	journal = {Wiley Interdisciplinary Reviews: Climate Change},
	author = {Carrassi, Alberto and Bocquet, Marc and Bertino, Laurent and Evensen, Geir},
	month = sep,
	year = {2018},
	note = {Publisher: Wiley-Blackwell},
	keywords = {Bayesian methods, data assimilation, ensemble methods, environmental prediction},
}

@incollection{Nikkar2013,
	title = {Energy {Stable} {High} {Order} {Finite} {Difference} {Methods} for {Hyperbolic} {Equations} in {Moving} {Coordinate} {Systems}},
	url = {https://doi.org/10.2514/6.2013-2579},
	booktitle = {21st {AIAA} {Computational} {Fluid} {Dynamics} {Conference}},
	publisher = {American Institute of Aeronautics and Astronautics},
	author = {Nikkar, Samira and Nordstrom, Jan},
	month = jun,
	year = {2013},
	doi = {10.2514/6.2013-2579},
	note = {Series Title: Fluid Dynamics and Co-located Conferences},
}

@techreport{noauthor_dimensionality_nodate,
	title = {Dimensionality {Reduction}},
}

@article{fredriksson_model-based_2010,
	title = {Model-based quantitative laser {Doppler} flowmetry in skin},
	volume = {15},
	issn = {1083-3668},
	doi = {10.1117/1.3484746},
	abstract = {Laser Doppler flowmetry (LDF) can be used for assessing the microcirculatory perfusion. However, conventional LDF (cLDF) gives only a relative perfusion estimate for an unknown measurement volume, with no information about the blood flow speed distribution. To overcome these limitations, a model-based analysis method for quantitative LDF (qLDF) is proposed. The method uses inverse Monte Carlo technique with an adaptive three-layer skin model. By analyzing the optimal model where measured and simulated LDF spectra detected at two different source-detector separations match, the absolute microcirculatory perfusion for a specified speed region in a predefined volume is determined. qLDF displayed errors{\textless}12\% when evaluated using simulations of physiologically relevant variations in the layer structure, in the optical properties of static tissue, and in blood absorption. Inhomogeneous models containing small blood vessels, hair, and sweat glands displayed errors{\textless}5\%. Evaluation models containing single larger blood vessels displayed significant errors but could be dismissed by residual analysis. In vivo measurements using local heat provocation displayed a higher perfusion increase with qLDF than cLDF, due to nonlinear effects in the latter. The qLDF showed that the perfusion increase occurred due to an increased amount of red blood cells with a speed{\textgreater}1 mm∕s.},
	number = {5},
	journal = {Journal of Biomedical Optics},
	author = {Fredriksson, Ingemar},
	month = sep,
	year = {2010},
	note = {Publisher: SPIE-Intl Soc Optical Eng},
	pages = {057002},
}

@article{fredriksson_machine_2019,
	title = {Machine learning in multiexposure laser speckle contrast imaging can replace conventional laser {Doppler} flowmetry},
	volume = {24},
	issn = {1560-2281},
	doi = {10.1117/1.jbo.24.1.016001},
	abstract = {Laser speckle contrast imaging (LSCI) enables video rate imaging of
blood flow. However, its relation to tissue blood perfusion is nonlinear
and depends strongly on exposure time. By contrast, the perfusion
estimate from the slower laser Doppler flowmetry (LDF) technique has a
relationship to blood perfusion that is better understood. Multiexposure
LSCI (MELSCI) enables a perfusion estimate closer to the actual
perfusion than that using a single exposure time. We present and
evaluate a method that utilizes contrasts from seven exposure times
between 1 and 64 ms to calculate a perfusion estimate that resembles the
perfusion estimate from LDF. The method is based on artificial neural
networks (ANN) for fast and accurate processing of MELSCI contrasts to
perfusion. The networks are trained using modeling of Doppler histograms
and speckle contrasts from tissue models. The importance of accounting
for noise is demonstrated. Results show that by using ANN, MELSCI data
can be processed to LDF perfusion with high accuracy, with a correlation
coefficient R = 1.000 for noise-free data, R = 0.993 when a moderate
degree of noise is present, and R = 0.995 for in vivo data from an
occlusion-release experiment. (C) The Authors. Published by SPIE under a
Creative Commons Attribution 4.0 Unported License.},
	number = {01},
	journal = {Journal of Biomedical Optics},
	author = {Fredriksson, Ingemar and Hultman, Martin and Strömberg, Tomas and Larsson, Marcus},
	month = jan,
	year = {2019},
	note = {Publisher: SPIE-Intl Soc Optical Eng},
	pages = {1},
}

@article{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02531},
}

@article{wang_gaussian_2016,
	title = {Gaussian process surrogates for failure detection: {A} {Bayesian} experimental design approach},
	volume = {313},
	issn = {10902716},
	doi = {10.1016/j.jcp.2016.02.053},
	abstract = {An important task of uncertainty quantification is to identify the probability of undesired events, in particular, system failures, caused by various sources of uncertainties. In this work we consider the construction of Gaussian process surrogates for failure detection and failure probability estimation. In particular, we consider the situation that the underlying computer models are extremely expensive, and in this setting, determining the sampling points in the state space is of essential importance. We formulate the problem as an optimal experimental design for Bayesian inferences of the limit state (i.e., the failure boundary) and propose an efficient numerical scheme to solve the resulting optimization problem. In particular, the proposed limit-state inference method is capable of determining multiple sampling points at a time, and thus it is well suited for problems where multiple computer simulations can be performed in parallel. The accuracy and performance of the proposed method is demonstrated by both academic and practical examples.},
	journal = {Journal of Computational Physics},
	author = {Wang, Hongqiao and Lin, Guang and Li, Jinglai},
	month = may,
	year = {2016},
	note = {Publisher: Academic Press Inc.},
	keywords = {Bayesian inference, Experimental design, Failure detection, Gaussian processes, Monte Carlo, Response surfaces, Uncertainty quantification},
	pages = {247--259},
}

@techreport{sapsis_output-weighted_2019,
	title = {Output-weighted optimal sampling for {Bayesian} regression and rare event statistics using few samples},
	abstract = {For many important problems the quantity of interest (or output) is an unknown function of the parameter space (or input), which is a random vector with known statistics. Since the dependence of the output on this random vector is unknown, the challenge is to identify its statistics, using the minimum number of function evaluations. This is a problem that can been seen in the context of active learning or optimal experimental design. We employ Bayesian regression to represent the derived model uncertainty due to finite and small number of input-output pairs. In this context we evaluate existing methods for optimal sample selection, such as model error minimization and mutual information maximization. We show that the commonly employed criteria in the literature do not take into account the output values of the existing input-output pairs. To overcome this deficiency we introduce a new criterion that explicitly takes into account the values of the output for the existing samples and adaptively selects inputs from regions or dimensions of the parameter space which have important contribution to the output. The new method allows for application to a large number of input variables, paving the way for optimal experimental design in very high-dimensions.},
	author = {Sapsis, Themistoklis P},
	year = {2019},
	keywords = {Active learning, Bayesian regression, Opti-mal sampling, Optimal experimental design, Rare extreme events},
}

@techreport{gal_dropout_2016,
	title = {Dropout as a {Bayesian} {Approximation}: {Representing} {Model} {Uncertainty} in {Deep} {Learning} {Zoubin} {Ghahramani}},
	url = {http://yarin.co.},
	abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison , Bayesian models offer a mathematically grounded framework to reason about model uncertainty , but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs-extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predic-tive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
	author = {Gal, Yarin and Uk, Zg201@cam Ac},
	year = {2016},
	keywords = {★},
}

@techreport{noauthor_karhunen-loeve_2008,
	title = {Karhunen-{Loeve} {Expansions} and their {Applications} {Limin} {Wang}},
	year = {2008},
}

@article{niiyama_murine_2009,
	title = {Murine model of hindlimb ischemia},
	issn = {1940087X},
	doi = {10.3791/1035},
	abstract = {In the United States, peripheral arterial disease (PAD) affects about 10 million individuals, and is also prevalent worldwide. Medical therapies forsymptomatic relief are limited. Surgical or endovascular interventions are useful for some individuals, but long-term results are often disappointing.As a result, there is a need for developing new therapies to treat PAD. The murine hindlimb ischemia preparation is a model of PAD, and is usefulfor testing new therapies. When compared to other models of tissue ischemia such as coronary or cerebral artery ligation, femoral artery ligationprovides for a simpler model of ischemic tissue. Other advantages of this model are the ease of access to the femoral artery and low mortality rate.In this video, we demonstrate the methodology for the murine model of unilateral hindimb ischemia. The specific materials and procedures forcreating and evaluating the model will be described, including the assessment of limb perfusion by laser Doppler imaging. This protocol can also beutilized for the transplantation and non-invasive tracking of cells, which is demonstrated by Huang et al. © 2009 Journal of Visualized Experiments.},
	number = {23},
	journal = {Journal of Visualized Experiments},
	author = {Niiyama, Hiroshi and Huang, Ngan F. and Rollins, Mark D. and Cooke, John P.},
	year = {2009},
	note = {Publisher: Journal of Visualized Experiments},
}

@techreport{bharai_steady_1982,
	title = {{STEADY} {FLOW} {IN} {A} {MODEL} {OF} {THE} {HUMAN} {CAROTID} {BIFURCATION}. {PART} {I}-{FLOW} {VISUALIZATION}*},
	abstract = {The geometry of a typical adult human carotid bifurcation, complete with the sinus, was established from a study of a large number of angiograms. A rigid model was constructed from glass and investigations were performed under steady flow conditions using flow visualization techniques over a range of upstream Reynolds numbers and flow division ratios through the branches representative of physiologic conditions expected in the human vasculature. The study reveals a complex flow field in which secondary flows play an important role. The separation regions occuring at the outer corners of the branching are also zones of Iow wall shear stress but are not regions of recirculation. The apex and side walls appear to be subjected to much higher shear stress. Comparison with pathologic data on Iocalization of atherosclerotic lesions indicates that zones susceptible to disease experience low or oscillatory shear stress while regions subject to higher shear are free of deposits.},
	author = {Bharai, B K and Mabon, R F and Giddens, D P},
	year = {1982},
	note = {Publication Title: J. Biomechanics
Volume: 15
Issue: 5},
	pages = {349--362},
}

@techreport{ku_pulsatile_nodate,
	title = {Pulsatile {Flow} and {Atherosclerosis} in the {Human} {Carotid} {Bifurcation} {Positive} {Correlation} between {Plaque} {Location} and {Low} and {Oscillating} {Shear} {Stress}},
	abstract = {Fluid velocities were measured by laser Doppler velocimetry under conditions of pulsatile flow in a scale model of the human carotid bifurcation. Flow velocity and wall shear stress at five axial and four circumferential positions were compared with intimal plaque thickness at corresponding locations in carotid bifurcations obtained from cadavers. Velocities and wall shear stresses during diastole were similar to those found previously under steady flow conditions, but these quantities oscillated in both magnitude and direction during the systolic phase. At the inner wall of the internal carotid sinus, in the region of the flow divider, wall shear stress was highest (systole = 41 dynes/cm 2 , diastole = 10 dynes/cm 2 , mean = 17 dynes/cm 2) and remained unidirectional during systole. Intimal thickening in this location was minimal. At the outer wall of the carotid sinus where intimal plaques were thickest, mean shear stress was low (-0. 5 dynes/cm 2) but the instantaneous shear stress oscillated between -7 and + 4 dynes/cm 2. Along the side walls of the sinus, intimal plaque thickness was greater than in the region of the flow divider and circumferential oscillations of shear stress were prominent. With all 20 axial and circumferential measurement locations considered, strong correlations were found between intimal thickness and the reciprocal of maximum shear stress (r = 0.90, p {\textless} 0.0005) or the reciprocal of mean shear stress (r = 0.82, p {\textless} 0.001). An index which takes into account oscillations of wall shear also correlated strongly with intimal thickness (r-0.82, p {\textless} 0.001). When only the inner wall and outer wall positions were taken into account, correlations of lesion thickness with the inverse of maximum wall shear and mean wall shear were 0.94 (p {\textless} 0.001) and 0.95 (p {\textless} 0.001), respectively, and with the oscillatory shear index, 0.93 (p {\textless} 0.001). These studies confirm earlier findings under steady flow conditions that plaques tend to form in areas of low, rather than high, shear stress, but indicate in addition that marked oscillations in the direction of wall shear may enhance atherogenesis. (Arteriosclerosis 5:293-302, May/June 1985) T he role of specific hemodynamic variables in the initiation and development of atherosclerotic plaques in human arteries can be assessed by correlating flow field measurements with the distribution of intimal lesions about branch ostia and bifurcations. flow velocity profiles in situ do not provide sufficient spatial resolution to describe the complex flow patterns at such locations, nor are current imaging techniques adequate for precise localization of early non-stenosing lesions. With the use of appropriate geometric and scaling parameters, rigid glass or plastic models can be used to visualize flow profiles and measure flow velocities representative of those occurring in human vessels. Postmortem human arteries are suitable for deter-minations of corresponding plaque location and size if appropriate pressure-fixation procedures are used. Previous comparison of steady flow measurements in scale models of the human carotid bifurcation with plaque deposition in a corresponding series of autopsy specimens 1 revealed that early lesions occurred principally in regions of flow separation, low wall shear stress, and departure from unidirectional 293},
	author = {Ku, David N and Giddens, Don P and Zarins, Christopher K and Glagov, Seymour},
}

@inproceedings{shaik_numerical_2006,
	title = {Numerical flow simulations of blood in arteries},
	volume = {5},
	isbn = {1-56347-807-2},
	doi = {10.2514/6.2006-294},
	abstract = {The objectives of this investigation are 1) to differentiate the effect of Newtonian and non-Newtonian fluid flow considering the three dimensional models of the artery, 2) to investigate the effect of arterial geometry by considering two models of the arterial bifurcation: (model 1) daughter and parent artery diameters are the same and (model 2) different arterial diameters for both parent and daughter arteries, and 3) to correlate the development of the atherosclerosis in the arteries for both steady and pulsatile flow simulations. In the computations, the non-Newtonian behavior of blood was described using the Carreau-Yasuda model. The predicted effect and the velocity pattern of non-Newtonian model was in qualitative agreement with the experimental measurements available in the literature. The effect of non-Newtonian flow on the shear stress was calculated on both the inner and the outer wall of the two arteries. The areas of extremes in shear stress (low or high), which correlate with the development of atherosclerosis in the arteries are identified for both steady and pulsatile flow cases. Pulsatile flow simulations resulted in showing the variance in the shear stresses at different time levels of the pulse cycle.},
	booktitle = {Collection of {Technical} {Papers} - 44th {AIAA} {Aerospace} {Sciences} {Meeting}},
	author = {Shaik, Eleyas and Hoffmann, Klaus A. and Dietiker, Jean Francois},
	year = {2006},
	pages = {3498--3511},
}

@techreport{rogowska_5_2008,
	title = {5 {Overview} and {Fundamentals} of {Medical} {Image} {Segmentation}},
	author = {Rogowska, Jadwiga},
	year = {2008},
	doi = {10.1016/B978-0-12-373904-9.50013-1},
	note = {Publication Title: HANDBOOK OF MEDICAL IMAGE PROCESSING AND ANALYSIS},
	keywords = {★},
	pages = {73--90},
}

@techreport{wu_study_2012,
	title = {Study on {De}-noising {Extraction} and {Digitization} {Methods} of {Blood} {Vessel}},
	author = {Wu, Cong},
	year = {2012},
}

@article{humeau-heurtier_microvascular_2015,
	title = {Microvascular blood flow monitoring with laser speckle contrast imaging using the generalized differences algorithm},
	volume = {98},
	issn = {10959319},
	doi = {10.1016/j.mvr.2014.12.003},
	abstract = {Laser speckle contrast imaging (LSCI) is a full-field optical technique to monitor microvascular blood flow with high spatial and temporal resolutions. It is used in many medical fields such as dermatology, vascular medicine, or neurosciences. However, LSCI leads to a large amount of data: image sampling frequency is often of several Hz and recordings usually last several minutes. Therefore, clinicians often perform regions of interest in which a spatial averaging of blood flow is performed and the result is followed with time. Unfortunately, this leads to a poor spatial resolution for the analyzed data. At the same time, a higher spatial resolution for the perfusion maps is wanted. To get over this dilemma we propose a new post-acquisition visual representation for LSCI perfusion data using the so-called generalized differences (GD) algorithm. From a stack of perfusion images, the procedure leads to a new single image with the same spatial resolution as the original images and this new image reflects perfusion changes. The algorithm is herein applied on simulated stacks of images and on experimental LSCI perfusion data acquired in three different situations with a commercialized laser speckle contrast imager. The results show that the GD algorithm provides a new way of visualizing LSCI perfusion data.},
	journal = {Microvascular Research},
	author = {Humeau-Heurtier, Anne and Mahé, Guillaume and Abraham, Pierre},
	month = mar,
	year = {2015},
	note = {Publisher: Academic Press Inc.},
	keywords = {Blood flow, Image analysis, Laser speckle, Medical and biological imaging, Microcirculation},
	pages = {54--61},
}

@article{gnyawali_retooling_2017,
	title = {Retooling {Laser} {Speckle} {Contrast} {Analysis} {Algorithm} to {Enhance} {Non}-{Invasive} {High} {Resolution} {Laser} {Speckle} {Functional} {Imaging} of {Cutaneous} {Microcirculation}},
	volume = {7},
	issn = {20452322},
	doi = {10.1038/srep41048},
	abstract = {Cutaneous microvasculopathy complicates wound healing. Functional assessment of gated individual dermal microvessels is therefore of outstanding interest. Functional performance of laser speckle contrast imaging (LSCI) systems is compromised by motion artefacts. To address such weakness, post-processing of stacked images is reported. We report the first post-processing of binary raw data from a high-resolution LSCI camera. Sharp images of low-flowing microvessels were enabled by introducing inverse variance in conjunction with speckle contrast in Matlab-based program code. Extended moving window averaging enhanced signal-To-noise ratio. Functional quantitative study of blood flow kinetics was performed on single gated microvessels using a free hand tool. Based on detection of flow in low-flow microvessels, a new sharp contrast image was derived. Thus, this work presents the first distinct image with quantitative microperfusion data from gated human foot microvasculature. This versatile platform is applicable to study a wide range of tissue systems including fine vascular network in murine brain without craniotomy as well as that in the murine dorsal skin. Importantly, the algorithm reported herein is hardware agnostic and is capable of post-processing binary raw data from any camera source to improve the sensitivity of functional flow data above and beyond standard limits of the optical system.},
	journal = {Scientific Reports},
	author = {Gnyawali, Surya C. and Blum, Kevin and Pal, Durba and Ghatak, Subhadip and Khanna, Savita and Roy, Sashwati and Sen, Chandan K.},
	month = jan,
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	keywords = {★},
}

@article{kochi_characterization_2013,
	title = {Characterization of the arterial anatomy of the murine hindlimb: {Functional} role in the design and understanding of ischemia models},
	volume = {8},
	issn = {19326203},
	doi = {10.1371/journal.pone.0084047},
	abstract = {Rationale: Appropriate ischemia models are required for successful studies of therapeutic angiogenesis. While collateral routes are known to be present within the innate vasculature, there are no reports describing the detailed vascular anatomy of the murine hindlimb. In addition, differences in the descriptions of anatomical names and locations in the literature impede understanding of the circulation and the design of hindlimb ischemia models. To understand better the collateral circulation in the whole hindlimb, clarification of all the feeding arteries of the hindlimb is required. Objective: The aim of this study is to reveal the detailed arterial anatomy and collateral routes in murine hindlimb to enable the appropriate design of therapeutic angiogenesis studies and to facilitate understanding of the circulation in ischemia models. Methods and Results: Arterial anatomy in the murine hindlimb was investigated by contrast-enhanced X-ray imaging and surgical dissection. The observed anatomy is shown in photographic images and in a schema. Previously unnoticed but relatively large arteries were observed in deep, cranial and lateral parts of the thigh. The data indicates that there are three collateral routes through the medial thigh, quadriceps femoris, and the biceps femoris muscles. Furthermore, anatomical variations were found at the origins of the three feeding arteries. Conclusions: The detailed arterial anatomy of murine hindlimb and collateral routes deduced from the anatomy are described. Limitations on designs of ischemia models in view of anatomical variations are proposed. These observations will contribute to the development of animal studies of therapeutic angiogenesis using murine hindlimb ischemia models.},
	number = {12},
	journal = {PLoS ONE},
	author = {Kochi, Takashi and Imai, Yoshimichi and Takeda, Atsushi and Watanabe, Yukiko and Mori, Shiro and Tachi, Masahiro and Kodama, Tetsuya},
	month = dec,
	year = {2013},
	keywords = {★},
}

@article{briers_laser_nodate,
	title = {Laser speckle contrast imaging: theoretical and practical limitations},
	url = {https://www.spiedigitallibrary.org/terms-of-use},
	doi = {10.1117/1},
	abstract = {When laser light illuminates a diffuse object, it produces a random interference effect known as a speckle pattern. If there is movement in the object, the speckles fluctuate in intensity. These fluctuations can provide information about the movement. A simple way of accessing this information is to image the speckle pattern with an exposure time longer than the shortest speckle fluctuation time scale-the fluctuations cause a blurring of the speckle, leading to a reduction in the local speckle contrast. Thus, velocity distributions are coded as speckle contrast variations. The same information can be obtained by using the Doppler effect, but producing a two-dimensional Doppler map requires either scanning of the laser beam or imaging with a high-speed camera: laser speckle contrast imaging (LSCI) avoids the need to scan and can be performed with a normal CCD-or CMOS-camera. LSCI is used primarily to map flow systems, especially blood flow. The development of LSCI is reviewed and its limitations and problems are investigated.},
	author = {Briers, David and Duncan, Donald D and Hirst, Evan and Kirkpatrick, Sean J and Larsson, Marcus and Steenbergen, Wiendelt and Stromberg, Tomas and Thompson, Oliver B},
	keywords = {blood flow, laser Doppler, laser speckle, medical imaging, perfusion Paper 130209R, time-varying speckle},
}

@techreport{abbeel_apprenticeship_nodate,
	title = {Apprenticeship {Learning} via {Inverse} {Reinforcement} {Learning}},
	abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is ex-pressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the ex-pert's unknown reward function.},
	author = {Abbeel, Pieter and Ng, Andrew Y},
	keywords = {★},
}

@article{tompson_accelerating_2016,
	title = {Accelerating {Eulerian} {Fluid} {Simulation} {With} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1607.03597},
	abstract = {Efficient simulation of the Navier-Stokes equations for fluid flow is a long standing problem in applied mathematics, for which state-of-the-art methods require large compute resources. In this work, we propose a data-driven approach that leverages the approximation power of deep-learning with the precision of standard solvers to obtain fast and highly realistic simulations. Our method solves the incompressible Euler equations using the standard operator splitting method, in which a large sparse linear system with many free parameters must be solved. We use a Convolutional Network with a highly tailored architecture, trained using a novel unsupervised learning framework to solve the linear system. We present real-time 2D and 3D simulations that outperform recently proposed data-driven methods; the obtained results are realistic and show good generalization properties.},
	author = {Tompson, Jonathan and Schlachter, Kristofer and Sprechmann, Pablo and Perlin, Ken},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.03597},
	keywords = {★},
}

@article{liu_reinforcement_2017,
	title = {Reinforcement {Learning} {Optimized} {Look}-{Ahead} {Energy} {Management} of a {Parallel} {Hybrid} {Electric} {Vehicle}},
	volume = {22},
	issn = {10834435},
	doi = {10.1109/TMECH.2017.2707338},
	abstract = {This paper presents a predictive energy management strategy for a parallel hybrid electric vehicle (HEV) based on velocity prediction and reinforcement learning (RL). The design procedure starts with modeling the parallel HEV as a systematic control-oriented model and defining a cost function. Fuzzy encoding and nearest neighbor approaches are proposed to achieve velocity prediction, and a finite-state Markov chain is exploited to learn transition probabilities of power demand. To determine the optimal control behaviors and power distribution between two energy sources, a novel RL-based energy management strategy is introduced. For comparison purposes, the two velocity prediction processes are examined by RL using the same realistic driving cycle. The look-ahead energy management strategy is contrasted with shortsighted and dynamic programming based counterparts, and further validated by hardware-in-the-loop test. The results demonstrate that the RL-optimized control is able to significantly reduce fuel consumption and computational time.},
	number = {4},
	journal = {IEEE/ASME Transactions on Mechatronics},
	author = {Liu, Teng and Hu, Xiaosong and Li, Shengbo Eben and Cao, Dongpu},
	month = aug,
	year = {2017},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Energy management, Markov chain (MC), hybrid electric vehicle (HEV), predictive control, reinforcement learning (RL)},
	pages = {1497--1507},
}

@article{rodwell_characteristics_2013,
	title = {Characteristics of occasional poor medium-range weather forecasts for {Europe}},
	volume = {94},
	issn = {00030007},
	doi = {10.1175/BAMS-D-12-00099.1},
	number = {9},
	journal = {Bulletin of the American Meteorological Society},
	author = {Rodwell, Mark J. and Magnusson, Linus and Bauer, Peter and Bechtold, Peter and Bonavita, Massimo and Cardinali, Carla and Diamantakis, Michail and Earnshaw, Paul and Garcia-Mendez, Antonio and Isaksen, Lars and Källén, Erland and Klocke, Daniel and Lopez, Philippe and McNally, Tony and Persson, Anders and Prates, Fernando and Wedi, Nils},
	month = sep,
	year = {2013},
	pages = {1393--1405},
}

@article{derakhti_predicting_2018,
	title = {Predicting the breaking strength of gravity water waves in deep and intermediate depth},
	volume = {848},
	issn = {14697645},
	doi = {10.1017/jfm.2018.352},
	abstract = {We revisit the classical but as yet unresolved problem of predicting the strength of breaking 2-D and 3-D gravity water waves, as quantified by the amount of wave energy dissipated per breaking event. Following Duncan (J. Fluid Mech., vol. 126, 1983, pp. 507-520), the wave energy dissipation rate per unit length of breaking crest may be related to the fifth moment of the wave speed and the non-dimensional breaking strength parameter b. We use a finite-volume Navier-Stokes solver with large-eddy simulation resolution and volume-of-fluid surface reconstruction (Derakhti \& Kirby, J. Fluid Mech., vol. 761, 2014a, pp. 464-506; J. Fluid Mech., vol. 790, 2016, pp. 553-581) to simulate nonlinear wave evolution, with a strong focus on breaking onset and postbreaking behaviour for representative cases of wave packets with breaking due to dispersive focusing and modulational instability. The present study uses these results to investigate the relationship between the breaking strength parameter b and the breaking onset parameter B proposed recently by Barthelemy et al. (J. Fluid Mech., vol. 841, 2018, pp. 463-488). The latter, formed from the local energy flux normalized by the local energy density and the local crest speed, simplifies, on the wave surface, to the ratio of fluid speed to crest speed. Following a wave crest, when B exceeds a generic threshold value at the wave crest (Barthelemy et al. 2018), breaking is imminent. We find a robust relationship between the breaking strength parameter b and the rate of change of breaking onset parameter dB/dt at the wave crest, as it transitions through the generic breaking onset threshold (B ∼ 0:85), scaled by the local period of the breaking wave. This result significantly refines previous efforts to express b in terms of a wave packet steepness parameter, which is difficult to define robustly and which does not provide a generically accurate forecast of the energy dissipated by breaking.},
	journal = {Journal of Fluid Mechanics},
	author = {Derakhti, Morteza and Banner, Michael L. and Kirby, James T.},
	month = aug,
	year = {2018},
	note = {Publisher: Cambridge University Press},
	keywords = {air/sea interactions, surface gravity waves, wave breaking},
	pages = {R2},
}

@article{gnyawali_retooling_2017-1,
	title = {Retooling {Laser} {Speckle} {Contrast} {Analysis} {Algorithm} to {Enhance} {Non}-{Invasive} {High} {Resolution} {Laser} {Speckle} {Functional} {Imaging} of {Cutaneous} {Microcirculation}},
	volume = {7},
	issn = {20452322},
	doi = {10.1038/srep41048},
	abstract = {Cutaneous microvasculopathy complicates wound healing. Functional assessment of gated individual dermal microvessels is therefore of outstanding interest. Functional performance of laser speckle contrast imaging (LSCI) systems is compromised by motion artefacts. To address such weakness, post-processing of stacked images is reported. We report the first post-processing of binary raw data from a high-resolution LSCI camera. Sharp images of low-flowing microvessels were enabled by introducing inverse variance in conjunction with speckle contrast in Matlab-based program code. Extended moving window averaging enhanced signal-To-noise ratio. Functional quantitative study of blood flow kinetics was performed on single gated microvessels using a free hand tool. Based on detection of flow in low-flow microvessels, a new sharp contrast image was derived. Thus, this work presents the first distinct image with quantitative microperfusion data from gated human foot microvasculature. This versatile platform is applicable to study a wide range of tissue systems including fine vascular network in murine brain without craniotomy as well as that in the murine dorsal skin. Importantly, the algorithm reported herein is hardware agnostic and is capable of post-processing binary raw data from any camera source to improve the sensitivity of functional flow data above and beyond standard limits of the optical system.},
	journal = {Scientific Reports},
	author = {Gnyawali, Surya C. and Blum, Kevin and Pal, Durba and Ghatak, Subhadip and Khanna, Savita and Roy, Sashwati and Sen, Chandan K.},
	month = jan,
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	keywords = {★},
}

@article{monteiro_rodrigues_observations_2018,
	title = {Observations on the perfusion recovery of regenerative angiogenesis in an ischemic limb model under hyperoxia},
	volume = {6},
	issn = {2051817X},
	doi = {10.14814/phy2.13736},
	abstract = {This study combines two well-known vascular research models, hyperoxia and hind limb ischemia, aiming to better characterize capacities of the hyperoxia challenge. We studied two groups of C57/BL6 male mice, a control (C) and a hind limb ischemia (HLI) group. Perfusion from both limbs was recorded in all animals by laser Doppler techniques under an oxygen (O2) saturated atmosphere, once for control and, during 35 days for the HLI group. We used a third set of normoxic animals for HLI morphometric control. The expected variability of responses was higher for the younger animals. In the HLI group, capillary density normalized at Day 21 as expected, but not microcirculatory physiology. In the operated limb, perfusion decreased dramatically following surgery (Day 4), as a slight reduction in the non-operated limb was also noted. Consistently, the response to hyperoxia was an increased perfusion in the ischemic limb and decreased perfusion in the contralateral limb. Only at Day 35, both limbs exhibited similar flows, although noticeably lower than Day 0. These observations help to understand some of the functional variability attributed to the hyperoxia model, by showing (i) differences in the circulation of the limb pairs to readjust a new perfusion set-point even after ischemia, an original finding implying that (ii) data from both limbs should be recorded when performing distal measurements in vivo. Our data demonstrate that the new vessels following HLI are not functionally normal, and this also affects the non-operated limb. These findings confirm the discriminative capacities of the hyperoxia challenge and suggest its potential utility to study other pathologies with vascular impact.},
	number = {12},
	journal = {Physiological Reports},
	author = {Monteiro Rodrigues, Luis and Silva, Henrique and Ferreira, Hugo and Renault, Marie Ange and Gadeau, Alain Pierre},
	month = jun,
	year = {2018},
	note = {Publisher: American Physiological Society},
	keywords = {Hind limb ischemia, hyperoxia, laser Doppler, mice, perfusion balance between limbs},
}

@article{seo_numerical_2013,
	title = {Numerical simulations of blood flow in arterial bifurcation models},
	volume = {25},
	issn = {1226119X},
	doi = {10.1007/s13367-013-0016-7},
	abstract = {In the study, two different arterial bifurcation model geometries were used in the flow simulation. The model 1 is assumed the internal carotid artery (ICA) and the external carotid artery (ECA) branches of the bifurcation aligned in parallel to each other, while the model 2 is the typical carotid geometry. In the computation the Non-Newtonian behavior of blood was described using Carreau model. Generally, in the comparison between Newtonian and Non-Newtonian results good agreement was observed in the velocity profiles, while some discrepancies were found in the temporal wall shear stress (WSS) distributions as well as pressure profiles due to the shear thinning behavior. The temporal evolution of WSS periodically increases and decreases closely that of the inlet velocity waveform. It was also observed that the reversed flow region in the ICA of model 2 is 2.5 times larger than that of model 1. As a result, the variation of the flow characteristics can be dependent on the geometry as well as the arterial bifurcation geometry plays an important role in the development of atherosclerosis. © 2013 The Korean Society of Rheology and Springer.},
	number = {3},
	journal = {Korea Australia Rheology Journal},
	author = {Seo, Taewon},
	month = aug,
	year = {2013},
	keywords = {Atherosclerosis, Bifurcation model, Blood flow, Carotid sinus, Wall shear stress, ★},
	pages = {153--161},
}

@misc{noauthor_error_interpolation_nodate,
	title = {error\_interpolation},
}

@article{Petcu2013,
	title = {The one‐dimensional shallow water equations with transparent boundary conditions},
	volume = {36},
	url = {http://umich.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwnZ1LT9tAEIBHiF6KEG9a84j2QBEcTL3rR9bHKCWqEBFSBefVbryWkEigsSPorT-hv7G_pDO7tgU5oV7iw07kOJ6Zndnd-QaAL_kDQxzp-zrkF4tp8YAfzSJamOSUu0jXuuHbtRiM0qtRfLkCsi2M8bCIbvWNzMQ5b7J2bapmk198nU71BfEsyROjcVOYdDPs_},
	doi = {10.1002/mma.1482},
	number = {15},
	journal = {Mathematical Methods in the Applied Sciences},
	author = {Petcu, Madalina and Temam, Roger},
	year = {2013},
	note = {Publisher: WILEY-BLACKWELL
ISBN: 0170-4214},
	pages = {1979--1994},
}

@article{ADAMY2010235,
	title = {A multilevel method for finite volume discretization of the two-dimensional nonlinear shallow-water equations},
	volume = {33},
	issn = {14635003},
	url = {http://www.sciencedirect.com/science/article/pii/S1463500310000156},
	doi = {10.1016/j.ocemod.2010.02.006},
	abstract = {In this article we propose and implement a multilevel method to simulate the solution of the two-dimensional nonlinear shallow-water equations. The multilevel method is based on a central-upwind finite volume scheme and uses new incremental unknowns which enable to preserve the numerical conservation of the scheme. The method is tested and analyzed on two and three levels of discretization on different test cases and turns out to furnish a good solution of the problems while saving CPU time. © 2010 Elsevier Ltd.},
	number = {3-4},
	journal = {Ocean Modelling},
	author = {Adamy, K. and Bousquet, A. and Faure, S. and Laminie, J. and Temam, R.},
	year = {2010},
	keywords = {Finite volume methods, Incremental unknowns, Multilevel methods, Shallow-water problem},
	pages = {235--256},
}

@article{ozgen,
	title = {Urban flood modeling using shallow water equations with depth-dependent anisotropic porosity},
	volume = {541},
	journal = {Journal of Hydrology},
	author = {Özgen, Ilhan and Zhao, Jiaheng and Liang, Dongfang and Hinkelmann, Reinhard},
	year = {2016},
	note = {Publisher: Elsevier B.V
ISBN: 0022-1694},
	pages = {1165--1184},
}

@article{BOUSQUET201366,
	title = {Multilevel finite volume methods and boundary conditions for geophysical flows},
	volume = {74},
	issn = {0045-7930},
	url = {http://www.sciencedirect.com/science/article/pii/S0045793013000108},
	doi = {https://doi.org/10.1016/j.compfluid.2013.01.001},
	abstract = {This review article concerns multi-level methods for finite volume discretizations. They are presented in the context of the non-viscous shallow water equations in space dimension one and two, although the ideas are of more general validity. The second, partly related topic, addressed in this article is that of boundary conditions also presented in the context of the inviscid shallow water equations. Beside algorithmic presentation, the article contains the discussion of some numerical stability issues and the results of some numerical simulations. The multi-level method is applied numerically to the equations of shallow water using a central-upwind scheme for finite volumes and we prove the stability for the linear shallow water equations in different situations. The question of the boundary conditions is illustrated for the two-dimensional equations in the reference case of the so-called Rossby soliton.},
	journal = {Computers \& Fluids},
	author = {Bousquet, Arthur and Marion, Martine and Petcu, Madalina and Temam, Roger},
	year = {2013},
	keywords = {Central-upwind, Finite volume, Multi-level method, Shallow water equations, Transparent boundary conditions},
	pages = {66--90},
}

@article{Faure2005,
	title = {Finite {Volume} {Discretization} and {Multilevel} {Methods} in {Flow} {Problems}},
	volume = {25},
	issn = {1573-7691},
	url = {https://doi.org/10.1007/s10915-004-4642-6},
	doi = {10.1007/s10915-004-4642-6},
	abstract = {This article is intended as a preliminary report on the implementation of a finite volume multilevel scheme for the discretization of the incompressible Navier--Stokes equations. As is well known the use of staggered grids (e.g. MAC grids, Perić et al. Comput. Fluids, 16(4), 389--403, (1988)) is a serious impediment for the implementation of multilevel schemes in the context of finite differences. This difficulty is circumvented here by the use of a colocated finite volume discretization (Faure et al. (2004a) Submitted, Perić et al. Comput. Fluids, 16(4), 389--403, (1988)), for which the algebra of multilevel methods is much simpler than in the context of MAC type finite differences. The general ideas and the numerical simulations are presented in this article in the simplified context of a two-dimensional Burgers equations; the two-, and three-dimensional Navier--Stokes equations introducing new difficulties related to the incompressibility condition and the time discretization, will be considered elsewhere (see Faure et al. (2004a) Submitted and Faure et al. (2004b), in preparation).},
	number = {1},
	journal = {Journal of Scientific Computing},
	author = {Faure, S and Laminie, J and Temam, R},
	month = oct,
	year = {2005},
	pages = {231--261},
}

@article{DUBOIS2005660,
	title = {Multilevel schemes for the shallow water equations},
	volume = {207},
	issn = {0021-9991},
	doi = {https://doi.org/10.1016/j.jcp.2005.01.025},
	abstract = {In this paper, we study a number of multilevel schemes for the numerical solution of the shallow water equations; new schemes and new perspectives of known schemes are examined. We consider the case of periodic boundary conditions. Spatial discretization is obtained using a Fourier spectral Galerkin method. For the time integration, two strategies are studied. The first one is based on scale separation, and we choose the time scheme (explicit or semi-implicit) as a function of the spatial scales (multilevel schemes). The second approach is based on a splitting of the operators, and we choose the time integration method as a function of the operator considered (multistep or fractional schemes). The numerical results obtained are compared with the explicit reference scheme (Leap–Frog scheme), and with the semi-implicit scheme (Leap–Frog scheme with Crank–Nicholson scheme for the gravity terms), both computed with a similar mesh. The drawback of the explicit reference scheme being the numerical stability constraint on the time step, and the drawback of the semi-implicit scheme being the dispersive error, the aim with the new schemes is to obtain schemes with less dispersive error than the semi-implicit scheme, and with better stability properties than the explicit reference scheme. The numerical results obtained show that the schemes proposed allow one to reduce the dispersive error and to increase the numerical stability at reduced cost.},
	number = {2},
	journal = {Journal of Computational Physics},
	author = {Dubois, T and Jauberteau, F and Temam, R M and Tribbia, J},
	year = {2005},
	keywords = {Fractional step methods, Gravity waves, Inertial waves, Multilevel methods, Shallow water equations},
	pages = {660--694},
}

@techreport{yeung_principal_2001,
	title = {Principal component analysis for clustering gene expression data},
	url = {http://www.cs.washington.edu/homes/kayee/pca},
	abstract = {Motivation: There is a great need to develop analytical methodology to analyze and to exploit the information contained in gene expression data. Because of the large number of genes and the complexity of biological networks, clustering is a useful exploratory technique for analysis of gene expression data. Other classical techniques, such as principal component analysis (PCA), have also been applied to analyze gene expression data. Using different data analysis techniques and different clustering algorithms to analyze the same data set can lead to very different conclusions. Our goal is to study the effectiveness of principal components (PCs) in capturing cluster structure. Specifically, using both real and synthetic gene expression data sets, we compared the quality of clusters obtained from the original data to the quality of clusters obtained after projecting onto subsets of the principal component axes. Results: Our empirical study showed that clustering with the PCs instead of the original variables does not necessarily improve, and often degrades, cluster quality. In particular, the first few PCs (which contain most of the variation in the data) do not necessarily capture most of the cluster structure. We also showed that clustering with PCs has different impact on different algorithms and different similarity metrics. Overall, we would not recommend PCA before clustering except in special circumstances.},
	author = {Yeung, K Y and Ruzzo, W L},
	year = {2001},
	note = {Publication Title: BIOINFORMATICS
Volume: 17
Issue: 9},
	pages = {763--774},
}

@techreport{wold_principal_nodate,
	title = {Principal {Component} {Analysis}},
	author = {Wold, Svante and Esbensen, Kim and Geladi, Paul},
}
