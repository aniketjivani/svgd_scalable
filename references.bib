
@article{papamarkou_zero_2014,
	title = {Zero {Variance} {Differential} {Geometric} {Markov} {Chain} {Monte} {Carlo} {Algorithms}},
	volume = {9},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-9/issue-1/Zero-Variance-Differential-Geometric-Markov-Chain-Monte-Carlo-Algorithms/10.1214/13-BA848.full},
	doi = {10.1214/13-BA848},
	abstract = {Differential geometric Markov Chain Monte Carlo (MCMC) strategies exploit the geometry of the target to achieve convergence in fewer MCMC iterations at the cost of increased computing time for each of the iterations. Such computational complexity is regarded as a potential shortcoming of geometric MCMC in practice. This paper suggests that part of the additional computing required by Hamiltonian Monte Carlo and Metropolis adjusted Langevin algorithms produces elements that allow concurrent implementation of the zero variance reduction technique for MCMC estimation. Therefore, zero variance geometric MCMC emerges as an inherently unified sampling scheme, in the sense that variance reduction and geometric exploitation of the parameter space can be performed simultaneously without exceeding the computational requirements posed by the geometric MCMC scheme alone. A MATLAB package is provided, which implements a generic code framework of the combined methodology for a range of models.},
	number = {1},
	urldate = {2025-07-07},
	journal = {Bayesian Analysis},
	author = {Papamarkou, Theodore and Mira, Antonietta and Girolami, Mark},
	month = mar,
	year = {2014},
	keywords = {Hamiltonian Monte Carlo, Metropolis adjusted Langevin algorithms, Metropolis-Hastings, control variates},
	pages = {97--128},
}

@misc{noauthor_14045053_nodate,
	title = {[1404.5053] {The} {Controlled} {Thermodynamic} {Integral} for {Bayesian} {Model} {Comparison}},
	url = {https://arxiv.org/abs/1404.5053},
	urldate = {2025-07-07},
}

@misc{si_scalable_2021,
	title = {Scalable {Control} {Variates} for {Monte} {Carlo} {Methods} via {Stochastic} {Optimization}},
	url = {http://arxiv.org/abs/2006.07487},
	doi = {10.48550/arXiv.2006.07487},
	abstract = {Control variates are a well-established tool to reduce the variance of Monte Carlo estimators. However, for large-scale problems including high-dimensional and large-sample settings, their advantages can be outweighed by a substantial computational cost. This paper considers control variates based on Stein operators, presenting a framework that encompasses and generalizes existing approaches that use polynomials, kernels and neural networks. A learning strategy based on minimising a variational objective through stochastic optimization is proposed, leading to scalable and effective control variates. Novel theoretical results are presented to provide insight into the variance reduction that can be achieved, and an empirical assessment, including applications to Bayesian inference, is provided in support.},
	urldate = {2025-07-07},
	publisher = {arXiv},
	author = {Si, Shijing and Oates, Chris J. and Duncan, Andrew B. and Carin, Lawrence and Briol, François-Xavier},
	month = jul,
	year = {2021},
	note = {arXiv:2006.07487 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ghosh_model_2019,
	title = {Model {Selection} in {Bayesian} {Neural} {Networks} via {Horseshoe} {Priors}},
	volume = {20},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v20/19-236.html},
	abstract = {The promise of augmenting accurate predictions provided by modern neural networks with well-calibrated predictive uncertainties has reinvigorated interest in Bayesian neural networks. However, model selection---even choosing the number of nodes---remains an open question. Poor choices can severely affect the quality of the produced uncertainties. In this paper, we explore continuous shrinkage priors, the horseshoe, and the regularized horseshoe distributions, for model selection in Bayesian neural networks. When placed over node pre-activations and coupled with appropriate variational approximations, we find that the strong shrinkage provided by the horseshoe is effective at turning off nodes that do not help explain the data. We demonstrate that our approach finds compact network structures even when the number of nodes required is grossly over-estimated. Moreover, the model selection over the number of nodes does not come at the expense of predictive or computational performance; in fact, we learn smaller networks with comparable predictive performance to current approaches. These effects are particularly apparent in sample-limited settings, such as small data sets and reinforcement learning.},
	number = {182},
	urldate = {2025-06-30},
	journal = {Journal of Machine Learning Research},
	author = {Ghosh, Soumya and Yao, Jiayu and Doshi-Velez, Finale},
	year = {2019},
	pages = {1--46},
}

@misc{fortuin_bayesian_2022,
	title = {Bayesian {Neural} {Network} {Priors} {Revisited}},
	url = {http://arxiv.org/abs/2102.06571},
	doi = {10.48550/arXiv.2102.06571},
	abstract = {Isotropic Gaussian priors are the de facto standard for modern Bayesian neural network inference. However, it is unclear whether these priors accurately reflect our true beliefs about the weight distributions or give optimal performance. To find better priors, we study summary statistics of neural network weights in networks trained using stochastic gradient descent (SGD). We find that convolutional neural network (CNN) and ResNet weights display strong spatial correlations, while fully connected networks (FCNNs) display heavy-tailed weight distributions. We show that building these observations into priors can lead to improved performance on a variety of image classification datasets. Surprisingly, these priors mitigate the cold posterior effect in FCNNs, but slightly increase the cold posterior effect in ResNets.},
	urldate = {2025-06-30},
	publisher = {arXiv},
	author = {Fortuin, Vincent and Garriga-Alonso, Adrià and Ober, Sebastian W. and Wenzel, Florian and Rätsch, Gunnar and Turner, Richard E. and Wilk, Mark van der and Aitchison, Laurence},
	month = mar,
	year = {2022},
	note = {arXiv:2102.06571 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{agrawal_optimal_2021,
	title = {Optimal {Bounds} between f-{Divergences} and {Integral} {Probability} {Metrics}},
	volume = {22},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v22/20-867.html},
	abstract = {The families of fff-divergences (e.g. the Kullback-Leibler divergence) and Integral Probability Metrics (e.g. total variation distance or maximum mean discrepancies) are widely used to quantify the similarity between probability distributions. In this work, we systematically study the relationship between these two families from the perspective of convex duality.  Starting from a tight variational representation of the fff-divergence, we derive a generalization of the moment-generating function, which we show exactly characterizes the best lower bound of the fff-divergence as a function of a given IPM. Using this characterization, we obtain new bounds while also recovering in a unified manner well-known results, such as Hoeffding's lemma, Pinsker's inequality and its extension to subgaussian functions, and the Hammersley-Chapman-Robbins bound. This characterization also allows us to prove new results on topological properties of the divergence which may be of independent interest.},
	number = {128},
	urldate = {2025-06-29},
	journal = {Journal of Machine Learning Research},
	author = {Agrawal, Rohit and Horel, Thibaut},
	year = {2021},
	pages = {1--59},
}

@misc{noauthor_deep_nodate,
	title = {Deep {Reinforcement} {Learning} {Leaderboard} - a {Hugging} {Face} {Space} by preslaff},
	url = {https://huggingface.co/spaces/preslaff/Deep-Reinforcement-Learning-Leaderboard},
	abstract = {This leaderboard app shows trained agents from a Deep Reinforcement Learning course, ranked by their performance. Users can search for their specific models by entering their user ID.},
	urldate = {2025-06-27},
}

@misc{hafner_learning_2019,
	title = {Learning {Latent} {Dynamics} for {Planning} from {Pixels}},
	url = {http://arxiv.org/abs/1811.04551},
	doi = {10.48550/arXiv.1811.04551},
	abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.},
	urldate = {2025-06-27},
	publisher = {arXiv},
	author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
	month = jun,
	year = {2019},
	note = {arXiv:1811.04551 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{wilson_gaussian_2011,
	title = {Gaussian {Process} {Regression} {Networks}},
	url = {http://arxiv.org/abs/1110.4411},
	doi = {10.48550/arXiv.1110.4411},
	abstract = {We introduce a new regression framework, Gaussian process regression networks (GPRN), which combines the structural properties of Bayesian neural networks with the non-parametric flexibility of Gaussian processes. This model accommodates input dependent signal and noise correlations between multiple response variables, input dependent length-scales and amplitudes, and heavy-tailed predictive distributions. We derive both efficient Markov chain Monte Carlo and variational Bayes inference procedures for this model. We apply GPRN as a multiple output regression and multivariate volatility model, demonstrating substantially improved performance over eight popular multiple output (multi-task) Gaussian process models and three multivariate volatility models on benchmark datasets, including a 1000 dimensional gene expression dataset.},
	urldate = {2025-06-26},
	publisher = {arXiv},
	author = {Wilson, Andrew Gordon and Knowles, David A. and Ghahramani, Zoubin},
	month = oct,
	year = {2011},
	note = {arXiv:1110.4411 [stat]},
	keywords = {Quantitative Finance - Statistical Finance, Statistics - Machine Learning, Statistics - Methodology},
}

@article{kontar_minimizing_2021,
	title = {Minimizing {Negative} {Transfer} of {Knowledge} in {Multivariate} {Gaussian} {Processes}: {A} {Scalable} and {Regularized} {Approach}},
	volume = {43},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Minimizing {Negative} {Transfer} of {Knowledge} in {Multivariate} {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1901.11512},
	doi = {10.1109/TPAMI.2020.2987482},
	abstract = {Recently there has been an increasing interest in the multivariate Gaussian process (MGP) which extends the Gaussian process (GP) to deal with multiple outputs. One approach to construct the MGP and account for non-trivial commonalities amongst outputs employs a convolution process (CP). The CP is based on the idea of sharing latent functions across several convolutions. Despite the elegance of the CP construction, it provides new challenges that need yet to be tackled. First, even with a moderate number of outputs, model building is extremely prohibitive due to the huge increase in computational demands and number of parameters to be estimated. Second, the negative transfer of knowledge may occur when some outputs do not share commonalities. In this paper we address these issues. We propose a regularized pairwise modeling approach for the MGP established using CP. The key feature of our approach is to distribute the estimation of the full multivariate model into a group of bivariate GPs which are individually built. Interestingly pairwise modeling turns out to possess unique characteristics, which allows us to tackle the challenge of negative transfer through penalizing the latent function that facilitates information sharing in each bivariate model. Predictions are then made through combining predictions from the bivariate models within a Bayesian framework. The proposed method has excellent scalability when the number of outputs is large and minimizes the negative transfer of knowledge between uncorrelated outputs. Statistical guarantees for the proposed method are studied and its advantageous features are demonstrated through numerical studies.},
	number = {10},
	urldate = {2025-06-26},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kontar, Raed and Raskutti, Garvesh and Zhou, Shiyu},
	month = oct,
	year = {2021},
	note = {arXiv:1901.11512 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {3508--3522},
}

@misc{noauthor_solving_nodate,
	title = {Solving the {Parametric} {Eigenvalue} {Problem} by {Taylor} {Series} and {Chebyshev} {Expansion}},
	url = {https://epubs.siam.org/doi/epdf/10.1137/23M1551961},
	language = {en},
	urldate = {2025-06-23},
	doi = {10.1137/23M1551961},
}

@article{barnard_national_2016,
	title = {The {National} {Eclipse} {Weather} {Experiment}: an assessment of citizen scientist weather observations},
	volume = {374},
	shorttitle = {The {National} {Eclipse} {Weather} {Experiment}},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsta.2015.0220},
	doi = {10.1098/rsta.2015.0220},
	abstract = {The National Eclipse Weather Experiment (NEWEx) was a citizen science project designed to assess the effects of the 20 March 2015 partial solar eclipse on the weather over the United Kingdom (UK). NEWEx had two principal objectives: to provide a spatial network of meteorological observations across the UK to aid the investigation of eclipse-induced weather changes, and to develop a nationwide public engagement activity-based participation of citizen scientists. In total, NEWEx collected 15 606 observations of air temperature, cloudiness and wind speed and direction from 309 locations across the UK, over a 3 h window spanning the eclipse period. The headline results were processed in near real time, immediately published online, and featured in UK national press articles on the day of the eclipse. Here, we describe the technical development of NEWEx and how the observations provided by the citizen scientists were analysed. By comparing the results of the NEWEx analyses with results from other investigations of the same eclipse using different observational networks, including measurements from the University of Reading’s Atmospheric Observatory, we demonstrate that NEWEx provided a fair representation of the change in the UK meteorological conditions throughout the eclipse. Despite the simplicity of the approach adopted, robust reductions in both temperature and wind speed during the eclipse were observed.
This article is part of the themed issue ‘Atmospheric effects of solar eclipses stimulated by the 2015 UK eclipse’.},
	number = {2077},
	urldate = {2025-06-20},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Barnard, L. and Portas, A. M. and Gray, S. L. and Harrison, R. G.},
	month = sep,
	year = {2016},
	keywords = {citizen science, eclipse, meteorology},
	pages = {20150220},
}

@article{guhaniyogi_compressed_2016,
	title = {Compressed {Gaussian} {Process} for {Manifold} {Regression}},
	volume = {17},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v17/14-230.html},
	abstract = {Nonparametric regression for large numbers of features (ppp) is an increasingly important problem. If the sample size nnn is massive, a common strategy is to partition the feature space, and then separately apply simple models to each partition set. This is not ideal when nnn is modest relative to ppp, and we propose an alternative approach relying on random compression of the feature vector combined with Gaussian process regression. The proposed approach is particularly motivated by the setting in which the response is conditionally independent of the features given the projection to a low dimensional manifold. Conditionally on the random compression matrix and a smoothness parameter, the posterior distribution for the regression surface and posterior predictive distributions are available analytically. Running the analysis in parallel for many random compression matrices and smoothness parameters, model averaging is used to combine the results. The algorithm can be implemented rapidly even in very large ppp and moderately large nnn nonparametric regression, has strong theoretical justification, and is found to yield state of the art predictive performance.},
	number = {69},
	urldate = {2025-06-19},
	journal = {Journal of Machine Learning Research},
	author = {Guhaniyogi, Rajarshi and Dunson, David B.},
	year = {2016},
	pages = {1--26},
}

@inproceedings{delbridge_randomly_2020,
	title = {Randomly {Projected} {Additive} {Gaussian} {Processes} for {Regression}},
	url = {https://proceedings.mlr.press/v119/delbridge20a.html},
	abstract = {Gaussian processes (GPs) provide flexible distributions over functions, with inductive biases controlled by a kernel. However, in many applications Gaussian processes can struggle with even moderate input dimensionality. Learning a low dimensional projection can help alleviate this curse of dimensionality, but introduces many trainable hyperparameters, which can be cumbersome, especially in the small data regime. We use additive sums of kernels for GP regression, where each kernel operates on a different random projection of its inputs. Surprisingly, we find that as the number of random projections increases, the predictive performance of this approach quickly converges to the performance of a kernel operating on the original full dimensional inputs, over a wide range of data sets, even if we are projecting into a single dimension. As a consequence, many problems can remarkably be reduced to one dimensional input spaces, without learning a transformation. We prove this convergence and its rate, and additionally propose a deterministic approach that converges more quickly than purely random projections. Moreover, we demonstrate our approach can achieve faster inference and improved predictive accuracy for high-dimensional inputs compared to kernels in the original input space.},
	language = {en},
	urldate = {2025-06-19},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Delbridge, Ian and Bindel, David and Wilson, Andrew Gordon},
	month = nov,
	year = {2020},
	pages = {2453--2463},
}

@misc{liu_stein_2018,
	title = {Stein {Variational} {Gradient} {Descent} as {Moment} {Matching}},
	url = {http://arxiv.org/abs/1810.11693},
	doi = {10.48550/arXiv.1810.11693},
	abstract = {Stein variational gradient descent (SVGD) is a non-parametric inference algorithm that evolves a set of particles to fit a given distribution of interest. We analyze the non-asymptotic properties of SVGD, showing that there exists a set of functions, which we call the Stein matching set, whose expectations are exactly estimated by any set of particles that satisfies the fixed point equation of SVGD. This set is the image of Stein operator applied on the feature maps of the positive definite kernel used in SVGD. Our results provide a theoretical framework for analyzing the properties of SVGD with different kernels, shedding insight into optimal kernel choice. In particular, we show that SVGD with linear kernels yields exact estimation of means and variances on Gaussian distributions, while random Fourier features enable probabilistic bounds for distributional approximation. Our results offer a refreshing view of the classical inference problem as fitting Stein's identity or solving the Stein equation, which may motivate more efficient algorithms.},
	urldate = {2025-06-16},
	publisher = {arXiv},
	author = {Liu, Qiang and Wang, Dilin},
	month = oct,
	year = {2018},
	note = {arXiv:1810.11693 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{depeweg_learning_2017,
	title = {Learning and {Policy} {Search} in {Stochastic} {Dynamical} {Systems} with {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1605.07127},
	doi = {10.48550/arXiv.1605.07127},
	abstract = {We present an algorithm for model-based reinforcement learning that combines Bayesian neural networks (BNNs) with random roll-outs and stochastic optimization for policy learning. The BNNs are trained by minimizing \${\textbackslash}alpha\$-divergences, allowing us to capture complicated statistical patterns in the transition dynamics, e.g. multi-modality and heteroskedasticity, which are usually missed by other common modeling approaches. We illustrate the performance of our method by solving a challenging benchmark where model-based approaches usually fail and by obtaining promising results in a real-world scenario for controlling a gas turbine.},
	urldate = {2025-06-16},
	publisher = {arXiv},
	author = {Depeweg, Stefan and Hernández-Lobato, José Miguel and Doshi-Velez, Finale and Udluft, Steffen},
	month = mar,
	year = {2017},
	note = {arXiv:1605.07127 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{najib_iterative_2024,
	title = {Iterative {Batch} {Reinforcement} {Learning} via {Safe} {Diversified} {Model}-based {Policy} {Search}},
	url = {http://arxiv.org/abs/2411.09722},
	doi = {10.48550/arXiv.2411.09722},
	abstract = {Batch reinforcement learning enables policy learning without direct interaction with the environment during training, relying exclusively on previously collected sets of interactions. This approach is, therefore, well-suited for high-risk and cost-intensive applications, such as industrial control. Learned policies are commonly restricted to act in a similar fashion as observed in the batch. In a real-world scenario, learned policies are deployed in the industrial system, inevitably leading to the collection of new data that can subsequently be added to the existing recording. The process of learning and deployment can thus take place multiple times throughout the lifespan of a system. In this work, we propose to exploit this iterative nature of applying offline reinforcement learning to guide learned policies towards efficient and informative data collection during deployment, leading to continuous improvement of learned policies while remaining within the support of collected data. We present an algorithmic methodology for iterative batch reinforcement learning based on ensemble-based model-based policy search, augmented with safety and, importantly, a diversity criterion.},
	urldate = {2025-06-16},
	publisher = {arXiv},
	author = {Najib, Amna and Depeweg, Stefan and Swazinna, Phillip},
	month = nov,
	year = {2024},
	note = {arXiv:2411.09722 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{xie_cost-aware_2025,
	title = {Cost-aware {Bayesian} {Optimization} via the {Pandora}'s {Box} {Gittins} {Index}},
	url = {http://arxiv.org/abs/2406.20062},
	doi = {10.48550/arXiv.2406.20062},
	abstract = {Bayesian optimization is a technique for efficiently optimizing unknown functions in a black-box manner. To handle practical settings where gathering data requires use of finite resources, it is desirable to explicitly incorporate function evaluation costs into Bayesian optimization policies. To understand how to do so, we develop a previously-unexplored connection between cost-aware Bayesian optimization and the Pandora's Box problem, a decision problem from economics. The Pandora's Box problem admits a Bayesian-optimal solution based on an expression called the Gittins index, which can be reinterpreted as an acquisition function. We study the use of this acquisition function for cost-aware Bayesian optimization, and demonstrate empirically that it performs well, particularly in medium-high dimensions. We further show that this performance carries over to classical Bayesian optimization without explicit evaluation costs. Our work constitutes a first step towards integrating techniques from Gittins index theory into Bayesian optimization.},
	urldate = {2025-06-15},
	publisher = {arXiv},
	author = {Xie, Qian and Astudillo, Raul and Frazier, Peter I. and Scully, Ziv and Terenin, Alexander},
	month = jan,
	year = {2025},
	note = {arXiv:2406.20062 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{scully_gittins_2025,
	title = {The {Gittins} {Index}: {A} {Design} {Principle} for {Decision}-{Making} {Under} {Uncertainty}},
	shorttitle = {The {Gittins} {Index}},
	url = {http://arxiv.org/abs/2506.10872},
	doi = {10.48550/arXiv.2506.10872},
	abstract = {The Gittins index is a tool that optimally solves a variety of decision-making problems involving uncertainty, including multi-armed bandit problems, minimizing mean latency in queues, and search problems like the Pandora's box model. However, despite the above examples and later extensions thereof, the space of problems that the Gittins index can solve perfectly optimally is limited, and its definition is rather subtle compared to those of other multi-armed bandit algorithms. As a result, the Gittins index is often regarded as being primarily a concept of theoretical importance, rather than a practical tool for solving decision-making problems. The aim of this tutorial is to demonstrate that the Gittins index can be fruitfully applied to practical problems. We start by giving an example-driven introduction to the Gittins index, then walk through several examples of problems it solves - some optimally, some suboptimally but still with excellent performance. Two practical highlights in the latter category are applying the Gittins index to Bayesian optimization, and applying the Gittins index to minimizing tail latency in queues.},
	urldate = {2025-06-14},
	publisher = {arXiv},
	author = {Scully, Ziv and Terenin, Alexander},
	month = jun,
	year = {2025},
	note = {arXiv:2506.10872 [math]},
	keywords = {Computer Science - Machine Learning, Computer Science - Performance, Mathematics - Optimization and Control, Mathematics - Probability, Statistics - Machine Learning},
}

@mastersthesis{persky_exploration_2021,
	title = {Exploration and {Exploitation}: {From} {Bandits} to {Bayesian} {Optimisation}},
	url = {https://www.mlmi.eng.cam.ac.uk/files/2020-2021_dissertations/exploration_and_exploitation.pdf},
	abstract = {This project aims to offer a theoretically principled method for approaching the task of
balancing exploration and exploitation in the setting of correlated multi-arm bandits, and
to extend this application to encompass Bayesian optimisation. Our method is an attempt
to generalise the Gittins index algorithm for multi-arm bandits to settings where bandit
reward distributions can be correlated and, following that, settings with a continuum of
bandits, leading to Bayesian optimisation. Our algorithm has no hyperparameters to set and
is theoretically principled in the sense that it is motivated by consideration of the optimality of
the Gittins index for independent bandits. We also derive an efficient algorithm for computing
the Gittins index based on Gaussian process approximations to the value function using cubic
spline covariance functions. Our Gittins based algorithm is demonstrated to outperform many
standard approaches to multi-arm bandits when the bandits are correlated and is competitive
with standard algorithms for Bayesian optimisation, especially outperforming those without
free hyperparameters.},
	school = {University of Cambridge},
	author = {Persky, Eli},
	year = {2021},
}

@article{oune_latent_2021,
	title = {Latent map {Gaussian} processes for mixed variable metamodeling},
	volume = {387},
	issn = {0045-7825},
	url = {https://www.sciencedirect.com/science/article/pii/S004578252100459X},
	doi = {10.1016/j.cma.2021.114128},
	abstract = {Gaussian processes (GPs) are ubiquitously used in sciences and engineering as metamodels. Standard GPs, however, can only handle numerical or quantitative variables. In this paper, we introduce latent map Gaussian processes (LMGPs) that inherit the attractive properties of GPs and are also applicable to mixed data which have both quantitative and qualitative inputs. The core idea behind LMGPs is to learn a continuous, low-dimensional latent space or manifold which encodes all qualitative inputs. To learn this manifold, we first assign a unique prior vector representation to each combination of qualitative inputs. We then use a low-rank linear map to project these priors on a manifold that characterizes the posterior representations. As the posteriors are quantitative, they can be directly used in any standard correlation function such as the Gaussian or Matern. Hence, the optimal map and the corresponding manifold, along with other hyperparameters of the correlation function, can be systematically learned via maximum likelihood estimation. Through a wide range of analytic and real-world examples, we demonstrate the advantages of LMGPs over state-of-the-art methods in terms of accuracy and versatility. In particular, we show that LMGPs can handle variable-length inputs, have an explainable neural network interpretation, and provide insights into how qualitative inputs affect the response or interact with each other. We also employ LMGPs in Bayesian optimization and illustrate that they can discover optimal compound compositions more efficiently than conventional methods that convert compositions to qualitative variables via manual featurization.},
	urldate = {2025-06-14},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Oune, Nicholas and Bostanabad, Ramin},
	month = dec,
	year = {2021},
	keywords = {Computer experiments, Emulation, Gaussian processes, Manifold learning, Metamodeling, Mixed-variable optimization},
	pages = {114128},
}

@article{laichter_statistical_2024,
	title = {Statistical analysis of flow field variations via independent component analysis},
	volume = {65},
	issn = {1432-1114},
	url = {https://doi.org/10.1007/s00348-024-03771-7},
	doi = {10.1007/s00348-024-03771-7},
	abstract = {The link between in-cylinder flow and subsequent combustion in a single-cylinder gasoline spark-ignition engine is analyzed via independent component analysis (ICA). Experimentally, the two in-plane components of the velocity are measured in the central cylinder plane by high-speed particle image velocimetry (PIV) with the engine running slightly lean at 1500 rpm in skip-fired mode. In 213 cycles, measurements are made during the late compression stroke before ignition with approximately 1° crank-angle temporal resolution. ICA then decomposes the set of 213 flow fields at each time step, yielding a set of “source” flow patterns—the independent components (IC). The temporal coherence between the ICs is then examined in a persistence analysis, comparing each IC with the one from the previous time step starting at ignition timing and going backwards in time. The results show which ICs persist how long throughout the compression stroke. To investigate the link between the ICs and combustion, the crank angle at which 10\% of the fuel are burned (CA10) in each cycle is correlated with the extent to which a given IC can be found in each flow field. The most persistent IC can be traced over more than half of the 70 degrees crank angle over which images were acquired. The IC that correlates best with CA10 visually more resembles some of the flow features found in conditional averaging of fast-burning versus slow-burning cycles.},
	language = {en},
	number = {3},
	urldate = {2025-06-12},
	journal = {Experiments in Fluids},
	author = {Laichter, J. and Kranz, P. and Kaiser, S. A.},
	month = feb,
	year = {2024},
	keywords = {Engine Technology, Flow cytometry, Internal Combustion Engines, Measurement Science and Instrumentation, Numerical Analysis, Signal Processing},
	pages = {32},
}

@misc{wang_stein_2019,
	title = {Stein {Variational} {Gradient} {Descent} {With} {Matrix}-{Valued} {Kernels}},
	url = {http://arxiv.org/abs/1910.12794},
	doi = {10.48550/arXiv.1910.12794},
	abstract = {Stein variational gradient descent (SVGD) is a particle-based inference algorithm that leverages gradient information for efficient approximate inference. In this work, we enhance SVGD by leveraging preconditioning matrices, such as the Hessian and Fisher information matrix, to incorporate geometric information into SVGD updates. We achieve this by presenting a generalization of SVGD that replaces the scalar-valued kernels in vanilla SVGD with more general matrix-valued kernels. This yields a significant extension of SVGD, and more importantly, allows us to flexibly incorporate various preconditioning matrices to accelerate the exploration in the probability landscape. Empirical results show that our method outperforms vanilla SVGD and a variety of baseline approaches over a range of real-world Bayesian inference tasks.},
	urldate = {2025-06-09},
	publisher = {arXiv},
	author = {Wang, Dilin and Tang, Ziyang and Bajaj, Chandrajit and Liu, Qiang},
	month = nov,
	year = {2019},
	note = {arXiv:1910.12794 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{salimans_progressive_2022,
	title = {Progressive {Distillation} for {Fast} {Sampling} of {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2202.00512},
	doi = {10.48550/arXiv.2202.00512},
	abstract = {Diffusion models have recently shown great promise for generative modeling, outperforming GANs on perceptual quality and autoregressive models at density estimation. A remaining downside is their slow sampling time: generating high quality samples takes many hundreds or thousands of model evaluations. Here we make two contributions to help eliminate this downside: First, we present new parameterizations of diffusion models that provide increased stability when using few sampling steps. Second, we present a method to distill a trained deterministic diffusion sampler, using many steps, into a new diffusion model that takes half as many sampling steps. We then keep progressively applying this distillation procedure to our model, halving the number of required sampling steps each time. On standard image generation benchmarks like CIFAR-10, ImageNet, and LSUN, we start out with state-of-the-art samplers taking as many as 8192 steps, and are able to distill down to models taking as few as 4 steps without losing much perceptual quality; achieving, for example, a FID of 3.0 on CIFAR-10 in 4 steps. Finally, we show that the full progressive distillation procedure does not take more time than it takes to train the original model, thus representing an efficient solution for generative modeling using diffusion at both train and test time.},
	urldate = {2025-06-09},
	publisher = {arXiv},
	author = {Salimans, Tim and Ho, Jonathan},
	month = jun,
	year = {2022},
	note = {arXiv:2202.00512 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{fop_model-based_2018,
	title = {Model-based {Clustering} with {Sparse} {Covariance} {Matrices}},
	url = {http://arxiv.org/abs/1711.07748},
	doi = {10.48550/arXiv.1711.07748},
	abstract = {Finite Gaussian mixture models are widely used for model-based clustering of continuous data. Nevertheless, since the number of model parameters scales quadratically with the number of variables, these models can be easily over-parameterized. For this reason, parsimonious models have been developed via covariance matrix decompositions or assuming local independence. However, these remedies do not allow for direct estimation of sparse covariance matrices nor do they take into account that the structure of association among the variables can vary from one cluster to the other. To this end, we introduce mixtures of Gaussian covariance graph models for model-based clustering with sparse covariance matrices. A penalized likelihood approach is employed for estimation and a general penalty term on the graph configurations can be used to induce different levels of sparsity and incorporate prior knowledge. Model estimation is carried out using a structural-EM algorithm for parameters and graph structure estimation, where two alternative strategies based on a genetic algorithm and an efficient stepwise search are proposed for inference. With this approach, sparse component covariance matrices are directly obtained. The framework results in a parsimonious model-based clustering of the data via a flexible model for the within-group joint distribution of the variables. Extensive simulated data experiments and application to illustrative datasets show that the method attains good classification performance and model quality.},
	urldate = {2025-06-03},
	publisher = {arXiv},
	author = {Fop, Michael and Murphy, Thomas Brendan and Scrucca, Luca},
	month = sep,
	year = {2018},
	note = {arXiv:1711.07748 [stat]},
	keywords = {Statistics - Computation, Statistics - Methodology},
}

@inproceedings{wang_stein_2018,
	title = {Stein {Variational} {Message} {Passing} for {Continuous} {Graphical} {Models}},
	url = {https://proceedings.mlr.press/v80/wang18l.html},
	abstract = {We propose a novel distributed inference algorithm for continuous graphical models, by extending Stein variational gradient descent (SVGD) to leverage the Markov dependency structure of the distribution of interest. Our approach combines SVGD with a set of structured local kernel functions defined on the Markov blanket of each node, which alleviates the curse of high dimensionality and simultaneously yields a distributed algorithm for decentralized inference tasks. We justify our method with theoretical analysis and show that the use of local kernels can be viewed as a new type of localized approximation that matches the target distribution on the conditional distributions of each node over its Markov blanket. Our empirical results show that our method outperforms a variety of baselines including standard MCMC and particle message passing methods.},
	language = {en},
	urldate = {2025-06-03},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wang, Dilin and Zeng, Zhe and Liu, Qiang},
	month = jul,
	year = {2018},
	pages = {5219--5227},
}

@article{andras_bayesian_2012,
	title = {A {Bayesian} interpretation of the particle swarm optimization and its kernel extension},
	volume = {7},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0048710},
	abstract = {Particle swarm optimization is a popular method for solving difficult optimization problems. There have been attempts to formulate the method in formal probabilistic or stochastic terms (e.g. bare bones particle swarm) with the aim to achieve more generality and explain the practical behavior of the method. Here we present a Bayesian interpretation of the particle swarm optimization. This interpretation provides a formal framework for incorporation of prior knowledge about the problem that is being solved. Furthermore, it also allows to extend the particle optimization method through the use of kernel functions that represent the intermediary transformation of the data into a different space where the optimization problem is expected to be easier to be resolved-such transformation can be seen as a form of prior knowledge about the nature of the optimization problem. We derive from the general Bayesian formulation the commonly used particle swarm methods as particular cases.},
	language = {eng},
	number = {11},
	journal = {PloS One},
	author = {Andras, Peter},
	year = {2012},
	pmid = {23144937},
	pmcid = {PMC3492439},
	keywords = {Algorithms, Bayes Theorem, Models, Theoretical, Problem Solving},
	pages = {e48710},
}

@misc{lian_rates_2007,
	title = {On rates of convergence for posterior distributions under misspecification},
	url = {http://arxiv.org/abs/math/0702126},
	doi = {10.48550/arXiv.math/0702126},
	abstract = {We extend the approach of Walker (2003, 2004) to the case of misspecified models. A sufficient condition for establishing rates of convergence is given based on a key identity involving martingales, which does not require construction of tests. We also show roughly that the result obtained by using tests can also be obtained by our approach, which demonstrates the potential wider applicability of this method.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Lian, Heng},
	month = feb,
	year = {2007},
	note = {arXiv:math/0702126},
	keywords = {Mathematics - Statistics Theory, Statistics - Statistics Theory},
}

@article{ramamoorthi_posterior_2015,
	title = {On {Posterior} {Concentration} in {Misspecified} {Models}},
	volume = {10},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-10/issue-4/On-Posterior-Concentration-in-Misspecified-Models/10.1214/15-BA941.full},
	doi = {10.1214/15-BA941},
	abstract = {We investigate the asymptotic behavior of Bayesian posterior distributions under independent and identically distributed (i.i.d.) misspecified models. More specifically, we study the concentration of the posterior distribution on neighborhoods of f⋆, the density that is closest in the Kullback–Leibler sense to the true model f0. We note, through examples, the need for assumptions beyond the usual Kullback–Leibler support assumption. We then investigate consistency with respect to a general metric under three assumptions, each based on a notion of divergence measure, and then apply these to a weighted L1-metric in convex models and non-convex models. Although a few results on this topic are available, we believe that these are somewhat inaccessible due, in part, to the technicalities and the subtle differences compared to the more familiar well-specified model case. One of our goals is to make some of the available results, especially that of Kleijn and van der Vaart (2006), more accessible. Unlike their paper, our approach does not require construction of test sequences. We also discuss a preliminary extension of the i.i.d. results to the independent but not identically distributed (i.n.i.d.) case.},
	number = {4},
	urldate = {2025-06-02},
	journal = {Bayesian Analysis},
	author = {Ramamoorthi, R. V. and Sriram, Karthik and Martin, Ryan},
	month = dec,
	year = {2015},
	keywords = {62C10, Bayesian, Kullback–Leibler, consistency, misspecified},
	pages = {759--789},
}

@article{lingras_rough_2011,
	title = {Rough clustering},
	volume = {1},
	copyright = {Copyright © 2011 John Wiley \& Sons, Inc.},
	issn = {1942-4795},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.16},
	doi = {10.1002/widm.16},
	abstract = {Traditional clustering partitions a group of objects into a number of nonoverlapping sets based on a similarity measure. In real world, the boundaries of these sets or clusters may not be clearly defined. Some of the objects may be almost equidistant from the center of multiple clusters. Traditional set theory mandates that these objects be assigned to a single cluster. Rough set theory can be used to represent the overlapping clusters. Rough sets provide more flexible representation than conventional sets, at the same time they are less descriptive than the fuzzy sets. This paper describes the basic concept of rough clustering based on k-means, genetic algorithms, Kohonen self-organizing maps, and support vector clustering. The discussion also includes a review of rough cluster validity measures, and applications of rough clustering to such diverse areas as forestry, medicine, medical imaging, web mining, super markets, and traffic engineering. © 2011 John Wiley \& Sons, Inc. WIREs Data Mining Knowl Discov 2011 1 64-72 DOI: 10.1002/widm.16 This article is categorized under: Technologies {\textgreater} Computational Intelligence Technologies {\textgreater} Machine Learning Technologies {\textgreater} Structure Discovery and Clustering},
	language = {en},
	number = {1},
	urldate = {2025-06-01},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Lingras, Pawan and Peters, Georg},
	year = {2011},
	pages = {64--72},
}

@misc{noauthor_physics-informed_nodate,
	title = {Physics-informed {Dyna}-style model-based deep reinforcement learning for dynamic control {\textbar} {Proceedings} of the {Royal} {Society} {A}: {Mathematical}, {Physical} and {Engineering} {Sciences}},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rspa.2021.0618},
	urldate = {2025-05-29},
}

@misc{min_policy_2020,
	title = {Policy {Gradient} {Optimization} of {Thompson} {Sampling} {Policies}},
	url = {http://arxiv.org/abs/2006.16507},
	doi = {10.48550/arXiv.2006.16507},
	abstract = {We study the use of policy gradient algorithms to optimize over a class of generalized Thompson sampling policies. Our central insight is to view the posterior parameter sampled by Thompson sampling as a kind of pseudo-action. Policy gradient methods can then be tractably applied to search over a class of sampling policies, which determine a probability distribution over pseudo-actions (i.e., sampled parameters) as a function of observed data. We also propose and compare policy gradient estimators that are specialized to Bayesian bandit problems. Numerical experiments demonstrate that direct policy search on top of Thompson sampling automatically corrects for some of the algorithm's known shortcomings and offers meaningful improvements even in long horizon problems where standard Thompson sampling is extremely effective.},
	urldate = {2025-05-29},
	publisher = {arXiv},
	author = {Min, Seungki and Moallemi, Ciamac C. and Russo, Daniel J.},
	month = jun,
	year = {2020},
	note = {arXiv:2006.16507 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{bonev_spherical_2023,
	title = {Spherical {Fourier} {Neural} {Operators}: {Learning} {Stable} {Dynamics} on the {Sphere}},
	shorttitle = {Spherical {Fourier} {Neural} {Operators}},
	url = {https://proceedings.mlr.press/v202/bonev23a.html},
	abstract = {Fourier Neural Operators (FNOs) have proven to be an efficient and effective method for resolution-independent operator learning in a broad variety of application areas across scientific machine learning. A key reason for their success is their ability to accurately model long-range dependencies in spatio-temporal data by learning global convolutions in a computationally efficient manner. To this end, FNOs rely on the discrete Fourier transform (DFT), however, DFTs cause visual and spectral artifacts as well as pronounced dissipation when learning operators in spherical coordinates by incorrectly assuming flat geometry. To overcome this limitation, we generalize FNOs on the sphere, introducing Spherical FNOs (SFNOs) for learning operators on spherical geometries. We apply SFNOs to forecasting atmo- spheric dynamics, and demonstrate stable autoregressive rollouts for a year of simulated time (1,460 steps), while retaining physically plausible dynamics. The SFNO has important implications for machine learning-based simulation of climate dynamics that could eventually help accelerate our response to climate change.},
	language = {en},
	urldate = {2025-05-24},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Bonev, Boris and Kurth, Thorsten and Hundt, Christian and Pathak, Jaideep and Baust, Maximilian and Kashinath, Karthik and Anandkumar, Anima},
	month = jul,
	year = {2023},
	pages = {2806--2823},
}

@inproceedings{ronneberger_u-net_2015,
	address = {Cham},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	isbn = {9783319245744},
	shorttitle = {U-{Net}},
	doi = {10.1007/978-3-319-24574-4_28},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	year = {2015},
	keywords = {Convolutional Layer, Data Augmentation, Deep Network, Ground Truth Segmentation, Training Image},
	pages = {234--241},
}

@inproceedings{chen_unsupervised_2014,
	title = {Unsupervised {Deep} {Haar} {Scattering} on {Graphs}},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/hash/34fde01345258939e718af181fc0f996-Abstract.html},
	abstract = {The classification of high-dimensional data defined on graphs is particularly difficult when the graph geometry is unknown. We introduce a Haar scattering transform on graphs, which computes invariant signal descriptors. It is implemented with a deep cascade of additions, subtractions and absolute values, which iteratively compute orthogonal Haar wavelet transforms. Multiscale neighborhoods of unknown graphs are estimated by minimizing an average total variation, with a pair matching algorithm of polynomial complexity. Supervised classification with dimension reduction is tested on data bases of scrambled images, and for signals sampled on unknown irregular grids on a sphere.},
	urldate = {2025-05-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Xu and Cheng, Xiuyuan and Mallat, Stéphane},
	year = {2014},
}

@inproceedings{janati_debiased_2020,
	title = {Debiased {Sinkhorn} barycenters},
	url = {https://proceedings.mlr.press/v119/janati20a.html},
	abstract = {Entropy regularization in optimal transport (OT) has been the driver of many recent interests for Wasserstein metrics and barycenters in machine learning. It allows to keep the appealing geometrical properties of the unregularized Wasserstein distance while having a significantly lower complexity thanks to Sinkhorn’s algorithm. However, entropy brings some inherent smoothing bias, resulting for example in blurred barycenters. This side effect has prompted an increasing temptation in the community to settle for a slower algorithm such as log-domain stabilized Sinkhorn which breaks the parallel structure that can be leveraged on GPUs, or even go back to unregularized OT. Here we show how this bias is tightly linked to the reference measure that defines the entropy regularizer and propose debiased Sinkhorn barycenters that preserve the best of worlds: fast Sinkhorn-like iterations without entropy smoothing. Theoretically, we prove that this debiasing is perfect for Gaussian distributions with equal variance. Empirically, we illustrate the reduced blurring and the computational advantage.},
	language = {en},
	urldate = {2025-05-21},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Janati, Hicham and Cuturi, Marco and Gramfort, Alexandre},
	month = nov,
	year = {2020},
	pages = {4692--4701},
}

@misc{cuturi_fast_2014,
	title = {Fast {Computation} of {Wasserstein} {Barycenters}},
	url = {http://arxiv.org/abs/1310.4375},
	doi = {10.48550/arXiv.1310.4375},
	abstract = {We present new algorithms to compute the mean of a set of empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter, is the measure that minimizes the sum of its Wasserstein distances to each element in that set. We propose two original algorithms to compute Wasserstein barycenters that build upon the subgradient method. A direct implementation of these algorithms is, however, too costly because it would require the repeated resolution of large primal and dual optimal transport problems to compute subgradients. Extending the work of Cuturi (2013), we propose to smooth the Wasserstein distance used in the definition of Wasserstein barycenters with an entropic regularizer and recover in doing so a strictly convex objective whose gradients can be computed for a considerably cheaper computational cost using matrix scaling algorithms. We use these algorithms to visualize a large family of images and to solve a constrained clustering problem.},
	urldate = {2025-05-21},
	publisher = {arXiv},
	author = {Cuturi, Marco and Doucet, Arnaud},
	month = jun,
	year = {2014},
	note = {arXiv:1310.4375 [stat]},
	keywords = {Statistics - Machine Learning},
}

@misc{chen_flow_2024,
	title = {Flow {Matching} on {General} {Geometries}},
	url = {http://arxiv.org/abs/2302.03660},
	doi = {10.48550/arXiv.2302.03660},
	abstract = {We propose Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. Riemannian Flow Matching bypasses these limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not require divergence computation, and computes its target vector field in closed-form. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. To extend to general geometries, we rely on the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on many real-world non-Euclidean datasets, and we demonstrate tractable training on general geometries, including triangular meshes with highly non-trivial curvature and boundaries.},
	urldate = {2025-05-20},
	publisher = {arXiv},
	author = {Chen, Ricky T. Q. and Lipman, Yaron},
	month = feb,
	year = {2024},
	note = {arXiv:2302.03660 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{lindgren_spde_2022,
	series = {Special {Issue}: {The} {Impact} of {Spatial} {Statistics}},
	title = {The {SPDE} approach for {Gaussian} and non-{Gaussian} fields: 10 years and still running},
	volume = {50},
	issn = {2211-6753},
	shorttitle = {The {SPDE} approach for {Gaussian} and non-{Gaussian} fields},
	url = {https://www.sciencedirect.com/science/article/pii/S2211675322000057},
	doi = {10.1016/j.spasta.2022.100599},
	abstract = {Gaussian processes and random fields have a long history, covering multiple approaches to representing spatial and spatio-temporal dependence structures, such as covariance functions, spectral representations, reproducing kernel Hilbert spaces, and graph based models. This article describes how the stochastic partial differential equation approach to generalising Matérn covariance models via Hilbert space projections connects with several of these approaches, with each connection being useful in different situations. In addition to an overview of the main ideas, some important extensions, theory, applications, and other recent developments are discussed. The methods include both Markovian and non-Markovian models, non-Gaussian random fields, non-stationary fields and space–time fields on arbitrary manifolds, and practical computational considerations.},
	urldate = {2025-05-20},
	journal = {Spatial Statistics},
	author = {Lindgren, Finn and Bolin, David and Rue, Håvard},
	month = aug,
	year = {2022},
	keywords = {Computational efficiency, Gaussian Markov random fields, INLA, Matérn covariances, Random fields, Stochastic partial differential equations},
	pages = {100599},
}

@misc{chen_flow_2024-1,
	title = {Flow {Matching} on {General} {Geometries}},
	url = {http://arxiv.org/abs/2302.03660},
	doi = {10.48550/arXiv.2302.03660},
	abstract = {We propose Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. Riemannian Flow Matching bypasses these limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not require divergence computation, and computes its target vector field in closed-form. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. To extend to general geometries, we rely on the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on many real-world non-Euclidean datasets, and we demonstrate tractable training on general geometries, including triangular meshes with highly non-trivial curvature and boundaries.},
	urldate = {2025-05-20},
	publisher = {arXiv},
	author = {Chen, Ricky T. Q. and Lipman, Yaron},
	month = feb,
	year = {2024},
	note = {arXiv:2302.03660 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{weng_bayesian_2011,
	title = {A {Bayesian} {Approximation} {Method} for {Online} {Ranking}},
	volume = {12},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v12/weng11a.html},
	abstract = {This paper describes a Bayesian approximation method to obtain online ranking algorithms for games with multiple teams and multiple players.  Recently for Internet games large online ranking systems are much needed.  We consider game models in which a k-team game is treated as several two-team games.  By approximating the expectation of teams' (or players') performances, we derive simple analytic update rules.  These update rules, without numerical integrations, are very easy to interpret and implement.  Experiments on game data show that the accuracy of our approach is competitive with state of the art systems such as TrueSkill, but the running time as well as the code is much shorter.},
	number = {9},
	urldate = {2025-05-16},
	journal = {Journal of Machine Learning Research},
	author = {Weng, Ruby C. and Lin, Chih-Jen},
	year = {2011},
	pages = {267--300},
}

@misc{chaouach_structured_2023,
	title = {Structured ambiguity sets for distributionally robust optimization},
	url = {http://arxiv.org/abs/2310.20657},
	doi = {10.48550/arXiv.2310.20657},
	abstract = {Distributionally robust optimization (DRO) incorporates robustness against uncertainty in the specification of probabilistic models. This paper focuses on mitigating the curse of dimensionality in data-driven DRO problems with optimal transport ambiguity sets. By exploiting independence across lower-dimensional components of the uncertainty, we construct structured ambiguity sets that exhibit a faster shrinkage as the number of collected samples increases. This narrows down the plausible models of the data-generating distribution and mitigates the conservativeness that the decisions of DRO problems over such ambiguity sets may face. We establish statistical guarantees for these structured ambiguity sets and provide dual reformulations of their associated DRO problems for a wide range of objective functions. The benefits of the approach are demonstrated in a numerical example.},
	urldate = {2025-05-15},
	publisher = {arXiv},
	author = {Chaouach, Lotfi M. and Oomen, Tom and Boskos, Dimitris},
	month = oct,
	year = {2023},
	note = {arXiv:2310.20657 [math]},
	keywords = {Mathematics - Optimization and Control},
}

@article{song_shapley_2016,
	title = {Shapley {Effects} for {Global} {Sensitivity} {Analysis}: {Theory} and {Computation}},
	volume = {4},
	shorttitle = {Shapley {Effects} for {Global} {Sensitivity} {Analysis}},
	url = {https://epubs.siam.org/doi/10.1137/15M1048070},
	doi = {10.1137/15M1048070},
	abstract = {This paper introduces generalized Sobol' indices, compares strategies for their estimation, and makes a systematic search for efficient estimators. Of particular interest are contrasts, sums of squares, and indices of bilinear form which allow a reduced number of function evaluations compared to alternatives. The bilinear framework includes some efficient estimators from Saltelli [Comput. Phys. Comm., 145 (2002), pp. 280--297] and Mauntz [Global Sensitivity Analysis of General Nonlinear Systems, Master's thesis, Imperial College, London, 2002] as well as some new estimators for specific variance components and mean dimensions. This paper also provides a bias corrected version of the estimator of Janon et al. [Asymptotic Normality and Efficiency of Two Sobol' Index Estimators, technical report, INRIA, Rocquencourt, France] and extends the bias correction to generalized Sobol' indices. Some numerical comparisons are given.},
	number = {1},
	urldate = {2025-05-15},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Song, Eunhye and Nelson, Barry L. and Staum, Jeremy},
	month = jan,
	year = {2016},
	pages = {1060--1083},
}

@article{lenzi_neural_2023,
	title = {Neural networks for parameter estimation in intractable models},
	volume = {185},
	issn = {0167-9473},
	url = {https://www.sciencedirect.com/science/article/pii/S0167947323000737},
	doi = {10.1016/j.csda.2023.107762},
	abstract = {The goal is to use deep learning models to estimate parameters in statistical models when standard likelihood estimation methods are computationally infeasible. For instance, inference for max-stable processes is exceptionally challenging even with small datasets, but simulation is straightforward. Data from model simulations are used to train deep neural networks and learn statistical parameters from max-stable models. The proposed neural network-based method provides a competitive alternative to current approaches, as demonstrated by considerable accuracy and computational time improvements. It serves as a proof of concept for deep learning in statistical parameter estimation and can be extended to other estimation problems.},
	urldate = {2025-05-10},
	journal = {Computational Statistics \& Data Analysis},
	author = {Lenzi, Amanda and Bessac, Julie and Rudi, Johann and Stein, Michael L.},
	month = sep,
	year = {2023},
	keywords = {Deep neural networks, Intractable likelihood, Max-stable distributions, Parameter estimation},
	pages = {107762},
}

@article{lindgren_explicit_2011,
	title = {An explicit link between {Gaussian} fields and {Gaussian} {Markov} random fields: the stochastic partial differential equation approach},
	volume = {73},
	copyright = {© 2011 Royal Statistical Society},
	issn = {1467-9868},
	shorttitle = {An explicit link between {Gaussian} fields and {Gaussian} {Markov} random fields},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2011.00777.x},
	doi = {10.1111/j.1467-9868.2011.00777.x},
	abstract = {Summary. Continuously indexed Gaussian fields (GFs) are the most important ingredient in spatial statistical modelling and geostatistics. The specification through the covariance function gives an intuitive interpretation of the field properties. On the computational side, GFs are hampered with the big n problem, since the cost of factorizing dense matrices is cubic in the dimension. Although computational power today is at an all time high, this fact seems still to be a computational bottleneck in many applications. Along with GFs, there is the class of Gaussian Markov random fields (GMRFs) which are discretely indexed. The Markov property makes the precision matrix involved sparse, which enables the use of numerical algorithms for sparse matrices, that for fields in only use the square root of the time required by general algorithms. The specification of a GMRF is through its full conditional distributions but its marginal properties are not transparent in such a parameterization. We show that, using an approximate stochastic weak solution to (linear) stochastic partial differential equations, we can, for some GFs in the Matérn class, provide an explicit link, for any triangulation of , between GFs and GMRFs, formulated as a basis function representation. The consequence is that we can take the best from the two worlds and do the modelling by using GFs but do the computations by using GMRFs. Perhaps more importantly, our approach generalizes to other covariance functions generated by SPDEs, including oscillating and non-stationary GFs, as well as GFs on manifolds. We illustrate our approach by analysing global temperature data with a non-stationary model defined on a sphere.},
	language = {en},
	number = {4},
	urldate = {2025-05-09},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Lindgren, Finn and Rue, Håvard and Lindström, Johan},
	year = {2011},
	keywords = {Approximate Bayesian inference, Covariance functions, Gaussian Markov random fields, Gaussian fields, Latent Gaussian models, Sparse matrices, Stochastic partial differential equations},
	pages = {423--498},
}

@misc{huser_modeling_2024,
	title = {Modeling of spatial extremes in environmental data science: {Time} to move away from max-stable processes},
	shorttitle = {Modeling of spatial extremes in environmental data science},
	url = {http://arxiv.org/abs/2401.17430},
	doi = {10.48550/arXiv.2401.17430},
	abstract = {Environmental data science for spatial extremes has traditionally relied heavily on max-stable processes. Even though the popularity of these models has perhaps peaked with statisticians, they are still perceived and considered as the `state-of-the-art' in many applied fields. However, while the asymptotic theory supporting the use of max-stable processes is mathematically rigorous and comprehensive, we think that it has also been overused, if not misused, in environmental applications, to the detriment of more purposeful and meticulously validated models. In this paper, we review the main limitations of max-stable process models, and strongly argue against their systematic use in environmental studies. Alternative solutions based on more flexible frameworks using the exceedances of variables above appropriately chosen high thresholds are discussed, and an outlook on future research is given, highlighting recommendations moving forward and the opportunities offered by hybridizing machine learning with extreme-value statistics.},
	urldate = {2025-05-07},
	publisher = {arXiv},
	author = {Huser, Raphaël and Opitz, Thomas and Wadsworth, Jennifer},
	month = jan,
	year = {2024},
	note = {arXiv:2401.17430 [stat]
version: 1},
	keywords = {Statistics - Methodology},
}

@misc{noauthor_220908546_nodate,
	title = {[2209.08546] {ActiveNeRF}: {Learning} where to {See} with {Uncertainty} {Estimation}},
	url = {https://arxiv.org/abs/2209.08546},
	urldate = {2025-05-06},
}

@article{burt_convergence_2020,
	title = {Convergence of {Sparse} {Variational} {Inference} in {Gaussian} {Processes} {Regression}},
	volume = {21},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v21/19-1015.html},
	abstract = {Gaussian processes are distributions over functions that are versatile and mathematically convenient priors in Bayesian modelling. However, their use is often impeded for data with large numbers of observations, NNN, due to the cubic (in NNN) cost of matrix operations used in exact inference. Many solutions have been proposed that rely on M≪NM≪NM {\textbackslash}ll N inducing variables to form an approximation at a cost of (NM2)O(NM2){\textbackslash}mathcal\{O\}{\textbackslash}left(NM{\textasciicircum}2{\textbackslash}right). While the computational cost appears linear in NNN, the true complexity depends on how MMM must scale with NNN to ensure a certain quality of the approximation. In this work, we investigate upper and lower bounds on how MMM needs to grow with NNN to ensure high quality approximations. We show that we can make the KL-divergence between the approximate model and the exact posterior arbitrarily small for a Gaussian-noise regression model with M≪NM≪NM {\textbackslash}ll N. Specifically, for the popular squared exponential kernel and DDD-dimensional Gaussian distributed covariates, M=((logN)D)M=O((log⁡N)D)M = {\textbackslash}mathcal\{O\}(({\textbackslash}log N){\textasciicircum}D) suffice and a method with an overall computational cost of (N(logN)2D(loglogN)2)O(N(log⁡N)2D(log⁡log⁡N)2){\textbackslash}mathcal\{O\}{\textbackslash}left(N({\textbackslash}log N){\textasciicircum}\{2D\}({\textbackslash}log {\textbackslash}log N){\textasciicircum}2{\textbackslash}right) can be used to perform inference.},
	number = {131},
	urldate = {2025-04-30},
	journal = {Journal of Machine Learning Research},
	author = {Burt, David R. and Rasmussen, Carl Edward and Wilk, Mark van der},
	year = {2020},
	pages = {1--63},
}

@misc{noauthor_231011527_nodate,
	title = {[2310.11527] {Thin} and {Deep} {Gaussian} {Processes}},
	url = {https://arxiv.org/abs/2310.11527},
	urldate = {2025-04-19},
}

@article{newell_nearly_2007,
	title = {A nearly universal solar wind-magnetosphere coupling function inferred from 10 magnetospheric state variables},
	volume = {112},
	copyright = {Copyright 2007 by the American Geophysical Union.},
	issn = {2156-2202},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2006JA012015},
	doi = {10.1029/2006JA012015},
	abstract = {We investigated whether one or a few coupling functions can represent best the interaction between the solar wind and the magnetosphere over a wide variety of magnetospheric activity. Ten variables which characterize the state of the magnetosphere were studied. Five indices from ground-based magnetometers were selected, namely Dst, Kp, AE, AU, and AL, and five from other sources, namely auroral power (Polar UVI), cusp latitude (sin(Λc)), b2i (both DMSP), geosynchronous magnetic inclination angle (GOES), and polar cap size (SuperDARN). These indices were correlated with more than 20 candidate solar wind coupling functions. One function, representing the rate magnetic flux is opened at the magnetopause, correlated best with 9 out of 10 indices of magnetospheric activity. This is dΦMP/dt = v4/3BT2/3sin8/3(θc/2), calculated from (rate IMF field lines approach the magnetopause, ∼v)(\% of IMF lines which merge, sin8/3(θc/2))(interplanetary field magnitude, BT)(merging line length, ∼(BMP/BT)1/3). The merging line length is based on flux matching between the solar wind and a dipole field and agrees with a superposed IMF on a vacuum dipole. The IMF clock angle dependence matches the merging rate reported (albeit with limited statistics) at high altitude. The nonlinearities of the magnetospheric response to BT and v are evident when the mean values of indices are plotted, in scatterplots, and in the superior correlations from dΦMP/dt. Our results show that a wide variety of magnetospheric phenomena can be predicted with reasonable accuracy (r {\textgreater} 0.80 in several cases) ab initio, that is without the time history of the target index, by a single function, estimating the dayside merging rate. Across all state variables studied (including AL, which is hard to predict, and polar cap size, which is hard to measure), dΦMP/dt accounts for about 57.2\% of the variance, compared to 50.9\% for EKL and 48.8\% for vBs. All data sets included at least thousands of points over many years, up to two solar cycles, with just two parameter fits, and the correlations are thus robust. The sole index which does not correlate best with dΦMP/dt is Dst, which correlates best (r = 0.87) with p1/2dΦMP/dt. If dΦMP/dt were credited with this success, its average score would be even higher.},
	language = {en},
	number = {A1},
	urldate = {2025-04-16},
	journal = {Journal of Geophysical Research: Space Physics},
	author = {Newell, P. T. and Sotirelis, T. and Liou, K. and Meng, C.-I. and Rich, F. J.},
	year = {2007},
	keywords = {coupling, function, prediction},
}

@article{narvekar_curriculum_2020,
	title = {Curriculum {Learning} for {Reinforcement} {Learning} {Domains}: {A} {Framework} and {Survey}},
	volume = {21},
	issn = {1533-7928},
	shorttitle = {Curriculum {Learning} for {Reinforcement} {Learning} {Domains}},
	url = {http://jmlr.org/papers/v21/20-212.html},
	abstract = {Reinforcement learning (RL) is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning (CL) in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research.},
	number = {181},
	urldate = {2025-04-08},
	journal = {Journal of Machine Learning Research},
	author = {Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E. and Stone, Peter},
	year = {2020},
	pages = {1--50},
}

@article{rios_compositionally-warped_2019,
	title = {Compositionally-warped {Gaussian} processes},
	volume = {118},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608019301856},
	doi = {10.1016/j.neunet.2019.06.012},
	abstract = {The Gaussian process (GP) is a nonparametric prior distribution over functions indexed by time, space, or other high-dimensional index set. The GP is a flexible model yet its limitation is given by its very nature: it can only model Gaussian marginal distributions. To model non-Gaussian data, a GP can be warped by a nonlinear transformation (or warping) as performed by warped GPs (WGPs) and more computationally-demanding alternatives such as Bayesian WGPs and deep GPs. However, the WGP requires a numerical approximation of the inverse warping for prediction, which increases the computational complexity in practice. To sidestep this issue, we construct a novel class of warpings consisting of compositions of multiple elementary functions, for which the inverse is known explicitly. We then propose the compositionally-warped GP (CWGP), a non-Gaussian generative model whose expressiveness follows from its deep compositional architecture, and its computational efficiency is guaranteed by the analytical inverse warping. Experimental validation using synthetic and real-world datasets confirms that the proposed CWGP is robust to the choice of warpings and provides more accurate point predictions, better trained models and shorter computation times than WGP.},
	urldate = {2025-04-07},
	journal = {Neural Networks},
	author = {Rios, Gonzalo and Tobar, Felipe},
	month = oct,
	year = {2019},
	keywords = {Function compositions, Gaussian process, Neural networks, Non-Gaussian models, Warped Gaussian processes},
	pages = {235--246},
}

@inproceedings{sharrock_coin_2023,
	title = {Coin {Sampling}: {Gradient}-{Based} {Bayesian} {Inference} without {Learning} {Rates}},
	shorttitle = {Coin {Sampling}},
	url = {https://proceedings.mlr.press/v202/sharrock23a.html},
	abstract = {In recent years, particle-based variational inference (ParVI) methods such as Stein variational gradient descent (SVGD) have grown in popularity as scalable methods for Bayesian inference. Unfortunately, the properties of such methods invariably depend on hyperparameters such as the learning rate, which must be carefully tuned by the practitioner in order to ensure convergence to the target measure at a suitable rate. In this paper, we introduce a suite of new particle-based methods for scalable Bayesian inference based on coin betting, which are entirely learning-rate free. We illustrate the performance of our approach on a range of numerical examples, including several high-dimensional models and datasets, demonstrating comparable performance to other ParVI algorithms with no need to tune a learning rate.},
	language = {en},
	urldate = {2025-04-03},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sharrock, Louis and Nemeth, Christopher},
	month = jul,
	year = {2023},
	pages = {30850--30882},
}

@inproceedings{de_bortoli_diffusion_2021,
	title = {Diffusion {Schrödinger} {Bridge} with {Applications} to {Score}-{Based} {Generative} {Modeling}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/940392f5f32a7ade1cc201767cf83e31-Abstract.html},
	abstract = {Progressively applying Gaussian noise transforms complex data distributions to approximately Gaussian. Reversing this dynamic defines a generative model. When the forward noising process is given by a Stochastic Differential Equation (SDE), Song et al (2021) demonstrate how the time inhomogeneous drift of the associated reverse-time SDE may be estimated using score-matching. A limitation of this approach is that the forward-time SDE must be run for a sufficiently long time for the final distribution to be approximately Gaussian. In contrast, solving the Schrödinger Bridge (SB) problem, i.e. an entropy-regularized optimal transport problem on path spaces, yields diffusions which generate samples from the data distribution in finite time. We present Diffusion SB (DSB), an original approximation of the Iterative Proportional Fitting (IPF) procedure to solve the SB problem, and provide theoretical analysis along with generative modeling experiments. The first DSB iteration recovers the methodology proposed by Song et al. (2021), with the flexibility of using shorter time intervals, as subsequent DSB iterations reduce the discrepancy between the final-time marginal of the forward (resp. backward) SDE with respect to the prior (resp. data) distribution. Beyond generative modeling, DSB offers a widely applicable computational optimal transport tool as the continuous state-space analogue of the popular Sinkhorn algorithm (Cuturi, 2013).},
	urldate = {2025-03-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {De Bortoli, Valentin and Thornton, James and Heng, Jeremy and Doucet, Arnaud},
	year = {2021},
	pages = {17695--17709},
}

@article{heng_diffusion_2024,
	title = {Diffusion {Schrödinger} {Bridges} for {Bayesian} {Computation}},
	volume = {39},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/journals/statistical-science/volume-39/issue-1/Diffusion-Schr%c3%b6dinger-Bridges-for-Bayesian-Computation/10.1214/23-STS908.full},
	doi = {10.1214/23-STS908},
	abstract = {Denoising diffusion models are a novel class of generative models that have recently become extremely popular in machine learning. In this paper, we describe how such ideas can also be used to sample from posterior distributions and, more generally, any target distribution whose density is known up to a normalizing constant. The key idea is to consider a forward “noising” diffusion initialized at the target distribution, which “transports” this latter to a normal distribution for long diffusion times. The time reversal of this process, the “denoising” diffusion, thus “transports” the normal distribution to the target distribution and can be approximated so as to sample from the target. To accelerate simulation, we show how one can introduce and approximate a Schrödinger bridge between these two distributions, that is, a diffusion which transports the normal to the target in finite time.},
	number = {1},
	urldate = {2025-03-24},
	journal = {Statistical Science},
	author = {Heng, Jeremy and Bortoli, Valentin De and Doucet, Arnaud},
	month = feb,
	year = {2024},
	keywords = {Optimal transport, Schrödinger bridge, Stochastic differential equation, Time reversal, score matching},
	pages = {90--99},
}

@inproceedings{fatras_learning_2020,
	title = {Learning with minibatch {Wasserstein} : asymptotic and gradient properties},
	shorttitle = {Learning with minibatch {Wasserstein}},
	url = {https://proceedings.mlr.press/v108/fatras20a.html},
	abstract = {Optimal transport distances are powerful tools to compare probability distributions and have found many applications in machine learning. Yet their algorithmic complexity prevents their direct use on large scale datasets. To overcome this challenge, practitioners compute these distances on minibatches i.e., they average the outcome of several smaller optimal transport problems. We propose in this paper an analysis of this practice, which effects are not well understood so far. We notably argue that it is equivalent to an implicit regularization of the original problem, with appealing properties such as unbiased estimators, gradients and a concentration bound around the expectation, but also with defects such as loss of distance property. Along with this theoretical analysis, we also conduct empirical experiments on gradient flows, GANs or color transfer that highlight the practical interest of this strategy.},
	language = {en},
	urldate = {2025-03-24},
	booktitle = {Proceedings of the {Twenty} {Third} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Fatras, Kilian and Zine, Younes and Flamary, Rémi and Gribonval, Remi and Courty, Nicolas},
	month = jun,
	year = {2020},
	pages = {2131--2141},
}

@misc{rebonato_most_2011,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {The {Most} {General} {Methodology} to {Create} a {Valid} {Correlation} {Matrix} for {Risk} {Management} and {Option} {Pricing} {Purposes}},
	url = {https://papers.ssrn.com/abstract=1969689},
	doi = {10.2139/ssrn.1969689},
	abstract = {We have presented two simple methods to produce a feasible (i.e. real, symmetric, and positivesemidefinite) correlation matrix when the econometric one is either noisy, unavailable, or inappropriate. The first method is to the knowledge of the authors more general than any of the approaches which have been proposed in the literature, and computationally faster. It can actually produce the optimal feasible solution in a sense specified by the user. The second method is, in principle, not as general, but we show that i) it is extremely fast and ii) it produces results very close to those obtained using the general procedure. It can therefore be used in its own right, or as a starting point for the general optimisation procedure, thereby making the latter even faster.},
	language = {en},
	urldate = {2025-03-20},
	publisher = {Social Science Research Network},
	author = {Rebonato, Riccardo and Jaeckel, Peter},
	month = dec,
	year = {2011},
	keywords = {Covariance, correlation, robustness},
}

@book{chomsky_aspects_1965,
	edition = {50},
	title = {Aspects of the {Theory} of {Syntax}},
	isbn = {9780262527408},
	url = {https://www.jstor.org/stable/j.ctt17kk81z},
	abstract = {Noam Chomsky's \textit{Aspects of the Theory of Syntax} , published in 1965, was a landmark work in generative grammar that introduced certain technical innovations still drawn upon in contemporary work. The fiftieth anniversary edition of this influential book includes a new preface by the author that identifies proposals that seem to be of lasting significance, reviews changes and improvements in the formulation and implementation of basic ideas, and addresses some of the controversies that arose over the general framework.Beginning in the mid-fifties and emanating largely from MIT, linguists developed an approach to linguistic theory and to the study of the structure of particular languages that diverged in many respects from conventional modern linguistics. Although the new approach was connected to the traditional study of languages, it differed enough in its specific conclusions about the structure of language to warrant a name, "generative grammar." Various deficiencies were discovered in the first attempts to formulate a theory of transformational generative grammar and in the descriptive analysis of particular languages that motivated these formulations. At the same time, it became apparent that these formulations can be extended and deepened. In this book, Chomsky reviews these developments and proposes a reformulation of the theory of transformational generative grammar that takes them into account. The emphasis in this study is syntax; semantic and phonological aspects of the language structure are discussed only insofar as they bear on syntactic theory.},
	urldate = {2025-03-20},
	publisher = {The MIT Press},
	author = {Chomsky, Noam},
	year = {1965},
}

@incollection{spino_mentalist_2018,
	title = {Mentalist {Learning} {Theory}},
	copyright = {Copyright © 2018 John Wiley \& Sons, Inc.},
	isbn = {9781118784235},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118784235.eelt0156},
	abstract = {The mentalist learning theory emphasizes the role of the mind in language acquisition by arguing that humans are born with an innate and biological capacity to learn languages. This theory was spearheaded by Noam Chomsky, and arose in response to B. F. Skinner's radical behaviorism. This summary traces the origins of the mentalist learning theory back to psychology, and explains how it is different from behaviorism. Although the mentalist learning theory was not designed to have pedagogical implications for second language learning, this summary also includes suggestions for language teachers that are compatible with the theory.},
	language = {en},
	urldate = {2025-03-19},
	booktitle = {The {TESOL} {Encyclopedia} of {English} {Language} {Teaching}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Spino, Le Anne and Loewen, Shawn},
	year = {2018},
	doi = {10.1002/9781118784235.eelt0156},
	keywords = {behaviorism, cognitive science, comprehensible input, first language acquisition, innatism, mentalist learning theory, second language acquisition, universal grammar},
	pages = {1--6},
}

@inproceedings{kurth_fourcastnet_2023,
	address = {New York, NY, USA},
	series = {{PASC} '23},
	title = {{FourCastNet}: {Accelerating} {Global} {High}-{Resolution} {Weather} {Forecasting} {Using} {Adaptive} {Fourier} {Neural} {Operators}},
	isbn = {9798400701900},
	shorttitle = {{FourCastNet}},
	url = {https://dl.acm.org/doi/10.1145/3592979.3593412},
	doi = {10.1145/3592979.3593412},
	abstract = {Extreme weather amplified by climate change is causing increasingly devastating impacts across the globe. The current use of physics-based numerical weather prediction (NWP) limits accuracy and resolution due to high computational cost and strict time-to-solution limits.We report that a data-driven deep learning Earth system emulator, FourCastNet, can predict global weather and generate medium-range forecasts five orders-of-magnitude faster than NWP while approaching state-of-the-art accuracy. FourCastNet is optimized and scales efficiently on three supercomputing systems: Selene, Perlmutter, and JUWELS Booster up to 3,808 NVIDIA A100 GPUs, attaining 140.8 petaFLOPS in mixed precision (11.9\% of peak at that scale). The time-to-solution for training FourCastNet measured on JUWELS Booster on 3,072 GPUs is 67.4 minutes, resulting in an 80,000 times faster time-to-solution relative to state-of-the-art NWP, in inference.FourCastNet produces accurate instantaneous weather predictions for a week in advance and enables enormous ensembles that could be used to improve predictions of rare weather extremes.},
	urldate = {2025-03-19},
	booktitle = {Proceedings of the {Platform} for {Advanced} {Scientific} {Computing} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Kurth, Thorsten and Subramanian, Shashank and Harrington, Peter and Pathak, Jaideep and Mardani, Morteza and Hall, David and Miele, Andrea and Kashinath, Karthik and Anandkumar, Anima},
	month = jun,
	year = {2023},
	pages = {1--11},
}

@misc{lipman_flow_2023,
	title = {Flow {Matching} for {Generative} {Modeling}},
	url = {http://arxiv.org/abs/2210.02747},
	doi = {10.48550/arXiv.2210.02747},
	abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.},
	urldate = {2025-03-18},
	publisher = {arXiv},
	author = {Lipman, Yaron and Chen, Ricky T. Q. and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
	month = feb,
	year = {2023},
	note = {arXiv:2210.02747 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{tong_improving_2023,
	title = {Improving and generalizing flow-based generative models with minibatch optimal transport},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=CD9Snc73AW},
	abstract = {Continuous normalizing flows (CNFs) are an attractive generative modeling technique, but they have been held back by limitations in their simulation-based maximum likelihood training. We introduce the generalized conditional flow matching (CFM) technique, a family of simulation-free training objectives for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. In contrast to both diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. A variant of our objective is optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference, as evaluated in our experiments. Furthermore, we show that when the true OT plan is available, our OT-CFM method approximates dynamic OT. Training CNFs with CFM improves results on a variety of conditional and unconditional generation tasks, such as inferring single cell dynamics, unsupervised image translation, and Schrödinger bridge inference. The Python code is available at https://github.com/atong01/conditional-flow-matching.},
	language = {en},
	urldate = {2025-03-18},
	journal = {Transactions on Machine Learning Research},
	author = {Tong, Alexander and Fatras, Kilian and Malkin, Nikolay and Huguet, Guillaume and Zhang, Yanlei and Rector-Brooks, Jarrid and Wolf, Guy and Bengio, Yoshua},
	month = nov,
	year = {2023},
}

@misc{tong_improving_2024,
	title = {Improving and generalizing flow-based generative models with minibatch optimal transport},
	url = {http://arxiv.org/abs/2302.00482},
	doi = {10.48550/arXiv.2302.00482},
	abstract = {Continuous normalizing flows (CNFs) are an attractive generative modeling technique, but they have been held back by limitations in their simulation-based maximum likelihood training. We introduce the generalized conditional flow matching (CFM) technique, a family of simulation-free training objectives for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. In contrast to both diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. A variant of our objective is optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference, as evaluated in our experiments. Furthermore, we show that when the true OT plan is available, our OT-CFM method approximates dynamic OT. Training CNFs with CFM improves results on a variety of conditional and unconditional generation tasks, such as inferring single cell dynamics, unsupervised image translation, and Schr{\textbackslash}"odinger bridge inference.},
	urldate = {2025-03-18},
	publisher = {arXiv},
	author = {Tong, Alexander and Fatras, Kilian and Malkin, Nikolay and Huguet, Guillaume and Zhang, Yanlei and Rector-Brooks, Jarrid and Wolf, Guy and Bengio, Yoshua},
	month = mar,
	year = {2024},
	note = {arXiv:2302.00482 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{liang_quantifying_2023,
	title = {Quantifying \& {Modeling} {Multimodal} {Interactions}: {An} {Information} {Decomposition} {Framework}},
	shorttitle = {Quantifying \& {Modeling} {Multimodal} {Interactions}},
	url = {http://arxiv.org/abs/2302.12247},
	doi = {10.48550/arXiv.2302.12247},
	abstract = {The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations are compared with human annotations. Finally, we demonstrate their usefulness in (1) quantifying interactions within multimodal datasets, (2) quantifying interactions captured by multimodal models, (3) principled approaches for model selection, and (4) three real-world case studies engaging with domain experts in pathology, mood prediction, and robotic perception where our framework helps to recommend strong multimodal models for each application.},
	urldate = {2025-03-18},
	publisher = {arXiv},
	author = {Liang, Paul Pu and Cheng, Yun and Fan, Xiang and Ling, Chun Kai and Nie, Suzanne and Chen, Richard and Deng, Zihao and Allen, Nicholas and Auerbach, Randy and Mahmood, Faisal and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
	month = dec,
	year = {2023},
	note = {arXiv:2302.12247 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Information Theory},
}

@inproceedings{foster_deep_2021,
	title = {Deep {Adaptive} {Design}: {Amortizing} {Sequential} {Bayesian} {Experimental} {Design}},
	shorttitle = {Deep {Adaptive} {Design}},
	url = {https://proceedings.mlr.press/v139/foster21a.html},
	abstract = {We introduce Deep Adaptive Design (DAD), a method for amortizing the cost of adaptive Bayesian experimental design that allows experiments to be run in real-time. Traditional sequential Bayesian optimal experimental design approaches require substantial computation at each stage of the experiment. This makes them unsuitable for most real-world applications, where decisions must typically be made quickly. DAD addresses this restriction by learning an amortized design network upfront and then using this to rapidly run (multiple) adaptive experiments at deployment time. This network represents a design policy which takes as input the data from previous steps, and outputs the next design using a single forward pass; these design decisions can be made in milliseconds during the live experiment. To train the network, we introduce contrastive information bounds that are suitable objectives for the sequential setting, and propose a customized network architecture that exploits key symmetries. We demonstrate that DAD successfully amortizes the process of experimental design, outperforming alternative strategies on a number of problems.},
	language = {en},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Foster, Adam and Ivanova, Desi R. and Malik, Ilyas and Rainforth, Tom},
	month = jul,
	year = {2021},
	pages = {3384--3395},
}

@inproceedings{gelberg_variational_2024,
	title = {Variational {Inference} {Failures} {Under} {Model} {Symmetries}: {Permutation} {Invariant} {Posteriors} for {Bayesian} {Neural} {Networks}},
	shorttitle = {Variational {Inference} {Failures} {Under} {Model} {Symmetries}},
	url = {https://proceedings.mlr.press/v251/gelberg24a.html},
	abstract = {Weight space symmetries in neural network architectures, such as permutation symmetries in MLPs, give rise to Bayesian neural network (BNN) posteriors with many equivalent modes. This multimodality poses a challenge for variational inference (VI) techniques, which typically rely on approximating the posterior with a unimodal distribution. In this work, we investigate the impact of weight space permutation symmetries on VI. We demonstrate, both theoretically and empirically, that these symmetries lead to biases in the approximate posterior, which degrade predictive performance and posterior fit if not explicitly accounted for. To mitigate this behavior, we leverage the symmetric structure of the posterior and devise a symmetrization mechanism for constructing permutation invariant variational posteriors. We show that the symmetrized distribution has a strictly better fit to the true posterior, and that it can be trained using the original ELBO objective with a modified KL regularization term. We demonstrate experimentally that our approach mitigates the aforementioned biases and results in improved predictions and a higher ELBO.},
	language = {en},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {Geometry}-grounded {Representation} {Learning} and {Generative} {Modeling} {Workshop} ({GRaM})},
	publisher = {PMLR},
	author = {Gelberg, Yoav and Ouderaa, Tycho F. A. van der and Wilk, Mark van der and Gal, Yarin},
	month = oct,
	year = {2024},
	pages = {233--248},
}

@article{qian_lift_2020,
	title = {Lift \& {Learn}: {Physics}-informed machine learning for large-scale nonlinear dynamical systems},
	volume = {406},
	issn = {0167-2789},
	shorttitle = {Lift \& {Learn}},
	url = {https://www.sciencedirect.com/science/article/pii/S0167278919307651},
	doi = {10.1016/j.physd.2020.132401},
	abstract = {We present Lift \& Learn, a physics-informed method for learning low-dimensional models for large-scale dynamical systems. The method exploits knowledge of a system’s governing equations to identify a coordinate transformation in which the system dynamics have quadratic structure. This transformation is called a lifting map because it often adds auxiliary variables to the system state. The lifting map is applied to data obtained by evaluating a model for the original nonlinear system. This lifted data is projected onto its leading principal components, and low-dimensional linear and quadratic matrix operators are fit to the lifted reduced data using a least-squares operator inference procedure. Analysis of our method shows that the Lift \& Learn models are able to capture the system physics in the lifted coordinates at least as accurately as traditional intrusive model reduction approaches. This preservation of system physics makes the Lift \& Learn models robust to changes in inputs. Numerical experiments on the FitzHugh–Nagumo neuron activation model and the compressible Euler equations demonstrate the generalizability of our model.},
	urldate = {2025-03-13},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Qian, Elizabeth and Kramer, Boris and Peherstorfer, Benjamin and Willcox, Karen},
	month = may,
	year = {2020},
	keywords = {Data-driven model reduction, Dynamical systems, Lifting map, Partial differential equations, Scientific machine learning},
	pages = {132401},
}

@article{chowdhary_sensitivity_2024,
	title = {Sensitivity {Analysis} of the {Information} {Gain} in {Infinite}-{Dimensional} {Bayesian} {Linear} {Inverse} {Problems}},
	volume = {14},
	issn = {2152-5080},
	url = {http://arxiv.org/abs/2310.16906},
	doi = {10.1615/Int.J.UncertaintyQuantification.2024051416},
	abstract = {We study the sensitivity of infinite-dimensional Bayesian linear inverse problems governed by partial differential equations (PDEs) with respect to modeling uncertainties. In particular, we consider derivative-based sensitivity analysis of the information gain, as measured by the Kullback-Leibler divergence from the posterior to the prior distribution. To facilitate this, we develop a fast and accurate method for computing derivatives of the information gain with respect to auxiliary model parameters. Our approach combines low-rank approximations, adjoint-based eigenvalue sensitivity analysis, and post-optimal sensitivity analysis. The proposed approach also paves way for global sensitivity analysis by computing derivative-based global sensitivity measures. We illustrate different aspects of the proposed approach using an inverse problem governed by a scalar linear elliptic PDE, and an inverse problem governed by the three-dimensional equations of linear elasticity, which is motivated by the inversion of the fault-slip field after an earthquake.},
	number = {6},
	urldate = {2025-03-05},
	journal = {International Journal for Uncertainty Quantification},
	author = {Chowdhary, Abhijit and Tong, Shanyin and Stadler, Georg and Alexanderian, Alen},
	year = {2024},
	note = {arXiv:2310.16906 [math]},
	keywords = {Computer Science - Numerical Analysis, Mathematics - Numerical Analysis},
	pages = {17--35},
}

@misc{song_revisiting_2019,
	title = {Revisiting the {Softmax} {Bellman} {Operator}: {New} {Benefits} and {New} {Perspective}},
	shorttitle = {Revisiting the {Softmax} {Bellman} {Operator}},
	url = {http://arxiv.org/abs/1812.00456},
	doi = {10.48550/arXiv.1812.00456},
	abstract = {The impact of softmax on the value function itself in reinforcement learning (RL) is often viewed as problematic because it leads to sub-optimal value (or Q) functions and interferes with the contraction properties of the Bellman operator. Surprisingly, despite these concerns, and independent of its effect on exploration, the softmax Bellman operator when combined with Deep Q-learning, leads to Q-functions with superior policies in practice, even outperforming its double Q-learning counterpart. To better understand how and why this occurs, we revisit theoretical properties of the softmax Bellman operator, and prove that \$(i)\$ it converges to the standard Bellman operator exponentially fast in the inverse temperature parameter, and \$(ii)\$ the distance of its Q function from the optimal one can be bounded. These alone do not explain its superior performance, so we also show that the softmax operator can reduce the overestimation error, which may give some insight into why a sub-optimal operator leads to better performance in the presence of value function approximation. A comparison among different Bellman operators is then presented, showing the trade-offs when selecting them.},
	urldate = {2025-02-19},
	publisher = {arXiv},
	author = {Song, Zhao and Parr, Ronald E. and Carin, Lawrence},
	month = may,
	year = {2019},
	note = {arXiv:1812.00456 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@phdthesis{edwards_exploration_2016,
	type = {phd},
	title = {Exploration and exploitation in {Bayes} sequential decision problems},
	copyright = {cc\_by\_nc\_nd},
	url = {https://eprints.lancs.ac.uk/id/eprint/84589/},
	abstract = {Bayes sequential decision problems are an extensive problem class with wide application. They involve taking actions in sequence in a system which has characteristics which are unknown or only partially known. These characteristics can be learnt over time as a result of our actions. Therefore we are faced with a trade-off between choosing actions that give desirable short term outcomes (exploitation) and actions that yield useful information about the system which can be used to improve longer term outcomes (exploration). Gittins indices provide an optimal method for a small but important subclass of these problems. Unfortunately the optimality of index methods does not hold generally and Gittins indices can be impractical to calculate for many problems. This has motivated the search for easy-to-calculate heuristics with general application. One such non-index method is the knowledge gradient heuristic. A thorough investigation of the method is made which identifies crucial weaknesses. Index and non-index variants are developed which avoid these weaknesses. The problem of choosing multiple website elements to present to user is an important problem relevant to many major web-based businesses. A Bayesian multi-armed bandit model is developed which captures the interactions between elements and the dual uncertainties of both user preferences and element quality. The problem has many challenging features but solution methods are proposed that are both easy to implement and which can be adapted to particular applications. Finally, easy-to-use software to calculate Gittins indices for Bernoulli and normal rewards has been developed as part of this thesis and has been made publicly available. The methodology used is presented together with a study of accuracy and speed.},
	language = {en},
	urldate = {2025-02-19},
	school = {Lancaster University},
	author = {Edwards, James and Glazebrook, Kevin and Fearnhead, Paul and Leslie, David},
	year = {2016},
}

@article{reisert_learning_2007,
	title = {Learning {Equivariant} {Functions} with {Matrix} {Valued} {Kernels}},
	volume = {8},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v8/reisert07a.html},
	abstract = {This paper presents a new class of matrix valued kernels that are
ideally suited to learn vector valued equivariant functions. Matrix
valued kernels are a natural generalization of the common notion of a
kernel.  We set the theoretical foundations of so called equivariant
matrix valued kernels.  We work out several properties of equivariant
kernels, we give an interpretation of their behavior and show
relations to scalar kernels. The notion of (ir)reducibility of group
representations is transferred into the framework of matrix valued
kernels. At the end to two exemplary applications are demonstrated.
We design a non-linear rotation and translation equivariant filter for
2D-images and propose an invariant object detector based on the
generalized Hough transform.},
	number = {15},
	urldate = {2025-02-19},
	journal = {Journal of Machine Learning Research},
	author = {Reisert, Marco and Burkhardt, Hans},
	year = {2007},
	pages = {385--408},
}

@misc{park_equivalence_2012,
	title = {On the equivalence between {Stein} and de {Bruijn} identities},
	url = {http://arxiv.org/abs/1202.0015},
	doi = {10.48550/arXiv.1202.0015},
	abstract = {This paper focuses on proving the equivalence between Stein's identity and de Bruijn's identity. Given some conditions, we prove that Stein's identity is equivalent to de Bruijn's identity. In addition, some extensions of de Bruijn's identity are presented. For arbitrary but fixed input and noise distributions, there exist relations between the first derivative of the differential entropy and the posterior mean. Moreover, the second derivative of the differential entropy is related to the Fisher information for arbitrary input and noise distributions. Several applications are presented to support the usefulness of the developed results in this paper.},
	urldate = {2025-02-17},
	publisher = {arXiv},
	author = {Park, Sangwoo and Serpedin, Erchin and Qaraqe, Khalid},
	month = jul,
	year = {2012},
	note = {arXiv:1202.0015 [cs]},
	keywords = {Computer Science - Information Theory, Mathematics - Information Theory},
}

@misc{park_equivalence_2012-1,
	title = {On the equivalence between {Stein} and de {Bruijn} identities},
	url = {http://arxiv.org/abs/1202.0015},
	doi = {10.48550/arXiv.1202.0015},
	abstract = {This paper focuses on proving the equivalence between Stein's identity and de Bruijn's identity. Given some conditions, we prove that Stein's identity is equivalent to de Bruijn's identity. In addition, some extensions of de Bruijn's identity are presented. For arbitrary but fixed input and noise distributions, there exist relations between the first derivative of the differential entropy and the posterior mean. Moreover, the second derivative of the differential entropy is related to the Fisher information for arbitrary input and noise distributions. Several applications are presented to support the usefulness of the developed results in this paper.},
	urldate = {2025-02-17},
	publisher = {arXiv},
	author = {Park, Sangwoo and Serpedin, Erchin and Qaraqe, Khalid},
	month = jul,
	year = {2012},
	note = {arXiv:1202.0015 [cs]},
	keywords = {Computer Science - Information Theory, Mathematics - Information Theory},
}

@misc{huang_reverse_2024,
	title = {Reverse {Diffusion} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/2307.02037},
	doi = {10.48550/arXiv.2307.02037},
	abstract = {We propose a Monte Carlo sampler from the reverse diffusion process. Unlike the practice of diffusion models, where the intermediary updates -- the score functions -- are learned with a neural network, we transform the score matching problem into a mean estimation one. By estimating the means of the regularized posterior distributions, we derive a novel Monte Carlo sampling algorithm called reverse diffusion Monte Carlo (rdMC), which is distinct from the Markov chain Monte Carlo (MCMC) methods. We determine the sample size from the error tolerance and the properties of the posterior distribution to yield an algorithm that can approximately sample the target distribution with any desired accuracy. Additionally, we demonstrate and prove under suitable conditions that sampling with rdMC can be significantly faster than that with MCMC. For multi-modal target distributions such as those in Gaussian mixture models, rdMC greatly improves over the Langevin-style MCMC sampling methods both theoretically and in practice. The proposed rdMC method offers a new perspective and solution beyond classical MCMC algorithms for the challenging complex distributions.},
	urldate = {2025-02-13},
	publisher = {arXiv},
	author = {Huang, Xunpeng and Dong, Hanze and Hao, Yifan and Ma, Yi-An and Zhang, Tong},
	month = mar,
	year = {2024},
	note = {arXiv:2307.02037 [stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@misc{liu_action-depedent_2018,
	title = {Action-depedent {Control} {Variates} for {Policy} {Optimization} via {Stein}'s {Identity}},
	url = {http://arxiv.org/abs/1710.11198},
	doi = {10.48550/arXiv.1710.11198},
	abstract = {Policy gradient methods have achieved remarkable successes in solving challenging reinforcement learning problems. However, it still often suffers from the large variance issue on policy gradient estimation, which leads to poor sample efficiency during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Motivated by the Stein's identity, our method extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more general action-dependent baseline functions. Empirical studies show that our method significantly improves the sample efficiency of the state-of-the-art policy gradient approaches.},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {Liu, Hao and Feng, Yihao and Mao, Yi and Zhou, Dengyong and Peng, Jian and Liu, Qiang},
	month = feb,
	year = {2018},
	note = {arXiv:1710.11198 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chaudhuri_parameter-free_2010,
	title = {A parameter-free hedging algorithm},
	url = {http://arxiv.org/abs/0903.2851},
	doi = {10.48550/arXiv.0903.2851},
	abstract = {We study the problem of decision-theoretic online learning (DTOL). Motivated by practical applications, we focus on DTOL when the number of actions is very large. Previous algorithms for learning in this framework have a tunable learning rate parameter, and a barrier to using online-learning in practical applications is that it is not understood how to set this parameter optimally, particularly when the number of actions is large. In this paper, we offer a clean solution by proposing a novel and completely parameter-free algorithm for DTOL. We introduce a new notion of regret, which is more natural for applications with a large number of actions. We show that our algorithm achieves good performance with respect to this new notion of regret; in addition, it also achieves performance close to that of the best bounds achieved by previous algorithms with optimally-tuned parameters, according to previous notions of regret.},
	urldate = {2025-02-11},
	publisher = {arXiv},
	author = {Chaudhuri, Kamalika and Freund, Yoav and Hsu, Daniel},
	month = jan,
	year = {2010},
	note = {arXiv:0903.2851 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{huan_optimal_2024,
	title = {Optimal experimental design: {Formulations} and computations},
	volume = {33},
	issn = {0962-4929, 1474-0508},
	shorttitle = {Optimal experimental design},
	url = {https://www.cambridge.org/core/journals/acta-numerica/article/optimal-experimental-design-formulations-and-computations/38BBD0DC1A0386FDF306B6C0167DF7D9},
	doi = {10.1017/S0962492924000023},
	abstract = {Questions of ‘how best to acquire data’ are essential to modelling and prediction in the natural and social sciences, engineering applications, and beyond. Optimal experimental design (OED) formalizes these questions and creates computational methods to answer them. This article presents a systematic survey of modern OED, from its foundations in classical design theory to current research involving OED for complex models. We begin by reviewing criteria used to formulate an OED problem and thus to encode the goal of performing an experiment. We emphasize the flexibility of the Bayesian and decision-theoretic approach, which encompasses information-based criteria that are well-suited to nonlinear and non-Gaussian statistical models. We then discuss methods for estimating or bounding the values of these design criteria; this endeavour can be quite challenging due to strong nonlinearities, high parameter dimension, large per-sample costs, or settings where the model is implicit. A complementary set of computational issues involves optimization methods used to find a design; we discuss such methods in the discrete (combinatorial) setting of observation selection and in settings where an exact design can be continuously parametrized. Finally we present emerging methods for sequential OED that build non-myopic design policies, rather than explicit designs; these methods naturally adapt to the outcomes of past experiments in proposing new experiments, while seeking coordination among all experiments to be performed. Throughout, we highlight important open questions and challenges.},
	language = {en},
	urldate = {2025-02-09},
	journal = {Acta Numerica},
	author = {Huan, Xun and Jagalur, Jayanth and Marzouk, Youssef},
	month = jul,
	year = {2024},
	keywords = {62-02, 62-08, 62B15, 62K05, 62L05, 65M32, 94A17},
	pages = {715--840},
}

@inproceedings{jiang_prioritized_2021,
	title = {Prioritized {Level} {Replay}},
	url = {https://proceedings.mlr.press/v139/jiang21b.html},
	abstract = {Environments with procedurally generated content serve as important benchmarks for testing systematic generalization in deep reinforcement learning. In this setting, each level is an algorithmically created environment instance with a unique configuration of its factors of variation. Training on a prespecified subset of levels allows for testing generalization to unseen levels. What can be learned from a level depends on the current policy, yet prior work defaults to uniform sampling of training levels independently of the policy. We introduce Prioritized Level Replay (PLR), a general framework for selectively sampling the next training level by prioritizing those with higher estimated learning potential when revisited in the future. We show TD-errors effectively estimate a level’s future learning potential and, when used to guide the sampling procedure, induce an emergent curriculum of increasingly difficult levels. By adapting the sampling of training levels, PLR significantly improves sample-efficiency and generalization on Procgen Benchmark—matching the previous state-of-the-art in test return—and readily combines with other methods. Combined with the previous leading method, PLR raises the state-of-the-art to over 76\% improvement in test return relative to standard RL baselines.},
	language = {en},
	urldate = {2025-02-09},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Jiang, Minqi and Grefenstette, Edward and Rocktäschel, Tim},
	month = jul,
	year = {2021},
	pages = {4940--4950},
}

@misc{ay_information_2019,
	title = {Information {Decomposition} based on {Cooperative} {Game} {Theory}},
	url = {http://arxiv.org/abs/1910.05979},
	doi = {10.48550/arXiv.1910.05979},
	abstract = {We offer a new approach to the information decomposition problem in information theory: given a 'target' random variable co-distributed with multiple 'source' variables, how can we decompose the mutual information into a sum of non-negative terms that quantify the contributions of each random variable, not only individually but also in combination? We derive our composition from cooperative game theory. It can be seen as assigning a "fair share" of the mutual information to each combination of the source variables. Our decomposition is based on a different lattice from the usual 'partial information decomposition' (PID) approach, and as a consequence our decomposition has a smaller number of terms: it has analogs of the synergy and unique information terms, but lacks terms corresponding to redundancy. Because of this, it is able to obey equivalents of the axioms known as 'local positivity' and 'identity', which cannot be simultaneously satisfied by a PID measure.},
	urldate = {2025-02-08},
	publisher = {arXiv},
	author = {Ay, Nihat and Polani, Daniel and Virgo, Nathaniel},
	month = oct,
	year = {2019},
	note = {arXiv:1910.05979 [cs]},
	keywords = {Computer Science - Information Theory, Mathematics - Information Theory},
}

@misc{noauthor_240205342v1_nodate,
	title = {[2402.05342v1] {Nonlinear} {Regression} {Analysis}},
	url = {https://arxiv.org/abs/2402.05342v1},
	urldate = {2025-02-08},
}

@article{long_multimodal_2022,
	title = {Multimodal information gain in {Bayesian} design of experiments},
	volume = {37},
	issn = {1613-9658},
	url = {https://doi.org/10.1007/s00180-021-01145-9},
	doi = {10.1007/s00180-021-01145-9},
	abstract = {One of the well-known challenges in optimal experimental design is how to efficiently estimate the nested integrations of the expected information gain. The Gaussian approximation and associated importance sampling have been shown to be effective at reducing the numerical costs. However, they may fail due to the non-negligible biases and the numerical instabilities. A new approach is developed to compute the expected information gain, when the posterior distribution is multimodal—a situation previously ignored by the methods aiming at accelerating the nested numerical integrations. Specifically, the posterior distribution is approximated using a mixture distribution constructed by multiple runs of global search for the modes and weighted local Laplace approximations. Under any given probability of capturing all the modes, we provide an estimation of the number of runs of searches, which is dimension independent. It is shown that the novel global-local multimodal approach can be significantly more accurate and more efficient than the other existing approaches, especially when the number of modes is large. The methods can be applied to the designs of experiments with both calibrated and uncalibrated observation noises.},
	language = {en},
	number = {2},
	urldate = {2025-02-08},
	journal = {Computational Statistics},
	author = {Long, Quan},
	month = apr,
	year = {2022},
	keywords = {Expected information gain, Gaussian mixture, Machine learning, Optimal design of experiments, Weighted Laplace approximation},
	pages = {865--885},
}

@misc{ramdas_game-theoretic_2023,
	title = {Game-theoretic statistics and safe anytime-valid inference},
	url = {http://arxiv.org/abs/2210.01948},
	doi = {10.48550/arXiv.2210.01948},
	abstract = {Safe anytime-valid inference (SAVI) provides measures of statistical evidence and certainty -- e-processes for testing and confidence sequences for estimation -- that remain valid at all stopping times, accommodating continuous monitoring and analysis of accumulating data and optional stopping or continuation for any reason. These measures crucially rely on test martingales, which are nonnegative martingales starting at one. Since a test martingale is the wealth process of a player in a betting game, SAVI centrally employs game-theoretic intuition, language and mathematics. We summarize the SAVI goals and philosophy, and report recent advances in testing composite hypotheses and estimating functionals in nonparametric settings.},
	urldate = {2025-02-07},
	publisher = {arXiv},
	author = {Ramdas, Aaditya and Grünwald, Peter and Vovk, Vladimir and Shafer, Glenn},
	month = jun,
	year = {2023},
	note = {arXiv:2210.01948 [math]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Information Theory, Mathematics - Information Theory, Mathematics - Statistics Theory, Statistics - Methodology, Statistics - Statistics Theory},
}

@misc{liu_meta-learning_2020,
	title = {Meta-learning {Transferable} {Representations} with a {Single} {Target} {Domain}},
	url = {http://arxiv.org/abs/2011.01418},
	doi = {10.48550/arXiv.2011.01418},
	abstract = {Recent works found that fine-tuning and joint training---two popular approaches for transfer learning---do not always improve accuracy on downstream tasks. First, we aim to understand more about when and why fine-tuning and joint training can be suboptimal or even harmful for transfer learning. We design semi-synthetic datasets where the source task can be solved by either source-specific features or transferable features. We observe that (1) pre-training may not have incentive to learn transferable features and (2) joint training may simultaneously learn source-specific features and overfit to the target. Second, to improve over fine-tuning and joint training, we propose Meta Representation Learning (MeRLin) to learn transferable features. MeRLin meta-learns representations by ensuring that a head fit on top of the representations with target training data also performs well on target validation data. We also prove that MeRLin recovers the target ground-truth model with a quadratic neural net parameterization and a source distribution that contains both transferable and source-specific features. On the same distribution, pre-training and joint training provably fail to learn transferable features. MeRLin empirically outperforms previous state-of-the-art transfer learning algorithms on various real-world vision and NLP transfer learning benchmarks.},
	urldate = {2025-02-07},
	publisher = {arXiv},
	author = {Liu, Hong and HaoChen, Jeff Z. and Wei, Colin and Ma, Tengyu},
	month = nov,
	year = {2020},
	note = {arXiv:2011.01418 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{chattopadhyay_joint_2013,
	title = {Joint {Transfer} and {Batch}-mode {Active} {Learning}},
	url = {https://proceedings.mlr.press/v28/chattopadhyay13.html},
	abstract = {Active learning and transfer learning are two different methodologies that address the common  problem of insufficient labels. Transfer learning addresses this problem by using the knowledge gained from a related and already labeled data source, whereas active learning focuses on selecting a small set of informative samples  for manual annotation. Recently, there has been much interest in developing frameworks that combine both transfer and active learning  methodologies. A few such frameworks reported in literature perform transfer and active learning in two separate stages. In this work, we present an integrated framework that performs transfer and active learning simultaneously by solving a  single convex optimization problem. The proposed framework computes the weights of source domain data and selects the samples from the target domain data simultaneously, by minimizing a common objective of reducing distribution difference between the data set consisting of reweighted source and the queried target domain data and the set of unlabeled target domain data. Comprehensive experiments on three real world data sets demonstrate that the proposed method improves the classification accuracy by 5\% to  10\% over the existing two-stage approach},
	language = {en},
	urldate = {2025-02-07},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chattopadhyay, Rita and Fan, Wei and Davidson, Ian and Panchanathan, Sethuraman and Ye, Jieping},
	month = may,
	year = {2013},
	pages = {253--261},
}

@article{reeger_adaptivity_2024,
	title = {Adaptivity in {Local} {Kernel} {Based} {Methods} for {Approximating} the {Action} of {Linear} {Operators}},
	volume = {46},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/10.1137/23M1598052},
	doi = {10.1137/23M1598052},
	abstract = {.We prove closed-form equations for the exact high-dimensional asymptotics of a family of first-order gradient-based methods, learning an estimator (e.g., M-estimator, shallow neural network) from observations on Gaussian data with empirical risk minimization. This includes widely used algorithms such as stochastic gradient descent (SGD) or Nesterov acceleration. The obtained equations match those resulting from the discretization of dynamical mean-field theory equations from statistical physics when applied to the corresponding gradient flow. Our proof method allows us to give an explicit description of how memory kernels build up in the effective dynamics and to include nonseparable update functions, allowing datasets with nonidentity covariance matrices. Finally, we provide numerical implementations of the equations for SGD with generic extensive batch size and constant learning rates.},
	number = {4},
	urldate = {2025-02-04},
	journal = {SIAM Journal on Scientific Computing},
	author = {Reeger, Jonah A.},
	month = aug,
	year = {2024},
	pages = {A2683--A2708},
}

@article{ton_spatial_2018,
	series = {One world, one health},
	title = {Spatial mapping with {Gaussian} processes and nonstationary {Fourier} features},
	volume = {28},
	issn = {2211-6753},
	url = {https://www.sciencedirect.com/science/article/pii/S2211675317302890},
	doi = {10.1016/j.spasta.2018.02.002},
	abstract = {The use of covariance kernels is ubiquitous in the field of spatial statistics. Kernels allow data to be mapped into high-dimensional feature spaces and can thus extend simple linear additive methods to nonlinear methods with higher order interactions. However, until recently, there has been a strong reliance on a limited class of stationary kernels such as the Matérn or squared exponential, limiting the expressiveness of these modelling approaches. Recent machine learning research has focused on spectral representations to model arbitrary stationary kernels and introduced more general representations that include classes of nonstationary kernels. In this paper, we exploit the connections between Fourier feature representations, Gaussian processes and neural networks to generalise previous approaches and develop a simple and efficient framework to learn arbitrarily complex nonstationary kernel functions directly from the data, while taking care to avoid overfitting using state-of-the-art methods from deep learning. We highlight the very broad array of kernel classes that could be created within this framework. We apply this to a time series dataset and a remote sensing problem involving land surface temperature in Eastern Africa. We show that without increasing the computational or storage complexity, nonstationary kernels can be used to improve generalisation performance and provide more interpretable results.},
	urldate = {2025-02-04},
	journal = {Spatial Statistics},
	author = {Ton, Jean-Francois and Flaxman, Seth and Sejdinovic, Dino and Bhatt, Samir},
	month = dec,
	year = {2018},
	keywords = {Gaussian process, Nonstationary, Random Fourier features, Spatial statistics},
	pages = {59--78},
}

@misc{laurent_symmetry-aware_2023,
	title = {A {Symmetry}-{Aware} {Exploration} of {Bayesian} {Neural} {Network} {Posteriors}},
	url = {http://arxiv.org/abs/2310.08287},
	doi = {10.48550/arXiv.2310.08287},
	abstract = {The distribution of the weights of modern deep neural networks (DNNs) - crucial for uncertainty quantification and robustness - is an eminently complex object due to its extremely high dimensionality. This paper proposes one of the first large-scale explorations of the posterior distribution of deep Bayesian Neural Networks (BNNs), expanding its study to real-world vision tasks and architectures. Specifically, we investigate the optimal approach for approximating the posterior, analyze the connection between posterior quality and uncertainty quantification, delve into the impact of modes on the posterior, and explore methods for visualizing the posterior. Moreover, we uncover weight-space symmetries as a critical aspect for understanding the posterior. To this extent, we develop an in-depth assessment of the impact of both permutation and scaling symmetries that tend to obfuscate the Bayesian posterior. While the first type of transformation is known for duplicating modes, we explore the relationship between the latter and L2 regularization, challenging previous misconceptions. Finally, to help the community improve our understanding of the Bayesian posterior, we will shortly release the first large-scale checkpoint dataset, including thousands of real-world models and our codes.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Laurent, Olivier and Aldea, Emanuel and Franchi, Gianni},
	month = oct,
	year = {2023},
	note = {arXiv:2310.08287 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{wiese_towards_2023,
	title = {Towards {Efficient} {MCMC} {Sampling} in {Bayesian} {Neural} {Networks} by {Exploiting} {Symmetry}},
	url = {http://arxiv.org/abs/2304.02902},
	doi = {10.48550/arXiv.2304.02902},
	abstract = {Bayesian inference in deep neural networks is challenging due to the high-dimensional, strongly multi-modal parameter posterior density landscape. Markov chain Monte Carlo approaches asymptotically recover the true posterior but are considered prohibitively expensive for large modern architectures. Local methods, which have emerged as a popular alternative, focus on specific parameter regions that can be approximated by functions with tractable integrals. While these often yield satisfactory empirical results, they fail, by definition, to account for the multi-modality of the parameter posterior. In this work, we argue that the dilemma between exact-but-unaffordable and cheap-but-inexact approaches can be mitigated by exploiting symmetries in the posterior landscape. Such symmetries, induced by neuron interchangeability and certain activation functions, manifest in different parameter values leading to the same functional output value. We show theoretically that the posterior predictive density in Bayesian neural networks can be restricted to a symmetry-free parameter reference set. By further deriving an upper bound on the number of Monte Carlo chains required to capture the functional diversity, we propose a straightforward approach for feasible Bayesian inference. Our experiments suggest that efficient sampling is indeed possible, opening up a promising path to accurate uncertainty quantification in deep learning.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Wiese, Jonas Gregor and Wimmer, Lisa and Papamarkou, Theodore and Bischl, Bernd and Günnemann, Stephan and Rügamer, David},
	month = apr,
	year = {2023},
	note = {arXiv:2304.02902 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{anily_subadditive_2014,
	title = {Subadditive and {Homogeneous} of {Degree} {One} {Games} {Are} {Totally} {Balanced}},
	volume = {62},
	issn = {0030-364X},
	url = {https://pubsonline.informs.org/doi/10.1287/opre.2014.1283},
	doi = {10.1287/opre.2014.1283},
	abstract = {A cooperative game with transferable utility is said to be homogeneous of degree one if for any integer m, the value of cloning m times all players at any given coalition, leads to m times the value of the original coalition. We show that this property coupled with subadditivity, guarantees the nonemptyness of the core of the game and of all its subgames, namely, the game is totally balanced. Examples for games stemming from the areas of retailing and of facility location are given.},
	number = {4},
	urldate = {2025-02-03},
	journal = {Operations Research},
	author = {Anily, Shoshana and Haviv, Moshe},
	month = aug,
	year = {2014},
	keywords = {cooperative, games/group decisions},
	pages = {788--793},
}

@incollection{monderer_chapter_2002,
	title = {Chapter 54 {Variations} on the shapley value},
	volume = {3},
	url = {https://www.sciencedirect.com/science/article/pii/S1574000502030175},
	abstract = {This survey captures the main contributions in the area described by the title that were published up to 1997. (Unfortunately, it does not capture all of them.) The variations that are the subject of this chapter are those axiomatically characterized solutions which are obtained by varying either the set of axioms that define the Shapley value, or the domain over which this value is defined, or both.In the first category, we deal mainly with probabilistic values. These are solutions that preserve one of the essential features of the Shapley value, namely, that they are given, for each player, by some averaging of the player's marginal contributions to coalitions, where the probabilistic weights depend on the coalitions only and not on the game. The Shapley value is the unique probabilistic value that is efficient and symmetric. We characterize and discuss two families of solutions: quasivalues, which are efficient probabilistic values, and semivalues, which are symmetric probabilistic values.In the second category, we deal with solutions that generalize the Shapley value by changing the domain over which the solution is defined. In such generalizations the solution is defined on pairs, consisting of a game and some structure on the set of players. The Shapley value is a special case of such a generalization in the sense that it coincides with the solution on the restricted domain in which the second argument is fixed to be the “trivial” one. Under this category we survey mostly solutions in which the structure is a partition of the set of the players, and a solution in which the structure is a graph, the vertices of which are the players.},
	urldate = {2025-02-03},
	booktitle = {Handbook of {Game} {Theory} with {Economic} {Applications}},
	publisher = {Elsevier},
	author = {Monderer, Dov and Samet, Dov},
	month = jan,
	year = {2002},
	doi = {10.1016/S1574-0005(02)03017-5},
	keywords = {Shapley value, non-symmetric values, probabilistic values, quasivalue, semivalue, weighted values},
	pages = {2055--2076},
}

@article{shi_physics-informed_2024,
	title = {Physics-informed neural network classification framework for reliability analysis},
	volume = {258},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417424020748},
	doi = {10.1016/j.eswa.2024.125207},
	abstract = {Reliability analysis is crucial for quantitatively evaluating structural safety amidst uncertainties, laying the foundation for reliability-based design optimization aimed at augmenting structural reliability and reducing economic expenditures, thereby offering substantial engineering benefits. However, performing reliability analysis on intricate structures, especially those requiring time-intensive finite element models, presents a formidable challenge. This study develops an innovative physics-informed neural network classification (PINNC) model to tackle this issue. In the PINNC model, the loss associated with the structural output state (i.e., safety or failed state) is defined as classification loss. Additionally, the structural output value (i.e., actual structural response) is treated as critical physical information, with its associated loss termed as physical loss. To facilitate a separate calculation of physical loss and classification loss, a parametric sigmoid activation function is used, establishing a link between the structural output value and the structural output state. The total loss is calculated as a weighted sum of both physical and classification losses. Unlike conventional neural network classification models that solely focus on classification loss, the PINNC model integrates both physical and classification losses, markedly improving the model’s efficacy in structural reliability analysis. Moreover, an adaptive framework is developed to incrementally include samples proximal to the limit state surface as new training data, thereby reinforcing the PINNC model’s computational precision. The effectiveness of the proposed PINNC framework in overcoming reliability analysis challenges is demonstrated through various applications.},
	urldate = {2025-01-30},
	journal = {Expert Systems with Applications},
	author = {Shi, Yan and Beer, Michael},
	month = dec,
	year = {2024},
	keywords = {Adaptive framework, Classification model, Physics-informed neural network, Reliability analysis, Weighted loss function},
	pages = {125207},
}

@misc{petrik_beyond_2019,
	title = {Beyond {Confidence} {Regions}: {Tight} {Bayesian} {Ambiguity} {Sets} for {Robust} {MDPs}},
	shorttitle = {Beyond {Confidence} {Regions}},
	url = {http://arxiv.org/abs/1902.07605},
	doi = {10.48550/arXiv.1902.07605},
	abstract = {Robust MDPs (RMDPs) can be used to compute policies with provable worst-case guarantees in reinforcement learning. The quality and robustness of an RMDP solution are determined by the ambiguity set---the set of plausible transition probabilities---which is usually constructed as a multi-dimensional confidence region. Existing methods construct ambiguity sets as confidence regions using concentration inequalities which leads to overly conservative solutions. This paper proposes a new paradigm that can achieve better solutions with the same robustness guarantees without using confidence regions as ambiguity sets. To incorporate prior knowledge, our algorithms optimize the size and position of ambiguity sets using Bayesian inference. Our theoretical analysis shows the safety of the proposed method, and the empirical results demonstrate its practical promise.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Petrik, Marek and Russell, Reazul Hasan},
	month = feb,
	year = {2019},
	note = {arXiv:1902.07605 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{gelman_prior_2006,
	title = {Prior distributions for variance parameters in hierarchical models (comment on article by {Browne} and {Draper})},
	volume = {1},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-1/issue-3/Prior-distributions-for-variance-parameters-in-hierarchical-models-comment-on/10.1214/06-BA117A.full},
	doi = {10.1214/06-BA117A},
	abstract = {Various noninformative prior distributions have been suggested for scale parameters in hierarchical models. We construct a new folded-noncentral-\$t\$ family of conditionally conjugate priors for hierarchical standard deviation parameters, and then consider noninformative and weakly informative priors in this family. We use an example to illustrate serious problems with the inverse-gamma family of "noninformative" prior distributions. We suggest instead to use a uniform prior on the hierarchical standard deviation, using the half-\$t\$ family when the number of groups is small and in other settings where a weakly informative prior is desired. We also illustrate the use of the half-\$t\$ family for hierarchical modeling of multiple variance parameters such as arise in the analysis of variance.},
	number = {3},
	urldate = {2025-01-27},
	journal = {Bayesian Analysis},
	author = {Gelman, Andrew},
	month = sep,
	year = {2006},
	keywords = {Bayesian inference, conditional conjugacy, folded-noncentral-\$t\$ distribution, half-\$t\$ distribution, hierarchical model, multilevel model, noninformative prior distribution, weakly informative prior distribution},
	pages = {515--534},
}

@inproceedings{du_compositional_2020,
	title = {Compositional {Visual} {Generation} with {Energy} {Based} {Models}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/49856ed476ad01fcff881d57e161d73f-Abstract.html},
	abstract = {A vital aspect of human intelligence is the ability to compose increasingly complex concepts out of simpler ideas, enabling both rapid learning and adaptation of knowledge. In this paper we show that energy-based models can exhibit this ability by directly combining probability distributions. Samples from the combined distribution correspond to compositions of concepts. For example, given a distribution for smiling faces, and another for male faces, we can combine them to generate smiling male faces. This allows us to generate natural images that simultaneously satisfy conjunctions, disjunctions, and negations of concepts. We evaluate compositional generation abilities of our model on the CelebA dataset of natural faces and synthetic 3D scene images. We also demonstrate other unique advantages of our model, such as the ability to continually learn and incorporate new concepts, or infer compositions of concept properties underlying an image.},
	urldate = {2025-01-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Du, Yilun and Li, Shuang and Mordatch, Igor},
	year = {2020},
	pages = {6637--6647},
}

@misc{han_stein_nodate,
	title = {Stein {Variational} {Gradient} {Descent} {Without} {Gradient}},
	url = {https://arxiv.org/abs/1806.02775},
	abstract = {Stein variational gradient decent (SVGD) has been shown to be a powerful approximate inference algorithm for complex distributions. However, the standard SVGD requires calculating the gradient of the target density and cannot be applied when the gradient is unavailable. In this work, we develop a gradient-free variant of SVGD (GF-SVGD), which replaces the true gradient with a surrogate gradient, and corrects the induced bias by re-weighting the gradients in a proper form. We show that our GF-SVGD can be viewed as the standard SVGD with a special choice of kernel, and hence directly inherits the theoretical properties of SVGD. We shed insights on the empirical choice of the surrogate gradient and propose an annealed GF-SVGD that leverages the idea of simulated annealing to improve the performance on high dimensional complex distributions. Empirical studies show that our method consistently outperforms a number of recent advanced gradient-free MCMC methods.},
	urldate = {2025-01-24},
	author = {Han, Jun and Liu, Qiang},
}

@misc{han_stein_2018,
	title = {Stein {Variational} {Gradient} {Descent} {Without} {Gradient}},
	url = {http://arxiv.org/abs/1806.02775},
	doi = {10.48550/arXiv.1806.02775},
	abstract = {Stein variational gradient decent (SVGD) has been shown to be a powerful approximate inference algorithm for complex distributions. However, the standard SVGD requires calculating the gradient of the target density and cannot be applied when the gradient is unavailable. In this work, we develop a gradient-free variant of SVGD (GF-SVGD), which replaces the true gradient with a surrogate gradient, and corrects the induced bias by re-weighting the gradients in a proper form. We show that our GF-SVGD can be viewed as the standard SVGD with a special choice of kernel, and hence directly inherits the theoretical properties of SVGD. We shed insights on the empirical choice of the surrogate gradient and propose an annealed GF-SVGD that leverages the idea of simulated annealing to improve the performance on high dimensional complex distributions. Empirical studies show that our method consistently outperforms a number of recent advanced gradient-free MCMC methods.},
	urldate = {2025-01-24},
	publisher = {arXiv},
	author = {Han, Jun and Liu, Qiang},
	month = jun,
	year = {2018},
	note = {arXiv:1806.02775 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{du_compositional_2024,
	title = {Compositional {Generative} {Modeling}: {A} {Single} {Model} is {Not} {All} {You} {Need}},
	shorttitle = {Compositional {Generative} {Modeling}},
	url = {http://arxiv.org/abs/2402.01103},
	doi = {10.48550/arXiv.2402.01103},
	abstract = {Large monolithic generative models trained on massive amounts of data have become an increasingly dominant approach in AI research. In this paper, we argue that we should instead construct large generative systems by composing smaller generative models together. We show how such a compositional generative approach enables us to learn distributions in a more data-efficient manner, enabling generalization to parts of the data distribution unseen at training time. We further show how this enables us to program and construct new generative models for tasks completely unseen at training. Finally, we show that in many cases, we can discover separate compositional components from data.},
	urldate = {2025-01-24},
	publisher = {arXiv},
	author = {Du, Yilun and Kaelbling, Leslie},
	month = jun,
	year = {2024},
	note = {arXiv:2402.01103 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{lindgren_explicit_2011-1,
	title = {An {Explicit} {Link} between {Gaussian} {Fields} and {Gaussian} {Markov} {Random} {Fields}: {The} {Stochastic} {Partial} {Differential} {Equation} {Approach}},
	volume = {73},
	issn = {1369-7412},
	shorttitle = {An {Explicit} {Link} between {Gaussian} {Fields} and {Gaussian} {Markov} {Random} {Fields}},
	url = {https://doi.org/10.1111/j.1467-9868.2011.00777.x},
	doi = {10.1111/j.1467-9868.2011.00777.x},
	abstract = {Continuously indexed Gaussian fields (GFs) are the most important ingredient in spatial statistical modelling and geostatistics. The specification through the covariance function gives an intuitive interpretation of the field properties. On the computational side, GFs are hampered with the big n problem, since the cost of factorizing dense matrices is cubic in the dimension. Although computational power today is at an all time high, this fact seems still to be a computational bottleneck in many applications. Along with GFs, there is the class of Gaussian Markov random fields (GMRFs) which are discretely indexed. The Markov property makes the precision matrix involved sparse, which enables the use of numerical algorithms for sparse matrices, that for fields in ℝ2 only use the square root of the time required by general algorithms. The specification of a GMRF is through its full conditional distributions but its marginal properties are not transparent in such a parameterization. We show that, using an approximate stochastic weak solution to (linear) stochastic partial differential equations, we can, for some GFs in the Matérn class, provide an explicit link, for any triangulation of ℝd, between GFs and GMRFs, formulated as a basis function representation. The consequence is that we can take the best from the two worlds and do the modelling by using GFs but do the computations by using GMRFs. Perhaps more importantly, our approach generalizes to other covariance functions generated by SPDEs, including oscillating and non-stationary GFs, as well as GFs on manifolds. We illustrate our approach by analysing global temperature data with a non-stationary model defined on a sphere.},
	number = {4},
	urldate = {2025-01-23},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Lindgren, Finn and Rue, Håvard and Lindström, Johan},
	month = sep,
	year = {2011},
	pages = {423--498},
}

@article{weng_bayesian_2011-1,
	title = {A {Bayesian} {Approximation} {Method} for {Online} {Ranking}},
	volume = {12},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v12/weng11a.html},
	abstract = {This paper describes a Bayesian approximation method to obtain online ranking algorithms for games with multiple teams and multiple players.  Recently for Internet games large online ranking systems are much needed.  We consider game models in which a k-team game is treated as several two-team games.  By approximating the expectation of teams' (or players') performances, we derive simple analytic update rules.  These update rules, without numerical integrations, are very easy to interpret and implement.  Experiments on game data show that the accuracy of our approach is competitive with state of the art systems such as TrueSkill, but the running time as well as the code is much shorter.},
	number = {9},
	urldate = {2025-01-21},
	journal = {Journal of Machine Learning Research},
	author = {Weng, Ruby C. and Lin, Chih-Jen},
	year = {2011},
	pages = {267--300},
}

@misc{adams_ranking_2011,
	title = {Ranking via {Sinkhorn} {Propagation}},
	url = {http://arxiv.org/abs/1106.1925},
	doi = {10.48550/arXiv.1106.1925},
	abstract = {It is of increasing importance to develop learning methods for ranking. In contrast to many learning objectives, however, the ranking problem presents difficulties due to the fact that the space of permutations is not smooth. In this paper, we examine the class of rank-linear objective functions, which includes popular metrics such as precision and discounted cumulative gain. In particular, we observe that expectations of these gains are completely characterized by the marginals of the corresponding distribution over permutation matrices. Thus, the expectations of rank-linear objectives can always be described through locations in the Birkhoff polytope, i.e., doubly-stochastic matrices (DSMs). We propose a technique for learning DSM-based ranking functions using an iterative projection operator known as Sinkhorn normalization. Gradients of this operator can be computed via backpropagation, resulting in an algorithm we call Sinkhorn propagation, or SinkProp. This approach can be combined with a wide range of gradient-based approaches to rank learning. We demonstrate the utility of SinkProp on several information retrieval data sets.},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Adams, Ryan Prescott and Zemel, Richard S.},
	month = jun,
	year = {2011},
	note = {arXiv:1106.1925 [stat]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{wang_shapley_2021,
	title = {Shapley {Explanation} {Networks}},
	url = {http://arxiv.org/abs/2104.02297},
	doi = {10.48550/arXiv.2104.02297},
	abstract = {Shapley values have become one of the most popular feature attribution explanation methods. However, most prior work has focused on post-hoc Shapley explanations, which can be computationally demanding due to its exponential time complexity and preclude model regularization based on Shapley explanations during training. Thus, we propose to incorporate Shapley values themselves as latent representations in deep models thereby making Shapley explanations first-class citizens in the modeling paradigm. This intrinsic explanation approach enables layer-wise explanations, explanation regularization of the model during training, and fast explanation computation at test time. We define the Shapley transform that transforms the input into a Shapley representation given a specific function. We operationalize the Shapley transform as a neural network module and construct both shallow and deep networks, called ShapNets, by composing Shapley modules. We prove that our Shallow ShapNets compute the exact Shapley values and our Deep ShapNets maintain the missingness and accuracy properties of Shapley values. We demonstrate on synthetic and real-world datasets that our ShapNets enable layer-wise Shapley explanations, novel Shapley regularizations during training, and fast computation while maintaining reasonable performance. Code is available at https://github.com/inouye-lab/ShapleyExplanationNetworks.},
	urldate = {2025-01-18},
	publisher = {arXiv},
	author = {Wang, Rui and Wang, Xiaoqian and Inouye, David I.},
	month = apr,
	year = {2021},
	note = {arXiv:2104.02297 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{liu_physics-informed_2021,
	title = {Physics-informed {Dyna}-style model-based deep reinforcement learning for dynamic control},
	volume = {477},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rspa.2021.0618},
	doi = {10.1098/rspa.2021.0618},
	abstract = {Model-based reinforcement learning (MBRL) is believed to have much higher sample efficiency compared with model-free algorithms by learning a predictive model of the environment. However, the performance of MBRL highly relies on the quality of the learned model, which is usually built in a black-box manner and may have poor predictive accuracy outside of the data distribution. The deficiencies of the learned model may prevent the policy from being fully optimized. Although some uncertainty analysis-based remedies have been proposed to alleviate this issue, model bias still poses a great challenge for MBRL. In this work, we propose to leverage the prior knowledge of underlying physics of the environment, where the governing laws are (partially) known. In particular, we developed a physics-informed MBRL framework, where governing equations and physical constraints are used to inform the model learning and policy search. By incorporating the prior information of the environment, the quality of the learned model can be notably improved, while the required interactions with the environment are significantly reduced, leading to better sample efficiency and learning performance. The effectiveness and merit have been demonstrated over a handful of classic control problems, where the environments are governed by canonical ordinary/partial differential equations.},
	number = {2255},
	urldate = {2025-01-17},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Liu, Xin-Yang and Wang, Jian-Xun},
	month = nov,
	year = {2021},
	keywords = {Kuramoto–Sivashinsky, flow control, physics-informed neural networks, reinforcement learning},
	pages = {20210618},
}

@misc{takamoto_pdebench_2024,
	title = {{PDEBENCH}: {An} {Extensive} {Benchmark} for {Scientific} {Machine} {Learning}},
	shorttitle = {{PDEBENCH}},
	url = {http://arxiv.org/abs/2210.07182},
	doi = {10.48550/arXiv.2210.07182},
	abstract = {Machine learning-based modeling of physical systems has experienced increased interest in recent years. Despite some impressive progress, there is still a lack of benchmarks for Scientific ML that are easy to use but still challenging and representative of a wide range of problems. We introduce PDEBench, a benchmark suite of time-dependent simulation tasks based on Partial Differential Equations (PDEs). PDEBench comprises both code and data to benchmark the performance of novel machine learning models against both classical numerical simulations and machine learning baselines. Our proposed set of benchmark problems contribute the following unique features: (1) A much wider range of PDEs compared to existing benchmarks, ranging from relatively common examples to more realistic and difficult problems; (2) much larger ready-to-use datasets compared to prior work, comprising multiple simulation runs across a larger number of initial and boundary conditions and PDE parameters; (3) more extensible source codes with user-friendly APIs for data generation and baseline results with popular machine learning models (FNO, U-Net, PINN, Gradient-Based Inverse Method). PDEBench allows researchers to extend the benchmark freely for their own purposes using a standardized API and to compare the performance of new models to existing baseline methods. We also propose new evaluation metrics with the aim to provide a more holistic understanding of learning methods in the context of Scientific ML. With those metrics we identify tasks which are challenging for recent ML methods and propose these tasks as future challenges for the community. The code is available at https://github.com/pdebench/PDEBench.},
	urldate = {2025-01-14},
	publisher = {arXiv},
	author = {Takamoto, Makoto and Praditia, Timothy and Leiteritz, Raphael and MacKinlay, Dan and Alesiani, Francesco and Pflüger, Dirk and Niepert, Mathias},
	month = aug,
	year = {2024},
	note = {arXiv:2210.07182 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Physics - Fluid Dynamics, Physics - Geophysics},
}

@misc{noauthor_unified_nodate,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html},
	urldate = {2025-01-13},
}

@article{chen_algorithms_2023,
	title = {Algorithms to estimate {Shapley} value feature attributions},
	volume = {5},
	copyright = {2023 Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-023-00657-x},
	doi = {10.1038/s42256-023-00657-x},
	abstract = {Feature attributions based on the Shapley value are popular for explaining machine learning models. However, their estimation is complex from both theoretical and computational standpoints. We disentangle this complexity into two main factors: the approach to removing feature information and the tractable estimation strategy. These two factors provide a natural lens through which we can better understand and compare 24 distinct algorithms. Based on the various feature-removal approaches, we describe the multiple types of Shapley value feature attributions and the methods to calculate each one. Then, based on the tractable estimation strategies, we characterize two distinct families of approaches: model-agnostic and model-specific approximations. For the model-agnostic approximations, we benchmark a wide class of estimation approaches and tie them to alternative yet equivalent characterizations of the Shapley value. For the model-specific approximations, we clarify the assumptions crucial to each method’s tractability for linear, tree and deep models. Finally, we identify gaps in the literature and promising future research directions.},
	language = {en},
	number = {6},
	urldate = {2025-01-13},
	journal = {Nature Machine Intelligence},
	author = {Chen, Hugh and Covert, Ian C. and Lundberg, Scott M. and Lee, Su-In},
	month = jun,
	year = {2023},
	keywords = {Applied mathematics, Computer science, Software},
	pages = {590--601},
}

@article{korzhyk_stackelberg_2011,
	title = {Stackelberg vs. {Nash} in {Security} {Games}: {An} {Extended} {Investigation} of {Interchangeability}, {Equivalence}, and {Uniqueness}},
	volume = {41},
	issn = {1076-9757},
	shorttitle = {Stackelberg vs. {Nash} in {Security} {Games}},
	url = {http://arxiv.org/abs/1401.3888},
	doi = {10.1613/jair.3269},
	abstract = {There has been significant recent interest in game-theoretic approaches to security, with much of the recent research focused on utilizing the leader-follower Stackelberg game model. Among the major applications are the ARMOR program deployed at LAX Airport and the IRIS program in use by the US Federal Air Marshals (FAMS). The foundational assumption for using Stackelberg games is that security forces (leaders), acting first, commit to a randomized strategy; while their adversaries (followers) choose their best response after surveillance of this randomized strategy. Yet, in many situations, a leader may face uncertainty about the follower's surveillance capability. Previous work fails to address how a leader should compute her strategy given such uncertainty. We provide five contributions in the context of a general class of security games. First, we show that the Nash equilibria in security games are interchangeable, thus alleviating the equilibrium selection problem. Second, under a natural restriction on security games, any Stackelberg strategy is also a Nash equilibrium strategy; and furthermore, the solution is unique in a class of security games of which ARMOR is a key exemplar. Third, when faced with a follower that can attack multiple targets, many of these properties no longer hold. Fourth, we show experimentally that in most (but not all) games where the restriction does not hold, the Stackelberg strategy is still a Nash equilibrium strategy, but this is no longer true when the attacker can attack multiple targets. Finally, as a possible direction for future research, we propose an extensive-form game model that makes the defender's uncertainty about the attacker's ability to observe explicit.},
	urldate = {2025-01-12},
	journal = {Journal of Artificial Intelligence Research},
	author = {Korzhyk, Dmytro and Yin, Zhengyu and Kiekintveld, Christopher and Conitzer, Vincent and Tambe, Milind},
	month = jun,
	year = {2011},
	note = {arXiv:1401.3888 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory},
	pages = {297--327},
}

@inproceedings{fiez_implicit_2020,
	title = {Implicit {Learning} {Dynamics} in {Stackelberg} {Games}: {Equilibria} {Characterization}, {Convergence} {Analysis}, and {Empirical} {Study}},
	shorttitle = {Implicit {Learning} {Dynamics} in {Stackelberg} {Games}},
	url = {https://proceedings.mlr.press/v119/fiez20a.html},
	abstract = {Contemporary work on learning in continuous games has commonly overlooked the hierarchical decision-making structure present in machine learning problems formulated as games, instead treating them as simultaneous play games and adopting the Nash equilibrium solution concept. We deviate from this paradigm and provide a comprehensive study of learning in Stackelberg games. This work provides insights into the optimization landscape of zero-sum games by establishing connections between Nash and Stackelberg equilibria along with the limit points of simultaneous gradient descent. We derive novel gradient-based learning dynamics emulating the natural structure of a Stackelberg game using the implicit function theorem and provide convergence analysis for deterministic and stochastic updates for zero-sum and general-sum games. Notably, in zero-sum games using deterministic updates, we show the only critical points the dynamics converge to are Stackelberg equilibria and provide a local convergence rate. Empirically, our learning dynamics mitigate rotational behavior and exhibit benefits for training generative adversarial networks compared to simultaneous gradient descent.},
	language = {en},
	urldate = {2025-01-12},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Fiez, Tanner and Chasnov, Benjamin and Ratliff, Lillian},
	month = nov,
	year = {2020},
	pages = {3133--3144},
}

@article{gin_deepgreen_2021,
	title = {{DeepGreen}: deep learning of {Green}’s functions for nonlinear boundary value problems},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	shorttitle = {{DeepGreen}},
	url = {https://www.nature.com/articles/s41598-021-00773-x},
	doi = {10.1038/s41598-021-00773-x},
	abstract = {Boundary value problems (BVPs) play a central role in the mathematical analysis of constrained physical systems subjected to external forces. Consequently, BVPs frequently emerge in nearly every engineering discipline and span problem domains including fluid mechanics, electromagnetics, quantum mechanics, and elasticity. The fundamental solution, or Green’s function, is a leading method for solving linear BVPs that enables facile computation of new solutions to systems under any external forcing. However, fundamental Green’s function solutions for nonlinear BVPs are not feasible since linear superposition no longer holds. In this work, we propose a flexible deep learning approach to solve nonlinear BVPs using a dual-autoencoder architecture. The autoencoders discover an invertible coordinate transform that linearizes the nonlinear BVP and identifies both a linear operator L and Green’s function G which can be used to solve new nonlinear BVPs. We find that the method succeeds on a variety of nonlinear systems including nonlinear Helmholtz and Sturm–Liouville problems, nonlinear elasticity, and a 2D nonlinear Poisson equation and can solve nonlinear BVPs at orders of magnitude faster than traditional methods without the need for an initial guess. The method merges the strengths of the universal approximation capabilities of deep learning with the physics knowledge of Green’s functions to yield a flexible tool for identifying fundamental solutions to a variety of nonlinear systems.},
	language = {en},
	number = {1},
	urldate = {2025-01-09},
	journal = {Scientific Reports},
	author = {Gin, Craig R. and Shea, Daniel E. and Brunton, Steven L. and Kutz, J. Nathan},
	month = nov,
	year = {2021},
	keywords = {Applied mathematics, Computational science, Engineering, Mathematics and computing, Software},
	pages = {21614},
}

@misc{wu_active_2021,
	title = {Active {Learning} for {Graph} {Neural} {Networks} via {Node} {Feature} {Propagation}},
	url = {http://arxiv.org/abs/1910.07567},
	doi = {10.48550/arXiv.1910.07567},
	abstract = {Graph Neural Networks (GNNs) for prediction tasks like node classification or edge prediction have received increasing attention in recent machine learning from graphically structured data. However, a large quantity of labeled graphs is difficult to obtain, which significantly limits the true success of GNNs. Although active learning has been widely studied for addressing label-sparse issues with other data types like text, images, etc., how to make it effective over graphs is an open question for research. In this paper, we present an investigation on active learning with GNNs for node classification tasks. Specifically, we propose a new method, which uses node feature propagation followed by K-Medoids clustering of the nodes for instance selection in active learning. With a theoretical bound analysis we justify the design choice of our approach. In our experiments on four benchmark datasets, the proposed method outperforms other representative baseline methods consistently and significantly.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Wu, Yuexin and Xu, Yichong and Singh, Aarti and Yang, Yiming and Dubrawski, Artur},
	month = nov,
	year = {2021},
	note = {arXiv:1910.07567 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{belghazi_mutual_2018,
	title = {Mutual {Information} {Neural} {Estimation}},
	url = {https://proceedings.mlr.press/v80/belghazi18a.html},
	abstract = {We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement the Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.},
	language = {en},
	urldate = {2025-01-07},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
	month = jul,
	year = {2018},
	pages = {531--540},
}

@inproceedings{rhodes_telescoping_2020,
	title = {Telescoping {Density}-{Ratio} {Estimation}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/33d3b157ddc0896addfb22fa2a519097-Abstract.html},
	abstract = {Density-ratio estimation via classification is a cornerstone of unsupervised learning. It has provided the foundation for state-of-the-art methods in representation learning and generative modelling, with the number of use-cases continuing to proliferate. However, it suffers from a critical limitation: it fails to accurately estimate ratios p/q for which the two densities differ significantly. Empirically, we find this occurs whenever the KL divergence between p and q exceeds tens of nats. To resolve this limitation, we introduce a new framework, telescoping density-ratio estimation (TRE), that enables the estimation of ratios between highly dissimilar densities in high-dimensional spaces. Our experiments demonstrate that TRE can yield substantial improvements over existing single-ratio methods for mutual information estimation, representation learning and energy-based modelling.},
	urldate = {2025-01-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rhodes, Benjamin and Xu, Kai and Gutmann, Michael U.},
	year = {2020},
	pages = {4905--4916},
}

@article{wirthl_global_2023,
	title = {Global sensitivity analysis based on {Gaussian}-process metamodelling for complex biomechanical problems},
	volume = {39},
	issn = {2040-7939, 2040-7947},
	url = {http://arxiv.org/abs/2202.01503},
	doi = {10.1002/cnm.3675},
	abstract = {Biomechanical models often need to describe very complex systems, organs or diseases, and hence also include a large number of parameters. One of the attractive features of physics-based models is that in those models (most) parameters have a clear physical meaning. Nevertheless, the determination of these parameters is often very elaborate and costly and shows a large scatter within the population. Hence, it is essential to identify the most important parameter for a particular problem at hand. In order to distinguish parameters which have a significant influence on a specific model output from non-influential parameters, we use sensitivity analysis, in particular the Sobol method as a global variance-based method. However, the Sobol method requires a large number of model evaluations, which is prohibitive for computationally expensive models. We therefore employ Gaussian processes as a metamodel for the underlying full model. Metamodelling introduces further uncertainty, which we also quantify. We demonstrate the approach by applying it to two different problems: nanoparticle-mediated drug delivery in a multiphase tumour-growth model, and arterial growth and remodelling. Even relatively small numbers of evaluations of the full model suffice to identify the influential parameters in both cases and to separate them from non-influential parameters. The approach also allows the quantification of higher-order interaction effects. We thus show that a variance-based global sensitivity analysis is feasible for computationally expensive biomechanical models. Different aspects of sensitivity analysis are covered including a transparent declaration of the uncertainties involved in the estimation process. Such a global sensitivity analysis not only helps to massively reduce costs for experimental determination of parameters but is also highly beneficial for inverse analysis of such complex models.},
	number = {3},
	urldate = {2025-01-07},
	journal = {International Journal for Numerical Methods in Biomedical Engineering},
	author = {Wirthl, Barbara and Brandstaeter, Sebastian and Nitzler, Jonas and Schrefler, Bernhard A. and Wall, Wolfgang A.},
	month = mar,
	year = {2023},
	note = {arXiv:2202.01503 [cs]},
	keywords = {Computer Science - Computational Engineering, Finance, and Science},
	pages = {e3675},
}

@article{owen_sobol_2014,
	title = {Sobol' {Indices} and {Shapley} {Value}},
	volume = {2},
	url = {https://epubs.siam.org/doi/10.1137/130936233},
	doi = {10.1137/130936233},
	abstract = {This paper introduces generalized Sobol' indices, compares strategies for their estimation, and makes a systematic search for efficient estimators. Of particular interest are contrasts, sums of squares, and indices of bilinear form which allow a reduced number of function evaluations compared to alternatives. The bilinear framework includes some efficient estimators from Saltelli [Comput. Phys. Comm., 145 (2002), pp. 280--297] and Mauntz [Global Sensitivity Analysis of General Nonlinear Systems, Master's thesis, Imperial College, London, 2002] as well as some new estimators for specific variance components and mean dimensions. This paper also provides a bias corrected version of the estimator of Janon et al. [Asymptotic Normality and Efficiency of Two Sobol' Index Estimators, technical report, INRIA, Rocquencourt, France] and extends the bias correction to generalized Sobol' indices. Some numerical comparisons are given.},
	number = {1},
	urldate = {2025-01-07},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Owen, Art B.},
	month = jan,
	year = {2014},
	pages = {245--251},
}

@inproceedings{lundberg_unified_2017,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	urldate = {2025-01-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lundberg, Scott M and Lee, Su-In},
	year = {2017},
}

@article{nannapaneni_reliability_2016,
	title = {Reliability analysis under epistemic uncertainty},
	volume = {155},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832016301119},
	doi = {10.1016/j.ress.2016.06.005},
	abstract = {This paper proposes a probabilistic framework to include both aleatory and epistemic uncertainty within model-based reliability estimation of engineering systems for individual limit states. Epistemic uncertainty is considered due to both data and model sources. Sparse point and/or interval data regarding the input random variables leads to uncertainty regarding their distribution types, distribution parameters, and correlations; this statistical uncertainty is included in the reliability analysis through a combination of likelihood-based representation, Bayesian hypothesis testing, and Bayesian model averaging techniques. Model errors, which include numerical solution errors and model form errors, are quantified through Gaussian process models and included in the reliability analysis. The probability integral transform is used to develop an auxiliary variable approach that facilitates a single-level representation of both aleatory and epistemic uncertainty. This strategy results in an efficient single-loop implementation of Monte Carlo simulation (MCS) and FORM/SORM techniques for reliability estimation under both aleatory and epistemic uncertainty. Two engineering examples are used to demonstrate the proposed methodology.},
	urldate = {2025-01-03},
	journal = {Reliability Engineering \& System Safety},
	author = {Nannapaneni, Saideep and Mahadevan, Sankaran},
	month = nov,
	year = {2016},
	keywords = {Auxiliary variable, Correlation, Epistemic uncertainty, First Order Reliability Method, Monte Carlo simulation, Reliability},
	pages = {9--20},
}

@article{chen_deepkriging_2022,
	title = {{DeepKriging}: {Spatially} {Dependent} {Deep} {Neural} {Networks} for {Spatial} {Prediction}},
	volume = {34},
	url = {https://www3.stat.sinica.edu.tw/statistica/J34N1/J34N113/J34N113.html},
	abstract = {In spatial statistics, a common objective is to predict values of a spatial process at unobserved locations by exploiting spatial dependence. Kriging provides the best linear unbiased predictor using covariance functions, and is often associated with Gaussian processes. However, for nonlinear predictions for nonGaussian and categorical data, the Kriging prediction is no longer optimal, and the associated variance is often overly optimistic. Although deep neural networks (DNNs) are widely used for general classification and prediction, they have not been studied thoroughly for data with spatial dependence. In this work, we propose a novel DNN structure for spatial prediction, where we capture the spatial dependence by adding an embedding layer of spatial coordinates with basis functions. We show in theory and simulation studies that the proposed DeepKriging method has a direct link to Kriging in the Gaussian case, and has multiple advantages over Kriging for nonGaussian and nonstationary data. That is, it provides nonlinear predictions, and thus has smaller approximation errors. Furthermore, it does not require operations on covariance matrices, and thus is scalable for large data sets. With sufficiently many hidden neurons, the proposed method provides an optimal prediction in terms of model capacity. In addition, we quantify prediction uncertainties based on density prediction, without assuming a data distribution. Finally, we apply the method to PM2.5 concentrations across the continental United States.},
	urldate = {2024-10-02},
	journal = {Statistica Sinica},
	author = {Chen, Wanfang and Li, Yuxiao and Reich, Brian J. and Sun, Ying},
	year = {2022},
	pages = {291--311},
}

@misc{lee_deep_2018,
	title = {Deep {Neural} {Networks} as {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1711.00165},
	doi = {10.48550/arXiv.1711.00165},
	abstract = {It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks.},
	urldate = {2025-01-02},
	publisher = {arXiv},
	author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
	month = mar,
	year = {2018},
	note = {arXiv:1711.00165 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{wendland_piecewise_1995,
	title = {Piecewise polynomial, positive definite and compactly supported radial functions of minimal degree},
	volume = {4},
	issn = {1572-9044},
	url = {https://doi.org/10.1007/BF02123482},
	doi = {10.1007/BF02123482},
	abstract = {We construct a new class of positive definite and compactly supported radial functions which consist of a univariate polynomial within their support. For given smoothness and space dimension it is proved that they are of minimal degree and unique up to a constant factor. Finally, we establish connections between already known functions of this kind.},
	language = {en},
	number = {1},
	urldate = {2025-01-02},
	journal = {Advances in Computational Mathematics},
	author = {Wendland, Holger},
	month = dec,
	year = {1995},
	keywords = {41A05, 41A15, 41A30, 41A63, 65D07, 65D10, B-splines, multivariate interpolation, positive definite functions},
	pages = {389--396},
}

@article{qiu_efficient_2024,
	title = {Efficient {Multimodal} {Sampling} via {Tempered} {Distribution} {Flow}},
	volume = {119},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2023.2198059},
	doi = {10.1080/01621459.2023.2198059},
	abstract = {Sampling from high-dimensional distributions is a fundamental problem in statistical research and practice. However, great challenges emerge when the target density function is unnormalized and contains isolated modes. We tackle this difficulty by fitting an invertible transformation mapping, called a transport map, between a reference probability measure and the target distribution, so that sampling from the target distribution can be achieved by pushing forward a reference sample through the transport map. We theoretically analyze the limitations of existing transport-based sampling methods using the Wasserstein gradient flow theory, and propose a new method called TemperFlow that addresses the multimodality issue. TemperFlow adaptively learns a sequence of tempered distributions to progressively approach the target distribution, and we prove that it overcomes the limitations of existing methods. Various experiments demonstrate the superior performance of this novel sampler compared to traditional methods, and we show its applications in modern deep learning tasks such as image generation. The programming code for the numerical experiments is available in the supplementary material.},
	number = {546},
	urldate = {2024-12-31},
	journal = {Journal of the American Statistical Association},
	author = {Qiu, Yixuan and Wang, Xiao},
	month = apr,
	year = {2024},
	keywords = {Deep neural network, Gradient flow, Markov chain Monte Carlo, Normalizing flow, Parallel tempering},
	pages = {1446--1460},
}

@inproceedings{asi_robustness_2023,
	title = {From {Robustness} to {Privacy} and {Back}},
	url = {https://proceedings.mlr.press/v202/asi23b.html},
	abstract = {We study the relationship between two desiderata of algorithms in statistical inference and machine learning—differential privacy and robustness to adversarial data corruptions. Their conceptual similarity was first observed by Dwork and Lei (STOC 2009), who observed that private algorithms satisfy robustness, and gave a general method for converting robust algorithms to private ones. However, all general methods for transforming robust algorithms into private ones lead to suboptimal error rates. Our work gives the first black-box transformation that converts any adversarially robust algorithm into one that satisfies pure differential privacy. Moreover, we show that for any low-dimensional estimation task, applying our transformation to an optimal robust estimator results in an optimal private estimator. Thus, we conclude that for any low-dimensional task, the optimal error rate for 𝜀ε{\textbackslash}varepsilon-differentially private estimators is essentially the same as the optimal error rate for estimators that are robust to adversarially corrupting 1/𝜀1/ε1/{\textbackslash}varepsilon training samples. We apply our transformation to obtain new optimal private estimators for several high-dimensional statistical tasks, including Gaussian linear regression and PCA. Finally, we present an extension of our transformation that leads to approximately differentially private algorithms whose error does not depend on the range of the output space, which is impossible under pure differential privacy.},
	language = {en},
	urldate = {2024-12-30},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Asi, Hilal and Ullman, Jonathan and Zakynthinou, Lydia},
	month = jul,
	year = {2023},
	pages = {1121--1146},
}

@inproceedings{asi_robustness_2023-1,
	address = {Honolulu, Hawaii, USA},
	series = {{ICML}'23},
	title = {From robustness to privacy and back},
	volume = {202},
	abstract = {We study the relationship between two desiderata of algorithms in statistical inference and machine learning--differential privacy and robustness to adversarial data corruptions. Their conceptual similarity was first observed by Dwork and Lei (STOC 2009), who observed that private algorithms satisfy robustness, and gave a general method for converting robust algorithms to private ones. However, all general methods for transforming robust algorithms into private ones lead to suboptimal error rates. Our work gives the first black-box transformation that converts any adversarially robust algorithm into one that satisfies pure differential privacy. Moreover, we show that for any low-dimensional estimation task, applying our transformation to an optimal robust estimator results in an optimal private estimator. Thus, we conclude that for any low-dimensional task, the optimal error rate for ε-differentially private estimators is essentially the same as the optimal error rate for estimators that are robust to adversarially corrupting 1/ε training samples. We apply our transformation to obtain new optimal private estimators for several high-dimensional tasks, including Gaussian (sparse) linear regression and PCA. Finally, we present an extension of our transformation that leads to approximate differentially private algorithms whose error does not depend on the range of the output space, which is impossible under pure differential privacy.},
	urldate = {2024-12-30},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {JMLR.org},
	author = {Asi, Hilal and Ullman, Jonathan and Zakynthinou, Lydia},
	month = jul,
	year = {2023},
	pages = {1121--1146},
}

@article{ginebra_measure_2007,
	title = {On the measure of the information in a statistical experiment},
	volume = {2},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-2/issue-1/On-the-measure-of-the-information-in-a-statistical-experiment/10.1214/07-BA207.full},
	doi = {10.1214/07-BA207},
	abstract = {Setting aside experimental costs, the choice of an experiment is usually formulated in terms of the maximization of a measure of information, often presented as an optimality design criterion. However, there does not seem to be a universal agreement on what objects can qualify as a valid measure of the information in an experiment. In this article we explicitly state a minimal set of requirements that must be satisfied by all such measures. Under that framework, the measure of the information in an experiment is equivalent to the measure of the variability of its likelihood ratio statistics or which is the same, it is equivalent to the measure of the variability of its posterior to prior ratio statistics and to the measure of the variability of the distribution of the posterior distributions yielded by it. The larger that variability, the more peaked the likelihood functions and posterior distributions that tend to be yielded by the experiment, and the more informative the experiment is. By going through various measures of variability, this paper uncovers the unifying link underlying well known information measures as well as information measures that are not yet recognized as such. The measure of the information in an experiment is then related to the measure of the information in a given observation from it. In this framework, the choice of experiment based on statistical merit only, is posed as a decision problem where the reward is a likelihood ratio or posterior distribution, the utility function is convex, the utility of the reward is the information observed, and the expected utility is the information in an experiment. Finally, the information in an experiment is linked to the information and to the uncertainty in a probability distribution, and we find that the measure of the information in an experiment is not always interpretable as the uncertainty in the prior minus the expected uncertainty in the posterior.},
	number = {1},
	urldate = {2024-12-30},
	journal = {Bayesian Analysis},
	author = {Ginebra, Josep},
	month = mar,
	year = {2007},
	keywords = {Convex ordering, Database Expansion Item, Design of experiments, Hellinger transform, Measure of association, Utility, divergence measure, likelihood ratio, location parameter, measure of diversity, measure of surprise, mutual information, optimal design, posterior to prior ratio, reference prior, stochastic ordering, sufficiency, uncertainty, value of information},
	pages = {167--211},
}

@inproceedings{rahimi_random_2007,
	title = {Random {Features} for {Large}-{Scale} {Kernel} {Machines}},
	volume = {20},
	url = {https://papers.nips.cc/paper_files/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html},
	abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user speciﬁed shift- invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classiﬁcation and regression tasks linear machine learning al- gorithms applied to these features outperform state-of-the-art large-scale kernel machines.},
	urldate = {2024-12-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rahimi, Ali and Recht, Benjamin},
	year = {2007},
}

@misc{noauthor_random_nodate,
	title = {Random {Features} for {Large}-{Scale} {Kernel} {Machines}},
	url = {https://papers.nips.cc/paper_files/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html},
	urldate = {2024-12-27},
}

@article{kast_positional_2024,
	title = {Positional embeddings for solving {PDEs} with evolutional deep neural networks},
	volume = {508},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999124002353},
	doi = {10.1016/j.jcp.2024.112986},
	abstract = {This work extends the paradigm of evolutional deep neural networks (EDNNs) to solving parametric time-dependent partial differential equations (PDEs) on domains with geometric structure. By introducing positional embeddings based on eigenfunctions of the Laplace-Beltrami operator, geometric properties are encoded intrinsically and Dirichlet, Neumann and periodic boundary conditions of the PDE solution are enforced directly through the neural network architecture. The proposed embeddings lead to improved error convergence for static PDEs and extend EDNNs towards computational domains of realistic complexity. Several steps are taken to improve performance of EDNNs: Solving the EDNN update equation with a Krylov solver avoids the explicit assembly of Jacobians and enables scaling to larger neural networks. Computational efficiency is further improved by an ad-hoc active sampling scheme that uses the PDE dynamics to effectively sample collocation points. A modified linearly implicit Rosenbrock method is proposed to alleviate the time step requirements of stiff PDEs. Lastly, a completely training-free approach, which automatically enforces initial conditions and only requires time integration, is compared against EDNNs that are trained on the initial conditions. We report results for the Korteweg-de Vries equation, a nonlinear heat equation and (nonlinear) advection-diffusion problems on domains with and without holes and various boundary conditions, to demonstrate the effectiveness of the method. The numerical results highlight EDNNs as a promising surrogate model for parametrized PDEs with slow decaying Kolmogorov n-width.},
	urldate = {2024-12-27},
	journal = {Journal of Computational Physics},
	author = {Kast, Mariella and Hesthaven, Jan S.},
	month = jul,
	year = {2024},
	keywords = {Many query applications, Partial differential equations, Reduced order models, Scientific machine learning},
	pages = {112986},
}

@misc{wilson_deep_2015,
	title = {Deep {Kernel} {Learning}},
	url = {http://arxiv.org/abs/1511.02222},
	doi = {10.48550/arXiv.1511.02222},
	abstract = {We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods. Specifically, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with benefits in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost \$O(n)\$ for \$n\$ training points, and predictions cost \$O(1)\$ per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with flexible kernel learning models, and stand-alone deep architectures.},
	urldate = {2024-12-27},
	publisher = {arXiv},
	author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
	month = nov,
	year = {2015},
	note = {arXiv:1511.02222 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@inproceedings{rahimi_random_2007-1,
	title = {Random {Features} for {Large}-{Scale} {Kernel} {Machines}},
	volume = {20},
	url = {https://papers.nips.cc/paper_files/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html},
	abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user speciﬁed shift- invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classiﬁcation and regression tasks linear machine learning al- gorithms applied to these features outperform state-of-the-art large-scale kernel machines.},
	urldate = {2024-12-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rahimi, Ali and Recht, Benjamin},
	year = {2007},
}

@misc{izmailov_semi-supervised_2019,
	title = {Semi-{Supervised} {Learning} with {Normalizing} {Flows}},
	url = {http://arxiv.org/abs/1912.13025},
	doi = {10.48550/arXiv.1912.13025},
	abstract = {Normalizing flows transform a latent distribution through an invertible neural network for a flexible and pleasingly simple approach to generative modelling, while preserving an exact likelihood. We propose FlowGMM, an end-to-end approach to generative semi supervised learning with normalizing flows, using a latent Gaussian mixture model. FlowGMM is distinct in its simplicity, unified treatment of labelled and unlabelled data with an exact likelihood, interpretability, and broad applicability beyond image data. We show promising results on a wide range of applications, including AG-News and Yahoo Answers text data, tabular data, and semi-supervised image classification. We also show that FlowGMM can discover interpretable structure, provide real-time optimization-free feature visualizations, and specify well calibrated predictive distributions.},
	urldate = {2024-12-26},
	publisher = {arXiv},
	author = {Izmailov, Pavel and Kirichenko, Polina and Finzi, Marc and Wilson, Andrew Gordon},
	month = dec,
	year = {2019},
	note = {arXiv:1912.13025 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{arora_survey_2020,
	title = {A {Survey} of {Inverse} {Reinforcement} {Learning}: {Challenges}, {Methods} and {Progress}},
	shorttitle = {A {Survey} of {Inverse} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1806.06877},
	doi = {10.48550/arXiv.1806.06877},
	abstract = {Inverse reinforcement learning (IRL) is the problem of inferring the reward function of an agent, given its policy or observed behavior. Analogous to RL, IRL is perceived both as a problem and as a class of methods. By categorically surveying the current literature in IRL, this article serves as a reference for researchers and practitioners of machine learning and beyond to understand the challenges of IRL and select the approaches best suited for the problem on hand. The survey formally introduces the IRL problem along with its central challenges such as the difficulty in performing accurate inference and its generalizability, its sensitivity to prior knowledge, and the disproportionate growth in solution complexity with problem size. The article elaborates how the current methods mitigate these challenges. We further discuss the extensions to traditional IRL methods for handling: inaccurate and incomplete perception, an incomplete model, multiple reward functions, and nonlinear reward functions. This survey concludes the discussion with some broad advances in the research area and currently open research questions.},
	urldate = {2024-12-24},
	publisher = {arXiv},
	author = {Arora, Saurabh and Doshi, Prashant},
	month = nov,
	year = {2020},
	note = {arXiv:1806.06877 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_certified_nodate,
	title = {Certified {Multifidelity} {Zeroth}-{Order} {Optimization}},
	url = {https://epubs.siam.org/doi/epdf/10.1137/23M1591086},
	language = {en},
	urldate = {2024-12-24},
	doi = {10.1137/23M1591086},
}

@misc{farquhar_towards_2019,
	title = {Towards {Robust} {Evaluations} of {Continual} {Learning}},
	url = {http://arxiv.org/abs/1805.09733},
	doi = {10.48550/arXiv.1805.09733},
	abstract = {Experiments used in current continual learning research do not faithfully assess fundamental challenges of learning continually. Instead of assessing performance on challenging and representative experiment designs, recent research has focused on increased dataset difficulty, while still using flawed experiment set-ups. We examine standard evaluations and show why these evaluations make some continual learning approaches look better than they are. We introduce desiderata for continual learning evaluations and explain why their absence creates misleading comparisons. Based on our desiderata we then propose new experiment designs which we demonstrate with various continual learning approaches and datasets. Our analysis calls for a reprioritization of research effort by the community.},
	urldate = {2024-12-24},
	publisher = {arXiv},
	author = {Farquhar, Sebastian and Gal, Yarin},
	month = jun,
	year = {2019},
	note = {arXiv:1805.09733 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{bansal_cold_2022,
	title = {Cold {Diffusion}: {Inverting} {Arbitrary} {Image} {Transforms} {Without} {Noise}},
	shorttitle = {Cold {Diffusion}},
	url = {http://arxiv.org/abs/2208.09392},
	doi = {10.48550/arXiv.2208.09392},
	abstract = {Standard diffusion models involve an image transform -- adding Gaussian noise -- and an image restoration operator that inverts this degradation. We observe that the generative behavior of diffusion models is not strongly dependent on the choice of image degradation, and in fact an entire family of generative models can be constructed by varying this choice. Even when using completely deterministic degradations (e.g., blur, masking, and more), the training and test-time update rules that underlie diffusion models can be easily generalized to create generative models. The success of these fully deterministic models calls into question the community's understanding of diffusion models, which relies on noise in either gradient Langevin dynamics or variational inference, and paves the way for generalized diffusion models that invert arbitrary processes. Our code is available at https://github.com/arpitbansal297/Cold-Diffusion-Models},
	urldate = {2024-12-23},
	publisher = {arXiv},
	author = {Bansal, Arpit and Borgnia, Eitan and Chu, Hong-Min and Li, Jie S. and Kazemi, Hamid and Huang, Furong and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
	month = aug,
	year = {2022},
	note = {arXiv:2208.09392 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{noauthor_220809392_nodate,
	title = {[2208.09392] {Cold} {Diffusion}: {Inverting} {Arbitrary} {Image} {Transforms} {Without} {Noise}},
	url = {https://arxiv.org/abs/2208.09392},
	urldate = {2024-12-23},
}

@misc{nguyen_efficient_2023,
	title = {Efficient {Training} of {Deep} {Equilibrium} {Models}},
	url = {http://arxiv.org/abs/2304.11663},
	doi = {10.48550/arXiv.2304.11663},
	abstract = {Deep equilibrium models (DEQs) have proven to be very powerful for learning data representations. The idea is to replace traditional (explicit) feedforward neural networks with an implicit fixed-point equation, which allows to decouple the forward and backward passes. In particular, training DEQ layers becomes very memory-efficient via the implicit function theorem. However, backpropagation through DEQ layers still requires solving an expensive Jacobian-based equation. In this paper, we introduce a simple but effective strategy to avoid this computational burden. Our method relies on the Jacobian approximation of Broyden's method after the forward pass to compute the gradients during the backward pass. Experiments show that simply re-using this approximation can significantly speed up the training while not causing any performance degradation.},
	urldate = {2024-12-18},
	publisher = {arXiv},
	author = {Nguyen, Bac and Mauch, Lukas},
	month = apr,
	year = {2023},
	note = {arXiv:2304.11663 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{lee_towards_2023,
	title = {Towards a {Rigorous} {Analysis} of {Mutual} {Information} in {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2308.15704},
	doi = {10.48550/arXiv.2308.15704},
	abstract = {Contrastive learning has emerged as a cornerstone in recent achievements of unsupervised representation learning. Its primary paradigm involves an instance discrimination task with a mutual information loss. The loss is known as InfoNCE and it has yielded vital insights into contrastive learning through the lens of mutual information analysis. However, the estimation of mutual information can prove challenging, creating a gap between the elegance of its mathematical foundation and the complexity of its estimation. As a result, drawing rigorous insights or conclusions from mutual information analysis becomes intricate. In this study, we introduce three novel methods and a few related theorems, aimed at enhancing the rigor of mutual information analysis. Despite their simplicity, these methods can carry substantial utility. Leveraging these approaches, we reassess three instances of contrastive learning analysis, illustrating their capacity to facilitate deeper comprehension or to rectify pre-existing misconceptions. Specifically, we investigate small batch size, mutual information as a measure, and the InfoMin principle.},
	urldate = {2024-12-18},
	publisher = {arXiv},
	author = {Lee, Kyungeun and Kim, Jaeill and Kang, Suhyun and Rhee, Wonjong},
	month = aug,
	year = {2023},
	note = {arXiv:2308.15704 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{noack_unifying_2024,
	title = {A {Unifying} {Perspective} on {Non}-{Stationary} {Kernels} for {Deeper} {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/2309.10068},
	doi = {10.48550/arXiv.2309.10068},
	abstract = {The Gaussian process (GP) is a popular statistical technique for stochastic function approximation and uncertainty quantification from data. GPs have been adopted into the realm of machine learning in the last two decades because of their superior prediction abilities, especially in data-sparse scenarios, and their inherent ability to provide robust uncertainty estimates. Even so, their performance highly depends on intricate customizations of the core methodology, which often leads to dissatisfaction among practitioners when standard setups and off-the-shelf software tools are being deployed. Arguably the most important building block of a GP is the kernel function which assumes the role of a covariance operator. Stationary kernels of the Mat{\textbackslash}'ern class are used in the vast majority of applied studies; poor prediction performance and unrealistic uncertainty quantification are often the consequences. Non-stationary kernels show improved performance but are rarely used due to their more complicated functional form and the associated effort and expertise needed to define and tune them optimally. In this perspective, we want to help ML practitioners make sense of some of the most common forms of non-stationarity for Gaussian processes. We show a variety of kernels in action using representative datasets, carefully study their properties, and compare their performances. Based on our findings, we propose a new kernel that combines some of the identified advantages of existing kernels.},
	urldate = {2024-12-16},
	publisher = {arXiv},
	author = {Noack, Marcus M. and Luo, Hengrui and Risser, Mark D.},
	month = sep,
	year = {2024},
	note = {arXiv:2309.10068 [stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{yu_training_2020,
	title = {Training {Deep} {Energy}-{Based} {Models} with f-{Divergence} {Minimization}},
	url = {http://arxiv.org/abs/2003.03463},
	doi = {10.48550/arXiv.2003.03463},
	abstract = {Deep energy-based models (EBMs) are very flexible in distribution parametrization but computationally challenging because of the intractable partition function. They are typically trained via maximum likelihood, using contrastive divergence to approximate the gradient of the KL divergence between data and model distribution. While KL divergence has many desirable properties, other f-divergences have shown advantages in training implicit density generative models such as generative adversarial networks. In this paper, we propose a general variational framework termed f-EBM to train EBMs using any desired f-divergence. We introduce a corresponding optimization algorithm and prove its local convergence property with non-linear dynamical systems theory. Experimental results demonstrate the superiority of f-EBM over contrastive divergence, as well as the benefits of training EBMs using f-divergences other than KL.},
	urldate = {2024-12-16},
	publisher = {arXiv},
	author = {Yu, Lantao and Song, Yang and Song, Jiaming and Ermon, Stefano},
	month = jul,
	year = {2020},
	note = {arXiv:2003.03463 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{lecun_energy-based_2007,
	title = {Energy-{Based} {Models}},
	url = {https://direct.mit.edu/books/edited-volume/3826/chapter/125508/Energy-Based-Models},
	doi = {10.7551/mitpress/7443.003.0014},
	language = {en},
	urldate = {2024-12-16},
	author = {LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, Marc'Aurelio and Huang, Fu Jie},
	month = jul,
	year = {2007},
}

@misc{song_how_2021,
	title = {How to {Train} {Your} {Energy}-{Based} {Models}},
	url = {http://arxiv.org/abs/2101.03288},
	doi = {10.48550/arXiv.2101.03288},
	abstract = {Energy-Based Models (EBMs), also known as non-normalized probabilistic models, specify probability density or mass functions up to an unknown normalizing constant. Unlike most other probabilistic models, EBMs do not place a restriction on the tractability of the normalizing constant, thus are more flexible to parameterize and can model a more expressive family of probability distributions. However, the unknown normalizing constant of EBMs makes training particularly difficult. Our goal is to provide a friendly introduction to modern approaches for EBM training. We start by explaining maximum likelihood training with Markov chain Monte Carlo (MCMC), and proceed to elaborate on MCMC-free approaches, including Score Matching (SM) and Noise Constrastive Estimation (NCE). We highlight theoretical connections among these three approaches, and end with a brief survey on alternative training methods, which are still under active research. Our tutorial is targeted at an audience with basic understanding of generative models who want to apply EBMs or start a research project in this direction.},
	urldate = {2024-12-16},
	publisher = {arXiv},
	author = {Song, Yang and Kingma, Diederik P.},
	month = feb,
	year = {2021},
	note = {arXiv:2101.03288 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bruineberg_emperors_2022,
	title = {The {Emperor}'s {New} {Markov} {Blankets}},
	volume = {45},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/emperors-new-markov-blankets/715C589A73DDF861DCF8997271DE0B8C},
	doi = {10.1017/S0140525X21002351},
	abstract = {The free energy principle, an influential framework in computational neuroscience and theoretical neurobiology, starts from the assumption that living systems ensure adaptive exchanges with their environment by minimizing the objective function of variational free energy. Following this premise, it claims to deliver a promising integration of the life sciences. In recent work, Markov blankets, one of the central constructs of the free energy principle, have been applied to resolve debates central to philosophy (such as demarcating the boundaries of the mind). The aim of this paper is twofold. First, we trace the development of Markov blankets starting from their standard application in Bayesian networks, via variational inference, to their use in the literature on active inference. We then identify a persistent confusion in the literature between the formal use of Markov blankets as an epistemic tool for Bayesian inference, and their novel metaphysical use in the free energy framework to demarcate the physical boundary between an agent and its environment. Consequently, we propose to distinguish between “Pearl blankets” to refer to the original epistemic use of Markov blankets and “Friston blankets” to refer to the new metaphysical construct. Second, we use this distinction to critically assess claims resting on the application of Markov blankets to philosophical problems. We suggest that this literature would do well in differentiating between two different research programmes: “inference with a model” and “inference within a model.” Only the latter is capable of doing metaphysical work with Markov blankets, but requires additional philosophical premises and cannot be justified by an appeal to the success of the mathematical framework alone.},
	language = {en},
	urldate = {2024-12-16},
	journal = {Behavioral and Brain Sciences},
	author = {Bruineberg, Jelle and Dołęga, Krzysztof and Dewhurst, Joe and Baltieri, Manuel},
	month = jan,
	year = {2022},
	keywords = {Bayesian inference, Markov blankets, active inference, free energy principle, scientific realism},
	pages = {e183},
}

@misc{yoon_dynamical_2022,
	title = {A dynamical systems based framework for dimension reduction},
	url = {http://arxiv.org/abs/2204.08155},
	doi = {10.48550/arXiv.2204.08155},
	abstract = {We propose a novel framework for learning a low-dimensional representation of data based on nonlinear dynamical systems, which we call dynamical dimension reduction (DDR). In the DDR model, each point is evolved via a nonlinear flow towards a lower-dimensional subspace; the projection onto the subspace gives the low-dimensional embedding. Training the model involves identifying the nonlinear flow and the subspace. Following the equation discovery method, we represent the vector field that defines the flow using a linear combination of dictionary elements, where each element is a pre-specified linear/nonlinear candidate function. A regularization term for the average total kinetic energy is also introduced and motivated by optimal transport theory. We prove that the resulting optimization problem is well-posed and establish several properties of the DDR method. We also show how the DDR method can be trained using a gradient-based optimization method, where the gradients are computed using the adjoint method from optimal control theory. The DDR method is implemented and compared on synthetic and example datasets to other dimension reductions methods, including PCA, t-SNE, and Umap.},
	urldate = {2024-12-16},
	publisher = {arXiv},
	author = {Yoon, Ryeongkyung and Osting, Braxton},
	month = apr,
	year = {2022},
	note = {arXiv:2204.08155 [stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@misc{collaboration_open_2024,
	title = {Open {X}-{Embodiment}: {Robotic} {Learning} {Datasets} and {RT}-{X} {Models}},
	shorttitle = {Open {X}-{Embodiment}},
	url = {http://arxiv.org/abs/2310.08864},
	doi = {10.48550/arXiv.2310.08864},
	abstract = {Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train generalist X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. More details can be found on the project website https://robotics-transformer-x.github.io.},
	urldate = {2024-12-12},
	publisher = {arXiv},
	author = {Collaboration, Open X.-Embodiment and O'Neill, Abby and Rehman, Abdul and Gupta, Abhinav and Maddukuri, Abhiram and Gupta, Abhishek and Padalkar, Abhishek and Lee, Abraham and Pooley, Acorn and Gupta, Agrim and Mandlekar, Ajay and Jain, Ajinkya and Tung, Albert and Bewley, Alex and Herzog, Alex and Irpan, Alex and Khazatsky, Alexander and Rai, Anant and Gupta, Anchit and Wang, Andrew and Kolobov, Andrey and Singh, Anikait and Garg, Animesh and Kembhavi, Aniruddha and Xie, Annie and Brohan, Anthony and Raffin, Antonin and Sharma, Archit and Yavary, Arefeh and Jain, Arhan and Balakrishna, Ashwin and Wahid, Ayzaan and Burgess-Limerick, Ben and Kim, Beomjoon and Schölkopf, Bernhard and Wulfe, Blake and Ichter, Brian and Lu, Cewu and Xu, Charles and Le, Charlotte and Finn, Chelsea and Wang, Chen and Xu, Chenfeng and Chi, Cheng and Huang, Chenguang and Chan, Christine and Agia, Christopher and Pan, Chuer and Fu, Chuyuan and Devin, Coline and Xu, Danfei and Morton, Daniel and Driess, Danny and Chen, Daphne and Pathak, Deepak and Shah, Dhruv and Büchler, Dieter and Jayaraman, Dinesh and Kalashnikov, Dmitry and Sadigh, Dorsa and Johns, Edward and Foster, Ethan and Liu, Fangchen and Ceola, Federico and Xia, Fei and Zhao, Feiyu and Frujeri, Felipe Vieira and Stulp, Freek and Zhou, Gaoyue and Sukhatme, Gaurav S. and Salhotra, Gautam and Yan, Ge and Feng, Gilbert and Schiavi, Giulio and Berseth, Glen and Kahn, Gregory and Yang, Guangwen and Wang, Guanzhi and Su, Hao and Fang, Hao-Shu and Shi, Haochen and Bao, Henghui and Amor, Heni Ben and Christensen, Henrik I. and Furuta, Hiroki and Bharadhwaj, Homanga and Walke, Homer and Fang, Hongjie and Ha, Huy and Mordatch, Igor and Radosavovic, Ilija and Leal, Isabel and Liang, Jacky and Abou-Chakra, Jad and Kim, Jaehyung and Drake, Jaimyn and Peters, Jan and Schneider, Jan and Hsu, Jasmine and Vakil, Jay and Bohg, Jeannette and Bingham, Jeffrey and Wu, Jeffrey and Gao, Jensen and Hu, Jiaheng and Wu, Jiajun and Wu, Jialin and Sun, Jiankai and Luo, Jianlan and Gu, Jiayuan and Tan, Jie and Oh, Jihoon and Wu, Jimmy and Lu, Jingpei and Yang, Jingyun and Malik, Jitendra and Silvério, João and Hejna, Joey and Booher, Jonathan and Tompson, Jonathan and Yang, Jonathan and Salvador, Jordi and Lim, Joseph J. and Han, Junhyek and Wang, Kaiyuan and Rao, Kanishka and Pertsch, Karl and Hausman, Karol and Go, Keegan and Gopalakrishnan, Keerthana and Goldberg, Ken and Byrne, Kendra and Oslund, Kenneth and Kawaharazuka, Kento and Black, Kevin and Lin, Kevin and Zhang, Kevin and Ehsani, Kiana and Lekkala, Kiran and Ellis, Kirsty and Rana, Krishan and Srinivasan, Krishnan and Fang, Kuan and Singh, Kunal Pratap and Zeng, Kuo-Hao and Hatch, Kyle and Hsu, Kyle and Itti, Laurent and Chen, Lawrence Yunliang and Pinto, Lerrel and Fei-Fei, Li and Tan, Liam and Fan, Linxi "Jim" and Ott, Lionel and Lee, Lisa and Weihs, Luca and Chen, Magnum and Lepert, Marion and Memmel, Marius and Tomizuka, Masayoshi and Itkina, Masha and Castro, Mateo Guaman and Spero, Max and Du, Maximilian and Ahn, Michael and Yip, Michael C. and Zhang, Mingtong and Ding, Mingyu and Heo, Minho and Srirama, Mohan Kumar and Sharma, Mohit and Kim, Moo Jin and Kanazawa, Naoaki and Hansen, Nicklas and Heess, Nicolas and Joshi, Nikhil J. and Suenderhauf, Niko and Liu, Ning and Palo, Norman Di and Shafiullah, Nur Muhammad Mahi and Mees, Oier and Kroemer, Oliver and Bastani, Osbert and Sanketi, Pannag R. and Miller, Patrick "Tree" and Yin, Patrick and Wohlhart, Paul and Xu, Peng and Fagan, Peter David and Mitrano, Peter and Sermanet, Pierre and Abbeel, Pieter and Sundaresan, Priya and Chen, Qiuyu and Vuong, Quan and Rafailov, Rafael and Tian, Ran and Doshi, Ria and Mart'in-Mart'in, Roberto and Baijal, Rohan and Scalise, Rosario and Hendrix, Rose and Lin, Roy and Qian, Runjia and Zhang, Ruohan and Mendonca, Russell and Shah, Rutav and Hoque, Ryan and Julian, Ryan and Bustamante, Samuel and Kirmani, Sean and Levine, Sergey and Lin, Shan and Moore, Sherry and Bahl, Shikhar and Dass, Shivin and Sonawani, Shubham and Tulsiani, Shubham and Song, Shuran and Xu, Sichun and Haldar, Siddhant and Karamcheti, Siddharth and Adebola, Simeon and Guist, Simon and Nasiriany, Soroush and Schaal, Stefan and Welker, Stefan and Tian, Stephen and Ramamoorthy, Subramanian and Dasari, Sudeep and Belkhale, Suneel and Park, Sungjae and Nair, Suraj and Mirchandani, Suvir and Osa, Takayuki and Gupta, Tanmay and Harada, Tatsuya and Matsushima, Tatsuya and Xiao, Ted and Kollar, Thomas and Yu, Tianhe and Ding, Tianli and Davchev, Todor and Zhao, Tony Z. and Armstrong, Travis and Darrell, Trevor and Chung, Trinity and Jain, Vidhi and Kumar, Vikash and Vanhoucke, Vincent and Zhan, Wei and Zhou, Wenxuan and Burgard, Wolfram and Chen, Xi and Chen, Xiangyu and Wang, Xiaolong and Zhu, Xinghao and Geng, Xinyang and Liu, Xiyuan and Liangwei, Xu and Li, Xuanlin and Pang, Yansong and Lu, Yao and Ma, Yecheng Jason and Kim, Yejin and Chebotar, Yevgen and Zhou, Yifan and Zhu, Yifeng and Wu, Yilin and Xu, Ying and Wang, Yixuan and Bisk, Yonatan and Dou, Yongqiang and Cho, Yoonyoung and Lee, Youngwoon and Cui, Yuchen and Cao, Yue and Wu, Yueh-Hua and Tang, Yujin and Zhu, Yuke and Zhang, Yunchu and Jiang, Yunfan and Li, Yunshuang and Li, Yunzhu and Iwasawa, Yusuke and Matsuo, Yutaka and Ma, Zehan and Xu, Zhuo and Cui, Zichen Jeff and Zhang, Zichen and Fu, Zipeng and Lin, Zipeng},
	month = jun,
	year = {2024},
	note = {arXiv:2310.08864 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{bahl_neural_2020,
	title = {Neural {Dynamic} {Policies} for {End}-to-{End} {Sensorimotor} {Learning}},
	url = {http://arxiv.org/abs/2012.02788},
	doi = {10.48550/arXiv.2012.02788},
	abstract = {The current dominant paradigm in sensorimotor control, whether imitation or reinforcement learning, is to train policies directly in raw action spaces such as torque, joint angle, or end-effector position. This forces the agent to make decisions individually at each timestep in training, and hence, limits the scalability to continuous, high-dimensional, and long-horizon tasks. In contrast, research in classical robotics has, for a long time, exploited dynamical systems as a policy representation to learn robot behaviors via demonstrations. These techniques, however, lack the flexibility and generalizability provided by deep learning or reinforcement learning and have remained under-explored in such settings. In this work, we begin to close this gap and embed the structure of a dynamical system into deep neural network-based policies by reparameterizing action spaces via second-order differential equations. We propose Neural Dynamic Policies (NDPs) that make predictions in trajectory distribution space as opposed to prior policy learning methods where actions represent the raw control space. The embedded structure allows end-to-end policy learning for both reinforcement and imitation learning setups. We show that NDPs outperform the prior state-of-the-art in terms of either efficiency or performance across several robotic control tasks for both imitation and reinforcement learning setups. Project video and code are available at https://shikharbahl.github.io/neural-dynamic-policies/},
	urldate = {2024-12-12},
	publisher = {arXiv},
	author = {Bahl, Shikhar and Mukadam, Mustafa and Gupta, Abhinav and Pathak, Deepak},
	month = dec,
	year = {2020},
	note = {arXiv:2012.02788 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@article{teh_energy-based_2003,
	title = {Energy-{Based} {Models} for {Sparse} {Overcomplete} {Representations}},
	volume = {4},
	issn = {ISSN 1533-7928},
	url = {https://www.jmlr.org/papers/v4/teh03a.html},
	abstract = {We present a new way of extending independent components analysis
(ICA) to overcomplete representations. In contrast to the causal
generative extensions of ICA which maintain marginal independence of
sources, we define features as deterministic (linear)
functions of the inputs. This assumption results in marginal
dependencies among the features, but conditional
independence of the features given the inputs.  By assigning energies
to the features a probability distribution over the input states is
defined through the Boltzmann distribution. Free parameters of this
model are trained using the contrastive divergence objective (Hinton, 2002).  When the number of features is equal to the number
of input dimensions this energy-based model reduces to noiseless ICA
and we show experimentally that the proposed learning algorithm is
able to perform blind source separation on speech data. In additional
experiments we train overcomplete energy-based models to extract
features from various standard data-sets containing speech, natural
images, hand-written digits and faces.},
	number = {Dec},
	urldate = {2024-12-12},
	journal = {Journal of Machine Learning Research},
	author = {Teh, Yee Whye and Welling, Max and Osindero, Simon and Hinton, Geoffrey E.},
	year = {2003},
	pages = {1235--1260},
}

@misc{farquhar_towards_2019-1,
	title = {Towards {Robust} {Evaluations} of {Continual} {Learning}},
	url = {http://arxiv.org/abs/1805.09733},
	doi = {10.48550/arXiv.1805.09733},
	abstract = {Experiments used in current continual learning research do not faithfully assess fundamental challenges of learning continually. Instead of assessing performance on challenging and representative experiment designs, recent research has focused on increased dataset difficulty, while still using flawed experiment set-ups. We examine standard evaluations and show why these evaluations make some continual learning approaches look better than they are. We introduce desiderata for continual learning evaluations and explain why their absence creates misleading comparisons. Based on our desiderata we then propose new experiment designs which we demonstrate with various continual learning approaches and datasets. Our analysis calls for a reprioritization of research effort by the community.},
	urldate = {2024-12-12},
	publisher = {arXiv},
	author = {Farquhar, Sebastian and Gal, Yarin},
	month = jun,
	year = {2019},
	note = {arXiv:1805.09733 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{singh_attention_2023,
	title = {Attention: {Marginal} {Probability} is {All} {You} {Need}?},
	shorttitle = {Attention},
	url = {http://arxiv.org/abs/2304.04556},
	doi = {10.48550/arXiv.2304.04556},
	abstract = {Attention mechanisms are a central property of cognitive systems allowing them to selectively deploy cognitive resources in a flexible manner. Attention has been long studied in the neurosciences and there are numerous phenomenological models that try to capture its core properties. Recently attentional mechanisms have become a dominating architectural choice of machine learning and are the central innovation of Transformers. The dominant intuition and formalism underlying their development has drawn on ideas of keys and queries in database management systems. In this work, we propose an alternative Bayesian foundation for attentional mechanisms and show how this unifies different attentional architectures in machine learning. This formulation allows to to identify commonality across different attention ML architectures as well as suggest a bridge to those developed in neuroscience. We hope this work will guide more sophisticated intuitions into the key properties of attention architectures and suggest new ones.},
	urldate = {2024-12-10},
	publisher = {arXiv},
	author = {Singh, Ryan and Buckley, Christopher L.},
	month = apr,
	year = {2023},
	note = {arXiv:2304.04556 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{tay_sparse_2020,
	title = {Sparse {Sinkhorn} {Attention}},
	url = {http://arxiv.org/abs/2002.11296},
	doi = {10.48550/arXiv.2002.11296},
	abstract = {We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers.},
	urldate = {2024-12-09},
	publisher = {arXiv},
	author = {Tay, Yi and Bahri, Dara and Yang, Liu and Metzler, Donald and Juan, Da-Cheng},
	month = feb,
	year = {2020},
	note = {arXiv:2002.11296 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{bengio_scheduled_2015,
	title = {Scheduled {Sampling} for {Sequence} {Prediction} with {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1506.03099},
	doi = {10.48550/arXiv.1506.03099},
	abstract = {Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used successfully in our winning entry to the MSCOCO image captioning challenge, 2015.},
	urldate = {2024-12-09},
	publisher = {arXiv},
	author = {Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
	month = sep,
	year = {2015},
	note = {arXiv:1506.03099 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{nikhila_ravi_sam_2024,
	title = {{SAM} 2: {Segment} {Anything} in {Images} and {Videos}},
	doi = {10.48550/arxiv.2408.00714},
	abstract = {We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.},
	journal = {arXiv.org},
	author = {{Nikhila Ravi} and {Valentin Gabeur} and {Yuan-Ting Hu} and {Ronghang Hu} and {Chaitanya K. Ryali} and {Tengyu Ma} and {Haitham Khedr} and {Roman Rädle} and {Chloé Rolland} and {Laura Gustafson} and {Eric Mintun} and {Junting Pan} and {Kalyan Vasudev Alwala} and {Nicolas Carion} and {Chao-Yuan Wu} and {Ross B. Girshick} and {Piotr Doll'ar} and {Christoph Feichtenhofer}},
	year = {2024},
	doi = {10.48550/arxiv.2408.00714},
	note = {ARXIV\_ID: 2408.00714
S2ID: 92a09cdfc19f3f582d89c28c1b4f386299cc69e1},
}

@misc{mohan_what_2024,
	title = {What {You} {See} is {Not} {What} {You} {Get}: {Neural} {Partial} {Differential} {Equations} and {The} {Illusion} of {Learning}},
	shorttitle = {What {You} {See} is {Not} {What} {You} {Get}},
	url = {http://arxiv.org/abs/2411.15101},
	doi = {10.48550/arXiv.2411.15101},
	abstract = {Differentiable Programming for scientific machine learning (SciML) has recently seen considerable interest and success, as it directly embeds neural networks inside PDEs, often called as NeuralPDEs, derived from first principle physics. Therefore, there is a widespread assumption in the community that NeuralPDEs are more trustworthy and generalizable than black box models. However, like any SciML model, differentiable programming relies predominantly on high-quality PDE simulations as "ground truth" for training. However, mathematics dictates that these are only discrete numerical approximations of the true physics. Therefore, we ask: Are NeuralPDEs and differentiable programming models trained on PDE simulations as physically interpretable as we think? In this work, we rigorously attempt to answer these questions, using established ideas from numerical analysis, experiments, and analysis of model Jacobians. Our study shows that NeuralPDEs learn the artifacts in the simulation training data arising from the discretized Taylor Series truncation error of the spatial derivatives. Additionally, NeuralPDE models are systematically biased, and their generalization capability is likely enabled by a fortuitous interplay of numerical dissipation and truncation error in the training dataset and NeuralPDE, which seldom happens in practical applications. This bias manifests aggressively even in relatively accessible 1-D equations, raising concerns about the veracity of differentiable programming on complex, high-dimensional, real-world PDEs, and in dataset integrity of foundation models. Further, we observe that the initial condition constrains the truncation error in initial-value problems in PDEs, thereby exerting limitations to extrapolation. Finally, we demonstrate that an eigenanalysis of model weights can indicate a priori if the model will be inaccurate for out-of-distribution testing.},
	urldate = {2024-12-07},
	publisher = {arXiv},
	author = {Mohan, Arvind and Chattopadhyay, Ashesh and Miller, Jonah},
	month = nov,
	year = {2024},
	note = {arXiv:2411.15101 [cs]},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics},
}

@misc{maddison__2015,
	title = {A* {Sampling}},
	url = {http://arxiv.org/abs/1411.0030},
	doi = {10.48550/arXiv.1411.0030},
	abstract = {The problem of drawing samples from a discrete distribution can be converted into a discrete optimization problem. In this work, we show how sampling from a continuous distribution can be converted into an optimization problem over continuous space. Central to the method is a stochastic process recently described in mathematical statistics that we call the Gumbel process. We present a new construction of the Gumbel process and A* sampling, a practical generic sampling algorithm that searches for the maximum of a Gumbel process using A* search. We analyze the correctness and convergence time of A* sampling and demonstrate empirically that it makes more efficient use of bound and likelihood evaluations than the most closely related adaptive rejection sampling-based algorithms.},
	urldate = {2024-12-07},
	publisher = {arXiv},
	author = {Maddison, Chris J. and Tarlow, Daniel and Minka, Tom},
	month = jan,
	year = {2015},
	note = {arXiv:1411.0030 [stat]},
	keywords = {Statistics - Computation, Statistics - Machine Learning},
}

@article{chen_solar_2024,
	title = {Solar {Imaging} {Data} {Analytics}: {A} {Selective} {Overview} of {Challenges} and {Opportunities}},
	volume = {1},
	issn = {null},
	shorttitle = {Solar {Imaging} {Data} {Analytics}},
	url = {https://doi.org/10.1080/29979676.2024.2391688},
	doi = {10.1080/29979676.2024.2391688},
	abstract = {We give a gentle introduction to solar imaging data, focusing on the challenges and opportunities of data-driven approaches for solar eruptions. We present various solar phenomena prediction problems that might benefit from statistical methods and describe available data products and software packages. State-of-the-art solar eruption forecasting models with data-driven approaches are summarized and discussed. Based on the characteristics of the datasets and state-of-the-art approaches, we point out several promising directions to explore from statistical modeling and computational perspectives.},
	number = {1},
	urldate = {2024-11-28},
	journal = {Statistics and Data Science in Imaging},
	author = {Chen, Yang and Manchester, Ward and Jin, Meng and Pevtsov, Alexei},
	month = jan,
	year = {2024},
	keywords = {Solar eruptions, Space weather, Spatial data, Tensor, Time series},
	pages = {2391688},
}

@inproceedings{zhong_symplectic_2020,
	title = {Symplectic {ODE}-{Net}: {Learning} {Hamiltonian} {Dynamics} with {Control}},
	shorttitle = {Symplectic {ODE}-{Net}},
	url = {https://iclr.cc/virtual_2020/poster_ryxmb1rKDS.html},
	abstract = {In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system, given by an ordinary differential equation (ODE), from observed state trajectories. To achieve better generalization with fewer training samples, SymODEN incorporates appropriate inductive bias by designing the associated computation graph in a physics-informed manner. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way, which can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even when the generalized coordinate data is embedded in a high-dimensional space or we can only access velocity data instead of generalized momentum. This framework, by offering interpretable, physically-consistent models for physical systems, opens up new possibilities for synthesizing model-based control strategies.},
	language = {en},
	urldate = {2024-11-28},
	author = {Zhong, Yaofeng Desmond and Dey, Biswadip and Chakraborty, Amit},
	month = apr,
	year = {2020},
}

@misc{lamb_professor_2016,
	title = {Professor {Forcing}: {A} {New} {Algorithm} for {Training} {Recurrent} {Networks}},
	shorttitle = {Professor {Forcing}},
	url = {http://arxiv.org/abs/1610.09038},
	doi = {10.48550/arXiv.1610.09038},
	abstract = {The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network's own one-step-ahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar.},
	urldate = {2024-11-26},
	publisher = {arXiv},
	author = {Lamb, Alex and Goyal, Anirudh and Zhang, Ying and Zhang, Saizheng and Courville, Aaron and Bengio, Yoshua},
	month = oct,
	year = {2016},
	note = {arXiv:1610.09038},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{bengio_scheduled_2015-1,
	title = {Scheduled {Sampling} for {Sequence} {Prediction} with {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1506.03099},
	doi = {10.48550/arXiv.1506.03099},
	abstract = {Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used successfully in our winning entry to the MSCOCO image captioning challenge, 2015.},
	urldate = {2024-11-26},
	publisher = {arXiv},
	author = {Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
	month = sep,
	year = {2015},
	note = {arXiv:1506.03099},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{ayyubi_progressive_2020,
	title = {Progressive {Growing} of {Neural} {ODEs}},
	url = {http://arxiv.org/abs/2003.03695},
	doi = {10.48550/arXiv.2003.03695},
	abstract = {Neural Ordinary Differential Equations (NODEs) have proven to be a powerful modeling tool for approximating (interpolation) and forecasting (extrapolation) irregularly sampled time series data. However, their performance degrades substantially when applied to real-world data, especially long-term data with complex behaviors (e.g., long-term trend across years, mid-term seasonality across months, and short-term local variation across days). To address the modeling of such complex data with different behaviors at different frequencies (time spans), we propose a novel progressive learning paradigm of NODEs for long-term time series forecasting. Specifically, following the principle of curriculum learning, we gradually increase the complexity of data and network capacity as training progresses. Our experiments with both synthetic data and real traffic data (PeMS Bay Area traffic data) show that our training methodology consistently improves the performance of vanilla NODEs by over 64\%.},
	urldate = {2024-11-26},
	publisher = {arXiv},
	author = {Ayyubi, Hammad A. and Yao, Yi and Divakaran, Ajay},
	month = mar,
	year = {2020},
	note = {arXiv:2003.03695},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{jakeman_pyapprox_2023,
	title = {{PyApprox}: {A} software package for sensitivity analysis, {Bayesian} inference, optimal experimental design, and multi-fidelity uncertainty quantification and surrogate modeling},
	volume = {170},
	issn = {1364-8152},
	shorttitle = {{PyApprox}},
	url = {https://www.sciencedirect.com/science/article/pii/S1364815223002116},
	doi = {10.1016/j.envsoft.2023.105825},
	abstract = {PyApprox is a Python-based one-stop-shop for probabilistic analysis of numerical models such as those used in the earth, environmental and engineering sciences. Easy to use and extendable tools are provided for constructing surrogates, sensitivity analysis, Bayesian inference, experimental design, and forward uncertainty quantification. The algorithms implemented represent a wide range of methods for model analysis developed over the past two decades, including recent advances in multi-fidelity approaches that use multiple model discretizations and/or simplified physics to significantly reduce the computational cost of various types of analyses. An extensive set of Benchmarks from the literature is also provided to facilitate the easy comparison of new or existing algorithms for a wide range of model analyses. This paper introduces PyApprox and its various features, and presents results demonstrating the utility of PyApprox on a benchmark problem modeling the advection of a tracer in groundwater.},
	urldate = {2024-11-25},
	journal = {Environmental Modelling \& Software},
	author = {Jakeman, J. D.},
	month = dec,
	year = {2023},
	keywords = {Bayesian inference, Decision making, Experimental design, Modeling, Multi-fidelity, Sensitivity analysis, Surrogate models, Uncertainty quantification},
	pages = {105825},
}

@misc{zhang_survey_2021,
	title = {A {Survey} on {Negative} {Transfer}},
	url = {http://arxiv.org/abs/2009.00909},
	doi = {10.48550/arXiv.2009.00909},
	abstract = {Transfer learning (TL) utilizes data or knowledge from one or more source domains to facilitate the learning in a target domain. It is particularly useful when the target domain has very few or no labeled data, due to annotation expense, privacy concerns, etc. Unfortunately, the effectiveness of TL is not always guaranteed. Negative transfer (NT), i.e., leveraging source domain data/knowledge undesirably reduces the learning performance in the target domain, has been a long-standing and challenging problem in TL. Various approaches have been proposed in the literature to handle it. However, there does not exist a systematic survey on the formulation of NT, the factors leading to NT, and the algorithms that mitigate NT. This paper fills this gap, by first introducing the definition of NT and its factors, then reviewing about fifty representative approaches for overcoming NT, according to four categories: secure transfer, domain similarity estimation, distant transfer, and NT mitigation. NT in related fields, e.g., multi-task learning, lifelong learning, and adversarial attacks, are also discussed.},
	urldate = {2024-11-22},
	publisher = {arXiv},
	author = {Zhang, Wen and Deng, Lingfei and Zhang, Lei and Wu, Dongrui},
	month = aug,
	year = {2021},
	note = {arXiv:2009.00909},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{kang_how_2024,
	title = {How {Far} is {Video} {Generation} from {World} {Model}: {A} {Physical} {Law} {Perspective}},
	shorttitle = {How {Far} is {Video} {Generation} from {World} {Model}},
	url = {http://arxiv.org/abs/2411.02385},
	doi = {10.48550/arXiv.2411.02385},
	abstract = {OpenAI's Sora highlights the potential of video generation for developing world models that adhere to fundamental physical laws. However, the ability of video generation models to discover such laws purely from visual data without human priors can be questioned. A world model learning the true law should give predictions robust to nuances and correctly extrapolate on unseen scenarios. In this work, we evaluate across three key scenarios: in-distribution, out-of-distribution, and combinatorial generalization. We developed a 2D simulation testbed for object movement and collisions to generate videos deterministically governed by one or more classical mechanics laws. This provides an unlimited supply of data for large-scale experimentation and enables quantitative evaluation of whether the generated videos adhere to physical laws. We trained diffusion-based video generation models to predict object movements based on initial frames. Our scaling experiments show perfect generalization within the distribution, measurable scaling behavior for combinatorial generalization, but failure in out-of-distribution scenarios. Further experiments reveal two key insights about the generalization mechanisms of these models: (1) the models fail to abstract general physical rules and instead exhibit "case-based" generalization behavior, i.e., mimicking the closest training example; (2) when generalizing to new cases, models are observed to prioritize different factors when referencing training data: color {\textgreater} size {\textgreater} velocity {\textgreater} shape. Our study suggests that scaling alone is insufficient for video generation models to uncover fundamental physical laws, despite its role in Sora's broader success. See our project page at https://phyworld.github.io},
	urldate = {2024-11-22},
	publisher = {arXiv},
	author = {Kang, Bingyi and Yue, Yang and Lu, Rui and Lin, Zhijie and Zhao, Yang and Wang, Kaixin and Huang, Gao and Feng, Jiashi},
	month = nov,
	year = {2024},
	note = {arXiv:2411.02385},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@article{ortega_thermodynamics_2013,
	title = {Thermodynamics as a theory of decision-making with information-processing costs},
	volume = {469},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rspa.2012.0683},
	doi = {10.1098/rspa.2012.0683},
	abstract = {Perfectly rational decision-makers maximize expected utility, but crucially ignore the resource costs incurred when determining optimal actions. Here, we propose a thermodynamically inspired formalization of bounded rational decision-making where information processing is modelled as state changes in thermodynamic systems that can be quantified by differences in free energy. By optimizing a free energy, bounded rational decision-makers trade off expected utility gains and information-processing costs measured by the relative entropy. As a result, the bounded rational decision-making problem can be rephrased in terms of well-known variational principles from statistical physics. In the limit when computational costs are ignored, the maximum expected utility principle is recovered. We discuss links to existing decision-making frameworks and applications to human decision-making experiments that are at odds with expected utility theory. Since most of the mathematical machinery can be borrowed from statistical physics, the main contribution is to re-interpret the formalism of thermodynamic free-energy differences in terms of bounded rational decision-making and to discuss its relationship to human decision-making experiments.},
	number = {2153},
	urldate = {2024-11-20},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Ortega, Pedro A. and Braun, Daniel A.},
	month = may,
	year = {2013},
	keywords = {bounded rationality, decision-making, information processing},
	pages = {20120683},
}

@misc{ortega_meta-learning_2019,
	title = {Meta-learning of {Sequential} {Strategies}},
	url = {http://arxiv.org/abs/1905.03030},
	doi = {10.48550/arXiv.1905.03030},
	abstract = {In this report we review memory-based meta-learning as a tool for building sample-efficient strategies that learn from past experience to adapt to any task within a target class. Our goal is to equip the reader with the conceptual foundations of this tool for building new, scalable agents that operate on broad domains. To do so, we present basic algorithmic templates for building near-optimal predictors and reinforcement learners which behave as if they had a probabilistic model that allowed them to efficiently exploit task structure. Furthermore, we recast memory-based meta-learning within a Bayesian framework, showing that the meta-learned strategies are near-optimal because they amortize Bayes-filtered data, where the adaptation is implemented in the memory dynamics as a state-machine of sufficient statistics. Essentially, memory-based meta-learning translates the hard problem of probabilistic sequential inference into a regression problem.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Ortega, Pedro A. and Wang, Jane X. and Rowland, Mark and Genewein, Tim and Kurth-Nelson, Zeb and Pascanu, Razvan and Heess, Nicolas and Veness, Joel and Pritzel, Alex and Sprechmann, Pablo and Jayakumar, Siddhant M. and McGrath, Tom and Miller, Kevin and Azar, Mohammad and Osband, Ian and Rabinowitz, Neil and György, András and Chiappa, Silvia and Osindero, Simon and Teh, Yee Whye and Hasselt, Hado van and Freitas, Nando de and Botvinick, Matthew and Legg, Shane},
	month = jul,
	year = {2019},
	note = {arXiv:1905.03030},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ledoit_nonlinear_2012,
	title = {Nonlinear shrinkage estimation of large-dimensional covariance matrices},
	volume = {40},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-40/issue-2/Nonlinear-shrinkage-estimation-of-large-dimensional-covariance-matrices/10.1214/12-AOS989.full},
	doi = {10.1214/12-AOS989},
	abstract = {Many statistical applications require an estimate of a covariance matrix and/or its inverse. When the matrix dimension is large compared to the sample size, which happens frequently, the sample covariance matrix is known to perform poorly and may suffer from ill-conditioning. There already exists an extensive literature concerning improved estimators in such situations. In the absence of further knowledge about the structure of the true covariance matrix, the most successful approach so far, arguably, has been shrinkage estimation. Shrinking the sample covariance matrix to a multiple of the identity, by taking a weighted average of the two, turns out to be equivalent to linearly shrinking the sample eigenvalues to their grand mean, while retaining the sample eigenvectors. Our paper extends this approach by considering nonlinear transformations of the sample eigenvalues. We show how to construct an estimator that is asymptotically equivalent to an oracle estimator suggested in previous work. As demonstrated in extensive Monte Carlo simulations, the resulting bona fide estimator can result in sizeable improvements over the sample covariance matrix and also over linear shrinkage.},
	number = {2},
	urldate = {2024-11-18},
	journal = {The Annals of Statistics},
	author = {Ledoit, Olivier and Wolf, Michael},
	month = apr,
	year = {2012},
	keywords = {15A52, 62G20, 62H12, Large-dimensional asymptotics, nonlinear shrinkage, rotation equivariance},
	pages = {1024--1060},
}

@article{dohare_loss_2024,
	title = {Loss of plasticity in deep continual learning},
	volume = {632},
	copyright = {2024 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07711-7},
	doi = {10.1038/s41586-024-07711-7},
	abstract = {Artificial neural networks, deep-learning methods and the backpropagation algorithm1 form the foundation of modern machine learning and artificial intelligence. These methods are almost always used in two phases, one in which the weights of the network are updated and one in which the weights are held constant while the network is used or evaluated. This contrasts with natural learning and many applications, which require continual learning. It has been unclear whether or not deep learning methods work in continual learning settings. Here we show that they do not—that standard deep-learning methods gradually lose plasticity in continual-learning settings until they learn no better than a shallow network. We show such loss of plasticity using the classic ImageNet dataset and reinforcement-learning problems across a wide range of variations in the network and the learning algorithm. Plasticity is maintained indefinitely only by algorithms that continually inject diversity into the network, such as our continual backpropagation algorithm, a variation of backpropagation in which a small fraction of less-used units are continually and randomly reinitialized. Our results indicate that methods based on gradient descent are not enough—that sustained deep learning requires a random, non-gradient component to maintain variability and plasticity.},
	language = {en},
	number = {8026},
	urldate = {2024-11-16},
	journal = {Nature},
	author = {Dohare, Shibhansh and Hernandez-Garcia, J. Fernando and Lan, Qingfeng and Rahman, Parash and Mahmood, A. Rupam and Sutton, Richard S.},
	month = aug,
	year = {2024},
	keywords = {Computer science, Human behaviour},
	pages = {768--774},
}

@article{sinha_stackelberg_2018,
	title = {Stackelberg security games: {Looking} beyond a decade of success},
	shorttitle = {Stackelberg security games},
	url = {https://ink.library.smu.edu.sg/sis_research/4792},
	doi = {10.24963/ijcai.2018/775},
	journal = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18),Stockholm, Sweden, July 13-19},
	author = {SINHA, Arunesh and FANG, Fei and AN, Bo and KIEKINTVELD, Christopher and TAMBE, Milind},
	month = jul,
	year = {2018},
	pages = {5494--5501},
}

@misc{huang_flowformer_2022,
	title = {{FlowFormer}: {A} {Transformer} {Architecture} for {Optical} {Flow}},
	shorttitle = {{FlowFormer}},
	url = {http://arxiv.org/abs/2203.16194},
	doi = {10.48550/arXiv.2203.16194},
	abstract = {We introduce optical Flow transFormer, dubbed as FlowFormer, a transformer-based neural network architecture for learning optical flow. FlowFormer tokenizes the 4D cost volume built from an image pair, encodes the cost tokens into a cost memory with alternate-group transformer (AGT) layers in a novel latent space, and decodes the cost memory via a recurrent transformer decoder with dynamic positional cost queries. On the Sintel benchmark, FlowFormer achieves 1.159 and 2.088 average end-point-error (AEPE) on the clean and final pass, a 16.5\% and 15.5\% error reduction from the best published result (1.388 and 2.47). Besides, FlowFormer also achieves strong generalization performance. Without being trained on Sintel, FlowFormer achieves 1.01 AEPE on the clean pass of Sintel training set, outperforming the best published result (1.29) by 21.7\%.},
	urldate = {2024-11-12},
	publisher = {arXiv},
	author = {Huang, Zhaoyang and Shi, Xiaoyu and Zhang, Chao and Wang, Qiang and Cheung, Ka Chun and Qin, Hongwei and Dai, Jifeng and Li, Hongsheng},
	month = sep,
	year = {2022},
	note = {arXiv:2203.16194},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{alfarano_estimating_2024,
	title = {Estimating optical flow: {A} comprehensive review of the state of the art},
	volume = {249},
	issn = {1077-3142},
	shorttitle = {Estimating optical flow},
	url = {https://www.sciencedirect.com/science/article/pii/S1077314224002418},
	doi = {10.1016/j.cviu.2024.104160},
	abstract = {Optical flow estimation is a crucial task in computer vision that provides low-level motion information. Despite recent advances, real-world applications still present significant challenges. This survey provides an overview of optical flow techniques and their application. For a comprehensive review, this survey covers both classical frameworks and the latest AI-based techniques. In doing so, we highlight the limitations of current benchmarks and metrics, underscoring the need for more representative datasets and comprehensive evaluation methods. The survey also highlights the importance of integrating industry knowledge and adopting training practices optimized for deep learning-based models. By addressing these issues, future research can aid the development of robust and efficient optical flow methods that can effectively address real-world scenarios.},
	urldate = {2024-11-12},
	journal = {Computer Vision and Image Understanding},
	author = {Alfarano, Andrea and Maiano, Luca and Papa, Lorenzo and Amerini, Irene},
	month = dec,
	year = {2024},
	keywords = {Computer Vision, Optical flow, motion estimation},
	pages = {104160},
}

@misc{herde_poseidon_2024,
	title = {Poseidon: {Efficient} {Foundation} {Models} for {PDEs}},
	shorttitle = {Poseidon},
	url = {http://arxiv.org/abs/2405.19101},
	doi = {10.48550/arXiv.2405.19101},
	abstract = {We introduce Poseidon, a foundation model for learning the solution operators of PDEs. It is based on a multiscale operator transformer, with time-conditioned layer norms that enable continuous-in-time evaluations. A novel training strategy leveraging the semi-group property of time-dependent PDEs to allow for significant scaling-up of the training data is also proposed. Poseidon is pretrained on a diverse, large scale dataset for the governing equations of fluid dynamics. It is then evaluated on a suite of 15 challenging downstream tasks that include a wide variety of PDE types and operators. We show that Poseidon exhibits excellent performance across the board by outperforming baselines significantly, both in terms of sample efficiency and accuracy. Poseidon also generalizes very well to new physics that is not seen during pretraining. Moreover, Poseidon scales with respect to model and data size, both for pretraining and for downstream tasks. Taken together, our results showcase the surprising ability of Poseidon to learn effective representations from a very small set of PDEs during pretraining in order to generalize well to unseen and unrelated PDEs downstream, demonstrating its potential as an effective, general purpose PDE foundation model. Finally, the Poseidon model as well as underlying pretraining and downstream datasets are open sourced, with code being available at https://github.com/camlab-ethz/poseidon and pretrained models and datasets at https://huggingface.co/camlab-ethz.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Herde, Maximilian and Raonić, Bogdan and Rohner, Tobias and Käppeli, Roger and Molinaro, Roberto and Bézenac, Emmanuel de and Mishra, Siddhartha},
	month = nov,
	year = {2024},
	note = {arXiv:2405.19101},
	keywords = {Computer Science - Machine Learning},
}

@misc{baevski_vq-wav2vec_2020,
	title = {vq-wav2vec: {Self}-{Supervised} {Learning} of {Discrete} {Speech} {Representations}},
	shorttitle = {vq-wav2vec},
	url = {http://arxiv.org/abs/1910.05453},
	doi = {10.48550/arXiv.1910.05453},
	abstract = {We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.},
	urldate = {2024-11-09},
	publisher = {arXiv},
	author = {Baevski, Alexei and Schneider, Steffen and Auli, Michael},
	month = feb,
	year = {2020},
	note = {arXiv:1910.05453},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{oord_neural_2018,
	title = {Neural {Discrete} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1711.00937},
	doi = {10.48550/arXiv.1711.00937},
	abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
	urldate = {2024-11-09},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
	month = may,
	year = {2018},
	note = {arXiv:1711.00937},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{ronneberger_u-net_2015,
	address = {Cham},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	isbn = {9783319245744},
	shorttitle = {U-{Net}},
	doi = {10.1007/978-3-319-24574-4_28},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	year = {2015},
	keywords = {Convolutional Layer, Data Augmentation, Deep Network, Ground Truth Segmentation, Training Image},
	pages = {234--241},
}

@misc{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	url = {http://arxiv.org/abs/1912.01703},
	doi = {10.48550/arXiv.1912.01703},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
	urldate = {2024-11-06},
	publisher = {arXiv},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	month = dec,
	year = {2019},
	note = {arXiv:1912.01703},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Statistics - Machine Learning},
}

@misc{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	doi = {10.48550/arXiv.1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2024-11-06},
	publisher = {arXiv},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv:1502.03167},
	keywords = {Computer Science - Machine Learning},
}

@misc{kang_how_2024-1,
	title = {How {Far} is {Video} {Generation} from {World} {Model}: {A} {Physical} {Law} {Perspective}},
	shorttitle = {How {Far} is {Video} {Generation} from {World} {Model}},
	url = {http://arxiv.org/abs/2411.02385},
	doi = {10.48550/arXiv.2411.02385},
	abstract = {OpenAI's Sora highlights the potential of video generation for developing world models that adhere to fundamental physical laws. However, the ability of video generation models to discover such laws purely from visual data without human priors can be questioned. A world model learning the true law should give predictions robust to nuances and correctly extrapolate on unseen scenarios. In this work, we evaluate across three key scenarios: in-distribution, out-of-distribution, and combinatorial generalization. We developed a 2D simulation testbed for object movement and collisions to generate videos deterministically governed by one or more classical mechanics laws. This provides an unlimited supply of data for large-scale experimentation and enables quantitative evaluation of whether the generated videos adhere to physical laws. We trained diffusion-based video generation models to predict object movements based on initial frames. Our scaling experiments show perfect generalization within the distribution, measurable scaling behavior for combinatorial generalization, but failure in out-of-distribution scenarios. Further experiments reveal two key insights about the generalization mechanisms of these models: (1) the models fail to abstract general physical rules and instead exhibit "case-based" generalization behavior, i.e., mimicking the closest training example; (2) when generalizing to new cases, models are observed to prioritize different factors when referencing training data: color {\textgreater} size {\textgreater} velocity {\textgreater} shape. Our study suggests that scaling alone is insufficient for video generation models to uncover fundamental physical laws, despite its role in Sora's broader success. See our project page at https://phyworld.github.io},
	urldate = {2024-11-06},
	publisher = {arXiv},
	author = {Kang, Bingyi and Yue, Yang and Lu, Rui and Lin, Zhijie and Zhao, Yang and Wang, Kaixin and Huang, Gao and Feng, Jiashi},
	month = nov,
	year = {2024},
	note = {arXiv:2411.02385},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{nguyen_hierarchical_2023,
	title = {Hierarchical {Sliced} {Wasserstein} {Distance}},
	url = {http://arxiv.org/abs/2209.13570},
	doi = {10.48550/arXiv.2209.13570},
	abstract = {Sliced Wasserstein (SW) distance has been widely used in different application scenarios since it can be scaled to a large number of supports without suffering from the curse of dimensionality. The value of sliced Wasserstein distance is the average of transportation cost between one-dimensional representations (projections) of original measures that are obtained by Radon Transform (RT). Despite its efficiency in the number of supports, estimating the sliced Wasserstein requires a relatively large number of projections in high-dimensional settings. Therefore, for applications where the number of supports is relatively small compared with the dimension, e.g., several deep learning applications where the mini-batch approaches are utilized, the complexities from matrix multiplication of Radon Transform become the main computational bottleneck. To address this issue, we propose to derive projections by linearly and randomly combining a smaller number of projections which are named bottleneck projections. We explain the usage of these projections by introducing Hierarchical Radon Transform (HRT) which is constructed by applying Radon Transform variants recursively. We then formulate the approach into a new metric between measures, named Hierarchical Sliced Wasserstein (HSW) distance. By proving the injectivity of HRT, we derive the metricity of HSW. Moreover, we investigate the theoretical properties of HSW including its connection to SW variants and its computational and sample complexities. Finally, we compare the computational cost and generative quality of HSW with the conventional SW on the task of deep generative modeling using various benchmark datasets including CIFAR10, CelebA, and Tiny ImageNet.},
	urldate = {2024-11-06},
	publisher = {arXiv},
	author = {Nguyen, Khai and Ren, Tongzheng and Nguyen, Huy and Rout, Litu and Nguyen, Tan and Ho, Nhat},
	month = feb,
	year = {2023},
	note = {arXiv:2209.13570},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@phdthesis{nadjahi_sliced-wasserstein_2021,
	type = {phdthesis},
	title = {Sliced-{Wasserstein} distance for large-scale machine learning : theory, methodology and extensions},
	shorttitle = {Sliced-{Wasserstein} distance for large-scale machine learning},
	url = {https://theses.hal.science/tel-03533097},
	abstract = {Many methods for statistical inference and generative modeling rely on a probability divergence to effectively compare two probability distributions. The Wasserstein distance, which emerges from optimal transport, has been an interesting choice, but suffers from computational and statistical limitations on large-scale settings. Several alternatives have then been proposed, including the Sliced-Wasserstein distance (SW), a metric that has been increasingly used in practice due to its computational benefits. However, there is little work regarding its theoretical properties. This thesis further explores the use of SW in modern statistical and machine learning problems, with a twofold objective: 1) provide new theoretical insights to understand in depth SW-based algorithms, and 2) design novel tools inspired by SW to improve its applicability and scalability. We first prove a set of asymptotic properties on the estimators obtained by minimizing SW, as well as a central limit theorem whose convergence rate is dimension-free. We also design a novel likelihood-free approximate inference method based on SW, which is theoretically grounded and scales well with the data size and dimension. Given that SW is commonly estimated with a simple Monte Carlo scheme, we then propose two approaches to alleviate the inefficiencies caused by the induced approximation error: on the one hand, we extend the definition of SW to introduce the Generalized Sliced-Wasserstein distances, and illustrate their advantages on generative modeling applications; on the other hand, we leverage concentration of measure results to formulate a new deterministic approximation for SW, which is computationally more efficient than the usual Monte Carlo technique and has nonasymptotical guarantees under a weak dependence condition. Finally, we define the general class of sliced probability divergences and investigate their topological and statistical properties; in particular, we establish that the sample complexity of any sliced divergence does not depend on the problem dimension.},
	language = {en},
	urldate = {2024-11-06},
	school = {Institut Polytechnique de Paris},
	author = {Nadjahi, Kimia},
	month = nov,
	year = {2021},
}

@article{chen_decent_nodate,
	title = {Decent {Estimate} of {CME} {Arrival} time from a {Data}-assimilated {Ensemble} in the {Alfvén} {Wave} {Solar} atmosphere {Model} ({DECADE}-{AWSoM})},
	url = {https://www.authorea.com/doi/full/10.22541/essoar.172745166.65966095/v1?commit=d39a4d7e97f7ffc6c0312425a5ab957b08bab57e},
	abstract = {Forecasting the arrival time of Earth-directed coronal mass ejections (CMEs) via physics-based simulations is an essential but challenging task in space weather research due to the complexity of the underlying physics and limited remote and in-situ ob},
	urldate = {2024-11-06},
	author = {Chen, Hongfan and Sachdeva, Nishtha and Huang, Zhenguang and Holst, Bartholomeus van der and Iv, Ward Beecher Manchester and Jivani, Aniket and Zou, Shasha and Chen, Yang and Huan, Xun and Toth, Gabor},
}

@misc{dockhorn_score-based_2022,
	title = {Score-{Based} {Generative} {Modeling} with {Critically}-{Damped} {Langevin} {Diffusion}},
	url = {http://arxiv.org/abs/2112.07068},
	doi = {10.48550/arXiv.2112.07068},
	abstract = {Score-based generative models (SGMs) have demonstrated remarkable synthesis quality. SGMs rely on a diffusion process that gradually perturbs the data towards a tractable distribution, while the generative model learns to denoise. The complexity of this denoising task is, apart from the data distribution itself, uniquely determined by the diffusion process. We argue that current SGMs employ overly simplistic diffusions, leading to unnecessarily complex denoising processes, which limit generative modeling performance. Based on connections to statistical mechanics, we propose a novel critically-damped Langevin diffusion (CLD) and show that CLD-based SGMs achieve superior performance. CLD can be interpreted as running a joint diffusion in an extended space, where the auxiliary variables can be considered "velocities" that are coupled to the data variables as in Hamiltonian dynamics. We derive a novel score matching objective for CLD and show that the model only needs to learn the score function of the conditional distribution of the velocity given data, an easier task than learning scores of the data directly. We also derive a new sampling scheme for efficient synthesis from CLD-based diffusion models. We find that CLD outperforms previous SGMs in synthesis quality for similar network architectures and sampling compute budgets. We show that our novel sampler for CLD significantly outperforms solvers such as Euler--Maruyama. Our framework provides new insights into score-based denoising diffusion models and can be readily used for high-resolution image synthesis. Project page and code: https://nv-tlabs.github.io/CLD-SGM.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Dockhorn, Tim and Vahdat, Arash and Kreis, Karsten},
	month = mar,
	year = {2022},
	note = {arXiv:2112.07068},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{farquhar_liberty_2020,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '20},
	title = {Liberty or depth: deep {Bayesian} neural nets do not need complex weight posterior approximations},
	isbn = {9781713829546},
	shorttitle = {Liberty or depth},
	abstract = {We challenge the longstanding assumption that the mean-field approximation for variational inference in Bayesian neural networks is severely restrictive, and show this is not the case in deep networks. We prove several results indicating that deep mean-field variational weight posteriors can induce similar distributions in function-space to those induced by shallower networks with complex weight posteriors. We validate our theoretical contributions empirically, both through examination of the weight posterior using Hamiltonian Monte Carlo in small models and by comparing diagonal- to structured-covariance in large settings. Since complex variational posteriors are often expensive and cumbersome to implement, our results suggest that using mean-field variational inference in a deeper model is both a practical and theoretically justified alternative to structured approximations.},
	urldate = {2024-11-04},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Farquhar, Sebastian and Smith, Lewis and Gal, Yarin},
	month = dec,
	year = {2020},
	pages = {4346--4357},
}

@misc{alvarez_bayesian_2016,
	title = {Bayesian inference for a covariance matrix},
	url = {http://arxiv.org/abs/1408.4050},
	doi = {10.48550/arXiv.1408.4050},
	abstract = {Covariance matrix estimation arises in multivariate problems including multivariate normal sampling models and regression models where random effects are jointly modeled, e.g. random-intercept, random-slope models. A Bayesian analysis of these problems requires a prior on the covariance matrix. Here we assess, through a simulation study and a real data set, the impact this prior choice has on posterior inference of the covariance matrix. Inverse Wishart distribution is the natural choice for a covariance matrix prior because its conjugacy on normal model and simplicity, is usually available in Bayesian statistical software. However inverse Wishart distribution presents some undesirable properties from a modeling point of view. It can be too restrictive because assume the same amount of prior information about every variance parameters and, more important, it shows a prior relationship between the variances and correlations. Some alternatives distributions has been proposed. The scaled inverse Wishart distribution, which give more flexibility on the variance priors conserving the conjugacy property but does not eliminate the prior relationship between variances and correlations. Secondly, it is possible to fit separate priors for individual correlations and standard deviations. This strategy eliminates any prior relationship within the covariance matrix parameters, but it is not conjugate and therefore computationally slow.},
	urldate = {2024-11-03},
	publisher = {arXiv},
	author = {Alvarez, Ignacio and Niemi, Jarad and Simpson, Matt},
	month = jul,
	year = {2016},
	note = {arXiv:1408.4050},
	keywords = {Statistics - Methodology},
}

@misc{noauthor_john_nodate,
	title = {John {Barnard}, {Robert} {McCulloch} and {Xiao}-{Li} {Meng} (2000). {Modeling} covariance matrices in terms of standard deviations and correlations, with application to shrinkage. {Vol}.10, {No}.4},
	url = {https://www3.stat.sinica.edu.tw/statistica/j10n4/j10n416/j10n416.htm},
	urldate = {2024-11-03},
}

@misc{noauthor_minimax_nodate,
	title = {Minimax {Optimal} {Estimation} of {KL} {Divergence} for {Continuous} {Distributions}},
	url = {https://ieeexplore.ieee.org/document/9142281},
	abstract = {Estimating Kullback-Leibler divergence from identical and independently distributed samples is an important problem in various domains. One simple and effective estimator is based on the k nearest neighbor distances between these samples. In this paper, we analyze the convergence rates of the bias and variance of this estimator. Furthermore, we derive a lower bound of the minimax mean square error and show that kNN method is asymptotically rate optimal.},
	language = {en-US},
	urldate = {2024-11-02},
}

@misc{lee_towards_2023-1,
	title = {Towards a {Rigorous} {Analysis} of {Mutual} {Information} in {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2308.15704},
	doi = {10.48550/arXiv.2308.15704},
	abstract = {Contrastive learning has emerged as a cornerstone in recent achievements of unsupervised representation learning. Its primary paradigm involves an instance discrimination task with a mutual information loss. The loss is known as InfoNCE and it has yielded vital insights into contrastive learning through the lens of mutual information analysis. However, the estimation of mutual information can prove challenging, creating a gap between the elegance of its mathematical foundation and the complexity of its estimation. As a result, drawing rigorous insights or conclusions from mutual information analysis becomes intricate. In this study, we introduce three novel methods and a few related theorems, aimed at enhancing the rigor of mutual information analysis. Despite their simplicity, these methods can carry substantial utility. Leveraging these approaches, we reassess three instances of contrastive learning analysis, illustrating their capacity to facilitate deeper comprehension or to rectify pre-existing misconceptions. Specifically, we investigate small batch size, mutual information as a measure, and the InfoMin principle.},
	urldate = {2024-10-31},
	publisher = {arXiv},
	author = {Lee, Kyungeun and Kim, Jaeill and Kang, Suhyun and Rhee, Wonjong},
	month = aug,
	year = {2023},
	note = {arXiv:2308.15704},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{rezende_taming_2018,
	title = {Taming {VAEs}},
	url = {http://arxiv.org/abs/1810.00597},
	doi = {10.48550/arXiv.1810.00597},
	abstract = {In spite of remarkable progress in deep latent variable generative modeling, training still remains a challenge due to a combination of optimization and generalization issues. In practice, a combination of heuristic algorithms (such as hand-crafted annealing of KL-terms) is often used in order to achieve the desired results, but such solutions are not robust to changes in model architecture or dataset. The best settings can often vary dramatically from one problem to another, which requires doing expensive parameter sweeps for each new case. Here we develop on the idea of training VAEs with additional constraints as a way to control their behaviour. We first present a detailed theoretical analysis of constrained VAEs, expanding our understanding of how these models work. We then introduce and analyze a practical algorithm termed Generalized ELBO with Constrained Optimization, GECO. The main advantage of GECO for the machine learning practitioner is a more intuitive, yet principled, process of tuning the loss. This involves defining of a set of constraints, which typically have an explicit relation to the desired model performance, in contrast to tweaking abstract hyper-parameters which implicitly affect the model behavior. Encouraging experimental results in several standard datasets indicate that GECO is a very robust and effective tool to balance reconstruction and compression constraints.},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Rezende, Danilo Jimenez and Viola, Fabio},
	month = oct,
	year = {2018},
	note = {arXiv:1810.00597},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ledoit_analytical_2020,
	title = {Analytical nonlinear shrinkage of large-dimensional covariance matrices},
	volume = {48},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-48/issue-5/Analytical-nonlinear-shrinkage-of-large-dimensional-covariance-matrices/10.1214/19-AOS1921.full},
	doi = {10.1214/19-AOS1921},
	abstract = {This paper establishes the first analytical formula for nonlinear shrinkage estimation of large-dimensional covariance matrices. We achieve this by identifying and mathematically exploiting a deep connection between nonlinear shrinkage and nonparametric estimation of the Hilbert transform of the sample spectral density. Previous nonlinear shrinkage methods were of numerical nature: QuEST requires numerical inversion of a complex equation from random matrix theory whereas NERCOME is based on a sample-splitting scheme. The new analytical method is more elegant and also has more potential to accommodate future variations or extensions. Immediate benefits are (i) that it is typically 1000 times faster with basically the same accuracy as QuEST and (ii) that it accommodates covariance matrices of dimension up to 10,000 and more. The difficult case where the matrix dimension exceeds the sample size is also covered.},
	number = {5},
	urldate = {2024-10-28},
	journal = {The Annals of Statistics},
	author = {Ledoit, Olivier and Wolf, Michael},
	month = oct,
	year = {2020},
	keywords = {15A52, 62G20, 62H12, Hilbert transform, Kernel estimation, rotation equivariance},
	pages = {3043--3065},
}

@article{ledoit_analytical_2020-1,
	title = {Analytical nonlinear shrinkage of large-dimensional covariance matrices},
	volume = {48},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-48/issue-5/Analytical-nonlinear-shrinkage-of-large-dimensional-covariance-matrices/10.1214/19-AOS1921.full},
	doi = {10.1214/19-AOS1921},
	abstract = {This paper establishes the first analytical formula for nonlinear shrinkage estimation of large-dimensional covariance matrices. We achieve this by identifying and mathematically exploiting a deep connection between nonlinear shrinkage and nonparametric estimation of the Hilbert transform of the sample spectral density. Previous nonlinear shrinkage methods were of numerical nature: QuEST requires numerical inversion of a complex equation from random matrix theory whereas NERCOME is based on a sample-splitting scheme. The new analytical method is more elegant and also has more potential to accommodate future variations or extensions. Immediate benefits are (i) that it is typically 1000 times faster with basically the same accuracy as QuEST and (ii) that it accommodates covariance matrices of dimension up to 10,000 and more. The difficult case where the matrix dimension exceeds the sample size is also covered.},
	number = {5},
	urldate = {2024-10-28},
	journal = {The Annals of Statistics},
	author = {Ledoit, Olivier and Wolf, Michael},
	month = oct,
	year = {2020},
	keywords = {15A52, 62G20, 62H12, Hilbert transform, Kernel estimation, rotation equivariance},
	pages = {3043--3065},
}

@misc{ament_unexpected_2024,
	title = {Unexpected {Improvements} to {Expected} {Improvement} for {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/2310.20708},
	doi = {10.48550/arXiv.2310.20708},
	abstract = {Expected Improvement (EI) is arguably the most popular acquisition function in Bayesian optimization and has found countless successful applications, but its performance is often exceeded by that of more recent methods. Notably, EI and its variants, including for the parallel and multi-objective settings, are challenging to optimize because their acquisition values vanish numerically in many regions. This difficulty generally increases as the number of observations, dimensionality of the search space, or the number of constraints grow, resulting in performance that is inconsistent across the literature and most often sub-optimal. Herein, we propose LogEI, a new family of acquisition functions whose members either have identical or approximately equal optima as their canonical counterparts, but are substantially easier to optimize numerically. We demonstrate that numerical pathologies manifest themselves in "classic" analytic EI, Expected Hypervolume Improvement (EHVI), as well as their constrained, noisy, and parallel variants, and propose corresponding reformulations that remedy these pathologies. Our empirical results show that members of the LogEI family of acquisition functions substantially improve on the optimization performance of their canonical counterparts and surprisingly, are on par with or exceed the performance of recent state-of-the-art acquisition functions, highlighting the understated role of numerical optimization in the literature.},
	urldate = {2024-10-27},
	publisher = {arXiv},
	author = {Ament, Sebastian and Daulton, Samuel and Eriksson, David and Balandat, Maximilian and Bakshy, Eytan},
	month = jan,
	year = {2024},
	note = {arXiv:2310.20708},
	keywords = {Computer Science - Machine Learning, Computer Science - Numerical Analysis, Mathematics - Numerical Analysis, Statistics - Machine Learning},
}

@article{berger_bayesian_2020,
	title = {Bayesian analysis of the covariance matrix of a multivariate normal distribution with a new class of priors},
	volume = {48},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-48/issue-4/Bayesian-analysis-of-the-covariance-matrix-of-a-multivariate-normal/10.1214/19-AOS1891.full},
	doi = {10.1214/19-AOS1891},
	abstract = {Bayesian analysis for the covariance matrix of a multivariate normal distribution has received a lot of attention in the last two decades. In this paper, we propose a new class of priors for the covariance matrix, including both inverse Wishart and reference priors as special cases. The main motivation for the new class is to have available priors—both subjective and objective—that do not “force eigenvalues apart,” which is a criticism of inverse Wishart and Jeffreys priors. Extensive comparison of these “shrinkage priors” with inverse Wishart and Jeffreys priors is undertaken, with the new priors seeming to have considerably better performance. A number of curious facts about the new priors are also observed, such as that the posterior distribution will be proper with just three vector observations from the multivariate normal distribution—regardless of the dimension of the covariance matrix—and that useful inference about features of the covariance matrix can be possible. Finally, a new MCMC algorithm is developed for this class of priors and is shown to be computationally effective for matrices of up to 100 dimensions.},
	number = {4},
	urldate = {2024-10-25},
	journal = {The Annals of Statistics},
	author = {Berger, James O. and Sun, Dongchu and Song, Chengyuan},
	month = aug,
	year = {2020},
	keywords = {62C10, 62F15, 62H10, 62H86, Covariance, inverse Wishart prior, objective priors, shrinkage priors},
	pages = {2381--2403},
}

@misc{nguyen_depth_2023,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {An in {Depth} {Introduction} to {Variational} {Bayes} {Note}},
	url = {https://papers.ssrn.com/abstract=4541076},
	doi = {10.2139/ssrn.4541076},
	abstract = {This note grew out of my attempt to understand variational inference (variational Bayes). Variational Bayesian approaches encompass a set of methodologies aimed at approximating inference problems that emerge within Bayesian inference and machine learning. These techniques have found extensive application across diverse fields of study. Within this note, we furnish a comprehensive guide to Variational Inference, with the specific intention of elucidating aspects often omitted in literature, thereby enhancing readers’ comprehension regarding the rationale behind the inclusion of particular equations in various papers and texts. This is accomplished by various examples ranging from simple to complex distributions. This note is inspired by Blei et al. (2003), Bishop (2006), Hoffman et al. (2010), Hoffman et al. (2013), Blei et al. (2017), Blei (2017), and Tran et al. (2021).},
	language = {en},
	urldate = {2024-10-25},
	publisher = {Social Science Research Network},
	author = {Nguyen, Duy},
	month = aug,
	year = {2023},
	keywords = {Gradient descent, Logistic regression, Massive data, Newton’s method, Optimal subsampling, Stochastic gradient descent},
}

@inproceedings{rainforth_nesting_2018,
	title = {On {Nesting} {Monte} {Carlo} {Estimators}},
	url = {https://proceedings.mlr.press/v80/rainforth18a.html},
	abstract = {Many problems in machine learning and statistics involve nested expectations and thus do not permit conventional Monte Carlo (MC) estimation. For such problems, one must nest estimators, such that terms in an outer estimator themselves involve calculation of a separate, nested, estimation. We investigate the statistical implications of nesting MC estimators, including cases of multiple levels of nesting, and establish the conditions under which they converge. We derive corresponding rates of convergence and provide empirical evidence that these rates are observed in practice. We further establish a number of pitfalls that can arise from naive nesting of MC estimators, provide guidelines about how these can be avoided, and lay out novel methods for reformulating certain classes of nested expectation problems into single expectations, leading to improved convergence rates. We demonstrate the applicability of our work by using our results to develop a new estimator for discrete Bayesian experimental design problems and derive error bounds for a class of variational objectives.},
	language = {en},
	urldate = {2024-10-25},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Rainforth, Tom and Cornish, Rob and Yang, Hongseok and Warrington, Andrew and Wood, Frank},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {4267--4276},
}

@article{huan_simulation-based_2013,
	title = {Simulation-based optimal {Bayesian} experimental design for nonlinear systems},
	volume = {232},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999112004597},
	doi = {10.1016/j.jcp.2012.08.013},
	abstract = {The optimal selection of experimental conditions is essential to maximizing the value of data for inference and prediction, particularly in situations where experiments are time-consuming and expensive to conduct. We propose a general mathematical framework and an algorithmic approach for optimal experimental design with nonlinear simulation-based models; in particular, we focus on finding sets of experiments that provide the most information about targeted sets of parameters. Our framework employs a Bayesian statistical setting, which provides a foundation for inference from noisy, indirect, and incomplete data, and a natural mechanism for incorporating heterogeneous sources of information. An objective function is constructed from information theoretic measures, reflecting expected information gain from proposed combinations of experiments. Polynomial chaos approximations and a two-stage Monte Carlo sampling method are used to evaluate the expected information gain. Stochastic approximation algorithms are then used to make optimization feasible in computationally intensive and high-dimensional settings. These algorithms are demonstrated on model problems and on nonlinear parameter inference problems arising in detailed combustion kinetics.},
	number = {1},
	urldate = {2024-10-25},
	journal = {Journal of Computational Physics},
	author = {Huan, Xun and Marzouk, Youssef M.},
	month = jan,
	year = {2013},
	keywords = {Bayesian inference, Chemical kinetics, Nonlinear experimental design, Optimal experimental design, Shannon information, Stochastic approximation, Uncertainty quantification},
	pages = {288--317},
}

@phdthesis{huan_numerical_2015,
	type = {Thesis},
	title = {Numerical approaches for sequential {Bayesian} optimal experimental design},
	copyright = {M.I.T. theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission. See provided URL for inquiries about permission.},
	url = {https://dspace.mit.edu/handle/1721.1/101442},
	abstract = {Experimental data play a crucial role in developing and refining models of physical systems. Some experiments can be more valuable than others, however. Well-chosen experiments can save substantial resources, and hence optimal experimental design (OED) seeks to quantify and maximize the value of experimental data. Common current practice for designing a sequence of experiments uses suboptimal approaches: batch (open-loop) design that chooses all experiments simultaneously with no feedback of information, or greedy (myopic) design that optimally selects the next experiment without accounting for future observations and dynamics. In contrast, sequential optimal experimental design (sOED) is free of these limitations. With the goal of acquiring experimental data that are optimal for model parameter inference, we develop a rigorous Bayesian formulation for OED using an objective that incorporates a measure of information gain. This framework is first demonstrated in a batch design setting, and then extended to sOED using a dynamic programming (DP) formulation. We also develop new numerical tools for sOED to accommodate nonlinear models with continuous (and often unbounded) parameter, design, and observation spaces. Two major techniques are employed to make solution of the DP problem computationally feasible. First, the optimal policy is sought using a one-step lookahead representation combined with approximate value iteration. This approximate dynamic programming method couples backward induction and regression to construct value function approximations. It also iteratively generates trajectories via exploration and exploitation to further improve approximation accuracy in frequently visited regions of the state space. Second, transport maps are used to represent belief states, which reflect the intermediate posteriors within the sequential design process. Transport maps offer a finite-dimensional representation of these generally non-Gaussian random variables, and also enable fast approximate Bayesian inference, which must be performed millions of times under nested combinations of optimization and Monte Carlo sampling. The overall sOED algorithm is demonstrated and verified against analytic solutions on a simple linear-Gaussian model. Its advantages over batch and greedy designs are then shown via a nonlinear application of optimal sequential sensing: inferring contaminant source location from a sensor in a time-dependent convection-diffusion system. Finally, the capability of the algorithm is tested for multidimensional parameter and design spaces in a more complex setting of the source inversion problem.},
	language = {eng},
	urldate = {2024-10-25},
	school = {Massachusetts Institute of Technology},
	author = {Huan, Xun},
	year = {2015},
}

@article{dong_variational_2025,
	title = {Variational {Bayesian} optimal experimental design with normalizing flows},
	volume = {433},
	issn = {0045-7825},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782524007126},
	doi = {10.1016/j.cma.2024.117457},
	abstract = {Bayesian optimal experimental design (OED) seeks experiments that maximize the expected information gain (EIG) in model parameters. Directly estimating the EIG using nested Monte Carlo is computationally expensive and requires an explicit likelihood. Variational OED (vOED), in contrast, estimates a lower bound of the EIG without likelihood evaluations by approximating the posterior distributions with variational forms, and then tightens the bound by optimizing its variational parameters. We introduce the use of normalizing flows (NFs) for representing variational distributions in vOED; we call this approach vOED-NFs. Specifically, we adopt NFs with a conditional invertible neural network architecture built from compositions of coupling layers, and enhanced with a summary network for data dimension reduction. We present Monte Carlo estimators to the lower bound along with gradient expressions to enable a gradient-based simultaneous optimization of the variational parameters and the design variables. The vOED-NFs algorithm is then validated in two benchmark problems, and demonstrated on a partial differential equation-governed application of cathodic electrophoretic deposition and an implicit likelihood case with stochastic modeling of aphid population. The findings suggest that a composition of 4–5 coupling layers is able to achieve lower EIG estimation bias, under a fixed budget of forward model runs, compared to previous approaches. The resulting NFs produce approximate posteriors that agree well with the true posteriors, able to capture non-Gaussian and multi-modal features effectively.},
	urldate = {2024-10-24},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Dong, Jiayuan and Jacobsen, Christian and Khalloufi, Mehdi and Akram, Maryam and Liu, Wanjiao and Duraisamy, Karthik and Huan, Xun},
	month = jan,
	year = {2025},
	keywords = {Conditional invertible neural networks, Coupling layers, Expected information gain, Implicit likelihood, Information lower bound, Normalizing flows, Uncertainty quantification, Variational inference},
	pages = {117457},
}

@inproceedings{foster_variational_2019,
	title = {Variational {Bayesian} {Optimal} {Experimental} {Design}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/d55cbf210f175f4a37916eafe6c04f0d-Abstract.html},
	abstract = {Bayesian optimal experimental design (BOED) is a principled framework for making efficient use of limited experimental resources. Unfortunately, its applicability is hampered by the difficulty of obtaining accurate estimates of the expected information gain (EIG) of an experiment. To address this, we introduce several classes of fast EIG estimators by building on ideas from amortized variational inference. We show theoretically and empirically that these estimators can provide significant gains in speed and accuracy over previous approaches. We further demonstrate the practicality of our approach on a number of end-to-end experiments.},
	urldate = {2024-10-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Foster, Adam and Jankowiak, Martin and Bingham, Elias and Horsfall, Paul and Teh, Yee Whye and Rainforth, Thomas and Goodman, Noah},
	year = {2019},
}

@misc{dey_dlkoopman_2023,
	title = {{DLKoopman}: {A} deep learning software package for {Koopman} theory},
	shorttitle = {{DLKoopman}},
	url = {http://arxiv.org/abs/2211.08992},
	doi = {10.48550/arXiv.2211.08992},
	abstract = {We present DLKoopman -- a software package for Koopman theory that uses deep learning to learn an encoding of a nonlinear dynamical system into a linear space, while simultaneously learning the linear dynamics. While several previous efforts have either restricted the ability to learn encodings, or been bespoke efforts designed for specific systems, DLKoopman is a generalized tool that can be applied to data-driven learning and optimization of any dynamical system. It can either be trained on data from individual states (snapshots) of a system and used to predict its unknown states, or trained on data from trajectories of a system and used to predict unknown trajectories for new initial states. DLKoopman is available on the Python Package Index (PyPI) as 'dlkoopman', and includes extensive documentation and tutorials. Additional contributions of the package include a novel metric called Average Normalized Absolute Error for evaluating performance, and a ready-to-use hyperparameter search module for improving performance.},
	urldate = {2024-10-24},
	publisher = {arXiv},
	author = {Dey, Sourya and Davis, Eric},
	month = jun,
	year = {2023},
	note = {arXiv:2211.08992},
	keywords = {Computer Science - Machine Learning, Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control},
}

@article{ledoit_analytical_2020-2,
	title = {Analytical nonlinear shrinkage of large-dimensional covariance matrices},
	volume = {48},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-48/issue-5/Analytical-nonlinear-shrinkage-of-large-dimensional-covariance-matrices/10.1214/19-AOS1921.full},
	doi = {10.1214/19-AOS1921},
	abstract = {This paper establishes the first analytical formula for nonlinear shrinkage estimation of large-dimensional covariance matrices. We achieve this by identifying and mathematically exploiting a deep connection between nonlinear shrinkage and nonparametric estimation of the Hilbert transform of the sample spectral density. Previous nonlinear shrinkage methods were of numerical nature: QuEST requires numerical inversion of a complex equation from random matrix theory whereas NERCOME is based on a sample-splitting scheme. The new analytical method is more elegant and also has more potential to accommodate future variations or extensions. Immediate benefits are (i) that it is typically 1000 times faster with basically the same accuracy as QuEST and (ii) that it accommodates covariance matrices of dimension up to 10,000 and more. The difficult case where the matrix dimension exceeds the sample size is also covered.},
	number = {5},
	urldate = {2024-10-24},
	journal = {The Annals of Statistics},
	author = {Ledoit, Olivier and Wolf, Michael},
	month = oct,
	year = {2020},
	keywords = {15A52, 62G20, 62H12, Hilbert transform, Kernel estimation, rotation equivariance},
	pages = {3043--3065},
}

@misc{werff_cnn_2024,
	title = {{CNN} vs. {Vision} {Transformer}: {A} {Practitioner}’s {Guide} to {Selecting} the {Right} {Model}},
	shorttitle = {{CNN} vs. {Vision} {Transformer}},
	url = {https://tobiasvanderwerff.github.io/2024/05/15/cnn-vs-vit.html},
	abstract = {Vision Transformers (ViTs) have become a popular model architecture in computer vision research, excelling in a variety of tasks and surpassing Convolutional Neural Networks (CNNs) in most benchmarks. As practitioners, we often face the dilemma of choosing the right architecture for our projects. This blog post aims to provide guidelines for making an informed decision on when to use CNNs versus ViTs, backed by empirical evidence and practical considerations.},
	language = {en},
	urldate = {2024-10-23},
	journal = {Tobias van der Werff},
	author = {Werff, Tobias van der},
	month = may,
	year = {2024},
}

@misc{geneva_transformers_2021,
	title = {Transformers for {Modeling} {Physical} {Systems}},
	url = {http://arxiv.org/abs/2010.03957},
	doi = {10.48550/arXiv.2010.03957},
	abstract = {Transformers are widely used in natural language processing due to their ability to model longer-term dependencies in text. Although these models achieve state-of-the-art performance for many language related tasks, their applicability outside of the natural language processing field has been minimal. In this work, we propose the use of transformer models for the prediction of dynamical systems representative of physical phenomena. The use of Koopman based embeddings provide a unique and powerful method for projecting any dynamical system into a vector representation which can then be predicted by a transformer. The proposed model is able to accurately predict various dynamical systems and outperform classical methods that are commonly used in the scientific machine learning literature.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Geneva, Nicholas and Zabaras, Nicholas},
	month = dec,
	year = {2021},
	note = {arXiv:2010.03957},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics},
}

@misc{amos_input_2016,
	title = {Input {Convex} {Neural} {Networks}},
	url = {https://arxiv.org/abs/1609.07152v3},
	abstract = {This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made input-convex with a minor modification, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases.},
	language = {en},
	urldate = {2024-10-23},
	journal = {arXiv.org},
	author = {Amos, Brandon and Xu, Lei and Kolter, J. Zico},
	month = sep,
	year = {2016},
}

@misc{perez_film_2017,
	title = {{FiLM}: {Visual} {Reasoning} with a {General} {Conditioning} {Layer}},
	shorttitle = {{FiLM}},
	url = {https://arxiv.org/abs/1709.07871v2},
	abstract = {We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.},
	language = {en},
	urldate = {2024-10-23},
	journal = {arXiv.org},
	author = {Perez, Ethan and Strub, Florian and de Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
	month = sep,
	year = {2017},
}

@article{chen_graph_2022,
	title = {Graph coarsening: from scientific computing to machine learning},
	volume = {79},
	issn = {2281-7875},
	shorttitle = {Graph coarsening},
	url = {https://doi.org/10.1007/s40324-021-00282-x},
	doi = {10.1007/s40324-021-00282-x},
	abstract = {The general method of graph coarsening or graph reduction has been a remarkably useful and ubiquitous tool in scientific computing and it is now just starting to have a similar impact in machine learning. The goal of this paper is to take a broad look into coarsening techniques that have been successfully deployed in scientific computing and see how similar principles are finding their way in more recent applications related to machine learning. In scientific computing, coarsening plays a central role in algebraic multigrid methods as well as the related class of multilevel incomplete LU factorizations. In machine learning, graph coarsening goes under various names, e.g., graph downsampling or graph reduction. Its goal in most cases is to replace some original graph by one which has fewer nodes, but whose structure and characteristics are similar to those of the original graph. As will be seen, a common strategy in these methods is to rely on spectral properties to define the coarse graph.},
	language = {en},
	number = {1},
	urldate = {2024-10-21},
	journal = {SeMA Journal},
	author = {Chen, Jie and Saad, Yousef and Zhang, Zechen},
	month = mar,
	year = {2022},
	keywords = {05C85, 65F10, 65N55m, 68T05, 94C15, Coarsening, Graph Coarsening, Graphs and Networks, Hierarchical methods. Graph Neural Networks, Multilevel methods},
	pages = {187--223},
}

@article{scott_bayes_2016,
	title = {Bayes and big data: the consensus {Monte} {Carlo} algorithm},
	volume = {11},
	issn = {1750-9653},
	shorttitle = {Bayes and big data},
	url = {https://doi.org/10.1080/17509653.2016.1142191},
	doi = {10.1080/17509653.2016.1142191},
	abstract = {A useful definition of ‘big data’ is data that is too big to process comfortably on a single machine, either because of processor, memory, or disk bottlenecks. Graphics processing units can alleviate the processor bottleneck, but memory or disk bottlenecks can only be eliminated by splitting data across multiple machines. Communication between large numbers of machines is expensive (regardless of the amount of data being communicated), so there is a need for algorithms that perform distributed approximate Bayesian analyses with minimal communication. Consensus Monte Carlo operates by running a separate Monte Carlo algorithm on each machine, and then averaging individual Monte Carlo draws across machines. Depending on the model, the resulting draws can be nearly indistinguishable from the draws that would have been obtained by running a single-machine algorithm for a very long time. Examples of consensus Monte Carlo are shown for simple models where single-machine solutions are available, for large single-layer hierarchical models, and for Bayesian additive regression trees (BART).},
	number = {2},
	urldate = {2024-10-21},
	journal = {International Journal of Management Science and Engineering Management},
	author = {Scott, Steven L. and Blocker, Alexander W. and Bonassi, Fernando V. and Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
	month = apr,
	year = {2016},
	pages = {78--88},
}

@inproceedings{rabinovich_variational_2015,
	title = {Variational {Consensus} {Monte} {Carlo}},
	volume = {28},
	url = {https://papers.nips.cc/paper_files/paper/2015/hash/e94550c93cd70fe748e6982b3439ad3b-Abstract.html},
	abstract = {Practitioners of Bayesian statistics have long depended on Markov chain Monte Carlo (MCMC) to obtain samples from intractable posterior distributions. Unfortunately, MCMC algorithms are typically serial, and do not scale to the large datasets typical of modern machine learning. The recently proposed consensus Monte Carlo algorithm removes this limitation by partitioning the data and drawing samples conditional on each partition in parallel (Scott et al, 2013). A fixed aggregation function then combines these samples, yielding approximate posterior samples. We introduce variational consensus Monte Carlo (VCMC), a variational Bayes algorithm that optimizes over aggregation functions to obtain samples from a distribution that better approximates the target. The resulting objective contains an intractable entropy term; we therefore derive a relaxation of the objective and show that the relaxed problem is blockwise concave under mild conditions. We illustrate the advantages of our algorithm on three inference tasks from the literature, demonstrating both the superior quality of the posterior approximation and the moderate overhead of the optimization step. Our algorithm achieves a relative error reduction (measured against serial MCMC) of up to 39\% compared to consensus Monte Carlo on the task of estimating 300-dimensional probit regression parameter expectations; similarly, it achieves an error reduction of 92\% on the task of estimating cluster comembership probabilities in a Gaussian mixture model with 8 components in 8 dimensions. Furthermore, these gains come at moderate cost compared to the runtime of serial MCMC, achieving near-ideal speedup in some instances.},
	urldate = {2024-10-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rabinovich, Maxim and Angelino, Elaine and Jordan, Michael I},
	year = {2015},
}

@misc{campbell_automated_2019,
	title = {Automated {Scalable} {Bayesian} {Inference} via {Hilbert} {Coresets}},
	url = {http://arxiv.org/abs/1710.05053},
	doi = {10.48550/arXiv.1710.05053},
	abstract = {The automation of posterior inference in Bayesian data analysis has enabled experts and nonexperts alike to use more sophisticated models, engage in faster exploratory modeling and analysis, and ensure experimental reproducibility. However, standard automated posterior inference algorithms are not tractable at the scale of massive modern datasets, and modifications to make them so are typically model-specific, require expert tuning, and can break theoretical guarantees on inferential quality. Building on the Bayesian coresets framework, this work instead takes advantage of data redundancy to shrink the dataset itself as a preprocessing step, providing fully-automated, scalable Bayesian inference with theoretical guarantees. We begin with an intuitive reformulation of Bayesian coreset construction as sparse vector sum approximation, and demonstrate that its automation and performance-based shortcomings arise from the use of the supremum norm. To address these shortcomings we develop Hilbert coresets, i.e., Bayesian coresets constructed under a norm induced by an inner-product on the log-likelihood function space. We propose two Hilbert coreset construction algorithms---one based on importance sampling, and one based on the Frank-Wolfe algorithm---along with theoretical guarantees on approximation quality as a function of coreset size. Since the exact computation of the proposed inner-products is model-specific, we automate the construction with a random finite-dimensional projection of the log-likelihood functions. The resulting automated coreset construction algorithm is simple to implement, and experiments on a variety of models with real and synthetic datasets show that it provides high-quality posterior approximations and a significant reduction in the computational cost of inference.},
	urldate = {2024-10-21},
	publisher = {arXiv},
	author = {Campbell, Trevor and Broderick, Tamara},
	month = feb,
	year = {2019},
	note = {arXiv:1710.05053},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
}

@inproceedings{agarwal_geometric_2007,
	title = {Geometric {Approximation} via {Coresets}},
	url = {https://www.semanticscholar.org/paper/Geometric-Approximation-via-Coresets-Agarwal-Har-Peled/9d713f1f79554a28b9788c0299cb07d34d782022},
	abstract = {The paradigm of coresets has recently emerged as a powerful tool for efficiently approximating various extent measures of a point set P . Using this paradigm, one quickly computes a small subset Q of P , called a coreset, that approximates the original set P and and then solves the problem on Q using a relatively inefficient algorithm. The solution for Q is then translated to an approximate solution to the original point set P . This paper describes the ways in which this paradigm has been successfully applied to various optimization and extent measure problems.},
	urldate = {2024-10-21},
	author = {Agarwal, P. and Har-Peled, Sariel and Varadarajan, Kasturi R.},
	year = {2007},
}

@inproceedings{huggins_coresets_2016,
	title = {Coresets for {Scalable} {Bayesian} {Logistic} {Regression}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/2b0f658cbffd284984fb11d90254081f-Abstract.html},
	abstract = {The use of Bayesian methods in large-scale data settings is attractive because of the rich hierarchical models, uncertainty quantification, and prior specification they provide. Standard Bayesian inference algorithms are computationally expensive, however, making their direct application to large datasets difficult or infeasible. Recent work on scaling Bayesian inference has focused on modifying the underlying algorithms to, for example, use only a random data subsample at each iteration. We leverage the insight that data is often redundant to instead obtain a weighted subset of the data (called a coreset) that is much smaller than the original dataset. We can then use this small coreset in any number of existing posterior inference algorithms without modification. In this paper, we develop an efficient coreset construction algorithm for Bayesian logistic regression models. We provide theoretical guarantees on the size and approximation quality of the coreset -- both for fixed, known datasets, and in expectation for a wide class of data generative models. Crucially, the proposed approach also permits efficient construction of the coreset in both streaming and parallel settings, with minimal additional effort. We demonstrate the efficacy of our approach on a number of synthetic and real-world datasets, and find that, in practice, the size of the coreset is independent of the original dataset size. Furthermore, constructing the coreset takes a negligible amount of time compared to that required to run MCMC on it.},
	urldate = {2024-10-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Huggins, Jonathan and Campbell, Trevor and Broderick, Tamara},
	year = {2016},
}

@inproceedings{hernandez-lobato_probabilistic_2015,
	title = {Probabilistic {Backpropagation} for {Scalable} {Learning} of {Bayesian} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v37/hernandez-lobatoc15.html},
	abstract = {Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights.},
	language = {en},
	urldate = {2024-10-19},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hernandez-Lobato, Jose Miguel and Adams, Ryan},
	month = jun,
	year = {2015},
	pages = {1861--1869},
}

@inproceedings{graves_practical_2011,
	title = {Practical {Variational} {Inference} for {Neural} {Networks}},
	volume = {24},
	url = {https://papers.nips.cc/paper_files/paper/2011/hash/7eb3c8be3d411e8ebfab08eba5f49632-Abstract.html},
	abstract = {Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural networks. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically reduce the number of network weights and lead to improved generalisation. Experimental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus.},
	urldate = {2024-10-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Graves, Alex},
	year = {2011},
}

@misc{cocola_hyper-reduced_2023,
	title = {Hyper-{Reduced} {Autoencoders} for {Efficient} and {Accurate} {Nonlinear} {Model} {Reductions}},
	url = {http://arxiv.org/abs/2303.09630},
	doi = {10.48550/arXiv.2303.09630},
	abstract = {Projection-based model order reduction on nonlinear manifolds has been recently proposed for problems with slowly decaying Kolmogorov n-width such as advection-dominated ones. These methods often use neural networks for manifold learning and showcase improved accuracy over traditional linear subspace-reduced order models. A disadvantage of the previously proposed methods is the potential high computational costs of training the networks on high-fidelity solution snapshots. In this work, we propose and analyze a novel method that overcomes this disadvantage by training a neural network only on subsampled versions of the high-fidelity solution snapshots. This method coupled with collocation-based hyper-reduction and Gappy-POD allows for efficient and accurate surrogate models. We demonstrate the validity of our approach on a 2d Burgers problem.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Cocola, Jorio and Tencer, John and Rizzi, Francesco and Parish, Eric and Blonigan, Patrick},
	month = mar,
	year = {2023},
	note = {arXiv:2303.09630},
	keywords = {Computer Science - Machine Learning, Computer Science - Numerical Analysis, Mathematics - Numerical Analysis, Physics - Computational Physics},
}

@misc{agrell_sequential_2020,
	title = {Sequential {Bayesian} optimal experimental design for structural reliability analysis},
	url = {http://arxiv.org/abs/2007.00402},
	doi = {10.48550/arXiv.2007.00402},
	abstract = {Structural reliability analysis is concerned with estimation of the probability of a critical event taking place, described by \$P(g({\textbackslash}textbf\{X\}) {\textbackslash}leq 0)\$ for some \$n\$-dimensional random variable \${\textbackslash}textbf\{X\}\$ and some real-valued function \$g\$. In many applications the function \$g\$ is practically unknown, as function evaluation involves time consuming numerical simulation or some other form of experiment that is expensive to perform. The problem we address in this paper is how to optimally design experiments, in a Bayesian decision theoretic fashion, when the goal is to estimate the probability \$P(g({\textbackslash}textbf\{X\}) {\textbackslash}leq 0)\$ using a minimal amount of resources. As opposed to existing methods that have been proposed for this purpose, we consider a general structural reliability model given in hierarchical form. We therefore introduce a general formulation of the experimental design problem, where we distinguish between the uncertainty related to the random variable \${\textbackslash}textbf\{X\}\$ and any additional epistemic uncertainty that we want to reduce through experimentation. The effectiveness of a design strategy is evaluated through a measure of residual uncertainty, and efficient approximation of this quantity is crucial if we want to apply algorithms that search for an optimal strategy. The method we propose is based on importance sampling combined with the unscented transform for epistemic uncertainty propagation. We implement this for the myopic (one-step look ahead) alternative, and demonstrate the effectiveness through a series of numerical experiments.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Agrell, Christian and Dahl, Kristina Rognlien},
	month = jul,
	year = {2020},
	note = {arXiv:2007.00402},
	keywords = {Statistics - Computation, Statistics - Methodology},
}

@article{soize_random_2005,
	series = {Special {Issue} on {Computational} {Methods} in {Stochastic} {Mechanics} and {Reliability} {Analysis}},
	title = {Random matrix theory for modeling uncertainties in computational mechanics},
	volume = {194},
	issn = {0045-7825},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782504003962},
	doi = {10.1016/j.cma.2004.06.038},
	abstract = {This paper deals with data uncertainties and model uncertainties issues in computational mechanics. If data uncertainties can be modeled by parametric probabilistic methods, for a given mean model, a nonparametric probabilistic approach can be used for modeling model uncertainties. The first part is devoted to random matrix theory for which we summarize previous published results and for which two new ensembles of random matrices useful for the nonparametric models are introduced. In a second part, the nonparametric probabilistic approach of random uncertainties is presented for linear dynamical systems and for nonlinear dynamical systems constituted of a linear part with additional localized nonlinearities. In a third part, a new method is proposed for estimating the parameters of the nonparametric approach from experiments. Finally, examples with experimental comparisons are given.},
	number = {12},
	urldate = {2024-10-14},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Soize, C.},
	month = apr,
	year = {2005},
	keywords = {Computational mechanics, Dynamical system, Nonlinear, Random matrix, Random uncertainties, Transient},
	pages = {1333--1366},
}

@article{aoues_benchmark_2010,
	title = {Benchmark study of numerical methods for reliability-based design optimization},
	volume = {41},
	issn = {1615-1488},
	url = {https://doi.org/10.1007/s00158-009-0412-2},
	doi = {10.1007/s00158-009-0412-2},
	abstract = {The reliability-based design optimization (RBDO) seeks for the best compromise between cost and safety, by considering system uncertainties. In order to overcome computational difficulties, many formulations have been recently developed, leading to confusion about what method should be selected for a given application, due to the lack of full-scale comparative studies. In this context, the present paper aims at giving an overview of various RBDO approaches which are tested on a benchmark constituted of four examples using mathematical and finite element models, with different levels of difficulties. The study is focused on the three main approaches, namely the two-level approach, the single loop approach and the decoupled approach; for each category, two RBDO formulations are discussed, implemented and tested for numerical examples. The benchmark study allows us to give comprehensive overview of various approaches, to give clear ideas about their capabilities and limitations, and to draw useful conclusions regarding robustness and numerical performance.},
	language = {en},
	number = {2},
	urldate = {2024-10-12},
	journal = {Structural and Multidisciplinary Optimization},
	author = {Aoues, Younes and Chateauneuf, Alaa},
	month = mar,
	year = {2010},
	keywords = {Numerical efficiency, Optimization, Performance measure, Reliability, Reliability index},
	pages = {277--294},
}

@article{bichon_efficient_2008,
	title = {Efficient {Global} {Reliability} {Analysis} for {Nonlinear} {Implicit} {Performance} {Functions}},
	volume = {46},
	issn = {0001-1452},
	url = {https://arc.aiaa.org/doi/10.2514/1.34321},
	doi = {10.2514/1.34321},
	number = {10},
	urldate = {2024-10-12},
	journal = {AIAA Journal},
	author = {Bichon, B. J. and Eldred, M. S. and Swiler, L. P. and Mahadevan, S. and McFarland, J. M.},
	month = oct,
	year = {2008},
	note = {Publisher: American Institute of Aeronautics and Astronautics},
	keywords = {Cumulative Distribution Function, Finite Element Analysis, Maximum Likelihood Estimation, Microelectromechanical Systems, Nelder Mead Simplex, Open Source Software, Residual Stress, Sandia National Laboratories, Surrogate Model, Young's Modulus},
	pages = {2459--2468},
}

@article{chaudhuri_certifiable_2021,
	title = {Certifiable {Risk}-{Based} {Engineering} {Design} {Optimization} {\textbar} {AIAA} {Journal}},
	volume = {60},
	url = {https://arc.aiaa.org/doi/10.2514/1.J060539},
	doi = {https://doi.org/10.2514/1.J060539},
	abstract = {Reliable, risk-averse design of complex engineering systems with optimized performance requires dealing with uncertainties. A conventional approach is to add safety margins to a design that was obtained from deterministic optimization. Safer engineering designs require appropriate cost and constraint function definitions that capture the risk associated with unwanted system behavior in the presence of uncertainties. The paper proposes two notions of certifiability. The first is based on accounting for the magnitude of failure to ensure data-informed conservativeness. The second is the ability to provide optimization convergence guarantees by preserving convexity. Satisfying these notions leads to certifiable risk-based design optimization (CRIBDO). In the context of CRIBDO, risk measures based on superquantile (aka conditional value-at-risk) and buffered probability of failure are analyzed. CRIBDO is contrasted with reliability-based design optimization (RBDO), in which uncertainties are accounted for via the probability of failure through a structural and a thermal design problem. A reformulation of the short column structural design problem leading to a convex CRIBDO problem is presented. The CRIBDO formulations capture more information about the problem to assign the appropriate conservativeness, exhibit superior optimization convergence by preserving properties of underlying functions, and alleviate the adverse effects of choosing hard failure thresholds required in RBDO.},
	number = {2},
	urldate = {2024-10-11},
	journal = {AIAA Journal},
	author = {Chaudhuri, Anirban and Kramer, Boris and Norton, Matthew and Royset, Johannes O.},
	year = {2021},
}

@article{hu_reliability-based_2024,
	title = {Reliability-based design optimization: a state-of-the-art review of its methodologies, applications, and challenges},
	volume = {67},
	issn = {1615-1488},
	shorttitle = {Reliability-based design optimization},
	url = {https://doi.org/10.1007/s00158-024-03884-x},
	doi = {10.1007/s00158-024-03884-x},
	abstract = {Reliability-based design optimization (RBDO) integrates various uncertainties into the design optimization process, offering a more realistic and robust approach compared to traditional deterministic design optimization methods. Thus, RBDO has emerged as a highly compelling and vital research direction within the design field. However, there is currently a dearth of comprehensive reviews on RBDO methodologies presented in a clear and concise manner. This paper aims to address this gap by providing a state-of-the-art review of RBDO methodologies across four key aspects: performance function evaluation, reliability analysis, optimization strategies and algorithms, and RBDO applications in five typical engineering fields. The paper commences by presenting basic RBDO formulations and providing an overall picture of various RBDO methodologies. Subsequently, performance function evaluation methodologies are explained and then categorized into three groups: physics-based performance function evaluation, data-driven performance function evaluation, and physics-informed performance function evaluation. Following this, two types of reliability analysis methodologies are introduced: time-independent reliability analysis and time-dependent reliability analysis. The review also delves into the realm of optimization strategies, with a comprehensive examination of three types: double-loop strategy, single-loop strategy, and decoupling strategy. Moreover, two types of optimization algorithms, the gradient-based algorithm and the meta-heuristic algorithm, are extensively surveyed. Each is scrutinized in terms of their specific methods, advantages, and disadvantages. In addition to methodological exploration, the paper scrutinizes RBDO applications in five engineering fields: wind engineering, aeronautical engineering, ocean engineering, bridge engineering, and vehicle engineering. These applications are carefully surveyed regarding the various uncertainties encountered, the methods employed, and the results of specific RBDO problems. The paper concludes by summarizing key challenges and charting the future work of RBDO research. It offers valuable insights that draw from the analysis of 174 surveyed papers, enabling readers to gain a comprehensive understanding of RBDO theories and facilitating the proper selection and development of appropriate methods for different RBDO stages and problems in diverse engineering contexts.},
	language = {en},
	number = {9},
	urldate = {2024-10-11},
	journal = {Structural and Multidisciplinary Optimization},
	author = {Hu, Weifei and Cheng, Sichuang and Yan, Jiquan and Cheng, Jin and Peng, Xiang and Cho, Hyunkyoo and Lee, Ikjin},
	month = oct,
	year = {2024},
	keywords = {Engineering applications, Optimization algorithm, Optimization strategy, Performance function evaluation, Reliability analysis, Reliability-based design optimization, Uncertainty},
	pages = {168},
}

@article{jerez_reliability-based_2022,
	title = {Reliability-based design optimization of structural systems under stochastic excitation: {An} overview},
	volume = {166},
	issn = {0888-3270},
	shorttitle = {Reliability-based design optimization of structural systems under stochastic excitation},
	url = {https://www.sciencedirect.com/science/article/pii/S0888327021007482},
	doi = {10.1016/j.ymssp.2021.108397},
	abstract = {This article presents a brief survey on some of the latest developments in the area of reliability-based design optimization of structural systems under stochastic excitation. The contributions are grouped into three main categories, namely, sequential optimization approaches, stochastic search based techniques, and schemes based on augmented reliability spaces. The different approaches are described and summarized. In addition, remarks are provided about their range of application, advantages, disadvantages, relevance, and potential research directions. The literature review indicates that computational aspects play a key role in the solution of this class of optimization problems. Besides, this overview suggests that methods for optimal design in stochastic structural dynamics are no longer restricted to academic-type of problems but they can be used as tools in a class of engineering design problems as well.},
	urldate = {2024-10-11},
	journal = {Mechanical Systems and Signal Processing},
	author = {Jerez, D. J. and Jensen, H. A. and Beer, M.},
	month = mar,
	year = {2022},
	keywords = {Metamodels, Optimization techniques, Reliability analysis, Sensitivity analysis, Simulation techniques, Stochastic dynamical systems, Stochastic optimization},
	pages = {108397},
}

@misc{noauthor_certifiable_nodate,
	title = {Certifiable {Risk}-{Based} {Engineering} {Design} {Optimization}},
	url = {https://arc.aiaa.org/doi/epdf/10.2514/1.J060539},
	language = {en},
	urldate = {2024-10-11},
	doi = {10.2514/1.J060539},
}

@misc{dattner_model_2023,
	title = {Model {Selection} for {Ordinary} {Differential} {Equations}: a {Statistical} {Testing} {Approach}},
	shorttitle = {Model {Selection} for {Ordinary} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2308.16438},
	doi = {10.48550/arXiv.2308.16438},
	abstract = {Ordinary differential equations (ODEs) are foundational in modeling intricate dynamics across a gamut of scientific disciplines. Yet, a possibility to represent a single phenomenon through multiple ODE models, driven by different understandings of nuances in internal mechanisms or abstraction levels, presents a model selection challenge. This study introduces a testing-based approach for ODE model selection amidst statistical noise. Rooted in the model misspecification framework, we adapt foundational insights from classical statistical paradigms (Vuong and Hotelling) to the ODE context, allowing for the comparison and ranking of diverse causal explanations without the constraints of nested models. Our simulation studies validate the theoretical robustness of our proposed test, revealing its consistent size and power. Real-world data examples further underscore the algorithm's applicability in practice. To foster accessibility and encourage real-world applications, we provide a user-friendly Python implementation of our model selection algorithm, bridging theoretical advancements with hands-on tools for the scientific community.},
	urldate = {2024-10-09},
	publisher = {arXiv},
	author = {Dattner, Itai and Gugushvili, Shota and Laskorunskyi, Oleksandr},
	month = aug,
	year = {2023},
	note = {arXiv:2308.16438},
	keywords = {Statistics - Methodology},
}

@misc{janik_complexity_2020,
	title = {Complexity for deep neural networks and other characteristics of deep feature representations},
	url = {https://arxiv.org/abs/2006.04791v2},
	abstract = {We define a notion of complexity, which quantifies the nonlinearity of the computation of a neural network, as well as a complementary measure of the effective dimension of feature representations. We investigate these observables both for trained networks for various datasets as well as explore their dynamics during training, uncovering in particular power law scaling. These observables can be understood in a dual way as uncovering hidden internal structure of the datasets themselves as a function of scale or depth. The entropic character of the proposed notion of complexity should allow to transfer modes of analysis from neuroscience and statistical physics to the domain of artificial neural networks. The introduced observables can be applied without any change to the analysis of biological neuronal systems.},
	language = {en},
	urldate = {2024-10-06},
	journal = {arXiv.org},
	author = {Janik, Romuald A. and Witaszczyk, Przemek},
	month = jun,
	year = {2020},
}

@misc{battaglia_relational_2018,
	title = {Relational inductive biases, deep learning, and graph networks},
	url = {http://arxiv.org/abs/1806.01261},
	doi = {10.48550/arXiv.1806.01261},
	abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
	urldate = {2024-10-05},
	publisher = {arXiv},
	author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
	month = oct,
	year = {2018},
	note = {arXiv:1806.01261 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{sanchez-gonzalez_learning_2020,
	title = {Learning to {Simulate} {Complex} {Physics} with {Graph} {Networks}},
	url = {https://arxiv.org/abs/2002.09405v2},
	abstract = {Here we present a machine learning framework and model implementation that can learn to simulate a wide variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework---which we term "Graph Network-based Simulators" (GNS)---represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework advances the state-of-the-art in learned physical simulation, and holds promise for solving a wide range of complex forward and inverse problems.},
	language = {en},
	urldate = {2024-10-05},
	journal = {arXiv.org},
	author = {Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter W.},
	month = feb,
	year = {2020},
}

@misc{toth_hamiltonian_2019,
	title = {Hamiltonian {Generative} {Networks}},
	url = {https://arxiv.org/abs/1909.13789v2},
	abstract = {The Hamiltonian formalism plays a central role in classical and quantum physics. Hamiltonians are the main tool for modelling the continuous time evolution of systems with conserved quantities, and they come equipped with many useful properties, like time reversibility and smooth interpolation in time. These properties are important for many machine learning problems - from sequence prediction to reinforcement learning and density modelling - but are not typically provided out of the box by standard tools such as recurrent neural networks. In this paper, we introduce the Hamiltonian Generative Network (HGN), the first approach capable of consistently learning Hamiltonian dynamics from high-dimensional observations (such as images) without restrictive domain assumptions. Once trained, we can use HGN to sample new trajectories, perform rollouts both forward and backward in time and even speed up or slow down the learned dynamics. We demonstrate how a simple modification of the network architecture turns HGN into a powerful normalising flow model, called Neural Hamiltonian Flow (NHF), that uses Hamiltonian dynamics to model expressive densities. We hope that our work serves as a first practical demonstration of the value that the Hamiltonian formalism can bring to deep learning.},
	language = {en},
	urldate = {2024-10-04},
	journal = {arXiv.org},
	author = {Toth, Peter and Rezende, Danilo Jimenez and Jaegle, Andrew and Racanière, Sébastien and Botev, Aleksandar and Higgins, Irina},
	month = sep,
	year = {2019},
}

@misc{wen_flipout_2018,
	title = {Flipout: {Efficient} {Pseudo}-{Independent} {Weight} {Perturbations} on {Mini}-{Batches}},
	shorttitle = {Flipout},
	url = {http://arxiv.org/abs/1803.04386},
	doi = {10.48550/arXiv.1803.04386},
	abstract = {Stochastic neural net weights are used in a variety of contexts, including regularization, Bayesian neural nets, exploration in reinforcement learning, and evolution strategies. Unfortunately, due to the large number of weights, all the examples in a mini-batch typically share the same weight perturbation, thereby limiting the variance reduction effect of large mini-batches. We introduce flipout, an efficient method for decorrelating the gradients within a mini-batch by implicitly sampling pseudo-independent weight perturbations for each example. Empirically, flipout achieves the ideal linear variance reduction for fully connected networks, convolutional networks, and RNNs. We find significant speedups in training neural networks with multiplicative Gaussian perturbations. We show that flipout is effective at regularizing LSTMs, and outperforms previous methods. Flipout also enables us to vectorize evolution strategies: in our experiments, a single GPU with flipout can handle the same throughput as at least 40 CPU cores using existing methods, equivalent to a factor-of-4 cost reduction on Amazon Web Services.},
	urldate = {2024-10-03},
	publisher = {arXiv},
	author = {Wen, Yeming and Vicol, Paul and Ba, Jimmy and Tran, Dustin and Grosse, Roger},
	month = apr,
	year = {2018},
	note = {arXiv:1803.04386 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{alemi_fixing_2017,
	title = {Fixing a {Broken} {ELBO}},
	url = {https://arxiv.org/abs/1711.00464v3},
	abstract = {Recent work in unsupervised representation learning has focused on learning deep directed latent-variable models. Fitting these models by maximizing the marginal likelihood or evidence is typically intractable, thus a common approximation is to maximize the evidence lower bound (ELBO) instead. However, maximum likelihood training (whether exact or approximate) does not necessarily result in a good latent representation, as we demonstrate both theoretically and empirically. In particular, we derive variational lower and upper bounds on the mutual information between the input and the latent variable, and use these bounds to derive a rate-distortion curve that characterizes the tradeoff between compression and reconstruction accuracy. Using this framework, we demonstrate that there is a family of models with identical ELBO, but different quantitative and qualitative characteristics. Our framework also suggests a simple new method to ensure that latent variable models with powerful stochastic decoders do not ignore their latent code.},
	language = {en},
	urldate = {2024-10-03},
	journal = {arXiv.org},
	author = {Alemi, Alexander A. and Poole, Ben and Fischer, Ian and Dillon, Joshua V. and Saurous, Rif A. and Murphy, Kevin},
	month = nov,
	year = {2017},
}

@misc{liu_two_2016,
	title = {Two {Methods} {For} {Wild} {Variational} {Inference}},
	url = {https://arxiv.org/abs/1612.00081v2},
	abstract = {Variational inference provides a powerful tool for approximate probabilistic in- ference on complex, structured models. Typical variational inference methods, however, require to use inference networks with computationally tractable proba- bility density functions. This largely limits the design and implementation of vari- ational inference methods. We consider wild variational inference methods that do not require tractable density functions on the inference networks, and hence can be applied in more challenging cases. As an example of application, we treat stochastic gradient Langevin dynamics (SGLD) as an inference network, and use our methods to automatically adjust the step sizes of SGLD, yielding significant improvement over the hand-designed step size schemes},
	language = {en},
	urldate = {2024-10-03},
	journal = {arXiv.org},
	author = {Liu, Qiang and Feng, Yihao},
	month = nov,
	year = {2016},
}

@misc{chang_kernel_2020,
	title = {Kernel {Stein} {Generative} {Modeling}},
	url = {http://arxiv.org/abs/2007.03074},
	doi = {10.48550/arXiv.2007.03074},
	abstract = {We are interested in gradient-based Explicit Generative Modeling where samples can be derived from iterative gradient updates based on an estimate of the score function of the data distribution. Recent advances in Stochastic Gradient Langevin Dynamics (SGLD) demonstrates impressive results with energy-based models on high-dimensional and complex data distributions. Stein Variational Gradient Descent (SVGD) is a deterministic sampling algorithm that iteratively transports a set of particles to approximate a given distribution, based on functional gradient descent that decreases the KL divergence. SVGD has promising results on several Bayesian inference applications. However, applying SVGD on high dimensional problems is still under-explored. The goal of this work is to study high dimensional inference with SVGD. We first identify key challenges in practical kernel SVGD inference in high-dimension. We propose noise conditional kernel SVGD (NCK-SVGD), that works in tandem with the recently introduced Noise Conditional Score Network estimator. NCK is crucial for successful inference with SVGD in high dimension, as it adapts the kernel to the noise level of the score estimate. As we anneal the noise, NCK-SVGD targets the real data distribution. We then extend the annealed SVGD with an entropic regularization. We show that this offers a flexible control between sample quality and diversity, and verify it empirically by precision and recall evaluations. The NCK-SVGD produces samples comparable to GANs and annealed SGLD on computer vision benchmarks, including MNIST and CIFAR-10.},
	urldate = {2024-10-03},
	publisher = {arXiv},
	author = {Chang, Wei-Cheng and Li, Chun-Liang and Mroueh, Youssef and Yang, Yiming},
	month = jul,
	year = {2020},
	note = {arXiv:2007.03074 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{higham_11_2008,
	series = {Other {Titles} in {Applied} {Mathematics}},
	title = {11. {Matrix} {Logarithm}},
	isbn = {9780898716467},
	url = {https://epubs.siam.org/doi/10.1137/1.9780898717778.ch11},
	abstract = {A logarithm of A ∈ ℂn×n is any matrix X such that eX = A. As we saw in Theorem 1.27, any nonsingular A has infinitely many logarithms. In this chapter A ∈ ℂn×n is assumed to have no eigenvalues on ℝ− and “log” always denotes the principal logarithm, which we recall from Theorem 1.31 is the unique logarithm whose spectrum lies in the strip \{ z : − π {\textless} Im(z) {\textless} π \}.The importance of the matrix logarithm can be ascribed to it being the inverse function of the matrix exponential and this intimate relationship leads to close connections between the theory and computational methods for the two functions.This chapter is organized as follows. We begin by developing some basic properties of the logarithm, including conditions under which the product formula log(BC) = log(B) + log(C) holds. Then we consider the Fréchet derivative and conditioning. The Mercator and Gregory series expansions are derived and various properties of the diagonal Padé approximants to the logarithm are explained. Two versions of the inverse scaling and squaring method are developed in some detail, one using the Schur form and the other working with full matrices. A Schur—Parlett algorithm employing inverse scaling and squaring on the diagonal blocks together with a special formula for 2 × 2 blocks is then derived. A numerical experiment comparing four different methods is then presented. Finally, an algorithm for evaluating the Fréchet derivative is described.11.1. Basic PropertiesWe begin with an integral expression for the logarithm.Theorem 11.1 (Richter). For A ∈ ℂn×n with no eigenvalues on ℝ−,log(A)=∫01(A−I)[t(A−I)+I]−1dt.11.1Proof. It suffices to prove the result for diagonalizable A, by Theorem 1.20, and hence it suffices to show that log x = ʃ01 (x − 1) [t(x − 1) + l]−1 dt for x ∈ ℂ lying off ℝ−; this latter equality is immediate.Now we turn to useful identities satisfied by the logarithm. Because of the multivalued nature of the logarithm it is not generally the case that log(eA) = A, though a sufficient condition for the equality is derived in Problem 1.39. To understand the scalar case of this equality we use the unwinding number of z ∈ ℂ defined byU(z)=z−log(ez)2πi.11.2},
	urldate = {2024-10-02},
	booktitle = {Functions of {Matrices}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Higham, Nicholas J.},
	month = jan,
	year = {2008},
	doi = {10.1137/1.9780898717778.ch11},
	pages = {269--286},
}

@article{rapisarda_parameterizing_2007,
	title = {Parameterizing correlations: a geometric interpretation},
	volume = {18},
	issn = {1471-678X},
	shorttitle = {Parameterizing correlations},
	url = {https://doi.org/10.1093/imaman/dpl010},
	doi = {10.1093/imaman/dpl010},
	abstract = {In this paper, we present a new interpretation of the parameterization of a correlation matrix proposed earlier by some authors (Jäckel \&amp; Rebonato, 1999). This interpretation is based on viewing any correlation matrix as the result of the scalar products of a suitable set of unit vectors in a multidimensional space, each rotated from all the others by generalized Euler angles. It is possible to exploit the intuitive nature of this approach in order to obtain more efficient optimization schemes when calibrating a reduced-form model to a desired correlation structure.},
	number = {1},
	urldate = {2024-10-02},
	journal = {IMA Journal of Management Mathematics},
	author = {Rapisarda, Francesco and Brigo, Damiano and Mercurio, Fabio},
	month = jan,
	year = {2007},
	pages = {55--73},
}

@article{pasari_machine_2022,
	title = {Machine learning algorithms in geostatistical data analysis: formulation and observation},
	volume = {1032},
	issn = {1755-1315},
	shorttitle = {Machine learning algorithms in geostatistical data analysis},
	url = {https://dx.doi.org/10.1088/1755-1315/1032/1/012008},
	doi = {10.1088/1755-1315/1032/1/012008},
	abstract = {Geostatistical analyses on spatiotemporal datasets have been increasingly popular on diverse disciplines including environmental sciences, soil and earth studies, meteorology, hydrology, and oceanography. They help in documenting and summarizing information to understand the variation of the process and other parameters. Due to high spatial variability and noise in the dataset, traditional geostatistical tools often fail to produce desirable results. In view of this, the present study proposes two machine learning methods, namely artificial neural network (ANN) and k-nearest neighbour (kNN), along with the kriging interpolation to derive temporal distribution of soil parameters. Results in terms of smooth porosity maps demonstrate the effectiveness of the proposed algorithm.},
	language = {en},
	number = {1},
	urldate = {2024-10-02},
	journal = {IOP Conference Series: Earth and Environmental Science},
	author = {Pasari, S. and Asudani, P. and Mehta, A.},
	month = jun,
	year = {2022},
	note = {Publisher: IOP Publishing},
	pages = {012008},
}

@misc{zhou_survey_2015,
	title = {A {Survey} on {Contextual} {Multi}-armed {Bandits}},
	url = {https://arxiv.org/abs/1508.03326v2},
	abstract = {In this survey we cover a few stochastic and adversarial contextual bandit algorithms. We analyze each algorithm's assumption and regret bound.},
	language = {en},
	urldate = {2024-10-02},
	journal = {arXiv.org},
	author = {Zhou, Li},
	month = aug,
	year = {2015},
}

@article{lucchetti_spherical_2024,
	title = {The {Spherical} {Parametrisation} for {Correlation} {Matrices} and its {Computational} {Advantages}},
	volume = {64},
	issn = {1572-9974},
	url = {https://doi.org/10.1007/s10614-023-10467-3},
	doi = {10.1007/s10614-023-10467-3},
	abstract = {In this paper, we analyse the computational advantages of the spherical parametrisation for correlation matrices in the context of Maximum Likelihood estimation via numerical optimisation. By using the special structure of correlation matrices, it is possible to define a bijective transformation of an \$\$n {\textbackslash}times n\$\$correlation matrix \$\$R\$\$into a vector of \$\$n(n-1)/2\$\$angles between 0 and \$\${\textbackslash}pi\$\$. After discussing the algebraic aspects of the problem, we provide examples of the use of the technique we propose in popular econometric models: the multivariate DCC-GARCH model, widely used in applied finance for large-scale problems, and the multivariate probit model, for which the computation of the likelihood is typically accomplished by simulated Maximum Likelihood. Our analysis reveals the conditions when the spherical parametrisation is advantageous; numerical optimisation algorithms are often more robust and efficient, especially when R is large and near-singular.},
	language = {en},
	number = {2},
	urldate = {2024-10-01},
	journal = {Computational Economics},
	author = {Lucchetti, Riccardo and Pedini, Luca},
	month = aug,
	year = {2024},
	keywords = {Correlation matrix, DCC-GARCH model, Multivariate probit, Spherical coordinates},
	pages = {1023--1046},
}

@article{ledoit_well-conditioned_2004,
	title = {A well-conditioned estimator for large-dimensional covariance matrices},
	volume = {88},
	issn = {0047-259X},
	url = {https://www.sciencedirect.com/science/article/pii/S0047259X03000964},
	doi = {10.1016/S0047-259X(03)00096-4},
	abstract = {Many applied problems require a covariance matrix estimator that is not only invertible, but also well-conditioned (that is, inverting it does not amplify estimation error). For large-dimensional covariance matrices, the usual estimator—the sample covariance matrix—is typically not well-conditioned and may not even be invertible. This paper introduces an estimator that is both well-conditioned and more accurate than the sample covariance matrix asymptotically. This estimator is distribution-free and has a simple explicit formula that is easy to compute and interpret. It is the asymptotically optimal convex linear combination of the sample covariance matrix with the identity matrix. Optimality is meant with respect to a quadratic loss function, asymptotically as the number of observations and the number of variables go to infinity together. Extensive Monte Carlo confirm that the asymptotic results tend to hold well in finite sample.},
	number = {2},
	urldate = {2024-10-01},
	journal = {Journal of Multivariate Analysis},
	author = {Ledoit, Olivier and Wolf, Michael},
	month = feb,
	year = {2004},
	keywords = {Condition number, Covariance matrix estimation, Empirical Bayes, General asymptotics, Shrinkage},
	pages = {365--411},
}

@book{maruyama_stein_2023,
	address = {Singapore},
	series = {{SpringerBriefs} in {Statistics}},
	title = {Stein {Estimation}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {9789819960767 9789819960774},
	url = {https://link.springer.com/10.1007/978-981-99-6077-4},
	language = {en},
	urldate = {2024-10-01},
	publisher = {Springer Nature},
	author = {Maruyama, Yuzo and Kubokawa, Tatsuya and Strawderman, William E.},
	year = {2023},
	doi = {10.1007/978-981-99-6077-4},
	keywords = {Admissibility, Bayes, James-Stein Estimator, Minimaxity, Stein Paradox},
}

@article{ledoit_power_2022,
	title = {The {Power} of ({Non}-){Linear} {Shrinking}: {A} {Review} and {Guide} to {Covariance} {Matrix} {Estimation}},
	volume = {20},
	issn = {1479-8409},
	shorttitle = {The {Power} of ({Non}-){Linear} {Shrinking}},
	url = {https://doi.org/10.1093/jjfinec/nbaa007},
	doi = {10.1093/jjfinec/nbaa007},
	abstract = {Many econometric and data-science applications require a reliable estimate of the covariance matrix, such as Markowitz’s portfolio selection. When the number of variables is of the same magnitude as the number of observations, this constitutes a difficult estimation problem; the sample covariance matrix certainly will not do. In this article, we review our work in this area, going back 15+ years. We have promoted various shrinkage estimators, which can be classified into linear and nonlinear. Linear shrinkage is simpler to understand, to derive, and to implement. But nonlinear shrinkage can deliver another level of performance improvement, especially if overlaid with stylized facts such as time-varying co-volatility or factor models.},
	number = {1},
	urldate = {2024-10-01},
	journal = {Journal of Financial Econometrics},
	author = {Ledoit, Olivier and Wolf, Michael},
	month = jan,
	year = {2022},
	pages = {187--218},
}

@misc{schafer_compression_2020,
	title = {Compression, inversion, and approximate {PCA} of dense kernel matrices at near-linear computational complexity},
	url = {http://arxiv.org/abs/1706.02205},
	doi = {10.48550/arXiv.1706.02205},
	abstract = {Dense kernel matrices \${\textbackslash}Theta {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}\{N {\textbackslash}times N\}\$ obtained from point evaluations of a covariance function \$G\$ at locations \${\textbackslash}\{ x\_\{i\} {\textbackslash}\}\_\{1 {\textbackslash}leq i {\textbackslash}leq N\} {\textbackslash}subset {\textbackslash}mathbb\{R\}{\textasciicircum}\{d\}\$ arise in statistics, machine learning, and numerical analysis. For covariance functions that are Green's functions of elliptic boundary value problems and homogeneously-distributed sampling points, we show how to identify a subset \$S {\textbackslash}subset {\textbackslash}\{ 1 , {\textbackslash}dots , N {\textbackslash}\}{\textasciicircum}2\$, with \${\textbackslash}\# S = O ( N {\textbackslash}log (N) {\textbackslash}log{\textasciicircum}\{d\} ( N /{\textbackslash}epsilon ) )\$, such that the zero fill-in incomplete Cholesky factorisation of the sparse matrix \${\textbackslash}Theta\_\{ij\} 1\_\{( i, j ) {\textbackslash}in S\}\$ is an \${\textbackslash}epsilon\$-approximation of \${\textbackslash}Theta\$. This factorisation can provably be obtained in complexity \$O ( N {\textbackslash}log( N ) {\textbackslash}log{\textasciicircum}\{d\}( N /{\textbackslash}epsilon) )\$ in space and \$O ( N {\textbackslash}log{\textasciicircum}\{2\}( N ) {\textbackslash}log{\textasciicircum}\{2d\}( N /{\textbackslash}epsilon) )\$ in time, improving upon the state of the art for general elliptic operators; we further present numerical evidence that \$d\$ can be taken to be the intrinsic dimension of the data set rather than that of the ambient space. The algorithm only needs to know the spatial configuration of the \$x\_\{i\}\$ and does not require an analytic representation of \$G\$. Furthermore, this factorization straightforwardly provides an approximate sparse PCA with optimal rate of convergence in the operator norm. Hence, by using only subsampling and the incomplete Cholesky factorization, we obtain, at nearly linear complexity, the compression, inversion and approximate PCA of a large class of covariance matrices. By inverting the order of the Cholesky factorization we also obtain a solver for elliptic PDE with complexity \$O ( N {\textbackslash}log{\textasciicircum}\{d\}( N /{\textbackslash}epsilon) )\$ in space and \$O ( N {\textbackslash}log{\textasciicircum}\{2d\}( N /{\textbackslash}epsilon) )\$ in time, improving upon the state of the art for general elliptic operators.},
	urldate = {2024-10-01},
	publisher = {arXiv},
	author = {Schäfer, Florian and Sullivan, T. J. and Owhadi, Houman},
	month = oct,
	year = {2020},
	note = {arXiv:1706.02205 [cs, math]},
	keywords = {65F30, 42C40, 65F50, 65N55, 65N75, 60G42, 68Q25, 68W40, Computer Science - Computational Complexity, Computer Science - Data Structures and Algorithms, Mathematics - Numerical Analysis, Mathematics - Probability},
}

@article{rothman_new_2010,
	title = {A new approach to {Cholesky}-based covariance regularization in high dimensions},
	volume = {97},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/25734107},
	abstract = {In this paper we propose a new regression interpretation of the Cholesky factor of the covariance matrix, as opposed to the well-known regression interpretation of the Cholesky factor of the inverse covariance, which leads to a new class of regularized covariance estimators suitable for high-dimensional problems. Regularizing the Cholesky factor of the covariance via this regression interpretation always results in a positive definite estimator. In particular, one can obtain a positive definite banded estimator of the covariance matrix at the same computational cost as the popular banded estimator of Bickel \& Levina (2008b), which is not guaranteed to be positive definite. We also establish theoretical connections between banding Cholesky factors of the covariance matrix and its inverse and constrained maximum likelihood estimation under the banding constraint, and compare the numerical performance of several methods in simulations and on a sonar data example.},
	number = {3},
	urldate = {2024-10-01},
	journal = {Biometrika},
	author = {Rothman, Adam J. and Levina, Elizaveta and Zhu, Ji},
	year = {2010},
	pages = {539--550},
}

@article{chiu_matrix-logarithmic_1996,
	title = {The {Matrix}-{Logarithmic} {Covariance} {Model}},
	volume = {91},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1996.10476677},
	doi = {10.1080/01621459.1996.10476677},
	abstract = {A flexible method is introduced to model the structure of a covariance matrix C and study the dependence of the covariances on explanatory variables by observing that for any real symmetric matrix A, the matrix exponential transformation, C = exp (A), is a positive definite matrix. Because there is no constraint on the possible values of the upper triangular elements on A, any possible structure of interest can be imposed on them. The method presented here is not intended to replace the existing special models available for a covariance matrix, but rather to provide a broad range of further structures that supplements existing methodology. Maximum likelihood estimation procedures are used to estimate the parameters, and the large-sample asymptotic properties are obtained. A simulation study and two real-life examples are given to illustrate the method introduced.},
	number = {433},
	urldate = {2024-09-30},
	journal = {Journal of the American Statistical Association},
	author = {Chiu, Tom Y. M. and Leonard, Tom and Tsui, Kam-Wah},
	month = mar,
	year = {1996},
	keywords = {Covariance matrix, Golden—Thompson inequality, Matrix exponential transformation, Maximum likelihood estimation, Volterra integral equation},
	pages = {198--210},
}

@article{lee_model_2020,
	title = {Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders},
	volume = {404},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999119306783},
	doi = {10.1016/j.jcp.2019.108973},
	abstract = {Nearly all model-reduction techniques project the governing equations onto a linear subspace of the original state space. Such subspaces are typically computed using methods such as balanced truncation, rational interpolation, the reduced-basis method, and (balanced) proper orthogonal decomposition (POD). Unfortunately, restricting the state to evolve in a linear subspace imposes a fundamental limitation to the accuracy of the resulting reduced-order model (ROM). In particular, linear-subspace ROMs can be expected to produce low-dimensional models with high accuracy only if the problem admits a fast decaying Kolmogorov n-width (e.g., diffusion-dominated problems). Unfortunately, many problems of interest exhibit a slowly decaying Kolmogorov n-width (e.g., advection-dominated problems). To address this, we propose a novel framework for projecting dynamical systems onto nonlinear manifolds using minimum-residual formulations at the time-continuous and time-discrete levels; the former leads to manifold Galerkin projection, while the latter leads to manifold least-squares Petrov–Galerkin (LSPG) projection. We perform analyses that provide insight into the relationship between these proposed approaches and classical linear-subspace reduced-order models; we also derive a posteriori discrete-time error bounds for the proposed approaches. In addition, we propose a computationally practical approach for computing the nonlinear manifold, which is based on convolutional autoencoders from deep learning. Finally, we demonstrate the ability of the method to significantly outperform even the optimal linear-subspace ROM on benchmark advection-dominated problems, thereby demonstrating the method's ability to overcome the intrinsic n-width limitations of linear subspaces.},
	urldate = {2024-09-30},
	journal = {Journal of Computational Physics},
	author = {Lee, Kookjin and Carlberg, Kevin T.},
	month = mar,
	year = {2020},
	keywords = {Autoencoders, Deep learning, Machine learning, Model reduction, Nonlinear manifolds, Optimal projection},
	pages = {108973},
}

@article{marin_approximate_2012,
	title = {Approximate {Bayesian} computational methods},
	volume = {22},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-011-9288-2},
	doi = {10.1007/s11222-011-9288-2},
	abstract = {Approximate Bayesian Computation (ABC) methods, also known as likelihood-free techniques, have appeared in the past ten years as the most satisfactory approach to intractable likelihood problems, first in genetics then in a broader spectrum of applications. However, these methods suffer to some degree from calibration difficulties that make them rather volatile in their implementation and thus render them suspicious to the users of more traditional Monte Carlo methods. In this survey, we study the various improvements and extensions brought on the original ABC algorithm in recent years.},
	language = {en},
	number = {6},
	urldate = {2024-09-30},
	journal = {Statistics and Computing},
	author = {Marin, Jean-Michel and Pudlo, Pierre and Robert, Christian P. and Ryder, Robin J.},
	month = nov,
	year = {2012},
	keywords = {ABC methodology, Artificial Intelligence, Bayesian model choice, Bayesian statistics, DIYABC, Likelihood-free methods},
	pages = {1167--1180},
}

@article{bucci_comparing_2022,
	title = {Comparing unconstrained parametrization methods for return covariance matrix prediction},
	volume = {32},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-022-10157-4},
	doi = {10.1007/s11222-022-10157-4},
	abstract = {Forecasting covariance matrices is a difficult task in many research fields since the predicted matrices should be at least positive semidefinite. This problem can be overcome by including constraints in the predictive model or through a parametrization of the matrices to be predicted. In this paper, we focus on the latter approach in a financial application and analyse four parametrizations of the covariance matrices of asset returns. The aim of the manuscript is to understand if the parametrizations of the covariance matrices exhibit differences in terms of predictive accuracy. To this end, we critically analyse their predictive performance through both a Monte Carlo simulation and an empirical application with daily and weekly realized covariance matrices of stock assets. Our findings highlight that the Cholesky decomposition and the parametrization recently introduced by Archakov and Hansen are the overall best-performing methods in terms of forecasting accuracy.},
	language = {en},
	number = {5},
	urldate = {2024-09-30},
	journal = {Statistics and Computing},
	author = {Bucci, Andrea and Ippoliti, Luigi and Valentini, Pasquale},
	month = oct,
	year = {2022},
	keywords = {Artificial Intelligence, Covariance model, Parametrization, Realized volatility, Volatility forecasting},
	pages = {90},
}

@misc{bischoff_practical_2024,
	title = {A {Practical} {Guide} to {Statistical} {Distances} for {Evaluating} {Generative} {Models} in {Science}},
	url = {http://arxiv.org/abs/2403.12636},
	doi = {10.48550/arXiv.2403.12636},
	abstract = {Generative models are invaluable in many fields of science because of their ability to capture high-dimensional and complicated distributions, such as photo-realistic images, protein structures, and connectomes. How do we evaluate the samples these models generate? This work aims to provide an accessible entry point to understanding popular notions of statistical distances, requiring only foundational knowledge in mathematics and statistics. We focus on four commonly used notions of statistical distances representing different methodologies: Using low-dimensional projections (Sliced-Wasserstein; SW), obtaining a distance using classifiers (Classifier Two-Sample Tests; C2ST), using embeddings through kernels (Maximum Mean Discrepancy; MMD), or neural networks (Fr{\textbackslash}'echet Inception Distance; FID). We highlight the intuition behind each distance and explain their merits, scalability, complexity, and pitfalls. To demonstrate how these distances are used in practice, we evaluate generative models from different scientific domains, namely a model of decision making and a model generating medical images. We showcase that distinct distances can give different results on similar data. Through this guide, we aim to help researchers to use, interpret, and evaluate statistical distances for generative models in science.},
	urldate = {2024-09-26},
	publisher = {arXiv},
	author = {Bischoff, Sebastian and Darcher, Alana and Deistler, Michael and Gao, Richard and Gerken, Franziska and Gloeckler, Manuel and Haxel, Lisa and Kapoor, Jaivardhan and Lappalainen, Janne K. and Macke, Jakob H. and Moss, Guy and Pals, Matthijs and Pei, Felix and Rapp, Rachel and Sağtekin, A. Erdem and Schröder, Cornelius and Schulz, Auguste and Stefanidi, Zinovia and Toyota, Shoji and Ulmer, Linda and Vetter, Julius},
	month = mar,
	year = {2024},
	note = {arXiv:2403.12636 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{alsup_multilevel_2022,
	title = {Multilevel {Stein} variational gradient descent with applications to {Bayesian} inverse problems},
	url = {https://proceedings.mlr.press/v145/alsup22a.html},
	abstract = {This work presents a multilevel variant of Stein variational gradient descent to more efficiently sample from target distributions. The key ingredient is a sequence of distributions with growing fidelity and costs that converges to the target distribution of interest. For example, such a sequence of distributions is given by a hierarchy of ever finer discretization levels of the forward model in Bayesian inverse problems. The proposed multilevel Stein variational gradient descent moves most of the iterations to lower, cheaper levels with the aim of requiring only a few iterations on the higher, more expensive levels when compared to the traditional, single-level Stein variational gradient descent variant that uses the highest-level distribution only. Under certain assumptions, in the mean-field limit, the error of the proposed multilevel Stein method decays by a log factor faster than the error of the single-level counterpart with respect to computational costs. Numerical experiments with Bayesian inverse problems show speedups of more than one order of magnitude of the proposed multilevel Stein method compared to the single-level variant that uses the highest level only.},
	language = {en},
	urldate = {2024-09-18},
	booktitle = {Proceedings of the 2nd {Mathematical} and {Scientific} {Machine} {Learning} {Conference}},
	publisher = {PMLR},
	author = {Alsup, Terrence and Venturi, Luca and Peherstorfer, Benjamin},
	month = apr,
	year = {2022},
	pages = {93--117},
}

@inproceedings{ba_towards_2019,
	title = {Towards {Characterizing} the {High}-dimensional {Bias} of {Kernel}-based {Particle} {Inference} {Algorithms}},
	url = {https://openreview.net/forum?id=Byg9KJn4Fr},
	abstract = {Particle-based inference algorithm is a promising method to efficiently generate samples for an intractable target distribution by iteratively updating a set of particles. As a noticeable example, Stein variational gradient descent (SVGD) provides a deterministic and computationally efficient update, but it is known to underestimate the variance in high dimensions, the mechanism of which is poorly understood. In this work we explore a connection between SVGD and MMD-based inference algorithm via Stein's lemma. By comparing the two update rules, we identify the source of bias in SVGD as a combination of high variance and deterministic bias, and empirically demonstrate that the removal of either factors leads to accurate estimation of the variance. In addition, for learning high-dimensional Gaussian target, we analytically derive the converged variance for both algorithms, and confirm that only SVGD suffers from the "curse of dimensionality".},
	language = {en},
	urldate = {2024-09-18},
	author = {Ba, Jimmy and Erdogdu, Murat A. and Ghassemi, Marzyeh and Suzuki, Taiji and Sun, Shengyang and Wu, Denny and Zhang, Tianzong},
	month = dec,
	year = {2019},
}

@misc{martin_informative_2021,
	title = {Informative priors for correlation matrices: {An} easy approach},
	shorttitle = {Informative priors for correlation matrices},
	url = {https://srmart.in/informative-priors-for-correlation-matrices-an-easy-approach/},
	language = {en-US},
	urldate = {2024-09-18},
	journal = {Stephen R. Martin, PhD},
	author = {Martin, Stephen},
	month = apr,
	year = {2021},
}

@misc{jantre_learning_2023,
	title = {Learning {Active} {Subspaces} for {Effective} and {Scalable} {Uncertainty} {Quantification} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2309.03061},
	doi = {10.48550/arXiv.2309.03061},
	abstract = {Bayesian inference for neural networks, or Bayesian deep learning, has the potential to provide well-calibrated predictions with quantified uncertainty and robustness. However, the main hurdle for Bayesian deep learning is its computational complexity due to the high dimensionality of the parameter space. In this work, we propose a novel scheme that addresses this limitation by constructing a low-dimensional subspace of the neural network parameters-referred to as an active subspace-by identifying the parameter directions that have the most significant influence on the output of the neural network. We demonstrate that the significantly reduced active subspace enables effective and scalable Bayesian inference via either Monte Carlo (MC) sampling methods, otherwise computationally intractable, or variational inference. Empirically, our approach provides reliable predictions with robust uncertainty estimates for various regression tasks.},
	urldate = {2024-09-14},
	publisher = {arXiv},
	author = {Jantre, Sanket and Urban, Nathan M. and Qian, Xiaoning and Yoon, Byung-Jun},
	month = sep,
	year = {2023},
	note = {arXiv:2309.03061 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@inproceedings{ramdas_decreasing_2015,
	address = {Austin, Texas},
	series = {{AAAI}'15},
	title = {On the decreasing power of kernel and distance based nonparametric hypothesis tests in high dimensions},
	isbn = {9780262511292},
	abstract = {This paper is about two related decision theoretic problems, nonparametric two-sample testing and independence testing. There is a belief that two recently proposed solutions, based on kernels and distances between pairs of points, behave well in high-dimensional settings. We identify different sources of misconception that give rise to the above belief. Specifically, we differentiate the hardness of estimation of test statistics from the hardness of testing whether these statistics are zero or not, and explicitly discuss a notion of "fair" alternative hypotheses for these problems as dimension increases. We then demonstrate that the power of these tests actually drops polynomially with increasing dimension against fair alternatives. We end with some theoretical insights and shed light on the median heuristic for kernel bandwidth selection. Our work advances the current understanding of the power of modern nonpara-metric hypothesis tests in high dimensions.},
	urldate = {2024-09-13},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Ramdas, Aaditya and Reddi, Sashank J. and Póczos, Barnabás and Singh, Aarti and Wasserman, Larry},
	month = jan,
	year = {2015},
	pages = {3571--3577},
}

@inproceedings{blundell_weight_2015,
	title = {Weight {Uncertainty} in {Neural} {Network}},
	url = {https://proceedings.mlr.press/v37/blundell15.html},
	abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	language = {en},
	urldate = {2024-09-05},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	month = jun,
	year = {2015},
	pages = {1613--1622},
}

@misc{english_kernelised_2024,
	title = {Kernelised {Normalising} {Flows}},
	url = {http://arxiv.org/abs/2307.14839},
	doi = {10.48550/arXiv.2307.14839},
	abstract = {Normalising Flows are non-parametric statistical models characterised by their dual capabilities of density estimation and generation. This duality requires an inherently invertible architecture. However, the requirement of invertibility imposes constraints on their expressiveness, necessitating a large number of parameters and innovative architectural designs to achieve good results. Whilst flow-based models predominantly rely on neural-network-based transformations for expressive designs, alternative transformation methods have received limited attention. In this work, we present Ferumal flow, a novel kernelised normalising flow paradigm that integrates kernels into the framework. Our results demonstrate that a kernelised flow can yield competitive or superior results compared to neural network-based flows whilst maintaining parameter efficiency. Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability.},
	urldate = {2024-09-04},
	publisher = {arXiv},
	author = {English, Eshant and Kirchler, Matthias and Lippert, Christoph},
	month = jun,
	year = {2024},
	note = {arXiv:2307.14839 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{wu_parallel_2016,
	title = {The {Parallel} {Knowledge} {Gradient} {Method} for {Batch} {Bayesian} {Optimization}},
	url = {https://arxiv.org/abs/1606.04414v4},
	abstract = {In many applications of black-box optimization, one can evaluate multiple points simultaneously, e.g. when evaluating the performances of several different neural network architectures in a parallel computing environment. In this paper, we develop a novel batch Bayesian optimization algorithm --- the parallel knowledge gradient method. By construction, this method provides the one-step Bayes-optimal batch of points to sample. We provide an efficient strategy for computing this Bayes-optimal batch of points, and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch Bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms, especially when function evaluations are noisy.},
	language = {en},
	urldate = {2024-08-27},
	journal = {arXiv.org},
	author = {Wu, Jian and Frazier, Peter I.},
	month = jun,
	year = {2016},
}

@article{ba_sequential_2018,
	title = {A {Sequential} {Maximum} {Projection} {Design} {Framework} for {Computer} {Experiments} with {Inert} {Factors}},
	volume = {28},
	issn = {1017-0405},
	url = {https://www.jstor.org/stable/44841929},
	abstract = {Many computer experiments involve a large number of input factors, but many of them are inert and only a subset are important. This paper develops a new sequential design framework that can accommodate multiple responses and quickly screen out inert factors so that the final design is space-filling with respect to the active factors. By folding over Latin hypercube designs with sliced structure, this sequential design can have flexible sample size in each stage and also ensure that each stage, as well as the whole combined design, are all approximately Latin hypercube designs. The sequential framework does not require prescribing the total sample size and, under the presence of inert factors, can lead to substantial savings in simulation resources. Even if all factors are important, the proposed sequential design can still achieve a similar overall space-filling property compared to a maximin Latin hypercube design optimized in a single stage.},
	number = {2},
	urldate = {2024-08-26},
	journal = {Statistica Sinica},
	author = {Ba, Shan and Myers, William R. and Wang, Dianpeng},
	year = {2018},
	note = {Publisher: Institute of Statistical Science, Academia Sinica},
	pages = {879--897},
}

@book{santner_design_2018,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {The {Design} and {Analysis} of {Computer} {Experiments}},
	copyright = {http://www.springer.com/tdm},
	isbn = {9781493988457 9781493988471},
	url = {http://link.springer.com/10.1007/978-1-4939-8847-1},
	urldate = {2024-08-26},
	publisher = {Springer},
	author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	year = {2018},
	doi = {10.1007/978-1-4939-8847-1},
	keywords = {Bayesian inference, Gaussian Process models, Latin hypercube designs, best linear unbiased predictors, calibration, computer experiment, experimental design, heuristic global approximation, log likelihood functions, sensitivity analysis, simulator output, stochastic process models, variable screening},
}

@article{wu_efficient_2017,
	title = {Efficient space-filling and near-orthogonality sequential {Latin} hypercube for computer experiments},
	volume = {324},
	issn = {0045-7825},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782516309252},
	doi = {10.1016/j.cma.2017.05.020},
	abstract = {In this paper, a general and efficient method for constructing LHDs possessing favorable characteristics of orthogonality and better computational efficiency is first proposed. Next, a sequentially enlarging strategy is derived based on the initial design. During the enlarging process, optimization of space-filling property is carried out, and a near-orthogonal space-filling augmented LHD is subsequently obtained. The enlarging process is repeated iteratively until the desired accuracy is achieved. Numerical examples and two mechanical applications are utilized in order to confirm and emphasize the effectiveness of this proposed method.},
	urldate = {2024-08-26},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Wu, Zeping and Wang, Donghui and Okolo, Patrick N. and Zhao, Kun and Zhang, Weihua},
	month = sep,
	year = {2017},
	keywords = {Latin hypercube design, Near-orthogonal design, Sequentially enlarging, Space-filling design},
	pages = {348--365},
}

@article{xiong_sequential_2013,
	title = {Sequential {Design} and {Analysis} of {High}-{Accuracy} and {Low}-{Accuracy} {Computer} {Codes}},
	volume = {55},
	issn = {0040-1706},
	url = {https://doi.org/10.1080/00401706.2012.723572},
	doi = {10.1080/00401706.2012.723572},
	abstract = {A growing trend in engineering and science is to use multiple computer codes with different levels of accuracy to study the same complex system. We propose a framework for sequential design and analysis of a pair of high-accuracy and low-accuracy computer codes. It first runs the two codes with a pair of nested Latin hypercube designs (NLHDs). Data from the initial experiment are used to fit a prediction model. If the accuracy of the fitted model is less than a prespecified threshold, the two codes are evaluated again with input values chosen in an elaborate fashion so that their expanded scenario sets still form a pair of NLHDs. The nested relationship between the two scenario sets makes it easier to model and calibrate the difference between the two sources. If necessary, this augmentation process can be repeated a number of times until the prediction model based on all available data has reasonable accuracy. The effectiveness of the proposed method is illustrated with several examples. Matlab codes are provided in the online supplement to this article.},
	number = {1},
	urldate = {2024-08-26},
	journal = {Technometrics},
	author = {Xiong, Shifeng and Qian, Peter Z. G. and Wu, C. F. Jeff},
	month = feb,
	year = {2013},
	keywords = {Gaussian process model, Kriging, Nested Latin hypercube design},
	pages = {37--46},
}

@misc{chakraborty_likelihood-free_2024,
	title = {A {Likelihood}-{Free} {Approach} to {Goal}-{Oriented} {Bayesian} {Optimal} {Experimental} {Design}},
	url = {http://arxiv.org/abs/2408.09582},
	doi = {10.48550/arXiv.2408.09582},
	abstract = {Conventional Bayesian optimal experimental design seeks to maximize the expected information gain (EIG) on model parameters. However, the end goal of the experiment often is not to learn the model parameters, but to predict downstream quantities of interest (QoIs) that depend on the learned parameters. And designs that offer high EIG for parameters may not translate to high EIG for QoIs. Goal-oriented optimal experimental design (GO-OED) thus directly targets to maximize the EIG of QoIs. We introduce LF-GO-OED (likelihood-free goal-oriented optimal experimental design), a computational method for conducting GO-OED with nonlinear observation and prediction models. LF-GO-OED is specifically designed to accommodate implicit models, where the likelihood is intractable. In particular, it builds a density ratio estimator from samples generated from approximate Bayesian computation (ABC), thereby sidestepping the need for likelihood evaluations or density estimations. The overall method is validated on benchmark problems with existing methods, and demonstrated on scientific applications of epidemiology and neural science.},
	urldate = {2024-08-26},
	publisher = {arXiv},
	author = {Chakraborty, Atlanta and Huan, Xun and Catanach, Tommie},
	month = aug,
	year = {2024},
	note = {arXiv:2408.09582 [stat]},
	keywords = {Statistics - Applications, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
}

@inproceedings{go_robust_2022,
	title = {Robust expected information gain for optimal {Bayesian} experimental design using ambiguity sets},
	url = {https://proceedings.mlr.press/v180/go22a.html},
	abstract = {The ranking of experiments by expected information gain (EIG) in Bayesian experimental design is sensitive to changes in the model’s prior distribution, and the approximation of EIG yielded by sampling will have errors similar to the use of a perturbed prior. We define and analyze Robust Expected Information Gain(REIG), a modification of the objective in EIG maximization by minimizing an affine relaxation of EIG over an ambiguity set of distributions that are close to the original prior in KL-divergence. We show that, when combined with a sampling-based approach to estimating EIG, REIG corresponds to a "log-sum-exp" stabilization of the samples used to estimate EIG, meaning that it can be efficiently implemented in practice. Numerical tests combining REIG with variational nested Monte Carlo (VNMC), adaptive contrastive estimation (ACE) and mutual information neural estimation (MINE) suggest that in practice REIG also compensates for the variability of under-sampled estimators.},
	language = {en},
	urldate = {2024-08-22},
	booktitle = {Proceedings of the {Thirty}-{Eighth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Go, Jinwoo and Isaac, Tobin},
	month = aug,
	year = {2022},
	pages = {728--737},
}

@article{letham_constrained_2019,
	title = {Constrained {Bayesian} {Optimization} with {Noisy} {Experiments}},
	volume = {14},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-14/issue-2/Constrained-Bayesian-Optimization-with-Noisy-Experiments/10.1214/18-BA1110.full},
	doi = {10.1214/18-BA1110},
	abstract = {Randomized experiments are the gold standard for evaluating the effects of changes to real-world systems. Data in these tests may be difficult to collect and outcomes may have high variance, resulting in potentially large measurement error. Bayesian optimization is a promising technique for efficiently optimizing multiple continuous parameters, but existing approaches degrade in performance when the noise level is high, limiting its applicability to many randomized experiments. We derive an expression for expected improvement under greedy batch optimization with noisy observations and noisy constraints, and develop a quasi-Monte Carlo approximation that allows it to be efficiently optimized. Simulations with synthetic functions show that optimization performance on noisy, constrained problems outperforms existing methods. We further demonstrate the effectiveness of the method with two real-world experiments conducted at Facebook: optimizing a ranking system, and optimizing server compiler flags.},
	number = {2},
	urldate = {2024-08-22},
	journal = {Bayesian Analysis},
	author = {Letham, Benjamin and Karrer, Brian and Ottoni, Guilherme and Bakshy, Eytan},
	month = jun,
	year = {2019},
	keywords = {Bayesian optimization, quasi-Monte Carlo methods, randomized experiments},
	pages = {495--519},
}

@misc{brochu_portfolio_2011,
	title = {Portfolio {Allocation} for {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/1009.5419},
	abstract = {Bayesian optimization with Gaussian processes has become an increasingly popular tool in the machine learning community. It is eﬃcient and can be used when very little is known about the objective function, making it popular in expensive black-box optimization scenarios. It uses Bayesian methods to sample the objective eﬃciently using an acquisition function which incorporates the model’s estimate of the objective and the uncertainty at any given point. However, there are several diﬀerent parameterized acquisition functions in the literature, and it is often unclear which one to use. Instead of using a single acquisition function, we adopt a portfolio of acquisition functions governed by an online multi-armed bandit strategy. We propose several portfolio strategies, the best of which we call GP-Hedge, and show that this method outperforms the best individual acquisition function. We also provide a theoretical bound on the algorithm’s performance.},
	language = {en},
	urldate = {2024-08-22},
	publisher = {arXiv},
	author = {Brochu, Eric and Hoffman, Matthew W. and de Freitas, Nando},
	month = mar,
	year = {2011},
	note = {arXiv:1009.5419 [cs]},
	keywords = {Computer Science - Machine Learning, G.1.6, G.3, I.2.6},
}

@misc{hvarfner_vanilla_2024,
	title = {Vanilla {Bayesian} {Optimization} {Performs} {Great} in {High} {Dimensions}},
	url = {http://arxiv.org/abs/2402.02229},
	doi = {10.48550/arXiv.2402.02229},
	abstract = {High-dimensional problems have long been considered the Achilles' heel of Bayesian optimization algorithms. Spurred by the curse of dimensionality, a large collection of algorithms aim to make it more performant in this setting, commonly by imposing various simplifying assumptions on the objective. In this paper, we identify the degeneracies that make vanilla Bayesian optimization poorly suited to high-dimensional tasks, and further show how existing algorithms address these degeneracies through the lens of lowering the model complexity. Moreover, we propose an enhancement to the prior assumptions that are typical to vanilla Bayesian optimization algorithms, which reduces the complexity to manageable levels without imposing structural restrictions on the objective. Our modification - a simple scaling of the Gaussian process lengthscale prior with the dimensionality - reveals that standard Bayesian optimization works drastically better than previously thought in high dimensions, clearly outperforming existing state-of-the-art algorithms on multiple commonly considered real-world high-dimensional tasks.},
	urldate = {2024-08-22},
	publisher = {arXiv},
	author = {Hvarfner, Carl and Hellsten, Erik Orm and Nardi, Luigi},
	month = jun,
	year = {2024},
	note = {arXiv:2402.02229 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{eriksson_scalable_2020,
	title = {Scalable {Global} {Optimization} via {Local} {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/1910.01739},
	doi = {10.48550/arXiv.1910.01739},
	abstract = {Bayesian optimization has recently emerged as a popular method for the sample-efficient optimization of expensive black-box functions. However, the application to high-dimensional problems with several thousand observations remains challenging, and on difficult problems Bayesian optimization is often not competitive with other paradigms. In this paper we take the view that this is due to the implicit homogeneity of the global probabilistic models and an overemphasized exploration that results from global acquisition. This motivates the design of a local probabilistic approach for global optimization of large-scale high-dimensional problems. We propose the \${\textbackslash}texttt\{TuRBO\}\$ algorithm that fits a collection of local models and performs a principled global allocation of samples across these models via an implicit bandit approach. A comprehensive evaluation demonstrates that \${\textbackslash}texttt\{TuRBO\}\$ outperforms state-of-the-art methods from machine learning and operations research on problems spanning reinforcement learning, robotics, and the natural sciences.},
	urldate = {2024-08-22},
	publisher = {arXiv},
	author = {Eriksson, David and Pearce, Michael and Gardner, Jacob R. and Turner, Ryan and Poloczek, Matthias},
	month = feb,
	year = {2020},
	note = {arXiv:1910.01739 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{hvarfner_vanilla_2024-1,
	title = {Vanilla {Bayesian} {Optimization} {Performs} {Great} in {High} {Dimensions}},
	url = {http://arxiv.org/abs/2402.02229},
	doi = {10.48550/arXiv.2402.02229},
	abstract = {High-dimensional problems have long been considered the Achilles' heel of Bayesian optimization algorithms. Spurred by the curse of dimensionality, a large collection of algorithms aim to make it more performant in this setting, commonly by imposing various simplifying assumptions on the objective. In this paper, we identify the degeneracies that make vanilla Bayesian optimization poorly suited to high-dimensional tasks, and further show how existing algorithms address these degeneracies through the lens of lowering the model complexity. Moreover, we propose an enhancement to the prior assumptions that are typical to vanilla Bayesian optimization algorithms, which reduces the complexity to manageable levels without imposing structural restrictions on the objective. Our modification - a simple scaling of the Gaussian process lengthscale prior with the dimensionality - reveals that standard Bayesian optimization works drastically better than previously thought in high dimensions, clearly outperforming existing state-of-the-art algorithms on multiple commonly considered real-world high-dimensional tasks.},
	urldate = {2024-08-22},
	publisher = {arXiv},
	author = {Hvarfner, Carl and Hellsten, Erik Orm and Nardi, Luigi},
	month = jun,
	year = {2024},
	note = {arXiv:2402.02229 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_gaussian_2024,
	title = {Do {Gaussian} {Processes} scale well with dimension?},
	url = {https://miguelgondu.com/blogposts/2024-03-16/when-does-vanilla-gpr-fail/},
	abstract = {Disputing folk knowledge about how Gaussian Processes scale},
	language = {en},
	urldate = {2024-08-22},
	month = mar,
	year = {2024},
	note = {Section: posts},
}

@misc{miller_targeted_2024,
	title = {Targeted {Variance} {Reduction}: {Robust} {Bayesian} {Optimization} of {Black}-{Box} {Simulators} with {Noise} {Parameters}},
	shorttitle = {Targeted {Variance} {Reduction}},
	url = {http://arxiv.org/abs/2403.03816},
	doi = {10.48550/arXiv.2403.03816},
	abstract = {The optimization of a black-box simulator over control parameters \${\textbackslash}mathbf\{x\}\$ arises in a myriad of scientific applications. In such applications, the simulator often takes the form \$f({\textbackslash}mathbf\{x\},{\textbackslash}boldsymbol\{{\textbackslash}theta\})\$, where \${\textbackslash}boldsymbol\{{\textbackslash}theta\}\$ are parameters that are uncertain in practice. Robust optimization aims to optimize the objective \${\textbackslash}mathbb\{E\}[f({\textbackslash}mathbf\{x\},{\textbackslash}boldsymbol\{{\textbackslash}Theta\})]\$, where \${\textbackslash}boldsymbol\{{\textbackslash}Theta\} {\textbackslash}sim {\textbackslash}mathcal\{P\}\$ is a random variable that models uncertainty on \${\textbackslash}boldsymbol\{{\textbackslash}theta\}\$. For this, existing black-box methods typically employ a two-stage approach for selecting the next point \$({\textbackslash}mathbf\{x\},{\textbackslash}boldsymbol\{{\textbackslash}theta\})\$, where \${\textbackslash}mathbf\{x\}\$ and \${\textbackslash}boldsymbol\{{\textbackslash}theta\}\$ are optimized separately via different acquisition functions. As such, these approaches do not employ a joint acquisition over \$({\textbackslash}mathbf\{x\},{\textbackslash}boldsymbol\{{\textbackslash}theta\})\$, and thus may fail to fully exploit control-to-noise interactions for effective robust optimization. To address this, we propose a new Bayesian optimization method called Targeted Variance Reduction (TVR). The TVR leverages a novel joint acquisition function over \$({\textbackslash}mathbf\{x\},{\textbackslash}boldsymbol\{{\textbackslash}theta\})\$, which targets variance reduction on the objective within the desired region of improvement. Under a Gaussian process surrogate on \$f\$, the TVR acquisition can be evaluated in closed form, and reveals an insightful exploration-exploitation-precision trade-off for robust black-box optimization. The TVR can further accommodate a broad class of non-Gaussian distributions on \${\textbackslash}mathcal\{P\}\$ via a careful integration of normalizing flows. We demonstrate the improved performance of TVR over the state-of-the-art in a suite of numerical experiments and an application to the robust design of automobile brake discs under operational uncertainty.},
	urldate = {2024-08-22},
	publisher = {arXiv},
	author = {Miller, John Joshua and Mak, Simon},
	month = mar,
	year = {2024},
	note = {arXiv:2403.03816 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{frazier_tutorial_2018,
	title = {A {Tutorial} on {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/1807.02811},
	doi = {10.48550/arXiv.1807.02811},
	abstract = {Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.},
	urldate = {2024-08-22},
	publisher = {arXiv},
	author = {Frazier, Peter I.},
	month = jul,
	year = {2018},
	note = {arXiv:1807.02811 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{katzfuss_scalable_2024,
	title = {Scalable {Bayesian} {Transport} {Maps} for {High}-{Dimensional} {Non}-{Gaussian} {Spatial} {Fields}},
	volume = {119},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2023.2197158},
	doi = {10.1080/01621459.2023.2197158},
	abstract = {A multivariate distribution can be described by a triangular transport map from the target distribution to a simple reference distribution. We propose Bayesian nonparametric inference on the transport map by modeling its components using Gaussian processes. This enables regularization and uncertainty quantification of the map estimation, while resulting in a closed-form and invertible posterior map. We then focus on inferring the distribution of a nonstationary spatial field from a small number of replicates. We develop specific transport-map priors that are highly flexible and are motivated by the behavior of a large class of stochastic processes. Our approach is scalable to high-dimensional distributions due to data-dependent sparsity and parallel computations. We also discuss extensions, including Dirichlet process mixtures for flexible marginals. We present numerical results to demonstrate the accuracy, scalability, and usefulness of our methods, including statistical emulation of non-Gaussian climate-model output. Supplementary materials for this article are available online.},
	number = {546},
	urldate = {2024-08-18},
	journal = {Journal of the American Statistical Association},
	author = {Katzfuss, Matthias and Schäfer, Florian},
	month = apr,
	year = {2024},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1080/01621459.2023.2197158},
	pages = {1409--1423},
}

@article{xu_tukey_2017,
	title = {Tukey g-and-h {Random} {Fields}},
	volume = {112},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2016.1205501},
	doi = {10.1080/01621459.2016.1205501},
	abstract = {We propose a new class of transGaussian random fields named Tukey g-and-h (TGH) random fields to model non-Gaussian spatial data. The proposed TGH random fields have extremely flexible marginal distributions, possibly skewed and/or heavy-tailed, and, therefore, have a wide range of applications. The special formulation of the TGH random field enables an automatic search for the most suitable transformation for the dataset of interest while estimating model parameters. Asymptotic properties of the maximum likelihood estimator and the probabilistic properties of the TGH random fields are investigated. An efficient estimation procedure, based on maximum approximated likelihood, is proposed and an extreme spatial outlier detection algorithm is formulated. Kriging and probabilistic prediction with TGH random fields are developed along with prediction confidence intervals. The predictive performance of TGH random fields is demonstrated through extensive simulation studies and an application to a dataset of total precipitation in the south east of the United States. Supplementary materials for this article are available online.},
	number = {519},
	urldate = {2024-08-18},
	journal = {Journal of the American Statistical Association},
	author = {Xu, Ganggang and Genton, Marc G.},
	month = jul,
	year = {2017},
	keywords = {Heavy tails, Kriging, Non-Gaussian random field, Probabilistic prediction, Skewness, Spatial outliers},
	pages = {1236--1249},
}

@article{wallin_geostatistical_2015,
	title = {Geostatistical {Modelling} {Using} {Non}-{Gaussian} {Matérn} {Fields}},
	volume = {42},
	issn = {0303-6898},
	url = {https://www.jstor.org/stable/24586900},
	abstract = {This work provides a class of non-Gaussian spatial Matérn fields which are useful for analysing geostatistical data. The models are constructed as solutions to stochastic partial differential equations driven by generalized hyperbolic noise and are incorporated in a standard geostatistical setting with irregularly spaced observations, measurement errors and covariates. A maximum likelihood estimation technique based on the Monte Carlo expectation-maximization algorithm is presented, and a Monte Carlo method for spatial prediction is derived. Finally, an application to precipitation data is presented, and the performance of the non-Gaussian models is compared with standard Gaussian and transformed Gaussian models through cross-validation.},
	number = {3},
	urldate = {2024-08-18},
	journal = {Scandinavian Journal of Statistics},
	author = {Wallin, Jonas and Bolin, David},
	year = {2015},
	pages = {872--890},
}

@inproceedings{ash_warm-starting_2020,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '20},
	title = {On warm-starting neural network training},
	isbn = {9781713829546},
	abstract = {In many real-world deployments of machine learning systems, data arrive piecemeal. These learning scenarios may be passive, where data arrive incrementally due to structural properties of the problem (e.g., daily financial data) or active, where samples are selected according to a measure of their quality (e.g., experimental design). In both of these cases, we are building a sequence of models that incorporate an increasing amount of data. We would like each of these models in the sequence to be performant and take advantage of all the data that are available to that point. Conventional intuition suggests that when solving a sequence of related optimization problems of this form, it should be possible to initialize using the solution of the previous iterate—to "warm start" the optimization rather than initialize from scratch—and see reductions in wall-clock time. However, in practice this warm-starting seems to yield poorer generalization performance than models that have fresh random initializations, even though the final training losses are similar. While it appears that some hyperparameter settings allow a practitioner to close this generalization gap, they seem to only do so in regimes that damage the wall-clock gains of the warm start. Nevertheless, it is highly desirable to be able to warm-start neural network training, as it would dramatically reduce the resource usage associated with the construction of performant deep learning systems. In this work, we take a closer look at this empirical phenomenon and try to understand when and how it occurs. We also provide a surprisingly simple trick that overcomes this pathology in several important situations, and present experiments that elucidate some of its properties.},
	urldate = {2024-08-14},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Ash, Jordan T. and Adams, Ryan P.},
	month = dec,
	year = {2020},
	pages = {3884--3894},
}

@phdthesis{frazier_knowledge-gradient_2009,
	address = {United States -- New Jersey},
	type = {Ph.{D}.},
	title = {Knowledge-gradient methods for statistical learning},
	copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
	url = {https://www.proquest.com/pqdtglobal/docview/304988451/abstract/7B4AD470733741DAPQ/1?sourcetype=Dissertations%20&%20Theses},
	abstract = {We consider the class of fully sequential Bayesian information collection problems, a class that includes ranking and selection problems, multi-armed bandit problems, and many others. Although optimal policies for such problems are generally known to exist and to satisfy Bellman's recursion, the curses of dimensionality prevent us from actually computing them except in a few very special cases. Motivated by this difficulty, we develop a general class of practical and theoretically well-founded information collection policies known as knowledge-gradient (KG) policies. KG policies have several attractive qualities: they are myopically optimal in general; they are asymptotically optimal in a broad class of problems; they are flexible and may be computed easily in a broad class of problems; and they perform well numerically in several well-studied ranking and selection problems compared with other state-of-the-art policies designed specifically for these problems.},
	language = {English},
	urldate = {2024-08-15},
	school = {Princeton University},
	author = {Frazier, Peter},
	year = {2009},
	keywords = {Applied sciences, Information collection, Knowledge gradients, Operations research, Pure sciences, Statistical learning, Statistics},
}

@misc{dong_variational_2024,
	title = {Variational {Bayesian} {Optimal} {Experimental} {Design} with {Normalizing} {Flows}},
	url = {http://arxiv.org/abs/2404.13056},
	doi = {10.48550/arXiv.2404.13056},
	abstract = {Bayesian optimal experimental design (OED) seeks experiments that maximize the expected information gain (EIG) in model parameters. Directly estimating the EIG using nested Monte Carlo is computationally expensive and requires an explicit likelihood. Variational OED (vOED), in contrast, estimates a lower bound of the EIG without likelihood evaluations by approximating the posterior distributions with variational forms, and then tightens the bound by optimizing its variational parameters. We introduce the use of normalizing flows (NFs) for representing variational distributions in vOED; we call this approach vOED-NFs. Specifically, we adopt NFs with a conditional invertible neural network architecture built from compositions of coupling layers, and enhanced with a summary network for data dimension reduction. We present Monte Carlo estimators to the lower bound along with gradient expressions to enable a gradient-based simultaneous optimization of the variational parameters and the design variables. The vOED-NFs algorithm is then validated in two benchmark problems, and demonstrated on a partial differential equation-governed application of cathodic electrophoretic deposition and an implicit likelihood case with stochastic modeling of aphid population. The findings suggest that a composition of 4--5 coupling layers is able to achieve lower EIG estimation bias, under a fixed budget of forward model runs, compared to previous approaches. The resulting NFs produce approximate posteriors that agree well with the true posteriors, able to capture non-Gaussian and multi-modal features effectively.},
	urldate = {2024-08-15},
	publisher = {arXiv},
	author = {Dong, Jiayuan and Jacobsen, Christian and Khalloufi, Mehdi and Akram, Maryam and Liu, Wanjiao and Duraisamy, Karthik and Huan, Xun},
	month = apr,
	year = {2024},
	note = {arXiv:2404.13056 [cs, stat]},
	keywords = {62K05, 94A17, 62C10, 62F15, Computer Science - Computational Engineering, Finance, and Science, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
}

@inproceedings{astudillo_bayesian_2019,
	title = {Bayesian {Optimization} of {Composite} {Functions}},
	url = {https://proceedings.mlr.press/v97/astudillo19a.html},
	abstract = {We consider optimization of composite objective functions, i.e., of the form 𝑓(𝑥)=𝑔(ℎ(𝑥))f(x)=g(h(x))f(x)=g(h(x)), where ℎhh is a black-box derivative-free expensive-to-evaluate function with vector-valued outputs, and 𝑔gg is a cheap-to-evaluate real-valued function. While these problems can be solved with standard Bayesian optimization, we propose a novel approach that exploits the composite structure of the objective function to substantially improve sampling efficiency. Our approach models ℎhh using a multi-output Gaussian process and chooses where to sample using the expected improvement evaluated on the implied non-Gaussian posterior on 𝑓ff, which we call expected improvement for composite functions (EI-CF). Although EI-CF cannot be computed in closed form, we provide a novel stochastic gradient estimator that allows its efficient maximization. We also show that our approach is asymptotically consistent, i.e., that it recovers a globally optimal solution as sampling effort grows to infinity, generalizing previous convergence results for classical expected improvement. Numerical experiments show that our approach dramatically outperforms standard Bayesian optimization benchmarks, reducing simple regret by several orders of magnitude.},
	language = {en},
	urldate = {2024-08-15},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Astudillo, Raul and Frazier, Peter},
	month = may,
	year = {2019},
	pages = {354--363},
}

@article{archakov_new_2021,
	title = {A {New} {Parametrization} of {Correlation} {Matrices}},
	volume = {89},
	copyright = {© 2021 The Econometric Society},
	issn = {1468-0262},
	url = {https://onlinelibrary-wiley-com.proxy.lib.umich.edu/doi/abs/10.3982/ECTA16910},
	doi = {10.3982/ECTA16910},
	abstract = {We introduce a novel parametrization of the correlation matrix. The reparametrization facilitates modeling of correlation and covariance matrices by an unrestricted vector, where positive definiteness is an innate property. This parametrization can be viewed as a generalization of Fisher's Z-transformation to higher dimensions and has a wide range of potential applications. An algorithm for reconstructing the unique n × n correlation matrix from any vector in is provided, and we derive its numerical complexity.},
	language = {en},
	number = {4},
	urldate = {2024-08-13},
	journal = {Econometrica},
	author = {Archakov, Ilya and Hansen, Peter Reinhard},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA16910},
	keywords = {Correlation matrix, Covariance modeling, Fisher transformation},
	pages = {1699--1715},
}

@techreport{weigand_matrix_2014,
	type = {Working {Paper}},
	title = {Matrix {Box}-{Cox} models for multivariate realized volatility},
	copyright = {http://www.econstor.eu/dspace/Nutzungsbedingungen},
	url = {https://www.econstor.eu/handle/10419/99972},
	abstract = {We propose exible models for multivariate realized volatility dynamics which involve generalizations of the Box-Cox transform to the matrix case. The matrix Box-Cox model of realized covariances (MBC-RCov) is based on transformations of the covariance matrix eigenvalues, while for the Box-Cox dynamic correlation (BC-DC) specification the variances are transformed individually and modeled jointly with the correlations. We estimate transformation parameters by a new multivariate semiparametric estimator and discuss bias-corrected point and density forecasting by simulation. The methods are applied to stock market data where excellent in-sample and out-of-sample performance is found.},
	language = {eng},
	number = {144},
	urldate = {2024-08-13},
	institution = {BGPE Discussion Paper},
	author = {Weigand, Roland},
	year = {2014},
}

@article{goldstein_distributional_2005,
	title = {Distributional {Transformations}, {Orthogonal} {Polynomials}, and {Stein} {Characterizations}},
	volume = {18},
	issn = {1572-9230},
	url = {https://doi.org/10.1007/s10959-004-2602-6},
	doi = {10.1007/s10959-004-2602-6},
	abstract = {A new class of distributional transformations is introduced, characterized by equations relating function weighted expectations of test functions on a given distribution to expectations of the transformed distribution on the test function’s higher order derivatives. The class includes the size and zero bias transformations, and when specializing to weighting by polynomial functions, relates distributional families closed under independent addition, and in particular the infinitely divisible distributions, to the family of transformations induced by their associated orthogonal polynomial systems. For these families, generalizing a well known property of size biasing, sums of independent variables are transformed by replacing summands chosen according to a multivariate distribution on its index set by independent variables whose distributions are transformed by members of that same family. A variety of the transformations associated with the classical orthogonal polynomial systems have as fixed points the original distribution, or a member of the same family with different parameter.},
	language = {en},
	number = {1},
	urldate = {2024-08-07},
	journal = {Journal of Theoretical Probability},
	author = {Goldstein, Larry and Reinert, Gesine},
	month = jan,
	year = {2005},
	keywords = {Orthogonal polynomials; size bias; zero bias; Stein’s method; distributional fixed points.},
	pages = {237--260},
}

@inproceedings{barber_information_2003,
	title = {Information {Maximization} in {Noisy} {Channels} : {A} {Variational} {Approach}},
	volume = {16},
	url = {https://papers.nips.cc/paper_files/paper/2003/hash/a6ea8471c120fe8cc35a2954c9b9c595-Abstract.html},
	abstract = {The maximisation of information transmission over noisy channels is a common, albeit generally computationally difficult problem. We approach the difficulty of computing the mutual information for noisy channels by using a variational approximation. The resulting IM algorithm is analagous to the EM algorithm, yet maximises mutual information, as opposed to likelihood. We apply the method to several practical examples, including linear compression, population encoding and CDMA.},
	urldate = {2024-08-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Barber, David and Agakov, Felix},
	year = {2003},
}

@inproceedings{bishop_variational_1999,
	title = {Variational principal components},
	volume = {1},
	url = {https://ieeexplore.ieee.org/abstract/document/819772},
	doi = {10.1049/cp:19991160},
	abstract = {One of the central issues in the use of principal component analysis (PCA) for data modelling is that of choosing the appropriate number of retained components. This problem was recently addressed through the formulation of a Bayesian treatment of PCA in terms of a probabilistic latent variable model. A central feature of this approach is that the effective dimensionality of the latent space is determined automatically as part of the Bayesian inference procedure. In common with most non-trivial Bayesian models, however, the required marginalizations are analytically intractable, and so an approximation scheme based on a local Gaussian representation of the posterior distribution was employed. In this paper we develop an alternative, variational formulation of Bayesian PCA, based on a factorial representation of the posterior distribution. This approach is computationally efficient, and unlike other approximation schemes, it maximizes a rigorous lower bound on the marginal log probability of the observed data.},
	urldate = {2024-08-07},
	booktitle = {1999 {Ninth} {International} {Conference} on {Artificial} {Neural} {Networks} {ICANN} 99. ({Conf}. {Publ}. {No}. 470)},
	author = {Bishop, C.M.},
	month = sep,
	year = {1999},
	note = {ISSN: 0537-9989},
	pages = {509--514 vol.1},
}

@article{semenoglou_image-based_2023,
	title = {Image-based time series forecasting: {A} deep convolutional neural network approach},
	volume = {157},
	issn = {0893-6080},
	shorttitle = {Image-based time series forecasting},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608022003902},
	doi = {10.1016/j.neunet.2022.10.006},
	abstract = {Inspired by the successful use of deep learning in computer vision, in this paper we introduce ForCNN, a novel deep learning method for univariate time series forecasting that mixes convolutional and dense layers in a single neural network. Instead of using conventional, numeric representations of time series data as input to the network, the proposed method considers visual representations of it in the form of images to directly produce point forecasts. Three variants of deep convolutional neural networks are examined to process the images, the first based on VGG-19, the second on ResNet-50, while the third on a self-designed architecture. The performance of the proposed approach is evaluated using time series of the M3 and M4 forecasting competitions. Our results suggest that image-based time series forecasting methods can outperform both standard and state-of-the-art forecasting models.},
	urldate = {2024-08-05},
	journal = {Neural Networks},
	author = {Semenoglou, Artemios-Anargyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
	month = jan,
	year = {2023},
	keywords = {Convolutional Neural Networks, Deep Learning, Forecasting, Images, M competitions, Time series},
	pages = {39--53},
}

@phdthesis{chen_predictive_2024,
	address = {United States -- Arizona},
	type = {Ph.{D}.},
	title = {Predictive {Modeling} for {Spatio}-{Temporal} {Data}: {From} {Gaussian} {Process} to {Deep} {Neural} {Networks}},
	copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
	shorttitle = {Predictive {Modeling} for {Spatio}-{Temporal} {Data}},
	url = {https://www.proquest.com/docview/3084680155/abstract/89F3130C7ED4FC7PQ/1},
	abstract = {Spatio-temporal (ST) data is ubiquitous across engineering applications, such as sensor networks, transportation systems, etc. This type of data is characterized by the integration of timestamps and geographic coordinates, providing a comprehensive understanding of how events or variables change in both temporal and spatial dimensions. Existing models handling ST data can be classified into two categories: statistical machine learning models and deep learning models. While statistical models provide interpretable results, they may face challenges with data complexity and scalability. In contrast, deep learning excels at capturing complex patterns but demands significant data and computational resources.
We explore both statistical machine learning and deep learning models to effectively handle spatio-temporal data across various engineering applications. In the first application, we concentrate on predicting the radiation pattern of 3D printed antennas for antenna design automation. Here, we extend the standard Gaussian process (GP) model to accommodate spatially structured inputs by incorporating the spatial closeness into the kernel function. We compare different kernel functions and utilize the fitted GP model together with Bayesian optimization to find optimal designs based on specified design objectives. Results show that our proposed kernel outperforms previous ones in both 2D and 3D spatially structured inputs.
In the second application, we introduce a deep learning framework for modeling trajectory data from multiple sources in a connected and autonomous vehicle (CAV) environment. We leverage Transformer and Graph Neural Networks (GNN) to capture temporal and spatial relations, respectively. Recognizing the distinct characteristics of data from sensor and communication technologies, we employ source-specific encoders to capture temporal dependencies. The trajectory dataset is collected in the CARLA simulator with synthesized data errors injected. Numerical experiments demonstrate that in a mixed traffic flow scenario, the integration of data from different sources enhances our understanding of the environment. This notably improves trajectory prediction accuracy, particularly in situations with a high CV market penetration rate.
In the third application, we extend the trajectory prediction task into a Vehicle-to-Infrastructure (V2I) setting. Here, we leverage trajectory data from both vehicle and infrastructure perspectives. We propose a conformal vehicle trajectory prediction framework with multi-view data integration. For trajectory data from each view, we employ established GNN based models to capture temporal dependencies, agent-agent interactions, and agent-lane relations. Then we utilize a cross-graph attention module to fuse node features from different views. The predicted multimodal trajectories are calibrated by a post-hoc conformal prediction module to get valid and efficient confidence regions, which is crucial for safety-critical tasks. The whole framework is evaluated on a real-world V2I dataset. The performance demonstrates its effectiveness and advantages over existing benchmarks.},
	language = {English},
	urldate = {2024-08-05},
	school = {The University of Arizona},
	author = {Chen, Xi},
	year = {2024},
	keywords = {Artificial intelligence, Deep learning, Engineering, Gaussian process, Industrial engineering, Predictive modeling, Spatio-temporal data, Statistics, Trajectory prediction, Transportation, Uncertainty quantification},
}

@article{tabak_correcting_2020,
	title = {Correcting nuisance variation using {Wasserstein} distance},
	volume = {8},
	issn = {2167-8359},
	url = {https://peerj.com/articles/8594},
	doi = {10.7717/peerj.8594},
	abstract = {Profiling cellular phenotypes from microscopic imaging can provide meaningful biological information resulting from various factors affecting the cells. One motivating application is drug development: morphological cell features can be captured from images, from which similarities between different drug compounds applied at different doses can be quantified. The general approach is to find a function mapping the images to an embedding space of manageable dimensionality whose geometry captures relevant features of the input images. An important known issue for such methods is separating relevant biological signal from nuisance variation. For example, the embedding vectors tend to be more correlated for cells that were cultured and imaged during the same week than for those from different weeks, despite having identical drug compounds applied in both cases. In this case, the particular batch in which a set of experiments were conducted constitutes the domain of the data; an ideal set of image embeddings should contain only the relevant biological information (e.g., drug effects). We develop a general framework for adjusting the image embeddings in order to “forget” domain-specific information while preserving relevant biological information. To achieve this, we minimize a loss function based on distances between marginal distributions (such as the Wasserstein distance) of embeddings across domains for each replicated treatment. For the dataset we present results with, the only replicated treatment happens to be the negative control treatment, for which we do not expect any treatment-induced cell morphology changes. We find that for our transformed embeddings (i) the underlying geometric structure is not only preserved but the embeddings also carry improved biological signal; and (ii) less domain-specific information is present.},
	language = {en},
	urldate = {2024-08-05},
	journal = {PeerJ},
	author = {Tabak, Gil and Fan, Minjie and Yang, Samuel and Hoyer, Stephan and Davis, Geoffrey},
	month = feb,
	year = {2020},
	pages = {e8594},
}

@article{laparra_iterative_2011,
	title = {Iterative {Gaussianization}: from {ICA} to {Random} {Rotations}},
	volume = {22},
	issn = {1045-9227, 1941-0093},
	shorttitle = {Iterative {Gaussianization}},
	url = {http://arxiv.org/abs/1602.00229},
	doi = {10.1109/TNN.2011.2106511},
	abstract = {Most signal processing problems involve the challenging task of multidimensional probability density function (PDF) estimation. In this work, we propose a solution to this problem by using a family of Rotation-based Iterative Gaussianization (RBIG) transforms. The general framework consists of the sequential application of a univariate marginal Gaussianization transform followed by an orthonormal transform. The proposed procedure looks for differentiable transforms to a known PDF so that the unknown PDF can be estimated at any point of the original domain. In particular, we aim at a zero mean unit covariance Gaussian for convenience. RBIG is formally similar to classical iterative Projection Pursuit (PP) algorithms. However, we show that, unlike in PP methods, the particular class of rotations used has no special qualitative relevance in this context, since looking for interestingness is not a critical issue for PDF estimation. The key difference is that our approach focuses on the univariate part (marginal Gaussianization) of the problem rather than on the multivariate part (rotation). This difference implies that one may select the most convenient rotation suited to each practical application. The differentiability, invertibility and convergence of RBIG are theoretically and experimentally analyzed. Relation to other methods, such as Radial Gaussianization (RG), one-class support vector domain description (SVDD), and deep neural networks (DNN) is also pointed out. The practical performance of RBIG is successfully illustrated in a number of multidimensional problems such as image synthesis, classification, denoising, and multi-information estimation.},
	number = {4},
	urldate = {2024-08-02},
	journal = {IEEE Transactions on Neural Networks},
	author = {Laparra, Valero and Camps-Valls, Gustavo and Malo, Jesús},
	month = apr,
	year = {2011},
	note = {arXiv:1602.00229 [stat]},
	keywords = {Statistics - Machine Learning},
	pages = {537--549},
}

@incollection{marzouk_introduction_2016,
	title = {An introduction to sampling via measure transport},
	url = {http://arxiv.org/abs/1602.05023},
	abstract = {We present the fundamentals of a measure transport approach to sampling. The idea is to construct a deterministic coupling---i.e., a transport map---between a complex "target" probability measure of interest and a simpler reference measure. Given a transport map, one can generate arbitrarily many independent and unweighted samples from the target simply by pushing forward reference samples through the map. We consider two different and complementary scenarios: first, when only evaluations of the unnormalized target density are available, and second, when the target distribution is known only through a finite collection of samples. We show that in both settings the desired transports can be characterized as the solutions of variational problems. We then address practical issues associated with the optimization--based construction of transports: choosing finite-dimensional parameterizations of the map, enforcing monotonicity, quantifying the error of approximate transports, and refining approximate transports by enriching the corresponding approximation spaces. Approximate transports can also be used to "Gaussianize" complex distributions and thus precondition conventional asymptotically exact sampling schemes. We place the measure transport approach in broader context, describing connections with other optimization--based samplers, with inference and density estimation schemes using optimal transport, and with alternative transformation--based approaches to simulation. We also sketch current work aimed at the construction of transport maps in high dimensions, exploiting essential features of the target distribution (e.g., conditional independence, low-rank structure). The approaches and algorithms presented here have direct applications to Bayesian computation and to broader problems of stochastic simulation.},
	urldate = {2024-08-02},
	author = {Marzouk, Youssef and Moselhy, Tarek and Parno, Matthew and Spantini, Alessio},
	year = {2016},
	note = {arXiv:1602.05023 [math, stat]},
	keywords = {Mathematics - Probability, Statistics - Computation, Statistics - Methodology},
	pages = {1--41},
}

@misc{alexanderian_brief_2023,
	title = {A brief note on the {Bayesian} {D}-optimality criterion},
	url = {http://arxiv.org/abs/2212.11466},
	doi = {10.48550/arXiv.2212.11466},
	abstract = {We consider finite-dimensional Bayesian linear inverse problems with Gaussian priors and additive Gaussian noise models. The goal of this note is to present a simple derivation of the well-known fact that solving the Bayesian D-optimal experimental design problem, i.e., maximizing the expected information gain, is equivalent to minimizing the log-determinant of posterior covariance operator. We focus on finite-dimensional inverse problems. However, the presentation is kept generic to facilitate extensions to infinite-dimensional inverse problems.},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Alexanderian, Alen},
	month = dec,
	year = {2023},
	note = {arXiv:2212.11466 [math, stat]},
	keywords = {Mathematics - Statistics Theory},
}

@article{alexanderian_bayesian_2016,
	title = {On {Bayesian} {A}- and {D}-{Optimal} {Experimental} {Designs} in {Infinite} {Dimensions}},
	volume = {11},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-11/issue-3/On-Bayesian-A--and-D-Optimal-Experimental-Designs-in/10.1214/15-BA969.full},
	doi = {10.1214/15-BA969},
	abstract = {We consider Bayesian linear inverse problems in infinite-dimensional separable Hilbert spaces, with a Gaussian prior measure and additive Gaussian noise model, and provide an extension of the concept of Bayesian D-optimality to the infinite-dimensional case. To this end, we derive the infinite-dimensional version of the expression for the Kullback–Leibler divergence from the posterior measure to the prior measure, which is subsequently used to derive the expression for the expected information gain. We also study the notion of Bayesian A-optimality in the infinite-dimensional setting, and extend the well known (in the finite-dimensional case) equivalence of the Bayes risk of the MAP estimator with the trace of the posterior covariance, for the Gaussian linear case, to the infinite-dimensional Hilbert space case.},
	number = {3},
	urldate = {2024-08-01},
	journal = {Bayesian Analysis},
	author = {Alexanderian, Alen and Gloor, Philip J. and Ghattas, Omar},
	month = sep,
	year = {2016},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {Bayes risk, Bayesian inference in Hilbert space, Bayesian optimal experimental design, Gaussian measure, Kullback–Leibler divergence, expected information gain},
	pages = {671--695},
}

@misc{izmailov_what_2021,
	title = {What {Are} {Bayesian} {Neural} {Network} {Posteriors} {Really} {Like}?},
	url = {http://arxiv.org/abs/2104.14421},
	doi = {10.48550/arXiv.2104.14421},
	abstract = {The posterior over Bayesian neural network (BNN) parameters is extremely high-dimensional and non-convex. For computational reasons, researchers approximate this posterior using inexpensive mini-batch methods such as mean-field variational inference or stochastic-gradient Markov chain Monte Carlo (SGMCMC). To investigate foundational questions in Bayesian deep learning, we instead use full-batch Hamiltonian Monte Carlo (HMC) on modern architectures. We show that (1) BNNs can achieve significant performance gains over standard training and deep ensembles; (2) a single long HMC chain can provide a comparable representation of the posterior to multiple shorter chains; (3) in contrast to recent studies, we find posterior tempering is not needed for near-optimal performance, with little evidence for a "cold posterior" effect, which we show is largely an artifact of data augmentation; (4) BMA performance is robust to the choice of prior scale, and relatively similar for diagonal Gaussian, mixture of Gaussian, and logistic priors; (5) Bayesian neural networks show surprisingly poor generalization under domain shift; (6) while cheaper alternatives such as deep ensembles and SGMCMC methods can provide good generalization, they provide distinct predictive distributions from HMC. Notably, deep ensemble predictive distributions are similarly close to HMC as standard SGLD, and closer than standard variational inference.},
	urldate = {2024-07-31},
	publisher = {arXiv},
	author = {Izmailov, Pavel and Vikram, Sharad and Hoffman, Matthew D. and Wilson, Andrew Gordon},
	month = apr,
	year = {2021},
	note = {arXiv:2104.14421 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{olivier_bayesian_2021,
	title = {Bayesian neural networks for uncertainty quantification in data-driven materials modeling},
	volume = {386},
	issn = {0045-7825},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782521004102},
	doi = {10.1016/j.cma.2021.114079},
	abstract = {Modern machine learning (ML) techniques, in conjunction with simulation-based methods, present remarkable potential for various scientific and engineering applications. Within the materials science field, these data-based methods can be used to build efficient structure–property linkages that can be further integrated within multi-scale simulations, or guide experiments in a materials discovery setting. However, a critical shortcoming of state-of-the-art ML techniques is their lack of reliable uncertainty/error estimates, which severely limits their use for materials or other engineering applications where data is often scarce and uncertainties are substantial. This paper presents methods for Bayesian learning of neural networks (NN) that allow consideration of both aleatoric uncertainties that account for the inherent stochasticity of the data-generating process, and epistemic uncertainties, which arise from consideration of limited amounts of data. In particular, algorithms based on approximate variational inference and (pseudo-)Bayesian model averaging achieve an appropriate trade-off between accuracy of the uncertainty estimates and accessible computational cost. The performance of these algorithms is first presented on simple 1D examples to illustrate their behavior in both extrapolation and interpolation settings. The approach is then applied for the prediction of homogenized and localized properties of a composite material. In this setting, data is generated from a finite element model, which permits a study of the behavior of the probabilistic learning algorithms under various amounts of aleatoric and epistemic uncertainties.},
	urldate = {2024-07-31},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Olivier, Audrey and Shields, Michael D. and Graham-Brady, Lori},
	month = dec,
	year = {2021},
	keywords = {Bayesian neural network, Data-driven materials modeling, Epistemic and aleatoric uncertainties, Probabilistic model averaging, Variational inference},
	pages = {114079},
}

@misc{holzmuller_framework_2023,
	title = {A {Framework} and {Benchmark} for {Deep} {Batch} {Active} {Learning} for {Regression}},
	url = {http://arxiv.org/abs/2203.09410},
	doi = {10.48550/arXiv.2203.09410},
	abstract = {The acquisition of labels for supervised learning can be expensive. To improve the sample efficiency of neural network regression, we study active learning methods that adaptively select batches of unlabeled data for labeling. We present a framework for constructing such methods out of (network-dependent) base kernels, kernel transformations, and selection methods. Our framework encompasses many existing Bayesian methods based on Gaussian process approximations of neural networks as well as non-Bayesian methods. Additionally, we propose to replace the commonly used last-layer features with sketched finite-width neural tangent kernels and to combine them with a novel clustering method. To evaluate different methods, we introduce an open-source benchmark consisting of 15 large tabular regression data sets. Our proposed method outperforms the state-of-the-art on our benchmark, scales to large data sets, and works out-of-the-box without adjusting the network architecture or training code. We provide open-source code that includes efficient implementations of all kernels, kernel transformations, and selection methods, and can be used for reproducing our results.},
	urldate = {2024-07-31},
	publisher = {arXiv},
	author = {Holzmüller, David and Zaverkin, Viktor and Kästner, Johannes and Steinwart, Ingo},
	month = aug,
	year = {2023},
	note = {arXiv:2203.09410 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{dhulipala_bayesian_2022,
	title = {Bayesian {Inference} with {Latent} {Hamiltonian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2208.06120},
	doi = {10.48550/arXiv.2208.06120},
	abstract = {When sampling for Bayesian inference, one popular approach is to use Hamiltonian Monte Carlo (HMC) and specifically the No-U-Turn Sampler (NUTS) which automatically decides the end time of the Hamiltonian trajectory. However, HMC and NUTS can require numerous numerical gradients of the target density, and can prove slow in practice. We propose Hamiltonian neural networks (HNNs) with HMC and NUTS for solving Bayesian inference problems. Once trained, HNNs do not require numerical gradients of the target density during sampling. Moreover, they satisfy important properties such as perfect time reversibility and Hamiltonian conservation, making them well-suited for use within HMC and NUTS because stationarity can be shown. We also propose an HNN extension called latent HNNs (L-HNNs), which are capable of predicting latent variable outputs. Compared to HNNs, L-HNNs offer improved expressivity and reduced integration errors. Finally, we employ L-HNNs in NUTS with an online error monitoring scheme to prevent sample degeneracy in regions of low probability density. We demonstrate L-HNNs in NUTS with online error monitoring on several examples involving complex, heavy-tailed, and high-local-curvature probability densities. Overall, L-HNNs in NUTS with online error monitoring satisfactorily inferred these probability densities. Compared to traditional NUTS, L-HNNs in NUTS with online error monitoring required 1--2 orders of magnitude fewer numerical gradients of the target density and improved the effective sample size (ESS) per gradient by an order of magnitude.},
	urldate = {2024-07-31},
	publisher = {arXiv},
	author = {Dhulipala, Somayajulu L. N. and Che, Yifeng and Shields, Michael D.},
	month = oct,
	year = {2022},
	note = {arXiv:2208.06120 [cs, stat]},
	keywords = {60J22, 68T07, 65C05, 37Jxx, 62F15, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
}

@misc{wang_stochastic_2023,
	title = {Stochastic {Deep}-{Ritz} for {Parametric} {Uncertainty} {Quantification}},
	url = {http://arxiv.org/abs/2206.00867},
	doi = {10.48550/arXiv.2206.00867},
	abstract = {Scientific machine learning has become an increasingly important tool in materials science and engineering. It is particularly well suited to tackle material problems involving many variables or to allow rapid construction of surrogates of material models, to name just a few. Mathematically, many problems in materials science and engineering can be cast as variational problems. However, handling of uncertainty, ever present in materials, in the context of variational formulations remains challenging for scientific machine learning. In this article, we propose a deep-learning-based numerical method for solving variational problems under uncertainty. Our approach seamlessly combines deep-learning approximation with Monte-Carlo sampling. The resulting numerical method is powerful yet remarkably simple. We assess its performance and accuracy on a number of variational problems.},
	urldate = {2024-07-29},
	publisher = {arXiv},
	author = {Wang, Ting and Knap, Jaroslaw},
	month = may,
	year = {2023},
	note = {arXiv:2206.00867 [cs, math]},
	keywords = {Mathematics - Numerical Analysis},
}

@article{abbiati_seismic_2021,
	title = {Seismic fragility analysis based on artificial ground motions and surrogate modeling of validated structural simulators},
	volume = {50},
	copyright = {© 2021 John Wiley \& Sons Ltd.},
	issn = {1096-9845},
	url = {https://onlinelibrary-wiley-com.proxy.lib.umich.edu/doi/abs/10.1002/eqe.3448},
	doi = {10.1002/eqe.3448},
	abstract = {This study introduces a computational framework for efficient and accurate seismic fragility analysis based on a combination of artificial ground motion modeling, polynomial-chaos-based global sensitivity analysis, and hierarchical kriging surrogate modeling. The framework follows the philosophy of the Performance-Based Earthquake Engineering PEER approach, where the fragility analysis is decoupled from hazard analysis. This study addresses three criticalities that are present in the current practice. Namely, reduced size of hazard-consistent size-specific ensembles of seismic records, validation of structural simulators against large-scale experiments, high computational cost for accurate fragility estimates. The effectiveness of the proposed framework is demonstrated for the Rio Torto Bridge, recently tested using hybrid simulation within the RETRO project.},
	language = {en},
	number = {9},
	urldate = {2024-07-28},
	journal = {Earthquake Engineering \& Structural Dynamics},
	author = {Abbiati, Giuseppe and Broccardo, Marco and Abdallah, Imad and Marelli, Stefano and Paolacci, Fabrizio},
	year = {2021},
	keywords = {artificial ground motion, fragility analysis, global sensitivity analysis, hierarchical kriging, polynomial chaos expansion, surrogate modeling},
	pages = {2314--2333},
}

@article{gidaris_kriging_2015,
	title = {Kriging metamodeling in seismic risk assessment based on stochastic ground motion models},
	volume = {44},
	copyright = {Copyright © 2015 John Wiley \& Sons, Ltd.},
	issn = {1096-9845},
	url = {https://onlinelibrary-wiley-com.proxy.lib.umich.edu/doi/abs/10.1002/eqe.2586},
	doi = {10.1002/eqe.2586},
	abstract = {An efficient computational framework is presented for seismic risk assessment within a modeling approach that utilizes stochastic ground motion models to describe the seismic hazard. The framework is based on the use of a kriging surrogate model (metamodel) to provide an approximate relationship between the structural response and the structural and ground motion parameters that are considered as uncertain. The stochastic character of the excitation is addressed by assuming that under the influence of the white noise (used within the ground motion model) the response follows a lognormal distribution. Once the surrogate model is established, a task that involves the formulation of an initial database to inform the metamodel development, it is then directly used for all response evaluations required to estimate seismic risk. The model prediction error stemming from the metamodel is directly incorporated within the seismic risk quantification and assessment, whereas an adaptive approach is developed to refine the database that informs the metamodel development. The ability to efficiently obtain derivative information through the kriging metamodel and its utility for various tasks within the probabilistic seismic risk assessment is also discussed. As an illustrative example, the assessment of seismic risk for a benchmark four-story concrete office building is presented. The potential that ground motions include near-fault characteristics is explicitly addressed within the context of this example. The implementation of the framework for the same structure equipped with fluid viscous dampers is also demonstrated. Copyright © 2015 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {14},
	urldate = {2024-07-28},
	journal = {Earthquake Engineering \& Structural Dynamics},
	author = {Gidaris, Ioannis and Taflanidis, Alexandros A. and Mavroeidis, George P.},
	year = {2015},
	keywords = {kriging, seismic risk, stochastic ground motion models, surrogate modeling},
	pages = {2377--2399},
}

@article{zhu_seismic_2023,
	title = {Seismic fragility analysis using stochastic polynomial chaos expansions},
	volume = {72},
	issn = {0266-8920},
	url = {https://www.sciencedirect.com/science/article/pii/S0266892023000024},
	doi = {10.1016/j.probengmech.2023.103413},
	abstract = {Within the performance-based earthquake engineering (PBEE) framework, the fragility model plays a pivotal role. Such a model represents the probability that the engineering demand parameter (EDP) exceeds a certain safety threshold given a set of selected intensity measures (IMs) that characterize the earthquake load. The-state-of-the art methods for fragility computation rely on full non-linear time–history analyses. Within this perimeter, there are two main approaches: the first relies on the selection and scaling of recorded ground motions; the second, based on random vibration theory, characterizes the seismic input with a parametric stochastic ground motion model (SGMM). The latter case has the great advantage that the problem of seismic risk analysis is framed as a forward uncertainty quantification problem. However, running classical full-scale Monte Carlo simulations is intractable because of the prohibitive computational cost of typical finite element models. Therefore, it is of great interest to define fragility models that link an EDP of interest with the SGMM parameters — which are regarded as IMs in this context. The computation of such fragility models is a challenge on its own and, despite a few recent studies, there is still an important research gap in this domain. This comes with no surprise as classical surrogate modeling techniques cannot be applied due to the stochastic nature of SGMM. This study tackles this computational challenge by using stochastic polynomial chaos expansions to represent the statistical dependence of EDP on IMs. More precisely, this surrogate model estimates the full conditional probability distribution of EDP conditioned on IMs. We compare the proposed approach with some state-of-the-art methods in two case studies. The numerical results show that the new method prevails over its competitors in estimating both the conditional distribution and the fragility functions.},
	urldate = {2024-07-28},
	journal = {Probabilistic Engineering Mechanics},
	author = {Zhu, Xujia and Broccardo, Marco and Sudret, Bruno},
	month = apr,
	year = {2023},
	keywords = {Earthquake engineering, Fragility functions, Polynomial chaos expansion, Stochastic simulator},
	pages = {103413},
}

@misc{huan_optimal_2024,
	title = {Optimal experimental design: {Formulations} and computations},
	shorttitle = {Optimal experimental design},
	url = {http://arxiv.org/abs/2407.16212},
	doi = {10.48550/arXiv.2407.16212},
	abstract = {Questions of `how best to acquire data' are essential to modeling and prediction in the natural and social sciences, engineering applications, and beyond. Optimal experimental design (OED) formalizes these questions and creates computational methods to answer them. This article presents a systematic survey of modern OED, from its foundations in classical design theory to current research involving OED for complex models. We begin by reviewing criteria used to formulate an OED problem and thus to encode the goal of performing an experiment. We emphasize the flexibility of the Bayesian and decision-theoretic approach, which encompasses information-based criteria that are well-suited to nonlinear and non-Gaussian statistical models. We then discuss methods for estimating or bounding the values of these design criteria; this endeavor can be quite challenging due to strong nonlinearities, high parameter dimension, large per-sample costs, or settings where the model is implicit. A complementary set of computational issues involves optimization methods used to find a design; we discuss such methods in the discrete (combinatorial) setting of observation selection and in settings where an exact design can be continuously parameterized. Finally we present emerging methods for sequential OED that build non-myopic design policies, rather than explicit designs; these methods naturally adapt to the outcomes of past experiments in proposing new experiments, while seeking coordination among all experiments to be performed. Throughout, we highlight important open questions and challenges.},
	urldate = {2024-07-24},
	publisher = {arXiv},
	author = {Huan, Xun and Jagalur, Jayanth and Marzouk, Youssef},
	month = jul,
	year = {2024},
	note = {arXiv:2407.16212 [cs, math, stat]},
	keywords = {Mathematics - Numerical Analysis, Statistics - Computation, Statistics - Methodology},
}

@article{czyz_beyond_2023,
	title = {Beyond {Normal}: {On} the {Evaluation} of {Mutual} {Information} {Estimators}},
	volume = {36},
	shorttitle = {Beyond {Normal}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/36b80eae70ff629d667f210e13497edf-Abstract-Conference.html},
	language = {en},
	urldate = {2024-07-23},
	journal = {Advances in Neural Information Processing Systems},
	author = {Czyż, Paweł and Grabowski, Frederic and Vogt, Julia and Beerenwinkel, Niko and Marx, Alexander},
	month = dec,
	year = {2023},
	pages = {16957--16990},
}

@inproceedings{mcallester_formal_2020,
	title = {Formal {Limitations} on the {Measurement} of {Mutual} {Information}},
	url = {https://proceedings.mlr.press/v108/mcallester20a.html},
	abstract = {Measuring mutual information from finite data is difficult. Recent work has considered variational methods maximizing a lower bound. In this paper, we prove that serious statistical limitations are inherent to any method of measuring mutual information. More specifically, we show that any distribution-free high-confidence lower bound on mutual information estimated from N samples cannot be larger than O(ln N).},
	language = {en},
	urldate = {2024-07-23},
	booktitle = {Proceedings of the {Twenty} {Third} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {McAllester, David and Stratos, Karl},
	month = jun,
	year = {2020},
	pages = {875--884},
}

@inproceedings{ranganath_operator_2016,
	title = {Operator {Variational} {Inference}},
	volume = {29},
	url = {https://papers.nips.cc/paper_files/paper/2016/hash/d947bf06a885db0d477d707121934ff8-Abstract.html},
	abstract = {Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (OPVI), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling---allowing inference to scale to massive data---as well as objectives that admit variational programs---a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images.},
	urldate = {2024-07-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ranganath, Rajesh and Tran, Dustin and Altosaar, Jaan and Blei, David},
	year = {2016},
}

@article{jivani_global_2023,
	title = {Global {Sensitivity} {Analysis} and {Uncertainty} {Quantification} for {Background} {Solar} {Wind} {Using} the {Alfvén} {Wave} {Solar} {Atmosphere} {Model}},
	volume = {21},
	copyright = {© 2023. The Authors.},
	issn = {1542-7390},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2022SW003262},
	doi = {10.1029/2022SW003262},
	abstract = {Modeling the impact of space weather events such as coronal mass ejections (CMEs) is crucial to protecting critical infrastructure. The Space Weather Modeling Framework is a state-of-the-art framework that offers full Sun-to-Earth simulations by computing the background solar wind, CME propagation, and magnetospheric impact. However, reliable long-term predictions of CME events require uncertainty quantification (UQ) and data assimilation. We take the first steps by performing global sensitivity analysis (GSA) and UQ for background solar wind simulations produced by the Alfvén Wave Solar atmosphere Model (AWSoM) for two Carrington rotations: CR2152 (solar maximum) and CR2208 (solar minimum). We conduct GSA by computing Sobol' indices that quantify contributions from model parameter uncertainty to the variance of solar wind speed and density at 1 au, both crucial quantities for CME propagation and strength. Sobol' indices also allow us to rank and retain only the most important parameters, which aids in the construction of smaller ensembles for the reduced-dimension parameter space. We present an efficient procedure for computing the Sobol' indices using polynomial chaos expansion surrogates and space-filling designs. The PCEs further enable inexpensive forward UQ. Overall, we identify three important model parameters: the multiplicative factor applied to the magnetogram, Poynting flux per magnetic field strength constant used at the inner boundary, and the coefficient of the perpendicular correlation length in the turbulent cascade model in AWSoM.},
	language = {en},
	number = {1},
	urldate = {2024-07-16},
	journal = {Space Weather},
	author = {Jivani, Aniket and Sachdeva, Nishtha and Huang, Zhenguang and Chen, Yang and van der Holst, Bart and Manchester, Ward and Iong, Daniel and Chen, Hongfan and Zou, Shasha and Huan, Xun and Toth, Gabor},
	year = {2023},
	keywords = {global sensitivity analysis, solar wind, space weather, uncertainty quantification},
	pages = {e2022SW003262},
}

@misc{foster_deep_2021,
	title = {Deep {Adaptive} {Design}: {Amortizing} {Sequential} {Bayesian} {Experimental} {Design}},
	shorttitle = {Deep {Adaptive} {Design}},
	url = {http://arxiv.org/abs/2103.02438},
	doi = {10.48550/arXiv.2103.02438},
	abstract = {We introduce Deep Adaptive Design (DAD), a method for amortizing the cost of adaptive Bayesian experimental design that allows experiments to be run in real-time. Traditional sequential Bayesian optimal experimental design approaches require substantial computation at each stage of the experiment. This makes them unsuitable for most real-world applications, where decisions must typically be made quickly. DAD addresses this restriction by learning an amortized design network upfront and then using this to rapidly run (multiple) adaptive experiments at deployment time. This network represents a design policy which takes as input the data from previous steps, and outputs the next design using a single forward pass; these design decisions can be made in milliseconds during the live experiment. To train the network, we introduce contrastive information bounds that are suitable objectives for the sequential setting, and propose a customized network architecture that exploits key symmetries. We demonstrate that DAD successfully amortizes the process of experimental design, outperforming alternative strategies on a number of problems.},
	urldate = {2024-07-15},
	publisher = {arXiv},
	author = {Foster, Adam and Ivanova, Desi R. and Malik, Ilyas and Rainforth, Tom},
	month = jun,
	year = {2021},
	note = {arXiv:2103.02438 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
}

@misc{balandat_botorch_2020,
	title = {{BoTorch}: {A} {Framework} for {Efficient} {Monte}-{Carlo} {Bayesian} {Optimization}},
	shorttitle = {{BoTorch}},
	url = {http://arxiv.org/abs/1910.06403},
	doi = {10.48550/arXiv.1910.06403},
	abstract = {Bayesian optimization provides sample-efficient global optimization for a broad range of applications, including automatic machine learning, engineering, physics, and experimental design. We introduce BoTorch, a modern programming framework for Bayesian optimization that combines Monte-Carlo (MC) acquisition functions, a novel sample average approximation optimization approach, auto-differentiation, and variance reduction techniques. BoTorch's modular design facilitates flexible specification and optimization of probabilistic models written in PyTorch, simplifying implementation of new acquisition functions. Our approach is backed by novel theoretical convergence results and made practical by a distinctive algorithmic foundation that leverages fast predictive distributions, hardware acceleration, and deterministic optimization. We also propose a novel "one-shot" formulation of the Knowledge Gradient, enabled by a combination of our theoretical and software contributions. In experiments, we demonstrate the improved sample efficiency of BoTorch relative to other popular libraries.},
	urldate = {2024-07-15},
	publisher = {arXiv},
	author = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel R. and Daulton, Samuel and Letham, Benjamin and Wilson, Andrew Gordon and Bakshy, Eytan},
	month = dec,
	year = {2020},
	note = {arXiv:1910.06403 [cs, math, stat]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{sudret_global_2008,
	series = {Bayesian {Networks} in {Dependability}},
	title = {Global sensitivity analysis using polynomial chaos expansions},
	volume = {93},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832007001329},
	doi = {10.1016/j.ress.2007.04.002},
	abstract = {Global sensitivity analysis (SA) aims at quantifying the respective effects of input random variables (or combinations thereof) onto the variance of the response of a physical or mathematical model. Among the abundant literature on sensitivity measures, the Sobol’ indices have received much attention since they provide accurate information for most models. The paper introduces generalized polynomial chaos expansions (PCE) to build surrogate models that allow one to compute the Sobol’ indices analytically as a post-processing of the PCE coefficients. Thus the computational cost of the sensitivity indices practically reduces to that of estimating the PCE coefficients. An original non intrusive regression-based approach is proposed, together with an experimental design of minimal size. Various application examples illustrate the approach, both from the field of global SA (i.e. well-known benchmark problems) and from the field of stochastic mechanics. The proposed method gives accurate results for various examples that involve up to eight input random variables, at a computational cost which is 2–3 orders of magnitude smaller than the traditional Monte Carlo-based evaluation of the Sobol’ indices.},
	number = {7},
	urldate = {2024-07-13},
	journal = {Reliability Engineering \& System Safety},
	author = {Sudret, Bruno},
	month = jul,
	year = {2008},
	keywords = {Analysis of variance, Generalized chaos, Global sensitivity analysis, Polynomial chaos, Regression, Sobol’ indices, Stochastic finite elements},
	pages = {964--979},
}

@book{rasmussen_gaussian_2005,
	title = {Gaussian {Processes} for {Machine} {Learning}},
	isbn = {9780262256834},
	url = {https://direct.mit.edu/books/book/2320/Gaussian-Processes-for-Machine-Learning},
	abstract = {A comprehensive and self-contained introduction to Gaussian processes, which provide a principled, practical, probabilistic approach to learning in kernel machines.
            Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.},
	language = {en},
	urldate = {2024-07-13},
	publisher = {The MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	month = nov,
	year = {2005},
	doi = {10.7551/mitpress/3206.001.0001},
}

@misc{reiser_uncertainty_2023,
	title = {Uncertainty {Quantification} and {Propagation} in {Surrogate}-based {Bayesian} {Inference}},
	url = {http://arxiv.org/abs/2312.05153},
	doi = {10.48550/arXiv.2312.05153},
	abstract = {Surrogate models are statistical or conceptual approximations for more complex simulation models. In this context, it is crucial to propagate the uncertainty induced by limited simulation budget and surrogate approximation error to predictions, inference, and subsequent decision-relevant quantities. However, quantifying and then propagating the uncertainty of surrogates is usually limited to special analytic cases or is otherwise computationally very expensive. In this paper, we propose a framework enabling a scalable, Bayesian approach to surrogate modeling with thorough uncertainty quantification, propagation, and validation. Specifically, we present three methods for Bayesian inference with surrogate models given measurement data. This is a task where the propagation of surrogate uncertainty is especially relevant, because failing to account for it may lead to biased and/or overconfident estimates of the parameters of interest. We showcase our approach in two detailed case studies for both linear and nonlinear modeling scenarios. Uncertainty propagation in surrogate models enables more reliable and safe approximation of expensive simulators and will therefore be useful in various fields of applications.},
	urldate = {2024-07-13},
	publisher = {arXiv},
	author = {Reiser, Philipp and Aguilar, Javier Enrique and Guthke, Anneli and Bürkner, Paul-Christian},
	month = dec,
	year = {2023},
	note = {arXiv:2312.05153 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{riis_bayesian_2023,
	title = {Bayesian {Active} {Learning} with {Fully} {Bayesian} {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/2205.10186},
	doi = {10.48550/arXiv.2205.10186},
	abstract = {The bias-variance trade-off is a well-known problem in machine learning that only gets more pronounced the less available data there is. In active learning, where labeled data is scarce or difficult to obtain, neglecting this trade-off can cause inefficient and non-optimal querying, leading to unnecessary data labeling. In this paper, we focus on active learning with Gaussian Processes (GPs). For the GP, the bias-variance trade-off is made by optimization of the two hyperparameters: the length scale and noise-term. Considering that the optimal mode of the joint posterior of the hyperparameters is equivalent to the optimal bias-variance trade-off, we approximate this joint posterior and utilize it to design two new acquisition functions. The first one is a Bayesian variant of Query-by-Committee (B-QBC), and the second is an extension that explicitly minimizes the predictive variance through a Query by Mixture of Gaussian Processes (QB-MGP) formulation. Across six simulators, we empirically show that B-QBC, on average, achieves the best marginal likelihood, whereas QB-MGP achieves the best predictive performance. We show that incorporating the bias-variance trade-off in the acquisition functions mitigates unnecessary and expensive data labeling.},
	urldate = {2024-07-12},
	publisher = {arXiv},
	author = {Riis, Christoffer and Antunes, Francisco and Hüttel, Frederik Boe and Azevedo, Carlos Lima and Pereira, Francisco Câmara},
	month = jan,
	year = {2023},
	note = {arXiv:2205.10186 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{tan_optimal_2022,
	title = {An {Optimal} {Transport} {Based} {Global} {Similarity} {Index} for {Remote} {Sensing} {Products} {Comparison}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/14/11/2546},
	doi = {10.3390/rs14112546},
	abstract = {Remote sensing products, such as land cover data products, are essential for a wide range of scientific studies and applications, and their quality evaluation and relative comparison have become a major issue that needs to be studied. Traditional methods, such as error matrices, are not effective in describing spatial distribution because they are based on a pixel-by-pixel comparison. In this paper, the relative quality comparison of two remote sensing products is turned into the difference measurement between the spatial distribution of pixels by proposing a max-sliced Wasserstein distance-based similarity index. According to optimal transport theory, the mathematical expression of the proposed similarity index is firstly clarified, and then its rationality is illustrated, and finally, experiments on three open land cover products (GLCFCS30, FROMGLC, CNLUCC) are conducted. Results show that based on this proposed similarity index-based relative quality comparison method, the spatial difference, including geometric shapes and spatial locations between two different remote sensing products in raster form, can be quantified. The method is particularly useful in cases where there exists misregistration between datasets, while pixel-based methods will lose their robustness.},
	language = {en},
	number = {11},
	urldate = {2024-07-11},
	journal = {Remote Sensing},
	author = {Tan, Yumin and Shi, Yanzhe and Xu, Le and Zhou, Kailei and Jing, Guifei and Wang, Xiaolu and Bai, Bingxin},
	month = jan,
	year = {2022},
	keywords = {Wasserstein distance, land cover, raster, similarity comparison},
	pages = {2546},
}

@misc{wu_quantifying_2021,
	title = {Quantifying {Uncertainty} in {Deep} {Spatiotemporal} {Forecasting}},
	url = {http://arxiv.org/abs/2105.11982},
	doi = {10.48550/arXiv.2105.11982},
	abstract = {Deep learning is gaining increasing popularity for spatiotemporal forecasting. However, prior works have mostly focused on point estimates without quantifying the uncertainty of the predictions. In high stakes domains, being able to generate probabilistic forecasts with confidence intervals is critical to risk assessment and decision making. Hence, a systematic study of uncertainty quantification (UQ) methods for spatiotemporal forecasting is missing in the community. In this paper, we describe two types of spatiotemporal forecasting problems: regular grid-based and graph-based. Then we analyze UQ methods from both the Bayesian and the frequentist point of view, casting in a unified framework via statistical decision theory. Through extensive experiments on real-world road network traffic, epidemics, and air quality forecasting tasks, we reveal the statistical and computational trade-offs for different UQ methods: Bayesian methods are typically more robust in mean prediction, while confidence levels obtained from frequentist methods provide more extensive coverage over data variations. Computationally, quantile regression type methods are cheaper for a single confidence interval but require re-training for different intervals. Sampling based methods generate samples that can form multiple confidence intervals, albeit at a higher computational cost.},
	urldate = {2024-07-11},
	publisher = {arXiv},
	author = {Wu, Dongxia and Gao, Liyao and Xiong, Xinyue and Chinazzi, Matteo and Vespignani, Alessandro and Ma, Yi-An and Yu, Rose},
	month = jun,
	year = {2021},
	note = {arXiv:2105.11982 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Applications, Statistics - Machine Learning},
}

@inproceedings{terejanu_active_2015,
	address = {Washington, DC},
	title = {Active data collection for inadequate models},
	url = {https://ieeexplore.ieee.org/abstract/document/7266592},
	urldate = {2024-07-09},
	publisher = {IEEE},
	author = {Terejanu, Gabriel},
	month = jul,
	year = {2015},
}

@article{straub_bayesian_2015,
	title = {Bayesian {Updating} with {Structural} {Reliability} {Methods}},
	volume = {141},
	url = {https://ascelibrary-org.proxy.lib.umich.edu/doi/epdf/10.1061/%28ASCE%29EM.1943-7889.0000839},
	abstract = {Bayesian updating is a powerful method to learn and calibrate models with data and observations. Because of the difﬁculties in-volved in computing the high-dimensional integrals necessary for Bayesian updating, Markov chain Monte Carlo (MCMC) sampling methodshave been developed and successfully applied for this task. The disadvantage of MCMC methods is the difﬁculty of ensuring the stationarity ofthe Markov chain. We present an alternative to MCMC that is particularly effective for updating mechanical and other computational models,termed Bayesian updating with structural reliability methods (BUS). With BUS, structural reliability methods are applied to compute the pos-terior distribution of uncertain model parameters and model outputs in general. An algorithm for the implementation of BUS is proposed, whichcan be interpreted as an enhancement of the classic rejection sampling algorithm for Bayesian updating. This algorithm is based on the subset simulation, and its efﬁciency is not dependent on the number of random variables in the model. The method is demonstrated by application toparameter identiﬁcation in a dynamic system, Bayesian updating of the material parameters of a structural system, and Bayesian updating ofa random ﬁeld–based ﬁnite-element model of a geotechnical site.},
	language = {en},
	number = {3},
	urldate = {2024-06-30},
	journal = {Journal of Engineering Mechanics},
	author = {Straub, Daniel and Papaioannou, Iason},
	month = mar,
	year = {2015},
}

@article{North2023,
	title = {A {Review} of {Data}‐{Driven} {Discovery} for {Dynamic} {Systems}},
	volume = {91},
	url = {https://onlinelibrary-wiley-com.proxy.lib.umich.edu/doi/full/10.1111/insr.12554},
	abstract = {Many real-world scientific processes are governed by complex non-linear dynamic systems that can be represented by differential equations. Recently, there has been an increased interest in learning, or discovering, the forms of the equations driving these complex non-linear dynamic systems using data-driven approaches. In this paper, we review the current literature on data-driven discovery for dynamic systems. We provide a categorisation to the different approaches for data-driven discovery and a unified mathematical framework to show the relationship between the approaches. Importantly, we discuss the role of statistics in the data-driven discovery field, describe a possible approach by which the problem can be cast in a statistical framework and provide avenues for future work.},
	number = {3},
	urldate = {2024-06-11},
	journal = {International Statistical Review},
	author = {North, Joshua and Wikle, Christopher and Schliep, Erin},
	year = {2023},
	pages = {464--492},
}

@article{Zahm2020,
	title = {Gradient-{Based} {Dimension} {Reduction} of {Multivariate} {Vector}-{Valued} {Functions}},
	volume = {42},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/18M1221837},
	doi = {10.1137/18M1221837},
	abstract = {We propose a multifidelity dimension reduction method to identify a low-dimensional structure present in many engineering models. The structure of interest arises when functions vary primarily on a low-dimensional subspace of the high-dimensional input space, while varying little along the complementary directions. Our approach builds on the gradient-based methodology of active subspaces, and exploits models of different fidelities to reduce the cost of performing dimension reduction through the computation of the active subspace matrix. We provide a nonasymptotic analysis of the number of gradient evaluations sufficient to achieve a prescribed error in the active subspace matrix, both in expectation and with high probability. We show that the sample complexity depends on a notion of intrinsic dimension of the problem, which can be much smaller than the dimension of the input space. We illustrate the benefits of such a multifidelity dimension reduction approach using numerical experiments with input spaces of up to two thousand dimensions.},
	number = {1},
	urldate = {2024-06-30},
	journal = {SIAM Journal on Scientific Computing},
	author = {Zahm, Olivier and Constantine, Paul G. and Prieur, Clémentine and Marzouk, Youssef M.},
	month = jan,
	year = {2020},
	pages = {A534--A558},
}

@article{yu_learning_2024,
	title = {Learning dynamical systems from data: {An} introduction to physics-guided deep learning},
	volume = {121},
	shorttitle = {Learning dynamical systems from data},
	url = {https://www.pnas.org/doi/10.1073/pnas.2311808121},
	doi = {10.1073/pnas.2311808121},
	abstract = {Modeling complex physical dynamics is a fundamental task in science and engineering. Traditional physics-based models are first-principled, explainable, and sample-efficient. However, they often rely on strong modeling assumptions and expensive numerical integration, requiring significant computational resources and domain expertise. While deep learning (DL) provides efficient alternatives for modeling complex dynamics, they require a large amount of labeled training data. Furthermore, its predictions may disobey the governing physical laws and are difficult to interpret. Physics-guided DL aims to integrate first-principled physical knowledge into data-driven methods. It has the best of both worlds and is well equipped to better solve scientific problems. Recently, this field has gained great progress and has drawn considerable interest across discipline Here, we introduce the framework of physics-guided DL with a special emphasis on learning dynamical systems. We describe the learning pipeline and categorize state-of-the-art methods under this framework. We also offer our perspectives on the open challenges and emerging opportunities.},
	number = {27},
	urldate = {2024-06-25},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Yu, Rose and Wang, Rui},
	month = jul,
	year = {2024},
	pages = {e2311808121},
}

@inproceedings{bonilla_multi-task_2007,
	title = {Multi-task {Gaussian} {Process} {Prediction}},
	volume = {20},
	url = {https://papers.nips.cc/paper/2007/hash/66368270ffd51418ec58bd793f2d9b1b-Abstract.html},
	abstract = {In this paper we investigate multi-task learning in the context of Gaussian Pro- cesses (GP). We propose a model that learns a shared covariance function on input-dependent features and a “free-form” covariance matrix over tasks. This al- lows for good ﬂexibility when modelling inter-task dependencies while avoiding the need for large amounts of data for training. We show that under the assump- tion of noise-free observations and a block design, predictions for a given task only depend on its target values and therefore a cancellation of inter-task trans- fer occurs. We evaluate the beneﬁts of our model on two practical applications: a compiler performance prediction problem and an exam score prediction task. Additionally, we make use of GP approximations and properties of our model in order to provide scalability to large data sets.},
	urldate = {2024-06-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bonilla, Edwin V and Chai, Kian and Williams, Christopher},
	year = {2007},
}

@misc{andonian_contrastive_2021,
	title = {Contrastive {Feature} {Loss} for {Image} {Prediction}},
	url = {http://arxiv.org/abs/2111.06934},
	doi = {10.48550/arXiv.2111.06934},
	abstract = {Training supervised image synthesis models requires a critic to compare two images: the ground truth to the result. Yet, this basic functionality remains an open problem. A popular line of approaches uses the L1 (mean absolute error) loss, either in the pixel or the feature space of pretrained deep networks. However, we observe that these losses tend to produce overly blurry and grey images, and other techniques such as GANs need to be employed to fight these artifacts. In this work, we introduce an information theory based approach to measuring similarity between two images. We argue that a good reconstruction should have high mutual information with the ground truth. This view enables learning a lightweight critic to "calibrate" a feature space in a contrastive manner, such that reconstructions of corresponding spatial patches are brought together, while other patches are repulsed. We show that our formulation immediately boosts the perceptual realism of output images when used as a drop-in replacement for the L1 loss, with or without an additional GAN loss.},
	urldate = {2024-06-21},
	publisher = {arXiv},
	author = {Andonian, Alex and Park, Taesung and Russell, Bryan and Isola, Phillip and Zhu, Jun-Yan and Zhang, Richard},
	month = nov,
	year = {2021},
	note = {arXiv:2111.06934 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{lyu_interpretation_2012,
	title = {Interpretation and {Generalization} of {Score} {Matching}},
	url = {http://arxiv.org/abs/1205.2629},
	doi = {10.48550/arXiv.1205.2629},
	abstract = {Score matching is a recently developed parameter learning method that is particularly effective to complicated high dimensional density models with intractable partition functions. In this paper, we study two issues that have not been completely resolved for score matching. First, we provide a formal link between maximum likelihood and score matching. Our analysis shows that score matching finds model parameters that are more robust with noisy training data. Second, we develop a generalization of score matching. Based on this generalization, we further demonstrate an extension of score matching to models of discrete data.},
	urldate = {2024-06-19},
	publisher = {arXiv},
	author = {Lyu, Siwei},
	month = may,
	year = {2012},
	note = {arXiv:1205.2629 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kattumannil_steins_2009,
	title = {On {Stein}’s identity and its applications},
	volume = {79},
	issn = {0167-7152},
	url = {https://www.sciencedirect.com/science/article/pii/S0167715209001060},
	doi = {10.1016/j.spl.2009.03.021},
	abstract = {Stein’s identity and its role in inference procedures has been widely discussed in the literature. In this paper, we provide a generalization of this identity applicable to a general class of distributions. We derive several existing results as special cases of our general identity and discuss some applications.},
	number = {12},
	urldate = {2024-06-17},
	journal = {Statistics \& Probability Letters},
	author = {Kattumannil, Sudheesh Kumar},
	month = jun,
	year = {2009},
	pages = {1444--1449},
}

@article{daw_overview_2022,
	title = {An {Overview} of {Univariate} and {Multivariate} {Karhunen} {Loève} {Expansions} in {Statistics}},
	volume = {23},
	issn = {2364-9569},
	url = {https://doi.org/10.1007/s41096-022-00122-9},
	doi = {10.1007/s41096-022-00122-9},
	abstract = {Dependent data are ubiquitous in statistics and across various subject matter domains, with dependencies across space, time, and variables. Basis expansions have proven quite effective in modeling such processes, particularly in the context of functional data and high-dimensional spatial, temporal, and spatio-temporal data. One of the most useful basis function representations is given by the Karhunen-Loève expansion (KLE), which is derived from the covariance kernel that controls the dependence of a random process, and can be expressed in terms of reproducing kernel Hilbert spaces. The KLE has been used in a wide variety of disciplines to solve many different types of problems, including dimension reduction, covariance estimation, and optimal spatial regionalization. Despite its utility in the univariate context, the multivariate KLE has been used much less frequently in statistics. This manuscript provides an overview of the KLE, with the goal of illustrating the utility of the univariate KLE and bringing the multivariate version to the attention of a wider audience of statisticians and data scientists. After deriving the KLE from a univariate perspective, we derive the multivariate version and illustrate the implementation of both via simulation and data examples.},
	language = {en},
	number = {2},
	urldate = {2024-06-18},
	journal = {Journal of the Indian Society for Probability and Statistics},
	author = {Daw, Ranadeep and Simpson, Matthew and Wikle, Christopher K. and Holan, Scott H. and Bradley, Jonathan R.},
	month = dec,
	year = {2022},
	pages = {285--326},
}

@article{sriperumbudur_density_2017,
	title = {Density {Estimation} in {Infinite} {Dimensional} {Exponential} {Families}},
	volume = {18},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v18/16-011.html},
	abstract = {In this paper, we consider an infinite dimensional exponential family P{\textbackslash}mathcal\{P\} of probability densities, which are parametrized by functions in a reproducing kernel Hilbert space H{\textbackslash}mathcal\{H\}, and show it to be quite rich in the sense that a broad class of densities on ℝdRd{\textbackslash}mathbb\{R\}{\textasciicircum}d can be approximated arbitrarily well in Kullback-Leibler (KL) divergence by elements in P{\textbackslash}mathcal\{P\}. Motivated by this approximation property, the paper addresses the question of estimating an unknown density p0p0p\_0 through an element in P{\textbackslash}mathcal\{P\}. Standard techniques like maximum likelihood estimation (MLE) or pseudo MLE (based on the method of sieves), which are based on minimizing the KL divergence between p0p0p\_0 and P{\textbackslash}mathcal\{P\}, do not yield practically useful estimators because of their inability to efficiently handle the log-partition function. We propose an estimator p̂ np{\textasciicircum}n{\textbackslash}hat\{p\}\_n based on minimizing the Fisher divergence, J(p0‖p)J(p0‖p)J(p\_0{\textbackslash}Vert p) between p0p0p\_0 and p∈p∈Pp{\textbackslash}in {\textbackslash}mathcal\{P\}, which involves solving a simple finite-dimensional linear system. When p0∈p0∈Pp\_0{\textbackslash}in{\textbackslash}mathcal\{P\}, we show that the proposed estimator is consistent, and provide a convergence rate of n−min\{23,2β+12β+2\}n−min\{23,2β+12β+2\}n{\textasciicircum}\{-{\textbackslash}min{\textbackslash}left{\textbackslash}\{{\textbackslash}frac\{2\}\{3\},{\textbackslash}frac\{2{\textbackslash}beta+1\}\{2{\textbackslash}beta+2\}{\textbackslash}right{\textbackslash}\}\} in Fisher divergence under the smoothness assumption that logp0∈(Cβ)log⁡p0∈R(Cβ){\textbackslash}log p\_0{\textbackslash}in{\textbackslash}mathcal\{R\}(C{\textasciicircum}{\textbackslash}beta) for some β≥0β≥0{\textbackslash}beta{\textbackslash}ge 0, where CCC is a certain Hilbert-Schmidt operator on H{\textbackslash}mathcal\{H\} and (Cβ)R(Cβ){\textbackslash}mathcal\{R\}(C{\textasciicircum}{\textbackslash}beta) denotes the image of CβCβC{\textasciicircum}{\textbackslash}beta. We also investigate the misspecified case of p0∉p0∉Pp\_0{\textbackslash}notin{\textbackslash}mathcal\{P\} and show that J(p0‖p̂ n)→infp∈J(p0‖p)J(p0‖p{\textasciicircum}n)→infp∈PJ(p0‖p)J(p\_0{\textbackslash}Vert{\textbackslash}hat\{p\}\_n){\textbackslash}rightarrow {\textbackslash}inf\_\{p{\textbackslash}in{\textbackslash}mathcal\{P\}\}J(p\_0{\textbackslash}Vert p) as n→∞n→∞n{\textbackslash}rightarrow {\textbackslash}infty, and provide a rate for this convergence under a similar smoothness condition as above. Through numerical simulations we demonstrate that the proposed estimator outperforms the non- parametric kernel density estimator, and that the advantage of the proposed estimator grows as ddd increases.},
	number = {57},
	urldate = {2024-06-17},
	journal = {Journal of Machine Learning Research},
	author = {Sriperumbudur, Bharath and Fukumizu, Kenji and Gretton, Arthur and Hyv{\textbackslash}"\{a\}rinen, Aapo and Kumar, Revant},
	year = {2017},
	pages = {1--59},
}

@misc{noack_2023_2,
	title = {A {Unifying} {Perspective} on {Non}-{Stationary} {Kernels} for {Deeper} {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/2309.10068},
	doi = {10.48550/arXiv.2309.10068},
	abstract = {The Gaussian process (GP) is a popular statistical technique for stochastic function approximation and uncertainty quantification from data. GPs have been adopted into the realm of machine learning in the last two decades because of their superior prediction abilities, especially in data-sparse scenarios, and their inherent ability to provide robust uncertainty estimates. Even so, their performance highly depends on intricate customizations of the core methodology, which often leads to dissatisfaction among practitioners when standard setups and off-the-shelf software tools are being deployed. Arguably the most important building block of a GP is the kernel function which assumes the role of a covariance operator. Stationary kernels of the Mat{\textbackslash}'ern class are used in the vast majority of applied studies; poor prediction performance and unrealistic uncertainty quantification are often the consequences. Non-stationary kernels show improved performance but are rarely used due to their more complicated functional form and the associated effort and expertise needed to define and tune them optimally. In this perspective, we want to help ML practitioners make sense of some of the most common forms of non-stationarity for Gaussian processes. We show a variety of kernels in action using representative datasets, carefully study their properties, and compare their performances. Based on our findings, we propose a new kernel that combines some of the identified advantages of existing kernels.},
	urldate = {2024-02-23},
	publisher = {arXiv},
	author = {Noack, Marcus M. and Luo, Hengrui and Risser, Mark D.},
	month = sep,
	year = {2023},
	note = {arXiv:2309.10068 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{noack_2023_1,
	title = {A {Unifying} {Perspective} on {Non}-{Stationary} {Kernels} for {Deeper} {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/2309.10068},
	doi = {10.48550/arXiv.2309.10068},
	abstract = {The Gaussian process (GP) is a popular statistical technique for stochastic function approximation and uncertainty quantification from data. GPs have been adopted into the realm of machine learning in the last two decades because of their superior prediction abilities, especially in data-sparse scenarios, and their inherent ability to provide robust uncertainty estimates. Even so, their performance highly depends on intricate customizations of the core methodology, which often leads to dissatisfaction among practitioners when standard setups and off-the-shelf software tools are being deployed. Arguably the most important building block of a GP is the kernel function which assumes the role of a covariance operator. Stationary kernels of the Mat{\textbackslash}'ern class are used in the vast majority of applied studies; poor prediction performance and unrealistic uncertainty quantification are often the consequences. Non-stationary kernels show improved performance but are rarely used due to their more complicated functional form and the associated effort and expertise needed to define and tune them optimally. In this perspective, we want to help ML practitioners make sense of some of the most common forms of non-stationarity for Gaussian processes. We show a variety of kernels in action using representative datasets, carefully study their properties, and compare their performances. Based on our findings, we propose a new kernel that combines some of the identified advantages of existing kernels.},
	urldate = {2024-04-06},
	publisher = {arXiv},
	author = {Noack, Marcus M. and Luo, Hengrui and Risser, Mark D.},
	month = sep,
	year = {2023},
	note = {arXiv:2309.10068 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability, Statistics - Machine Learning},
}

@article{zozor_debruijn_2015,
	title = {{deBruijn} identities: {From} {Shannon}, {Kullback}-{Leibler} and {Fisher} to generalized phi-entropies, phi-divergences and phi-{Fisher} informations},
	volume = {1641},
	issn = {0094-243X},
	shorttitle = {{deBruijn} identities},
	url = {https://doi.org/10.1063/1.4906018},
	doi = {10.1063/1.4906018},
	number = {1},
	urldate = {2024-06-17},
	journal = {AIP Conference Proceedings},
	author = {Zozor, Steeve and Brossier, Jean-Marc},
	month = jan,
	year = {2015},
	pages = {522--529},
}

@misc{huggins_practical_2018,
	title = {Practical bounds on the error of {Bayesian} posterior approximations: {A} nonasymptotic approach},
	shorttitle = {Practical bounds on the error of {Bayesian} posterior approximations},
	url = {http://arxiv.org/abs/1809.09505},
	doi = {10.48550/arXiv.1809.09505},
	abstract = {Bayesian inference typically requires the computation of an approximation to the posterior distribution. An important requirement for an approximate Bayesian inference algorithm is to output high-accuracy posterior mean and uncertainty estimates. Classical Monte Carlo methods, particularly Markov Chain Monte Carlo, remain the gold standard for approximate Bayesian inference because they have a robust finite-sample theory and reliable convergence diagnostics. However, alternative methods, which are more scalable or apply to problems where Markov Chain Monte Carlo cannot be used, lack the same finite-data approximation theory and tools for evaluating their accuracy. In this work, we develop a flexible new approach to bounding the error of mean and uncertainty estimates of scalable inference algorithms. Our strategy is to control the estimation errors in terms of Wasserstein distance, then bound the Wasserstein distance via a generalized notion of Fisher distance. Unlike computing the Wasserstein distance, which requires access to the normalized posterior distribution, the Fisher distance is tractable to compute because it requires access only to the gradient of the log posterior density. We demonstrate the usefulness of our Fisher distance approach by deriving bounds on the Wasserstein error of the Laplace approximation and Hilbert coresets. We anticipate that our approach will be applicable to many other approximate inference methods such as the integrated Laplace approximation, variational inference, and approximate Bayesian computation},
	urldate = {2024-06-11},
	publisher = {arXiv},
	author = {Huggins, Jonathan H. and Campbell, Trevor and Kasprzak, Mikołaj and Broderick, Tamara},
	month = oct,
	year = {2018},
	note = {arXiv:1809.09505 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Computation, Statistics - Machine Learning},
}

@misc{yang_variational_2019,
	title = {Variational approximations using {Fisher} divergence},
	url = {http://arxiv.org/abs/1905.05284},
	doi = {10.48550/arXiv.1905.05284},
	abstract = {Modern applications of Bayesian inference involve models that are sufficiently complex that the corresponding posterior distributions are intractable and must be approximated. The most common approximation is based on Markov chain Monte Carlo, but these can be expensive when the data set is large and/or the model is complex, so more efficient variational approximations have recently received considerable attention. The traditional variational methods, that seek to minimize the Kullback--Leibler divergence between the posterior and a relatively simple parametric family, provide accurate and efficient estimation of the posterior mean, but often does not capture other moments, and have limitations in terms of the models to which they can be applied. Here we propose the construction of variational approximations based on minimizing the Fisher divergence, and develop an efficient computational algorithm that can be applied to a wide range of models without conjugacy or potentially unrealistic mean-field assumptions. We demonstrate the superior performance of the proposed method for the benchmark case of logistic regression.},
	urldate = {2024-06-11},
	publisher = {arXiv},
	author = {Yang, Yue and Martin, Ryan and Bondell, Howard},
	month = may,
	year = {2019},
	note = {arXiv:1905.05284 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
}

@article{franceschini_model-based_2008,
	series = {Model-{Based} {Experimental} {Analysis}},
	title = {Model-based design of experiments for parameter precision: {State} of the art},
	volume = {63},
	issn = {0009-2509},
	shorttitle = {Model-based design of experiments for parameter precision},
	url = {https://www.sciencedirect.com/science/article/pii/S0009250907008871},
	doi = {10.1016/j.ces.2007.11.034},
	abstract = {Due to the wide use and key importance of mathematical models in process engineering, experiment design is becoming an essential tool for the rapid building and validation of these mechanistic models. Several experiment design techniques have been developed in the past and applied successfully to a wide range of systems. This paper is focused on the so-called model-based design of experiments (DOE) and aims at presenting an up-to-date state of the art in this important field. In order to provide an adequate and thorough background to this technique, a detailed description of the key elements of a model identification procedure (the model itself, the experiment, the statistical tools, etc.) and the major steps of a model-building strategy are introduced before focusing on the experiment design for parameter precision, which is the topic of this survey. An overview and critical analysis of the state of the art in this sector are proposed. The main contributions to model-based experiment design procedures in terms of novel criteria, mathematical formulations and numerical implementations are highlighted. A list of the most recent applications of these techniques in various fields (from chemical kinetics to biological modelling) is then presented highlighting the key role of model-based DOE in the process engineering area.},
	number = {19},
	urldate = {2024-06-11},
	journal = {Chemical Engineering Science},
	author = {Franceschini, Gaia and Macchietto, Sandro},
	month = oct,
	year = {2008},
	keywords = {Mathematical modelling, Model validation, Model-based experiment design, Non-linear dynamics, Optimisation, Parameter identification, Process engineering},
	pages = {4846--4872},
}

@article{espie_optimal_1989,
	title = {The optimal design of dynamic experiments},
	volume = {35},
	copyright = {Copyright © 1989 American Institute of Chemical Engineers},
	issn = {1547-5905},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aic.690350206},
	doi = {10.1002/aic.690350206},
	abstract = {A robust and efficient algorithm is developed to calculate dynamic inputs for optimal experimental designs. Different objective functions are presented to allow designs for both model discrimination and the improvement of parameter precision. Time-varying inputs are calculated by reformulating the optimal design problem as an optimal control problem. This approach can provide large improvements in the ability to discriminate among a series of models, and then increase the accuracy of the resulting parameters.},
	language = {en},
	number = {2},
	urldate = {2024-06-11},
	journal = {AIChE Journal},
	author = {Espie, D. and Macchietto, S.},
	year = {1989},
	pages = {223--229},
}

@misc{botev_which_2021,
	title = {Which priors matter? {Benchmarking} models for learning latent dynamics},
	shorttitle = {Which priors matter?},
	url = {http://arxiv.org/abs/2111.05458},
	doi = {10.48550/arXiv.2111.05458},
	abstract = {Learning dynamics is at the heart of many important applications of machine learning (ML), such as robotics and autonomous driving. In these settings, ML algorithms typically need to reason about a physical system using high dimensional observations, such as images, without access to the underlying state. Recently, several methods have proposed to integrate priors from classical mechanics into ML models to address the challenge of physical reasoning from images. In this work, we take a sober look at the current capabilities of these models. To this end, we introduce a suite consisting of 17 datasets with visual observations based on physical systems exhibiting a wide range of dynamics. We conduct a thorough and detailed comparison of the major classes of physically inspired methods alongside several strong baselines. While models that incorporate physical priors can often learn latent spaces with desirable properties, our results demonstrate that these methods fail to significantly improve upon standard techniques. Nonetheless, we find that the use of continuous and time-reversible dynamics benefits models of all classes.},
	urldate = {2024-06-11},
	publisher = {arXiv},
	author = {Botev, Aleksandar and Jaegle, Andrew and Wirnsberger, Peter and Hennes, Daniel and Higgins, Irina},
	month = nov,
	year = {2021},
	note = {arXiv:2111.05458 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{wang_stochastic_2022,
	title = {Stochastic {Deep}-{Ritz} for {Parametric} {Uncertainty} {Quantification}},
	url = {https://arxiv.org/abs/2206.00867v3},
	abstract = {Scientific machine learning has become an increasingly important tool in materials science and engineering. It is particularly well suited to tackle material problems involving many variables or to allow rapid construction of surrogates of material models, to name just a few. Mathematically, many problems in materials science and engineering can be cast as variational problems. However, handling of uncertainty, ever present in materials, in the context of variational formulations remains challenging for scientific machine learning. In this article, we propose a deep-learning-based numerical method for solving variational problems under uncertainty. Our approach seamlessly combines deep-learning approximation with Monte-Carlo sampling. The resulting numerical method is powerful yet remarkably simple. We assess its performance and accuracy on a number of variational problems.},
	language = {en},
	urldate = {2024-06-07},
	journal = {arXiv.org},
	author = {Wang, Ting and Knap, Jaroslaw},
	month = jun,
	year = {2022},
}

@article{guy_distributed_2020,
	title = {A {Distributed} {Active} {Subspace} {Method} for {Scalable} {Surrogate} {Modeling} of {Function} {Valued} {Outputs}},
	volume = {85},
	issn = {1573-7691},
	url = {https://doi.org/10.1007/s10915-020-01346-2},
	doi = {10.1007/s10915-020-01346-2},
	abstract = {We present a distributed active subspace method for training surrogate models of complex physical processes with high-dimensional inputs and function valued outputs. Specifically, we represent the model output with a truncated Karhunen–Loève (KL) expansion, screen the structure of the input space with respect to each KL mode via the active subspace method, and finally form an overall surrogate model of the output by combining surrogates of individual output KL modes. To ensure scalable computation of the gradients of the output KL modes, needed in active subspace discovery, we rely on adjoint-based gradient computation. The proposed method combines benefits of active subspace methods for input dimension reduction and KL expansions used for spectral representation of the output field. We provide a mathematical framework for the proposed method and conduct an error analysis of the mixed KL active subspace approach. Specifically, we provide an error estimate that quantifies errors due to active subspace projection and truncated KL expansion of the output. We demonstrate the numerical performance of the surrogate modeling approach with an application example from biotransport.},
	language = {en},
	number = {2},
	urldate = {2024-06-06},
	journal = {Journal of Scientific Computing},
	author = {Guy, Hayley and Alexanderian, Alen and Yu, Meilin},
	month = oct,
	year = {2020},
	keywords = {Biotransport, Dimension reduction, Distributed active subspace, Function valued outputs, Karhunen–Loève expansion, Porous medium flow},
	pages = {36},
}

@misc{snoek_input_2014,
	title = {Input {Warping} for {Bayesian} {Optimization} of {Non}-stationary {Functions}},
	url = {https://arxiv.org/abs/1402.0929v3},
	abstract = {Bayesian optimization has proven to be a highly effective methodology for the global optimization of unknown, expensive and multimodal functions. The ability to accurately model distributions over functions is critical to the effectiveness of Bayesian optimization. Although Gaussian processes provide a flexible prior over functions which can be queried efficiently, there are various classes of functions that remain difficult to model. One of the most frequently occurring of these is the class of non-stationary functions. The optimization of the hyperparameters of machine learning algorithms is a problem domain in which parameters are often manually transformed a priori, for example by optimizing in "log-space," to mitigate the effects of spatially-varying length scale. We develop a methodology for automatically learning a wide family of bijective transformations or warpings of the input space using the Beta cumulative distribution function. We further extend the warping framework to multi-task Bayesian optimization so that multiple tasks can be warped into a jointly stationary space. On a set of challenging benchmark optimization tasks, we observe that the inclusion of warping greatly improves on the state-of-the-art, producing better results faster and more reliably.},
	language = {en},
	urldate = {2024-06-06},
	journal = {arXiv.org},
	author = {Snoek, Jasper and Swersky, Kevin and Zemel, Richard S. and Adams, Ryan P.},
	month = feb,
	year = {2014},
}

@article{tsitsiklis_efficient_1995,
	title = {Efficient algorithms for globally optimal trajectories},
	volume = {40},
	issn = {1558-2523},
	url = {https://ieeexplore-ieee-org.proxy.lib.umich.edu/abstract/document/412624},
	doi = {10.1109/9.412624},
	abstract = {We present serial and parallel algorithms for solving a system of equations that arises from the discretization of the Hamilton-Jacobi equation associated to a trajectory optimization problem of the following type. A vehicle starts at a prespecified point x/sub o/ and follows a unit speed trajectory x(t) inside a region in /spl Rscr//sup m/ until an unspecified time T that the region is exited. A trajectory minimizing a cost function of the form /spl int//sub 0//sup T/ r(x(t))dt+q(x(T)) is sought. The discretized Hamilton-Jacobi equation corresponding to this problem is usually solved using iterative methods. Nevertheless, assuming that the function r is positive, we are able to exploit the problem structure and develop one-pass algorithms for the discretized problem. The first algorithm resembles Dijkstra's shortest path algorithm and runs in time O(n log n), where n is the number of grid points. The second algorithm uses a somewhat different discretization and borrows some ideas from a variation of Dial's shortest path algorithm (1969) that we develop here; it runs in time O(n), which is the best possible, under some fairly mild assumptions. Finally, we show that the latter algorithm can be efficiently parallelized: for two-dimensional problems and with p processors, its running time becomes O(n/p), provided that p=O(/spl radic/n/log n).{\textless}{\textgreater}},
	number = {9},
	urldate = {2024-06-05},
	journal = {IEEE Transactions on Automatic Control},
	author = {Tsitsiklis, J.N.},
	month = sep,
	year = {1995},
	keywords = {Computational complexity, Cost function, Equations, Iterative algorithms, Iterative methods, Operations research, Optimal control, Optimization methods, Parallel algorithms, Vehicles},
	pages = {1528--1538},
}

@inproceedings{galioto_bayesian_2020,
	title = {Bayesian {Identification} of {Hamiltonian} {Dynamics} from {Symplectic} {Data}},
	url = {https://ieeexplore.ieee.org/document/9303852},
	doi = {10.1109/CDC42340.2020.9303852},
	abstract = {We propose a Bayesian probabilistic formulation for system identification of Hamiltonian systems. This approach uses an approximate marginal Markov Chain Monte Carlo algorithm to directly discover a system Hamiltonian from data. Our approach improves upon existing methods in two ways: first we encode the fact that the data generating process is symplectic directly into our learning objective, and second we utilize a learning objective that simultaneously accounts for unknown parameters, model form, and measurement noise. This objective is the log marginal posterior of a probabilistic model that embeds a symplectic and reversible integrator within an uncertain dynamical system. We demonstrate that the resulting learning problem yields dynamical systems that have improved accuracy and reduced predictive uncertainty compared to existing state-of-the-art approaches. Simulation results are shown on the Hénon-Heiles Hamiltonian system.},
	urldate = {2024-06-05},
	booktitle = {2020 59th {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Galioto, Nicholas and Gorodetsky, Alex A.},
	month = dec,
	year = {2020},
	note = {ISSN: 2576-2370},
	keywords = {Approximation algorithms, Bayes methods, Dynamical systems, Mathematical model, Predictive models, Probabilistic logic, Uncertainty},
	pages = {1190--1195},
}

@article{lawrence_probabilistic_2005,
	title = {Probabilistic {Non}-linear {Principal} {Component} {Analysis} with {Gaussian} {Process} {Latent} {Variable} {Models}},
	volume = {6},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v6/lawrence05a.html},
	abstract = {Summarising a high dimensional data set with a low dimensional embedding
is a standard approach for exploring its structure. In this paper
we provide an overview of some existing techniques for discovering
such embeddings. We then introduce a novel probabilistic interpretation
of principal component analysis (PCA) that we term dual probabilistic
PCA (DPPCA). The DPPCA model has the additional advantage that the
linear mappings from the embedded space can easily be non-linearised
through Gaussian processes. We refer to this model as a Gaussian process
latent variable model (GP-LVM). Through analysis of the GP-LVM objective
function, we relate the model to popular spectral techniques such
as kernel PCA and multidimensional scaling. We then review a practical
algorithm for GP-LVMs in the context of large data sets and develop
it to also handle discrete valued data and missing attributes. We
demonstrate the model on a range of real-world and artificially generated
data sets.},
	number = {60},
	urldate = {2024-06-05},
	journal = {Journal of Machine Learning Research},
	author = {Lawrence, Neil},
	year = {2005},
	pages = {1783--1816},
}

@article{beckmann_probabilistic_2004,
	title = {Probabilistic independent component analysis for functional magnetic resonance imaging},
	volume = {23},
	issn = {0278-0062},
	doi = {10.1109/TMI.2003.822821},
	abstract = {We present an integrated approach to probabilistic independent component analysis (ICA) for functional MRI (FMRI) data that allows for nonsquare mixing in the presence of Gaussian noise. In order to avoid overfitting, we employ objective estimation of the amount of Gaussian noise through Bayesian analysis of the true dimensionality of the data, i.e., the number of activation and non-Gaussian noise sources. This enables us to carry out probabilistic modeling and achieves an asymptotically unique decomposition of the data. It reduces problems of interpretation, as each final independent component is now much more likely to be due to only one physical or physiological process. We also describe other improvements to standard ICA, such as temporal prewhitening and variance normalization of timeseries, the latter being particularly useful in the context of dimensionality reduction when weak activation is present. We discuss the use of prior information about the spatiotemporal nature of the source processes, and an alternative-hypothesis testing approach for inference, using Gaussian mixture models. The performance of our approach is illustrated and evaluated on real and artificial FMRI data, and compared to the spatio-temporal accuracy of results obtained from classical ICA and GLM analyses.},
	language = {eng},
	number = {2},
	journal = {IEEE transactions on medical imaging},
	author = {Beckmann, Christian F. and Smith, Stephen M.},
	month = feb,
	year = {2004},
	pmid = {14964560},
	keywords = {Algorithms, Brain, Cerebral Cortex, Humans, Image Enhancement, Image Interpretation, Computer-Assisted, Magnetic Resonance Imaging, Models, Neurological, Models, Statistical, Neurons, Phantoms, Imaging, Principal Component Analysis, Reproducibility of Results, Sensitivity and Specificity, Stochastic Processes, Vision, Ocular},
	pages = {137--152},
}

@misc{noauthor_independent_nodate,
	title = {Independent component analysis: algorithms and applications - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608000000265},
	urldate = {2024-06-05},
}

@article{bach_kernel_2002,
	title = {Kernel {Independent} {Component} {Analysis}},
	volume = {3},
	issn = {ISSN 1533-7928},
	url = {https://www.jmlr.org/papers/v3/bach02a.html},
	abstract = {We present a class of algorithms for independent component
analysis (ICA) which use contrast functions based on canonical
correlations in a reproducing kernel Hilbert space.  On the one
hand, we show that our contrast functions are related to mutual
information and have desirable mathematical properties as measures
of statistical dependence. On the other hand, building on recent
developments in kernel methods, we show that these criteria and
their derivatives can be computed efficiently. Minimizing these
criteria leads to flexible and robust algorithms for ICA. We
illustrate with simulations involving a wide variety of source
distributions, showing that our algorithms outperform many of the
presently known algorithms.},
	number = {Jul},
	urldate = {2024-06-05},
	journal = {Journal of Machine Learning Research},
	author = {Bach, Francis R. and Jordan, Michael I.},
	year = {2002},
	pages = {1--48},
}

@article{caldeira_deeply_2020,
	title = {Deeply uncertain: comparing methods of uncertainty quantification in deep learning algorithms},
	volume = {2},
	issn = {2632-2153},
	shorttitle = {Deeply uncertain},
	url = {https://dx.doi.org/10.1088/2632-2153/aba6f3},
	doi = {10.1088/2632-2153/aba6f3},
	abstract = {We present a comparison of methods for uncertainty quantification (UQ) in deep learning algorithms in the context of a simple physical system. Three of the most common uncertainty quantification methods—Bayesian neural networks (BNNs), concrete dropout (CD), and deep ensembles (DEs) — are compared to the standard analytic error propagation. We discuss this comparison in terms endemic to both machine learning (‘epistemic’ and ‘aleatoric’) and the physical sciences (‘statistical’ and ‘systematic’). The comparisons are presented in terms of simulated experimental measurements of a single pendulum—a prototypical physical system for studying measurement and analysis techniques. Our results highlight some pitfalls that may occur when using these UQ methods. For example, when the variation of noise in the training set is small, all methods predicted the same relative uncertainty independently of the inputs. This issue is particularly hard to avoid in BNN. On the other hand, when the test set contains samples far from the training distribution, we found that no methods sufficiently increased the uncertainties associated to their predictions. This problem was particularly clear for CD. In light of these results, we make some recommendations for usage and interpretation of UQ methods.},
	language = {en},
	number = {1},
	urldate = {2024-06-04},
	journal = {Machine Learning: Science and Technology},
	author = {Caldeira, João and Nord, Brian},
	month = dec,
	year = {2020},
	pages = {015002},
}

@article{dass_laplace_2017,
	title = {Laplace based approximate posterior inference for differential equation models},
	volume = {27},
	issn = {0960-3174, 1573-1375},
	url = {http://arxiv.org/abs/1607.07203},
	doi = {10.1007/s11222-016-9647-0},
	abstract = {Ordinary differential equations are arguably the most popular and useful mathematical tool for describing physical and biological processes in the real world. Often, these physical and biological processes are observed with errors, in which case the most natural way to model such data is via regression where the mean function is defined by an ordinary differential equation believed to provide an understanding of the underlying process. These regression based dynamical models are called differential equation models. Parameter inference from differential equation models poses computational challenges mainly due to the fact that analytic solutions to most differential equations are not available. In this paper, we propose an approximation method for obtaining the posterior distribution of parameters in differential equation models. The approximation is done in two steps. In the first step, the solution of a differential equation is approximated by the general one-step method which is a class of numerical methods for ordinary differential equations including the Euler and the Runge-Kutta procedures; in the second step, nuisance parameters are marginalized using Laplace approximation. The proposed Laplace approximated posterior gives a computationally fast alternative to the full Bayesian computational scheme (such as Markov Chain Monte Carlo) and produces more accurate and stable estimators than the popular smoothing methods (called collocation methods) based on frequentist procedures. For a theoretical support of the proposed method, we prove that the Laplace approximated posterior converges to the actual posterior under certain conditions and analyze the relation between the order of numerical error and its Laplace approximation. The proposed method is tested on simulated data sets and compared with the other existing methods.},
	number = {3},
	urldate = {2024-06-04},
	journal = {Statistics and Computing},
	author = {Dass, Sarat C. and Lee, Jaeyong and Lee, Kyoungjae and Park, Jonghun},
	month = may,
	year = {2017},
	note = {arXiv:1607.07203 [stat]},
	keywords = {Statistics - Methodology},
	pages = {679--698},
}

@misc{ffjord_2018_grathwohl,
	title = {{FFJORD}: {Free}-form {Continuous} {Dynamics} for {Scalable} {Reversible} {Generative} {Models}},
	shorttitle = {{FFJORD}},
	url = {http://arxiv.org/abs/1810.01367},
	doi = {10.48550/arXiv.1810.01367},
	abstract = {A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
	month = oct,
	year = {2018},
	note = {arXiv:1810.01367 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{hyvarinen_nonlinear_2023,
	title = {Nonlinear {Independent} {Component} {Analysis} for {Principled} {Disentanglement} in {Unsupervised} {Deep} {Learning}},
	url = {http://arxiv.org/abs/2303.16535},
	doi = {10.48550/arXiv.2303.16535},
	abstract = {A central problem in unsupervised deep learning is how to find useful representations of high-dimensional data, sometimes called "disentanglement". Most approaches are heuristic and lack a proper theoretical foundation. In linear representation learning, independent component analysis (ICA) has been successful in many applications areas, and it is principled, i.e., based on a well-defined probabilistic model. However, extension of ICA to the nonlinear case has been problematic due to the lack of identifiability, i.e., uniqueness of the representation. Recently, nonlinear extensions that utilize temporal structure or some auxiliary information have been proposed. Such models are in fact identifiable, and consequently, an increasing number of algorithms have been developed. In particular, some self-supervised algorithms can be shown to estimate nonlinear ICA, even though they have initially been proposed from heuristic perspectives. This paper reviews the state-of-the-art of nonlinear ICA theory and algorithms.},
	urldate = {2024-06-03},
	publisher = {arXiv},
	author = {Hyvarinen, Aapo and Khemakhem, Ilyes and Morioka, Hiroshi},
	month = sep,
	year = {2023},
	note = {arXiv:2303.16535 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{greydanus_hamiltonian_2019,
	title = {Hamiltonian {Neural} {Networks}},
	url = {http://arxiv.org/abs/1906.01563},
	doi = {10.48550/arXiv.1906.01563},
	abstract = {Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time.},
	urldate = {2024-06-02},
	publisher = {arXiv},
	author = {Greydanus, Sam and Dzamba, Misko and Yosinski, Jason},
	month = sep,
	year = {2019},
	note = {arXiv:1906.01563 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@article{hennig_probabilistic_2015,
	title = {Probabilistic {Numerics} and {Uncertainty} in {Computations}},
	doi = {10.1098/rspa.2015.0142},
	abstract = {We deliver a call to arms for probabilistic numerical methods: algorithms for numerical tasks, including linear algebra, integration, optimization and solving differential equations, that return uncertainties in their calculations. Such uncertainties, arising from the loss of precision induced by numerical calculation with limited time or hardware, are important for much contemporary science and industry. Within applications such as climate science and astrophysics, the need to make decisions on the basis of computations with large and complex data have led to a renewed focus on the management of numerical uncertainty. We describe how several seminal classic numerical methods can be interpreted naturally as probabilistic inference. We then show that the probabilistic view suggests new algorithms that can flexibly be adapted to suit application specifics, while delivering improved empirical performance. We provide concrete illustrations of the benefits of probabilistic numeric algorithms on real scientific problems from astrometry and astronomical imaging, while highlighting open problems with these new algorithms. Finally, we describe how probabilistic numerical methods provide a coherent framework for identifying the uncertainty in calculations performed with a combination of numerical algorithms (e.g. both numerical optimizers and differential equation solvers), potentially allowing the diagnosis (and control) of error sources in computations.},
	journal = {Proceedings of The Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Hennig, Philipp and Osborne, Michael A. and Girolami, Mark},
	year = {2015},
	pmid = {26346321},
	pmcid = {4528661},
}

@article{oates_modern_2019,
	title = {A {Modern} {Retrospective} on {Probabilistic} {Numerics}},
	doi = {10.1007/s11222-019-09902-z},
	abstract = {This article attempts to place the emergence of probabilistic numerics as a mathematical–statistical research field within its historical context and to explore how its gradual development can be related both to applications and to a modern formal treatment. We highlight in particular the parallel contributions of Sul\$'\$din and Larkin in the 1960s and how their pioneering early ideas have reached a degree of maturity in the intervening period, mediated by paradigms such as average-case analysis and information-based complexity. We provide a subjective assessment of the state of research in probabilistic numerics and highlight some difficulties to be addressed by future works.},
	journal = {Statistics and Computing},
	author = {Oates, Chris J. and Oates, Chris J. and Sullivan, Timothy J. and Sullivan, Timothy},
	year = {2019},
	pmid = {null},
	pmcid = {null},
}

@article{schober_probabilistic_2019,
	title = {A probabilistic model for the numerical solution of initial value problems},
	doi = {10.1007/s11222-017-9798-7},
	abstract = {We study connections between ordinary differential equation (ODE) solvers and probabilistic regression methods in statistics. We provide a new view of probabilistic ODE solvers as active inference agents operating on stochastic differential equation models that estimate the unknown initial value problem (IVP) solution from approximate observations of the solution derivative, as provided by the ODE dynamics. Adding to this picture, we show that several multistep methods of Nordsieck form can be recasted as Kalman filtering on q-times integrated Wiener processes. Doing so provides a family of IVP solvers that return a Gaussian posterior measure, rather than a point estimate. We show that some such methods have low computational overhead, nontrivial convergence order, and that the posterior has a calibrated concentration rate. Additionally, we suggest a step size adaptation algorithm which completes the proposed method to a practically useful implementation, which we experimentally evaluate using a representative set of standard codes in the DETEST benchmark set.},
	journal = {Statistics and Computing},
	author = {Schober, Michael and Schober, Michael and Särkkä, Simon and Särkkä, Simo and Hennig, Philipp and Hennig, Philipp},
	year = {2019},
	pmid = {null},
	pmcid = {null},
}

@article{tronarp_bayesian_2020,
	title = {Bayesian {ODE} {Solvers}: {The} {Maximum} {A} {Posteriori} {Estimate}},
	doi = {10.1007/s11222-021-09993-7},
	abstract = {It has recently been established that the numerical solution of ordinary differential equations can be posed as a nonlinear Bayesian inference problem, which can be approximately solved via Gaussian filtering and smoothing, whenever a Gauss–Markov prior is used. In this paper the class of ν times differentiable linear time invariant Gauss–Markov priors is considered. A taxonomy of Gaussian estimators is established, with the maximum a posteriori estimate at the top of the hierarchy, which can be computed with the iterated extended Kalman smoother. The remaining three classes are termed explicit, semi-implicit, and implicit, which are in similarity with the classical notions corresponding to conditions on the vector field, under which the filter update produces a local maximum a posteriori estimate. The maximum a posteriori estimate corresponds to an optimal interpolant in the reproducing Hilbert space associated with the prior, which in the present case is equivalent to a Sobolev space of smoothness {\textbackslash}nu+1. Consequently, using methods from scattered data approximation and nonlinear analysis in Sobolev spaces, it is shown that the maximum a posteriori estimate converges to the true solution at a polynomial rate in the fill-distance (maximum step size) subject to mild conditions on the vector field. The methodology developed provides a novel and more natural approach to study the convergence of these estimators than classical methods of convergence analysis. The methods and theoretical results are demonstrated in numerical examples.},
	journal = {arXiv: Numerical Analysis},
	author = {Tronarp, Filip and Tronarp, Filip and Särkkä, Simo and Särkkä, Simo and Hennig, Philipp and Hennig, Philipp},
	year = {2020},
	pmid = {null},
	pmcid = {null},
}

@article{raissi_numerical_2018,
	title = {Numerical {Gaussian} {Processes} for {Time}-{Dependent} and {Nonlinear} {Partial} {Differential} {Equations}},
	doi = {10.1137/17m1120762},
	abstract = {We introduce the concept of numerical Gaussian processes, which we define as Gaussian processes with covariance functions resulting from temporal discretization of time-dependent partial differential equations. Numerical Gaussian processes, by construction, are designed to deal with cases where (a) all we observe are noisy data on black-box initial conditions, and (b) we are interested in quantifying the uncertainty associated with such noisy data in our solutions to time-dependent partial differential equations. Our method circumvents the need for spatial discretization of the differential operators by proper placement of Gaussian process priors. This is an attempt to construct structured and data-efficient learning machines, which are explicitly informed by the underlying physics that possibly generated the observed data. The effectiveness of the proposed approach is demonstrated through several benchmark problems involving linear and nonlinear time-dependent operators. In all examples, we are able to r...},
	journal = {SIAM Journal on Scientific Computing},
	author = {Raissi, Maziar and Raissi, Maziar and Perdikaris, Paris and Perdikaris, Paris and Karniadakis, George Em and Karniadakis, George Em},
	year = {2018},
	pmid = {null},
	pmcid = {null},
}

@article{cockayne_bayesian_2017,
	title = {Bayesian {Probabilistic} {Numerical} {Methods}},
	doi = {10.1137/17m1139357},
	abstract = {The emergent field of probabilistic numerics has thus far lacked clear statistical principals. This paper establishes Bayesian probabilistic numerical methods as those which can be cast as solutions to certain inverse problems within the Bayesian framework. This allows us to establish general conditions under which Bayesian probabilistic numerical methods are well-defined, encompassing both non-linear and non-Gaussian models. For general computation, a numerical approximation scheme is proposed and its asymptotic convergence established. The theoretical development is then extended to pipelines of computation, wherein probabilistic numerical methods are composed to solve more challenging numerical tasks. The contribution highlights an important research frontier at the interface of numerical analysis and uncertainty quantification, with a challenging industrial application presented.},
	journal = {arXiv: Methodology},
	author = {Cockayne, Jon and Cockayne, Jon and Oates, Chris J. and Oates, Chris J. and Sullivan, Tim and Sullivan, Timothy and Girolami, Mark and Girolami, Mark},
	year = {2017},
	pmid = {null},
	pmcid = {null},
}

@article{fasshauer_solving_1999,
	title = {Solving differential equations with radial basis functions: multilevel methods and smoothing},
	doi = {10.1023/a:1018919824891},
	abstract = {Some of the meshless radial basis function methods used for the numerical solution of partial differential equations are reviewed. In particular, the differences between globally and locally supported methods are discussed, and for locally supported methods the important role of smoothing within a multilevel framework is demonstrated. A possible connection between multigrid finite elements and multilevel radial basis function methods with smoothing is explored. Various numerical examples are also provided throughout the paper.},
	journal = {Advances in Computational Mathematics},
	author = {Fasshauer, Gregory E. and Fasshauer, Gregory E.},
	year = {1999},
	pmid = {null},
	pmcid = {null},
}

@article{wang_bayesian_2021,
	title = {Bayesian numerical methods for nonlinear partial differential equations},
	doi = {10.1007/s11222-021-10030-w},
	abstract = {The numerical solution of differential equations can be formulated as an inference problem to which formal statistical approaches can be applied. However, nonlinear partial differential equations (PDEs) pose substantial challenges from an inferential perspective, most notably the absence of explicit conditioning formula. This paper extends earlier work on linear PDEs to a general class of initial value problems specified by nonlinear PDEs, motivated by problems for which evaluations of the right-hand-side, initial conditions, or boundary conditions of the PDE have a high computational cost. The proposed method can be viewed as exact Bayesian inference under an approximate likelihood, which is based on discretisation of the nonlinear differential operator. Proof-of-concept experimental results demonstrate that meaningful probabilistic uncertainty quantification for the unknown solution of the PDE can be performed, while controlling the number of times the right-hand-side, initial and boundary conditions are evaluated. A suitable prior model for the solution of the PDE is identified using novel theoretical analysis of the sample path properties of Matérn processes, which may be of independent interest.},
	journal = {Statistics and Computing},
	author = {Wang, Junyang and Wang, Junyang and Cockayne, Jon and Cockayne, Jon and Cockayne, Jon and Chkrebtii, Oksana A. and Chkrebtii, Oksana and Sullivan, Timothy J. and Sullivan, Timothy and Oates, Chris J. and Oates, Chris J.},
	year = {2021},
	pmid = {null},
	pmcid = {null},
}

@misc{denoeux_theory_2010,
	address = {Brest},
	title = {Theory of belief functions - {An} introduction},
	language = {en},
	author = {Denœux, Thierry},
	month = mar,
	year = {2010},
}

@article{papamakarios_normalizing_2021,
	title = {Normalizing {Flows} for {Probabilistic} {Modeling} and {Inference}},
	volume = {22},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v22/19-1028.html},
	abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
	number = {57},
	urldate = {2024-05-30},
	journal = {Journal of Machine Learning Research},
	author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
	year = {2021},
	pages = {1--64},
}

@article{merrill_empirical_2021,
	title = {An {Empirical} {Study} of {Bayesian} {Optimization}: {Acquisition} {Versus} {Partition}},
	volume = {22},
	issn = {1533-7928},
	shorttitle = {An {Empirical} {Study} of {Bayesian} {Optimization}},
	url = {http://jmlr.org/papers/v22/18-220.html},
	abstract = {Bayesian optimization (BO) is a popular framework for black-box optimization. Two classes of BO approaches have shown promising empirical performance while providing strong theoretical guarantees. The first class optimizes an acquisition function to select points, which is typically computationally expensive and can only be done approximately. The second class of algorithms use systematic space partitioning, which is much cheaper computationally but the selection is typically less informed. This points to a potential trade-off between the computational complexity and empirical performance of these algorithms. The current literature, however, only provides a sparse sampling of empirical comparison points, giving little insight into this trade-off. The primary contribution of this work is to conduct a comprehensive, repeatable evaluation within a common software framework, which we provide as an open-source package. Our results give strong evidence about the relative performance of these methods and reveal a consistent top performer, even when accounting for overall computation time.},
	number = {4},
	urldate = {2024-05-30},
	journal = {Journal of Machine Learning Research},
	author = {Merrill, Erich and Fern, Alan and Fern, Xiaoli and Dolatnia, Nima},
	year = {2021},
	pages = {1--25},
}

@phdthesis{hauth_advances_2024,
	type = {Thesis},
	title = {Advances in {Intuitive} {Priors} and {Scalable} {Algorithms} for {Bayesian} {Deep} {Neural} {Network} {Models} in {Scientific} {Applications}},
	url = {http://deepblue.lib.umich.edu/handle/2027.42/193183},
	abstract = {In recent years, deep learning (DL) algorithms have gained widespread use in scientific and engineering fields, promising insights into complex trends within extensive datasets. However, these models typically lack transparency and interpretability and the quantification of uncertainty in DL models, especially related to the understanding derived from the quality and quantity of training data, remains an underexplored research area. This dissertation addresses this gap by advancing Bayesian uncertainty quantification (UQ) methods in large-scale deep neural networks (DNNs), specifically constructing Bayesian neural networks (BNNs). 

This dissertation proposes two methodological improvements to BNNs: the first is a parameter subselection procedure that leverages gradient based sensitivity analysis to select only the most impactful DL parameters for Bayesian inference; the second contribution is a prior selection methodology that weighs both expert knowledge of the predictive space alongside as well as desirable regularizing effects in the weight space. 

This dissertation goes on to implement Bayesian neural networks and these novel methodologies in four unique scientific machine learning case studies, two related to physics simulations and two related to real-world health data. These case studies include: a novel framework for remotely detecting ice accumulation on helicopter rotor blades and assessing flight performance degradation; an investigation on the temporal evolution of uncertainty in Bayesian graph convolutional neural networks when predicting stress response in polycrystalline materials; an investigation of the uncertainty in the state-of-the-art U-NET model for brain tumor segmentation; and a novel framework for automatically assessing physical therapy patient performance on balance training exercises, along with preliminary approaches for future exercise recommendation. 

By drawing new insights into model uncertainty across diverse science and engineering applications, this research aims to provide greater understanding of uncertainty in Bayesian neural networks, to help mitigate the consequences of model overconfidence, and to provide critical metrics for decision-making and data collection.},
	language = {en\_US},
	urldate = {2024-05-29},
	author = {Hauth, Jeremiah},
	year = {2024},
	doi = {10.7302/22828},
}

@incollection{tracey_upgrading_2018,
	series = {{AIAA} {SciTech} {Forum}},
	title = {Upgrading from {Gaussian} {Processes} to {Student}’s-{T} {Processes}},
	url = {https://arc.aiaa.org/doi/10.2514/6.2018-1659},
	urldate = {2024-05-24},
	booktitle = {2018 {AIAA} {Non}-{Deterministic} {Approaches} {Conference}},
	publisher = {American Institute of Aeronautics and Astronautics},
	author = {Tracey, Brendan D. and Wolpert, David},
	month = jan,
	year = {2018},
	doi = {10.2514/6.2018-1659},
	keywords = {Aerodynamic Force, Aerospace Designs, Angle of Attack, Bayesian Optimization, Computational Fluid Dynamics, Computing, Cumulative Distribution Function, Nelder Mead Simplex, Optimization Algorithm, Wing Twist},
}

@misc{amini_deep_2019,
	title = {Deep {Evidential} {Regression}},
	url = {https://arxiv.org/abs/1910.02600v2},
	abstract = {Deterministic neural networks (NNs) are increasingly being deployed in safety critical domains, where calibrated, robust, and efficient measures of uncertainty are crucial. In this paper, we propose a novel method for training non-Bayesian NNs to estimate a continuous target as well as its associated evidence in order to learn both aleatoric and epistemic uncertainty. We accomplish this by placing evidential priors over the original Gaussian likelihood function and training the NN to infer the hyperparameters of the evidential distribution. We additionally impose priors during training such that the model is regularized when its predicted evidence is not aligned with the correct output. Our method does not rely on sampling during inference or on out-of-distribution (OOD) examples for training, thus enabling efficient and scalable uncertainty learning. We demonstrate learning well-calibrated measures of uncertainty on various benchmarks, scaling to complex computer vision tasks, as well as robustness to adversarial and OOD test samples.},
	language = {en},
	urldate = {2024-05-24},
	journal = {arXiv.org},
	author = {Amini, Alexander and Schwarting, Wilko and Soleimany, Ava and Rus, Daniela},
	month = oct,
	year = {2019},
}

@techreport{sentz_combination_2002,
	title = {Combination of {Evidence} in {Dempster}-{Shafer} {Theory}},
	url = {https://www.osti.gov/biblio/800792},
	abstract = {Dempster-Shafer theory offers an alternative to traditional probabilistic theory for the mathematical representation of uncertainty. The significant innovation of this framework is that it allows for the allocation of a probability mass to sets or intervals. Dempster-Shafer theory does not require an assumption regarding the probability of the individual constituents of the set or interval. This is a potentially valuable tool for the evaluation of risk and reliability in engineering applications when it is not possible to obtain a precise measurement from experiments, or when knowledge is obtained from expert elicitation. An important aspect of this theory is the combination of evidence obtained from multiple sources and the modeling of conflict between them. This report surveys a number of possible combination rules for Dempster-Shafer structures and provides examples of the implementation of these rules for discrete and interval-valued data.},
	language = {English},
	number = {SAND2002-0835},
	urldate = {2024-05-24},
	institution = {Sandia National Lab. (SNL-NM), Albuquerque, NM (United States); Sandia National Lab. (SNL-CA), Livermore, CA (United States)},
	author = {Sentz, Kari and Ferson, Scott},
	month = apr,
	year = {2002},
	doi = {10.2172/800792},
}

@article{zhuang_learned_2021,
	title = {Learned discretizations for passive scalar advection in a two-dimensional turbulent flow},
	volume = {6},
	url = {https://link.aps.org/doi/10.1103/PhysRevFluids.6.064605},
	doi = {10.1103/PhysRevFluids.6.064605},
	abstract = {The computational cost of fluid simulations increases rapidly with grid resolution. This has given a hard limit on the ability of simulations to accurately resolve small-scale features of complex flows. Here we use a machine learning approach to learn a numerical discretization that retains high accuracy even when the solution is under-resolved with classical methods. We apply this approach to passive scalar advection in a two-dimensional turbulent flow. The method maintains the same accuracy as traditional high-order flux-limited advection solvers, while using 4× lower grid resolution in each dimension. The machine learning component is tightly integrated with traditional finite-volume schemes and can be trained via an end-to-end differentiable programming framework. The solver can achieve near-peak hardware utilization on CPUs and accelerators via convolutional filters.},
	number = {6},
	urldate = {2024-05-21},
	journal = {Physical Review Fluids},
	author = {Zhuang, Jiawei and Kochkov, Dmitrii and Bar-Sinai, Yohai and Brenner, Michael P. and Hoyer, Stephan},
	month = jun,
	year = {2021},
	pages = {064605},
}

@misc{noauthor_probabilistic_2024,
	title = {Probabilistic {Machine} {Learning} {Methods} for {Spatio}-{Temporal} {Data}},
	url = {https://curate.nd.edu/articles/dataset/Probabilistic_Machine_Learning_Methods_for_Spatio-Temporal_Data/25595235/1},
	doi = {10.7274/25595235.v1},
	abstract = {This dissertation presents multiple novel methodological advancements in the realm of machine learning (ML) for spatio-temporal data applications. Traditional machine learning approaches typically have difficultly producing both accurate point predictions and adequate uncertainty quantification for these data, especially in instances where the data themselves are sampled at a fine temporal scale. This is due to the fact that inference on these complex ML models is notably difficult and can impose a significant computational burden. The challenge of forecasting spatio-temporal data is further heightened when attempting to ensure the forecast themselves obey any known physical laws which dictate or influence the underlying data structure.

We explore the current challenges in properly quantifying the uncertainty of forecasts for spatio-temporal data applications stemming from contemporary ML models. Methods are introduced to not only calibrate the uncertainty estimates such that proper coverage is achieved but also so there is a realistic expansion of the uncertainty through time. These contemporary ML models are also adapted such that the physical processes present throughout that data are used to inform the learning procedures, so that the forecasts themselves are influenced to be more physically compliant. We demonstrate the power in combining ML models in an ensemble to improve model accuracy in predicting nonstationary, complex temporal data. Finally, a general comparison is made to explore the benefits and drawbacks of ML approaches to time-series forecasting versus the popular and standard statistical approaches, and as a guide to explain how these newfound advanced ML modelling techniques are not necessarily meant to act as a universal best approach for prediction and forecasting.},
	language = {en},
	urldate = {2024-05-21},
	publisher = {University of Notre Dame},
	month = may,
	year = {2024},
}

@misc{romero_towards_2022,
	title = {Towards a {General} {Purpose} {CNN} for {Long} {Range} {Dependencies} in \${N}\${D}},
	url = {http://arxiv.org/abs/2206.03398},
	doi = {10.48550/arXiv.2206.03398},
	abstract = {The use of Convolutional Neural Networks (CNNs) is widespread in Deep Learning due to a range of desirable model properties which result in an efficient and effective machine learning framework. However, performant CNN architectures must be tailored to specific tasks in order to incorporate considerations such as the input length, resolution, and dimentionality. In this work, we overcome the need for problem-specific CNN architectures with our Continuous Convolutional Neural Network (CCNN): a single CNN architecture equipped with continuous convolutional kernels that can be used for tasks on data of arbitrary resolution, dimensionality and length without structural changes. Continuous convolutional kernels model long range dependencies at every layer, and remove the need for downsampling layers and task-dependent depths needed in current CNN architectures. We show the generality of our approach by applying the same CCNN to a wide set of tasks on sequential (1\${\textbackslash}mathrm\{D\}\$) and visual data (2\${\textbackslash}mathrm\{D\}\$). Our CCNN performs competitively and often outperforms the current state-of-the-art across all tasks considered.},
	urldate = {2024-05-21},
	publisher = {arXiv},
	author = {Romero, David W. and Knigge, David M. and Gu, Albert and Bekkers, Erik J. and Gavves, Efstratios and Tomczak, Jakub M. and Hoogendoorn, Mark},
	month = jul,
	year = {2022},
	note = {arXiv:2206.03398 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{giordano_black_2024,
	title = {Black {Box} {Variational} {Inference} with a {Deterministic} {Objective}: {Faster}, {More} {Accurate}, and {Even} {More} {Black} {Box}},
	volume = {25},
	issn = {1533-7928},
	shorttitle = {Black {Box} {Variational} {Inference} with a {Deterministic} {Objective}},
	url = {http://jmlr.org/papers/v25/23-1015.html},
	abstract = {Automatic differentiation variational inference (ADVI) offers fast and easy-to-use posterior approximation in multiple modern probabilistic programming languages. However, its stochastic optimizer lacks clear convergence criteria and requires tuning parameters. Moreover, ADVI inherits the poor posterior uncertainty estimates of mean-field variational Bayes (MFVB). We introduce "deterministic ADVI" (DADVI) to address these issues. DADVI replaces the intractable MFVB objective with a fixed Monte Carlo approximation, a technique known in the stochastic optimization literature as the "sample average approximation" (SAA). By optimizing an approximate but deterministic objective, DADVI can use off-the-shelf second-order optimization, and, unlike standard mean-field ADVI, is amenable to more accurate posterior covariances via linear response (LR). In contrast to existing worst-case theory, we show that, on certain classes of common statistical problems, DADVI and the SAA can perform well with relatively few samples even in very high dimensions, though we also show that such favorable results cannot extend to variational approximations that are too expressive relative to mean-field ADVI. We show on a variety of real-world problems that DADVI reliably finds good solutions with default settings (unlike ADVI) and, together with LR covariances, is typically faster and more accurate than standard ADVI.},
	number = {18},
	urldate = {2024-05-21},
	journal = {Journal of Machine Learning Research},
	author = {Giordano, Ryan and Ingram, Martin and Broderick, Tamara},
	year = {2024},
	pages = {1--39},
}

@article{fries_lasdi_2022,
	title = {{LaSDI}: {Parametric} {Latent} {Space} {Dynamics} {Identification}},
	volume = {399},
	issn = {00457825},
	shorttitle = {{LaSDI}},
	url = {http://arxiv.org/abs/2203.02076},
	doi = {10.1016/j.cma.2022.115436},
	abstract = {Enabling fast and accurate physical simulations with data has become an important area of computational physics to aid in inverse problems, design-optimization, uncertainty quantification, and other various decision-making applications. This paper presents a data-driven framework for parametric latent space dynamics identification procedure that enables fast and accurate simulations. The parametric model is achieved by building a set of local latent space model and designing an interaction among them. An individual local latent space dynamics model achieves accurate solution in a trust region. By letting the set of trust region to cover the whole parameter space, our model shows an increase in accuracy with an increase in training data. We introduce two different types of interaction mechanisms, i.e., point-wise and region-based approach. Both linear and nonlinear data compression techniques are used. We illustrate the framework of Latent Space Dynamics Identification (LaSDI) enable a fast and accurate solution process on various partial differential equations, i.e., Burgers' equations, radial advection problem, and nonlinear heat conduction problem, achieving \$O(100)\$x speed-up and \$O(1){\textbackslash}\%\$ relative error with respect to the corresponding full order models.},
	urldate = {2024-05-20},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Fries, William and He, Xiaolong and Choi, Youngsoo},
	month = sep,
	year = {2022},
	note = {arXiv:2203.02076 [cs, math]},
	keywords = {Mathematics - Numerical Analysis},
	pages = {115436},
}

@misc{noauthor_intense_nodate,
	title = {Intense space weather storms {October} 19-{November} 07, 2003},
	url = {https://repository.library.noaa.gov/view/noaa/6995},
	urldate = {2024-05-20},
}

@misc{noauthor_intense_nodate-1,
	title = {Intense space weather storms {October} 19-{November} 07, 2003},
	url = {https://repository.library.noaa.gov/view/noaa/6995},
	urldate = {2024-05-20},
}

@inproceedings{finzi_simplifying_2020,
	title = {Simplifying {Hamiltonian} and {Lagrangian} {Neural} {Networks} via {Explicit} {Constraints}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/9f655cc8884fda7ad6d8a6fb15cc001e-Abstract.html},
	urldate = {2024-05-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Finzi, Marc and Wang, Ke Alexander and Wilson, Andrew G},
	year = {2020},
	pages = {13880--13889},
}

@misc{liu_stein_2017,
	title = {Stein {Variational} {Gradient} {Descent} as {Gradient} {Flow}},
	url = {http://arxiv.org/abs/1704.07520},
	doi = {10.48550/arXiv.1704.07520},
	abstract = {Stein variational gradient descent (SVGD) is a deterministic sampling algorithm that iteratively transports a set of particles to approximate given distributions, based on an efficient gradient-based update that guarantees to optimally decrease the KL divergence within a function space. This paper develops the first theoretical analysis on SVGD, discussing its weak convergence properties and showing that its asymptotic behavior is captured by a gradient flow of the KL divergence functional under a new metric structure induced by Stein operator. We also provide a number of results on Stein operator and Stein's identity using the notion of weak derivative, including a new proof of the distinguishability of Stein discrepancy under weak conditions.},
	urldate = {2024-05-18},
	publisher = {arXiv},
	author = {Liu, Qiang},
	month = nov,
	year = {2017},
	note = {arXiv:1704.07520 [stat]},
	keywords = {Statistics - Machine Learning},
}

@misc{fedeli_geometrically-exact_2017,
	title = {Geometrically-exact time-integration mesh-free schemes for advection-diffusion problems derived from optimal transportation theory and their connection with particle methods},
	url = {http://arxiv.org/abs/1703.02165},
	doi = {10.48550/arXiv.1703.02165},
	abstract = {We develop an Optimal Transportation Meshfree (OTM) particle method for advection-diffusion in which the concentration or density of the diffusive species is approximated by Dirac measures. We resort to an incremental variational principle for purposes of time discretization of the diffusive step. This principle characterizes the evolution of the density as a competition between the Wasserstein distance between two consecutive densities and entropy. Exploiting the structure of the Euler-Lagrange equations, we approximate the density as a collection of Diracs. The interpolation of the incremental transport map is effected through mesh-free max-ent interpolation. Remarkably, the resulting update is geometrically exact with respect to advection and volume. We present three-dimensional examples of application that illustrate the scope and robustness of the method.},
	urldate = {2024-05-18},
	publisher = {arXiv},
	author = {Fedeli, Livio and Pandolfi, Anna and Ortiz, Michael},
	month = mar,
	year = {2017},
	note = {arXiv:1703.02165 [math]},
	keywords = {Mathematics - Numerical Analysis},
}

@inproceedings{zhong_benchmarking_2021,
	title = {Benchmarking {Energy}-{Conserving} {Neural} {Networks} for {Learning} {Dynamics} from {Data}},
	url = {https://proceedings.mlr.press/v144/zhong21a.html},
	abstract = {The last few years have witnessed an increased interest in incorporating physics-informed inductive bias in deep learning frameworks. In particular, a growing volume of literature has been exploring ways to enforce energy conservation while using neural networks for learning dynamics from observed time-series data. In this work, we present a comparative analysis of the energy-conserving neural networks - for example, deep Lagrangian network, Hamiltonian neural network, etc. - wherein the underlying physics is encoded in their computation graph. We focus on ten neural network models and explain the similarities and differences between the models. We compare their performance in 4 different physical systems. Our result highlights that using a high-dimensional coordinate system and then imposing restrictions via explicit constraints can lead to higher accuracy in the learned dynamics. We also point out the possibility of leveraging some of these energy-conserving models to design energy-based controllers.},
	language = {en},
	urldate = {2024-05-17},
	booktitle = {Proceedings of the 3rd {Conference} on {Learning} for {Dynamics} and {Control}},
	publisher = {PMLR},
	author = {Zhong, Yaofeng Desmond and Dey, Biswadip and Chakraborty, Amit},
	month = may,
	year = {2021},
	pages = {1218--1229},
}

@article{bhouri_gaussian_2022,
	title = {Gaussian processes meet {NeuralODEs}: a {Bayesian} framework for learning the dynamics of partially observed systems from scarce and noisy data},
	volume = {380},
	shorttitle = {Gaussian processes meet {NeuralODEs}},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsta.2021.0201},
	doi = {10.1098/rsta.2021.0201},
	abstract = {We present a machine learning framework (GP-NODE) for Bayesian model discovery from partial, noisy and irregular observations of nonlinear dynamical systems. The proposed method takes advantage of differentiable programming to propagate gradient information through ordinary differential equation solvers and perform Bayesian inference with respect to unknown model parameters using Hamiltonian Monte Carlo sampling and Gaussian Process priors over the observed system states. This allows us to exploit temporal correlations in the observed data, and efficiently infer posterior distributions over plausible models with quantified uncertainty. The use of the Finnish Horseshoe as a sparsity-promoting prior for free model parameters also enables the discovery of parsimonious representations for the latent dynamics. A series of numerical studies is presented to demonstrate the effectiveness of the proposed GP-NODE method including predator–prey systems, systems biology and a 50-dimensional human motion dynamical system.

This article is part of the theme issue ‘Data-driven prediction in dynamical systems’.},
	number = {2229},
	urldate = {2024-05-17},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Bhouri, Mohamed Aziz and Perdikaris, Paris},
	month = jun,
	year = {2022},
	keywords = {dynamical systems, model discovery, scientific machine learning, uncertainty quantification},
	pages = {20210201},
}

@misc{dandekar_bayesian_2022,
	title = {Bayesian {Neural} {Ordinary} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2012.07244},
	doi = {10.48550/arXiv.2012.07244},
	abstract = {Recently, Neural Ordinary Differential Equations has emerged as a powerful framework for modeling physical simulations without explicitly defining the ODEs governing the system, but instead learning them via machine learning. However, the question: "Can Bayesian learning frameworks be integrated with Neural ODE's to robustly quantify the uncertainty in the weights of a Neural ODE?" remains unanswered. In an effort to address this question, we primarily evaluate the following categories of inference methods: (a) The No-U-Turn MCMC sampler (NUTS), (b) Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) and (c) Stochastic Langevin Gradient Descent (SGLD). We demonstrate the successful integration of Neural ODEs with the above Bayesian inference frameworks on classical physical systems, as well as on standard machine learning datasets like MNIST, using GPU acceleration. On the MNIST dataset, we achieve a posterior sample accuracy of 98.5\% on the test ensemble of 10,000 images. Subsequently, for the first time, we demonstrate the successful integration of variational inference with normalizing flows and Neural ODEs, leading to a powerful Bayesian Neural ODE object. Finally, considering a predator-prey model and an epidemiological system, we demonstrate the probabilistic identification of model specification in partially-described dynamical systems using universal ordinary differential equations. Together, this gives a scientific machine learning tool for probabilistic estimation of epistemic uncertainties.},
	urldate = {2024-05-17},
	publisher = {arXiv},
	author = {Dandekar, Raj and Chung, Karen and Dixit, Vaibhav and Tarek, Mohamed and Garcia-Valadez, Aslan and Vemula, Krishna Vishal and Rackauckas, Chris},
	month = feb,
	year = {2022},
	note = {arXiv:2012.07244 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@phdthesis{axerio-cilies_predicting_2012,
	address = {United States -- California},
	type = {Ph.{D}.},
	title = {Predicting {Formula} 1 {Tire} {Aerodynamics}: {Sensitivities}, {Uncertainties} and {Optimization}},
	copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
	shorttitle = {Predicting {Formula} 1 {Tire} {Aerodynamics}},
	url = {https://www.proquest.com/docview/2454420037/abstract/30D8EEE5EF06431DPQ/1},
	abstract = {The efficient design of complex engineering devices requires detailed understanding of the physical processes. These may either negatively or positively affect performance even in the presence of variability in the operating environment or uncertainties in manufacturing. The focus of this work is the prediction of the aerodynamics associated with an isolated, stationary and rotating Formula 1 tire and brake assembly. The end goal is to design a better performing tire assembly in the presence of uncertainties, while reducing as much as possible the computational effort required. This required innovations in computational modeling, uncertainty analysis, and optimization techniques.
This application is extremely challenging due to the complex fluid dynamics processes such as impingement, jetting, large-scale separation, and wake recovery, inherent in the problem. Previous literature has indicated differing mechanisms explaining the dominant features such as the pressure loading, wake structures and unsteadiness. Limited work has been published on the aerodynamics of realistic tire geometry with specific emphasis on advanced turbulence closures (i.e. large eddy simulation - LES) and optimization under uncertainty; this work provides the first comprehensive reference for this problem. In this work we use large-scale simulations to validate turbulence closures against experimental particle image velocimetry results. We then for the first time quantify the uncertainty associated with Formula 1 tires through sensitivity analysis, and finally optimize the shape of a brake duct under these uncertainties using novel algorithms.
Part I of this thesis focuses on a deterministic analysis of a realistic tire and brake assembly geometry. The numerical simulations were setup to reproduce a controlled experiment performed at Stanford, in a low-speed close-loop wind tunnel. Multiple turbulence closures are evaluated to determine the closure that most accurately matches experimental results. The results show that LES is more accurate than Reynolds averaged Navier-Stokes (RANS) equations at predicting experimental results. The bluff body flow around an isolated tire in a closed wind tunnel is shown to be inherently unsteady. Time accurate methods (i.e. Unsteady RANS, LES) that capture the unsteadiness of the flow specifically in the near wake, contact patch, and hub regions are more precise at predicting macro-scale structures. A novel RANS wall treatment for moving walls is introduced to correctly represent situations in which the tangential velocity of the wall does not match the freestream velocity; this allows the use of coarser meshes without penalizing dramatically the prediction quality.
Part II of this thesis focuses on sensitivity, uncertainty quantification and optimization. This is a very important issue in computational engineering, especially in highly competitive environments such as in Formula 1. Current design practice based on a nominal scenario is gradually being replaced with an optimization process that targets realistic race conditions where temperatures, track condition, and 'dirty' air all impact the overall performance of the car. Sensitivities associated with inflow conditions, treatment of rotating components, geometrical tire uncertainties, contact patch assumptions, and turbulence model closure are quantified and ranked in order of importance. In this work, multi-dimensional response surfaces are constructed to relate input variables to output quantities of interest and to compute confidence intervals on the numerical predictions.
A classic non-dominated sorting based multi-objective evolutionary algorithm is used to optimize the brake cooling duct to achieve the highest air flow mass capture with the least possible tire drag. We continued this analysis by considering the effect of uncertainties in the optimization framework. An extension of the optimization algorithm, the Probabilistic Non-dominated Sorting Genetic Algorithm (P-NSGA) is proposed and tested. The result is a tight coupling between the uncertainty quantification procedure and the optimization procedure giving the most robust brake duct design in the presence of uncertainty.
With the availability of high-performance computing clusters, the design, optimization, and aerodynamic prediction of extremely complex configurations is becoming increasingly prevalent. In order to efficiently explore the full parameter space for design purposes or to assess the effect of uncertainties, multiple deterministic simulations (or an 'ensemble') need to be performed. In this work, a simulations environment, Leland, has also been developed to schedule, monitor and stir the calculation ensemble and extract runtime information as well as simulations results and statistics on the fly. Leland is a dynamic scheduler that starting from a small ensemble automatically selects the new candidate simulations to be performed to increase the efficiency of both the optimization procedure and the uncertainty quantification method. Leland is equipped with an auto-tuning strategy for optimal load balancing and fault tolerance checks to ensure that a simulation or a processor stall is detected and does not impact the overall ensemble.},
	language = {English},
	urldate = {2024-05-10},
	school = {Stanford University},
	author = {Axerio-Cilies, John},
	year = {2012},
	note = {ISBN: 9798672114958},
	keywords = {Brake assembly geometry, Fault tolerance, Optimal load balancing},
}

@article{seshadri_density-matching_2016,
	title = {A density-matching approach for optimization under uncertainty},
	volume = {305},
	issn = {0045-7825},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782516300883},
	doi = {10.1016/j.cma.2016.03.006},
	abstract = {Modern computers enable methods for design optimization that account for uncertainty in the system—so-called optimization under uncertainty (OUU). We propose a metric for OUU that measures the distance between a designer-specified probability density function of the system response (the target) and the system response’s density function at a given design. We study an OUU formulation that minimizes this distance metric over all designs. We discretize the objective function with numerical quadrature, and we approximate the response density function with a Gaussian kernel density estimate. We offer heuristics for addressing issues that arise in this formulation, and we apply the approach to a CFD-based airfoil shape optimization problem. We qualitatively compare the density-matching approach to a multi-objective robust design optimization to gain insight into the method.},
	urldate = {2024-05-10},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Seshadri, Pranay and Constantine, Paul and Iaccarino, Gianluca and Parks, Geoffrey},
	month = jun,
	year = {2016},
	keywords = {Density-matching, Design under uncertainty, Optimization under uncertainty},
	pages = {562--578},
}

@misc{noauthor_horsetail_nodate,
	title = {Horsetail matching: a flexible approach to optimization under uncertainty*},
	shorttitle = {Horsetail matching},
	url = {https://www.tandfonline.com/doi/epdf/10.1080/0305215X.2017.1327581?needAccess=true},
	language = {en},
	urldate = {2024-05-10},
	note = {ISSN: 0305-215X},
}

@inproceedings{beland_bayesian_nodate,
	address = {Long Beach, CA, USA},
	title = {Bayesian {Optimization} {Under} {Uncertainty}},
	url = {https://bayesopt.github.io/papers/2017/15.pdf},
	abstract = {We consider the problem of robust optimization, where it is sought to design a
system such that it sustains a specified measure of performance under uncertainty.
This problem is challenging since modeling a complex system under uncertainty
can be expensive and for most real-world problems robust optimization will not
be computationally viable. In this paper, we propose a Bayesian methodology to
efficiently solve a class of robust optimization problems that arise in engineering
design under uncertainty. The central idea is to use Gaussian process models of loss
functions (or robustness metrics) together with appropriate acquisition functions to
guide the search for a robust optimal solution. Numerical studies on a test problem
are presented to demonstrate the efficacy of the proposed approach.},
	booktitle = {{NeurIPS} {Workshop} on {Bayesian} {Optimization}},
	author = {Beland, Justin and Nair, Prasanth},
}

@misc{noauthor_bayesopt_nodate,
	title = {{BayesOpt} 2017},
	url = {https://bayesopt.github.io/accepted.html},
	urldate = {2024-05-10},
}

@misc{noauthor_bayesopt_nodate-1,
	title = {{BayesOpt} 2017},
	url = {https://bayesopt.github.io/accepted.html},
	urldate = {2024-05-10},
}

@article{lazaro-gredilla_sparse_2010,
	title = {Sparse {Spectrum} {Gaussian} {Process} {Regression}},
	volume = {11},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v11/lazaro-gredilla10a.html},
	abstract = {We present a new sparse Gaussian Process (GP) model for regression. The key novel idea is to sparsify the spectral representation of the GP. This leads to a simple, practical algorithm for regression tasks. We compare the achievable trade-offs between predictive accuracy and computational requirements, and show that these are typically superior to existing state-of-the-art sparse approximations. We discuss both the weight space and function space representations, and note that the new construction implies priors over functions which are always stationary, and can approximate any covariance function in this class.},
	number = {63},
	urldate = {2024-05-08},
	journal = {Journal of Machine Learning Research},
	author = {Lázaro-Gredilla, Miguel and Quiñnero-Candela, Joaquin and Rasmussen, Carl Edward and {An\&\#237} and Figueiras-Vidal, bal R.},
	year = {2010},
	pages = {1865--1881},
}

@misc{wilson_maximizing_2018,
	title = {Maximizing acquisition functions for {Bayesian} optimization},
	url = {http://arxiv.org/abs/1805.10196},
	doi = {10.48550/arXiv.1805.10196},
	abstract = {Bayesian optimization is a sample-efficient approach to global optimization that relies on theoretically motivated value heuristics (acquisition functions) to guide its search process. Fully maximizing acquisition functions produces the Bayes' decision rule, but this ideal is difficult to achieve since these functions are frequently non-trivial to optimize. This statement is especially true when evaluating queries in parallel, where acquisition functions are routinely non-convex, high-dimensional, and intractable. We first show that acquisition functions estimated via Monte Carlo integration are consistently amenable to gradient-based optimization. Subsequently, we identify a common family of acquisition functions, including EI and UCB, whose properties not only facilitate but justify use of greedy approaches for their maximization.},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Wilson, James T. and Hutter, Frank and Deisenroth, Marc Peter},
	month = dec,
	year = {2018},
	note = {arXiv:1805.10196 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{anastasiou_steins_2023,
	title = {Stein’s {Method} {Meets} {Computational} {Statistics}: {A} {Review} of {Some} {Recent} {Developments}},
	volume = {38},
	issn = {0883-4237, 2168-8745},
	shorttitle = {Stein’s {Method} {Meets} {Computational} {Statistics}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-38/issue-1/Steins-Method-Meets-Computational-Statistics--A-Review-of-Some/10.1214/22-STS863.full},
	doi = {10.1214/22-STS863},
	abstract = {Stein’s method compares probability distributions through the study of a class of linear operators called Stein operators. While mainly studied in probability and used to underpin theoretical statistics, Stein’s method has led to significant advances in computational statistics in recent years. The goal of this survey is to bring together some of these recent developments, and in doing so, to stimulate further research into the successful field of Stein’s method and statistics. The topics we discuss include tools to benchmark and compare sampling methods such as approximate Markov chain Monte Carlo, deterministic alternatives to sampling methods, control variate techniques, parameter estimation and goodness-of-fit testing.},
	number = {1},
	urldate = {2024-05-02},
	journal = {Statistical Science},
	author = {Anastasiou, Andreas and Barp, Alessandro and Briol, François-Xavier and Ebner, Bruno and Gaunt, Robert E. and Ghaderinezhad, Fatemeh and Gorham, Jackson and Gretton, Arthur and Ley, Christophe and Liu, Qiang and Mackey, Lester and Oates, Chris J. and Reinert, Gesine and Swan, Yvik},
	month = feb,
	year = {2023},
	keywords = {Stein’s method, approximate Markov chain Monte Carlo, control variates, goodness-of-fit testing, likelihood ratio, maximum likelihood estimator, prior sensitivity, sample quality, variational inference},
	pages = {120--139},
}

@misc{noauthor_optimization_nodate,
	title = {Optimization of {Multi}-{Fidelity} {Computer} {Experiments} via the {EQIE} {Criterion}},
	url = {https://www.tandfonline.com/doi/epdf/10.1080/00401706.2016.1142902?needAccess=true},
	language = {en},
	urldate = {2024-05-02},
}

@misc{feng_learning_2017,
	title = {Learning to {Draw} {Samples} with {Amortized} {Stein} {Variational} {Gradient} {Descent}},
	url = {http://arxiv.org/abs/1707.06626},
	doi = {10.48550/arXiv.1707.06626},
	abstract = {We propose a simple algorithm to train stochastic neural networks to draw samples from given target distributions for probabilistic inference. Our method is based on iteratively adjusting the neural network parameters so that the output changes along a Stein variational gradient direction (Liu \& Wang, 2016) that maximally decreases the KL divergence with the target distribution. Our method works for any target distribution specified by their unnormalized density function, and can train any black-box architectures that are differentiable in terms of the parameters we want to adapt. We demonstrate our method with a number of applications, including variational autoencoder (VAE) with expressive encoders to model complex latent space structures, and hyper-parameter learning of MCMC samplers that allows Bayesian inference to adaptively improve itself when seeing more data.},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {Feng, Yihao and Wang, Dilin and Liu, Qiang},
	month = oct,
	year = {2017},
	note = {arXiv:1707.06626 [stat]},
	keywords = {Statistics - Machine Learning},
}

@article{anastasiou_steins_2023-1,
	title = {Stein’s {Method} {Meets} {Computational} {Statistics}: {A} {Review} of {Some} {Recent} {Developments}},
	volume = {38},
	issn = {0883-4237, 2168-8745},
	shorttitle = {Stein’s {Method} {Meets} {Computational} {Statistics}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-38/issue-1/Steins-Method-Meets-Computational-Statistics--A-Review-of-Some/10.1214/22-STS863.full},
	doi = {10.1214/22-STS863},
	abstract = {Stein’s method compares probability distributions through the study of a class of linear operators called Stein operators. While mainly studied in probability and used to underpin theoretical statistics, Stein’s method has led to significant advances in computational statistics in recent years. The goal of this survey is to bring together some of these recent developments, and in doing so, to stimulate further research into the successful field of Stein’s method and statistics. The topics we discuss include tools to benchmark and compare sampling methods such as approximate Markov chain Monte Carlo, deterministic alternatives to sampling methods, control variate techniques, parameter estimation and goodness-of-fit testing.},
	number = {1},
	urldate = {2024-05-01},
	journal = {Statistical Science},
	author = {Anastasiou, Andreas and Barp, Alessandro and Briol, François-Xavier and Ebner, Bruno and Gaunt, Robert E. and Ghaderinezhad, Fatemeh and Gorham, Jackson and Gretton, Arthur and Ley, Christophe and Liu, Qiang and Mackey, Lester and Oates, Chris J. and Reinert, Gesine and Swan, Yvik},
	month = feb,
	year = {2023},
	keywords = {Stein’s method, approximate Markov chain Monte Carlo, control variates, goodness-of-fit testing, likelihood ratio, maximum likelihood estimator, prior sensitivity, sample quality, variational inference},
	pages = {120--139},
}

@misc{eriksson_scalable_2020,
	title = {Scalable {Global} {Optimization} via {Local} {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/1910.01739},
	doi = {10.48550/arXiv.1910.01739},
	abstract = {Bayesian optimization has recently emerged as a popular method for the sample-efficient optimization of expensive black-box functions. However, the application to high-dimensional problems with several thousand observations remains challenging, and on difficult problems Bayesian optimization is often not competitive with other paradigms. In this paper we take the view that this is due to the implicit homogeneity of the global probabilistic models and an overemphasized exploration that results from global acquisition. This motivates the design of a local probabilistic approach for global optimization of large-scale high-dimensional problems. We propose the \${\textbackslash}texttt\{TuRBO\}\$ algorithm that fits a collection of local models and performs a principled global allocation of samples across these models via an implicit bandit approach. A comprehensive evaluation demonstrates that \${\textbackslash}texttt\{TuRBO\}\$ outperforms state-of-the-art methods from machine learning and operations research on problems spanning reinforcement learning, robotics, and the natural sciences.},
	urldate = {2024-04-26},
	publisher = {arXiv},
	author = {Eriksson, David and Pearce, Michael and Gardner, Jacob R. and Turner, Ryan and Poloczek, Matthias},
	month = feb,
	year = {2020},
	note = {arXiv:1910.01739 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{wilson_maximizing_2018-1,
	title = {Maximizing acquisition functions for {Bayesian} optimization},
	url = {http://arxiv.org/abs/1805.10196},
	doi = {10.48550/arXiv.1805.10196},
	abstract = {Bayesian optimization is a sample-efficient approach to global optimization that relies on theoretically motivated value heuristics (acquisition functions) to guide its search process. Fully maximizing acquisition functions produces the Bayes' decision rule, but this ideal is difficult to achieve since these functions are frequently non-trivial to optimize. This statement is especially true when evaluating queries in parallel, where acquisition functions are routinely non-convex, high-dimensional, and intractable. We first show that acquisition functions estimated via Monte Carlo integration are consistently amenable to gradient-based optimization. Subsequently, we identify a common family of acquisition functions, including EI and UCB, whose properties not only facilitate but justify use of greedy approaches for their maximization.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Wilson, James T. and Hutter, Frank and Deisenroth, Marc Peter},
	month = dec,
	year = {2018},
	note = {arXiv:1805.10196 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{wilson_maximizing_2018-2,
	title = {Maximizing acquisition functions for {Bayesian} optimization},
	url = {http://arxiv.org/abs/1805.10196},
	doi = {10.48550/arXiv.1805.10196},
	abstract = {Bayesian optimization is a sample-efficient approach to global optimization that relies on theoretically motivated value heuristics (acquisition functions) to guide its search process. Fully maximizing acquisition functions produces the Bayes' decision rule, but this ideal is difficult to achieve since these functions are frequently non-trivial to optimize. This statement is especially true when evaluating queries in parallel, where acquisition functions are routinely non-convex, high-dimensional, and intractable. We first show that acquisition functions estimated via Monte Carlo integration are consistently amenable to gradient-based optimization. Subsequently, we identify a common family of acquisition functions, including EI and UCB, whose properties not only facilitate but justify use of greedy approaches for their maximization.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Wilson, James T. and Hutter, Frank and Deisenroth, Marc Peter},
	month = dec,
	year = {2018},
	note = {arXiv:1805.10196 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{pan_physics-informed_2020,
	title = {Physics-{Informed} {Probabilistic} {Learning} of {Linear} {Embeddings} of {Nonlinear} {Dynamics} with {Guaranteed} {Stability}},
	volume = {19},
	url = {https://epubs.siam.org/doi/10.1137/19M1267246},
	doi = {10.1137/19M1267246},
	abstract = {This paper describes a method for learning low-dimensional approximations of nonlinear dynamical systems, based on neural network approximations of the underlying Koopman operator. Extended Dynamic Mode Decomposition (EDMD) provides a useful data-driven approximation of the Koopman operator for analyzing dynamical systems. This paper addresses a fundamental problem associated with EDMD: a trade-off between representational capacity of the dictionary and overfitting due to insufficient data. A new neural network architecture combining an autoencoder with linear recurrent dynamics in the encoded state is used to learn a low-dimensional and highly informative Koopman-invariant subspace of observables. A method is also presented for balanced model reduction of overspecified EDMD systems in feature space. Nonlinear reconstruction using partially linear multikernel regression aims to improve reconstruction accuracy from the low-dimensional state when the data has complex but intrinsically low-dimensional structure. The techniques demonstrate the ability to identify Koopman eigenfunctions of the unforced Duffing equation, create accurate low-dimensional models of an unstable cylinder wake flow, and make short-time predictions of the chaotic Kuramoto--Sivashinsky equation.},
	number = {1},
	urldate = {2024-04-11},
	journal = {SIAM Journal on Applied Dynamical Systems},
	author = {Pan, Shaowu and Duraisamy, Karthik},
	month = jan,
	year = {2020},
	pages = {480--509},
}

@misc{hensman_gaussian_2013,
	title = {Gaussian {Processes} for {Big} {Data}},
	url = {http://arxiv.org/abs/1309.6835},
	doi = {10.48550/arXiv.1309.6835},
	abstract = {We introduce stochastic variational inference for Gaussian process models. This enables the application of Gaussian process (GP) models to data sets containing millions of data points. We show how GPs can be vari- ationally decomposed to depend on a set of globally relevant inducing variables which factorize the model in the necessary manner to perform variational inference. Our ap- proach is readily extended to models with non-Gaussian likelihoods and latent variable models based around Gaussian processes. We demonstrate the approach on a simple toy problem and two real world data sets.},
	urldate = {2024-04-10},
	publisher = {arXiv},
	author = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D.},
	month = sep,
	year = {2013},
	note = {arXiv:1309.6835 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{titsias_bayesian_2010,
	title = {Bayesian {Gaussian} {Process} {Latent} {Variable} {Model}},
	url = {https://proceedings.mlr.press/v9/titsias10a.html},
	abstract = {We introduce a variational inference framework for training the Gaussian process latent variable model and thus performing Bayesian nonlinear dimensionality reduction. This method allows us to variationally integrate out the input variables of the Gaussian process and compute a lower bound on the exact marginal likelihood of the nonlinear latent variable model. The maximization of the variational lower bound provides a Bayesian training procedure that is robust to overfitting and can automatically select the dimensionality of the nonlinear latent space. We demonstrate our method on real world datasets. The focus in this paper is on dimensionality reduction problems, but the methodology is more general. For example, our algorithm is immediately applicable for training Gaussian process models in the presence of missing or uncertain inputs.},
	language = {en},
	urldate = {2024-04-10},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Titsias, Michalis and Lawrence, Neil D.},
	month = mar,
	year = {2010},
	pages = {844--851},
}

@inproceedings{titsias_variational_2009,
	title = {Variational {Learning} of {Inducing} {Variables} in {Sparse} {Gaussian} {Processes}},
	url = {https://proceedings.mlr.press/v5/titsias09a.html},
	abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs  are defined to be variational parameters  which are selected by minimizing  the Kullback-Leibler divergence between  the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
	language = {en},
	urldate = {2024-04-10},
	booktitle = {Proceedings of the {Twelth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Titsias, Michalis},
	month = apr,
	year = {2009},
	pages = {567--574},
}

@inproceedings{snelson_sparse_2005,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'05},
	title = {Sparse {Gaussian} processes using pseudo-inputs},
	abstract = {We present a new Gaussian process (GP) regression model whose co-variance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M ≪ N, where N is the number of real data points, and hence obtain a sparse regression method which has O(M2N) training cost and O(M2) prediction cost per test case. We also find hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian regression model with particular input dependent noise. The method turns out to be closely related to several other sparse GP approaches, and we discuss the relation in detail. We finally demonstrate its performance on some large data sets, and make a direct comparison to other sparse GP methods. We show that our method can match full GP performance with small M, i.e. very sparse solutions, and it significantly outperforms other approaches in this regime.},
	urldate = {2024-04-10},
	booktitle = {Proceedings of the 18th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Snelson, Edward and Ghahramani, Zoubin},
	month = dec,
	year = {2005},
	pages = {1257--1264},
}

@misc{betancourt_hamiltonian_2013,
	title = {Hamiltonian {Monte} {Carlo} for {Hierarchical} {Models}},
	url = {http://arxiv.org/abs/1312.0906},
	doi = {10.48550/arXiv.1312.0906},
	abstract = {Hierarchical modeling provides a framework for modeling the complex interactions typical of problems in applied statistics. By capturing these relationships, however, hierarchical models also introduce distinctive pathologies that quickly limit the efficiency of most common methods of in- ference. In this paper we explore the use of Hamiltonian Monte Carlo for hierarchical models and demonstrate how the algorithm can overcome those pathologies in practical applications.},
	urldate = {2024-04-10},
	publisher = {arXiv},
	author = {Betancourt, M. J. and Girolami, Mark},
	month = dec,
	year = {2013},
	note = {arXiv:1312.0906 [stat]},
	keywords = {Statistics - Methodology},
}

@misc{finzi_simplifying_2020-1,
	title = {Simplifying {Hamiltonian} and {Lagrangian} {Neural} {Networks} via {Explicit} {Constraints}},
	url = {http://arxiv.org/abs/2010.13581},
	doi = {10.48550/arXiv.2010.13581},
	abstract = {Reasoning about the physical world requires models that are endowed with the right inductive biases to learn the underlying dynamics. Recent works improve generalization for predicting trajectories by learning the Hamiltonian or Lagrangian of a system rather than the differential equations directly. While these methods encode the constraints of the systems using generalized coordinates, we show that embedding the system into Cartesian coordinates and enforcing the constraints explicitly with Lagrange multipliers dramatically simplifies the learning problem. We introduce a series of challenging chaotic and extended-body systems, including systems with N-pendulums, spring coupling, magnetic fields, rigid rotors, and gyroscopes, to push the limits of current approaches. Our experiments show that Cartesian coordinates with explicit constraints lead to a 100x improvement in accuracy and data efficiency.},
	urldate = {2024-04-10},
	publisher = {arXiv},
	author = {Finzi, Marc and Wang, Ke Alexander and Wilson, Andrew Gordon},
	month = oct,
	year = {2020},
	note = {arXiv:2010.13581 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Physics - Computational Physics, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
}

@inproceedings{zhong_benchmarking_2021-1,
	title = {Benchmarking {Energy}-{Conserving} {Neural} {Networks} for {Learning} {Dynamics} from {Data}},
	url = {https://proceedings.mlr.press/v144/zhong21a.html},
	abstract = {The last few years have witnessed an increased interest in incorporating physics-informed inductive bias in deep learning frameworks. In particular, a growing volume of literature has been exploring ways to enforce energy conservation while using neural networks for learning dynamics from observed time-series data. In this work, we present a comparative analysis of the energy-conserving neural networks - for example, deep Lagrangian network, Hamiltonian neural network, etc. - wherein the underlying physics is encoded in their computation graph. We focus on ten neural network models and explain the similarities and differences between the models. We compare their performance in 4 different physical systems. Our result highlights that using a high-dimensional coordinate system and then imposing restrictions via explicit constraints can lead to higher accuracy in the learned dynamics. We also point out the possibility of leveraging some of these energy-conserving models to design energy-based controllers.},
	language = {en},
	urldate = {2024-04-10},
	booktitle = {Proceedings of the 3rd {Conference} on {Learning} for {Dynamics} and {Control}},
	publisher = {PMLR},
	author = {Zhong, Yaofeng Desmond and Dey, Biswadip and Chakraborty, Amit},
	month = may,
	year = {2021},
	pages = {1218--1229},
}

@article{hoffman_stochastic_2013,
	title = {Stochastic {Variational} {Inference}},
	volume = {14},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v14/hoffman13a.html},
	abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
	number = {40},
	urldate = {2024-04-06},
	journal = {Journal of Machine Learning Research},
	author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
	year = {2013},
	pages = {1303--1347},
}

@misc{liu_stein_2019,
	title = {Stein {Variational} {Gradient} {Descent}: {A} {General} {Purpose} {Bayesian} {Inference} {Algorithm}},
	shorttitle = {Stein {Variational} {Gradient} {Descent}},
	url = {http://arxiv.org/abs/1608.04471},
	doi = {10.48550/arXiv.1608.04471},
	abstract = {We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.},
	urldate = {2024-04-06},
	publisher = {arXiv},
	author = {Liu, Qiang and Wang, Dilin},
	month = sep,
	year = {2019},
	note = {arXiv:1608.04471 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{chen_projected_2020,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '20},
	title = {Projected stein variational gradient descent},
	isbn = {978-1-71382-954-6},
	abstract = {The curse of dimensionality is a longstanding challenge in Bayesian inference in high dimensions. In this work, we propose a projected Stein variational gradient descent (pSVGD) method to overcome this challenge by exploiting the fundamental property of intrinsic low dimensionality of the data informed subspace stemming from ill-posedness of such problems. We adaptively construct the subspace using a gradient information matrix of the log-likelihood, and apply pSVGD to the much lower-dimensional coefficients of the parameter projection. The method is demonstrated to be more accurate and efficient than SVGD. It is also shown to be more scalable with respect to the number of parameters, samples, data points, and processor cores via experiments with parameters dimensions ranging from the hundreds to the tens of thousands.},
	urldate = {2024-04-06},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Chen, Peng and Ghattas, Omar},
	month = dec,
	year = {2020},
	pages = {1947--1958},
}

@misc{liu_kernelized_2016,
	title = {A {Kernelized} {Stein} {Discrepancy} for {Goodness}-of-fit {Tests} and {Model} {Evaluation}},
	url = {http://arxiv.org/abs/1602.03253},
	doi = {10.48550/arXiv.1602.03253},
	abstract = {We derive a new discrepancy statistic for measuring differences between two probability distributions based on combining Stein's identity with the reproducing kernel Hilbert space theory. We apply our result to test how well a probabilistic model fits a set of observations, and derive a new class of powerful goodness-of-fit tests that are widely applicable for complex and high dimensional distributions, even for those with computationally intractable normalization constants. Both theoretical and empirical properties of our methods are studied thoroughly.},
	urldate = {2024-04-06},
	publisher = {arXiv},
	author = {Liu, Qiang and Lee, Jason D. and Jordan, Michael I.},
	month = jul,
	year = {2016},
	note = {arXiv:1602.03253 [stat]},
	keywords = {Statistics - Machine Learning},
}

@inproceedings{liu_short_2016,
	title = {A {Short} {Introduction} to {Kernelized} {Stein} {Discrepancy}},
	url = {https://www.semanticscholar.org/paper/A-Short-Introduction-to-Kernelized-Stein-Liu/eef3d4bfac79b383050900ebe8b64f7d9a042b70},
	abstract = {Machine learning and statistics are essentially about understanding data using models (typically probabilistic models). Discrepancy measures that can tell the consistency between data and models are extremely useful, and provide foundations for algorithms for all kinds of tasks, including model evaluation (telling how well a model fits the data), frequentist parameter learning (finding the model that minimizes the discrepancy with data), as well as sampling for Bayesian inference (finding a set of points ("data") to approximate the posterior distribution). See Figure 1.},
	urldate = {2024-04-06},
	author = {Liu, Qiang},
	year = {2016},
}

@article{plumlee_bayesian_2017,
	title = {Bayesian {Calibration} of {Inexact} {Computer} {Models}},
	volume = {112},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2016.1211016},
	doi = {10.1080/01621459.2016.1211016},
	abstract = {Bayesian calibration is used to study computer models in the presence of both a calibration parameter and model bias. The parameter in the predominant methodology is left undefined. This results in an issue, where the posterior of the parameter is suboptimally broad. There has been no generally accepted alternatives to date. This article proposes using Bayesian calibration, where the prior distribution on the bias is orthogonal to the gradient of the computer model. Problems associated with Bayesian calibration are shown to be mitigated through analytic results in addition to examples. Supplementary materials for this article are available online.},
	number = {519},
	urldate = {2024-04-05},
	journal = {Journal of the American Statistical Association},
	author = {Plumlee, Matthew},
	month = jul,
	year = {2017},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2016.1211016},
	keywords = {Calibration, Computer experiments, Deterministic models, Gaussian processes, Identifiability, Kriging, Model inadequacy, Orthogonal processes, Uncertainty quantification},
	pages = {1274--1285},
}

@article{kovachki_neural_2023,
	title = {Neural {Operator}: {Learning} {Maps} {Between} {Function} {Spaces} {With} {Applications} to {PDEs}},
	volume = {24},
	issn = {1533-7928},
	shorttitle = {Neural {Operator}},
	url = {http://jmlr.org/papers/v24/21-1524.html},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite dimensional Euclidean spaces or finite sets. We propose a generalization of neural networks to learn operators, termed neural operators, that map between infinite dimensional function spaces. We formulate the neural operator as a composition of linear integral operators and nonlinear activation functions. We prove a universal approximation theorem for our proposed neural operator, showing that it can approximate any given nonlinear continuous operator. The proposed neural operators are also discretization-invariant, i.e., they share the same model parameters among different discretization of the underlying function spaces. Furthermore, we introduce four classes of efficient parameterization, viz., graph neural operators, multi-pole graph neural operators, low-rank neural operators, and Fourier neural operators. An important application for neural operators is learning surrogate maps for the solution operators of partial differential equations (PDEs). We consider standard PDEs such as the Burgers, Darcy subsurface flow, and the Navier-Stokes equations, and show that the proposed neural operators have superior performance compared to existing machine learning based methodologies, while being several orders of magnitude faster than conventional PDE solvers.},
	number = {89},
	urldate = {2024-04-05},
	journal = {Journal of Machine Learning Research},
	author = {Kovachki, Nikola and Li, Zongyi and Liu, Burigede and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	year = {2023},
	pages = {1--97},
}

@misc{viswanath_neural_2023,
	title = {Neural {Operator}: {Is} data all you need to model the world? {An} insight into the impact of {Physics} {Informed} {Machine} {Learning}},
	shorttitle = {Neural {Operator}},
	url = {http://arxiv.org/abs/2301.13331},
	doi = {10.48550/arXiv.2301.13331},
	abstract = {Numerical approximations of partial differential equations (PDEs) are routinely employed to formulate the solution of physics, engineering and mathematical problems involving functions of several variables, such as the propagation of heat or sound, fluid flow, elasticity, electrostatics, electrodynamics, and more. While this has led to solving many complex phenomena, there are some limitations. Conventional approaches such as Finite Element Methods (FEMs) and Finite Differential Methods (FDMs) require considerable time and are computationally expensive. In contrast, data driven machine learning-based methods such as neural networks provide a faster, fairly accurate alternative, and have certain advantages such as discretization invariance and resolution invariance. This article aims to provide a comprehensive insight into how data-driven approaches can complement conventional techniques to solve engineering and physics problems, while also noting some of the major pitfalls of machine learning-based approaches. Furthermore, we highlight, a novel and fast machine learning-based approach ({\textasciitilde}1000x) to learning the solution operator of a PDE operator learning. We will note how these new computational approaches can bring immense advantages in tackling many problems in fundamental and applied physics.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Viswanath, Hrishikesh and Rahman, Md Ashiqur and Vyas, Abhijeet and Shor, Andrey and Medeiros, Beatriz and Hernandez, Stephanie and Prameela, Suhas Eswarappa and Bera, Aniket},
	month = sep,
	year = {2023},
	note = {arXiv:2301.13331 [physics]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Physics - Computational Physics},
}

@article{prabhudesai_lowering_2023,
	title = {Lowering the computational barrier: {Partially} {Bayesian} neural networks for transparency in medical imaging {AI}},
	volume = {5},
	issn = {2624-9898},
	shorttitle = {Lowering the computational barrier},
	url = {https://www.frontiersin.org/articles/10.3389/fcomp.2023.1071174},
	doi = {10.3389/fcomp.2023.1071174},
	abstract = {Deep Neural Networks (DNNs) can provide clinicians with fast and accurate predictions that are highly valuable for high-stakes medical decision-making, such as in brain tumor segmentation and treatment planning. However, these models largely lack transparency about the uncertainty in their predictions, potentially giving clinicians a false sense of reliability that may lead to grave consequences in patient care. Growing calls for Transparent and Responsible AI have promoted Uncertainty Quantification (UQ) to capture and communicate uncertainty in a systematic and principled manner. However, traditional Bayesian UQ methods remain prohibitively costly for large, million-dimensional tumor segmentation DNNs such as the U-Net. In this work, we discuss a computationally-efficient UQ approach via the partially Bayesian neural networks (pBNN). In pBNN, only a single layer, strategically selected based on gradient-based sensitivity analysis, is targeted for Bayesian inference. We illustrate the effectiveness of pBNN in capturing the full uncertainty for a 7.8-million parameter U-Net. We also demonstrate how practitioners and model developers can use the pBNN's predictions to better understand the model’s capabilities and behavior.},
	language = {English},
	urldate = {2024-04-04},
	journal = {Frontiers in Computer Science},
	author = {Prabhudesai, Snehal and Hauth, Jeremiah and Guo, Dingkun and Rao, Arvind and Banovic, Nikola and Huan, Xun},
	month = feb,
	year = {2023},
	note = {Publisher: Frontiers},
	keywords = {Bayesian uncertainty, Transparency, Tumor segmentation, medical decision-making, responsible ai},
}

@article{zhu_bayesian_2018,
	title = {Bayesian deep convolutional encoder–decoder networks for surrogate modeling and uncertainty quantification},
	volume = {366},
	issn = {0021-9991},
	url = {https://doi.org/10.1016/j.jcp.2018.04.018},
	doi = {10.1016/j.jcp.2018.04.018},
	abstract = {We are interested in the development of surrogate models for uncertainty quantification and propagation in problems governed by stochastic PDEs using a deep convolutional encoder–decoder network in a similar fashion to approaches considered in deep learning for image-to-image regression tasks. Since normal neural networks are data-intensive and cannot provide predictive uncertainty, we propose a Bayesian approach to convolutional neural nets. A recently introduced variational gradient descent algorithm based on Stein's method is scaled to deep convolutional networks to perform approximate Bayesian inference on millions of uncertain network parameters. This approach achieves state of the art performance in terms of predictive accuracy and uncertainty quantification in comparison to other approaches in Bayesian neural networks as well as techniques that include Gaussian processes and ensemble methods even when the training data size is relatively small. To evaluate the performance of this approach, we consider standard uncertainty quantification tasks for flow in heterogeneous media using limited training data consisting of permeability realizations and the corresponding velocity and pressure fields. The performance of the surrogate model developed is very good even though there is no underlying structure shared between the input (permeability) and output (flow/pressure) fields as is often the case in the image-to-image regression models used in computer vision problems. Studies are performed with an underlying stochastic input dimensionality up to 4225 where most other uncertainty quantification methods fail. Uncertainty propagation tasks are considered and the predictive output Bayesian statistics are compared to those obtained with Monte Carlo estimates. • Bayesian Convolutional Encoder–Decoder Deep Networks for Uncertainty Quantification Tasks. • Integrating Stein variational inference for exploring the high-dimensional posterior distribution of the network parameters. • Addressing the curse of dimensionality showing applications in porous media flows with permeability dimensionality of 4225.},
	number = {C},
	urldate = {2024-04-04},
	journal = {Journal of Computational Physics},
	author = {Zhu, Yinhao and Zabaras, Nicholas},
	month = aug,
	year = {2018},
	keywords = {Bayesian neural networks, Convolutional encoder–decoder networks, Deep learning, Porous media flows, Uncertainty quantification},
	pages = {415--447},
}

@misc{houlsby_bayesian_2011,
	title = {Bayesian {Active} {Learning} for {Classification} and {Preference} {Learning}},
	url = {http://arxiv.org/abs/1112.5745},
	doi = {10.48550/arXiv.1112.5745},
	abstract = {Information theoretic active learning has been widely studied for probabilistic models. For simple regression an optimal myopic policy is easily tractable. However, for other tasks and with more complex models, such as classification with nonparametric models, the optimal solution is harder to compute. Current approaches make approximations to achieve tractability. We propose an approach that expresses information gain in terms of predictive entropies, and apply this method to the Gaussian Process Classifier (GPC). Our approach makes minimal approximations to the full information theoretic objective. Our experimental performance compares favourably to many popular active learning algorithms, and has equal or lower computational complexity. We compare well to decision theoretic approaches also, which are privy to more information and require much more computational time. Secondly, by developing further a reformulation of binary preference learning to a classification problem, we extend our algorithm to Gaussian Process preference learning.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Houlsby, Neil and Huszár, Ferenc and Ghahramani, Zoubin and Lengyel, Máté},
	month = dec,
	year = {2011},
	note = {arXiv:1112.5745 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{rainforth2023,
	title = {Modern {Bayesian} {Experimental} {Design}},
	url = {http://arxiv.org/abs/2302.14545},
	doi = {10.48550/arXiv.2302.14545},
	abstract = {Bayesian experimental design (BED) provides a powerful and general framework for optimizing the design of experiments. However, its deployment often poses substantial computational challenges that can undermine its practical use. In this review, we outline how recent advances have transformed our ability to overcome these challenges and thus utilize BED effectively, before discussing some key areas for future development in the field.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Rainforth, Tom and Foster, Adam and Ivanova, Desi R. and Smith, Freddie Bickford},
	month = nov,
	year = {2023},
	note = {arXiv:2302.14545 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
}

@misc{kossen_active_2022,
	title = {Active {Surrogate} {Estimators}: {An} {Active} {Learning} {Approach} to {Label}-{Efficient} {Model} {Evaluation}},
	shorttitle = {Active {Surrogate} {Estimators}},
	url = {http://arxiv.org/abs/2202.06881},
	doi = {10.48550/arXiv.2202.06881},
	abstract = {We propose Active Surrogate Estimators (ASEs), a new method for label-efficient model evaluation. Evaluating model performance is a challenging and important problem when labels are expensive. ASEs address this active testing problem using a surrogate-based estimation approach that interpolates the errors of points with unknown labels, rather than forming a Monte Carlo estimator. ASEs actively learn the underlying surrogate, and we propose a novel acquisition strategy, XWED, that tailors this learning to the final estimation task. We find that ASEs offer greater label-efficiency than the current state-of-the-art when applied to challenging model evaluation problems for deep neural networks.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Kossen, Jannik and Farquhar, Sebastian and Gal, Yarin and Rainforth, Tom},
	month = oct,
	year = {2022},
	note = {arXiv:2202.06881 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{smith_prediction-oriented_2023,
	title = {Prediction-{Oriented} {Bayesian} {Active} {Learning}},
	url = {https://proceedings.mlr.press/v206/bickfordsmith23a.html},
	abstract = {Information-theoretic approaches to active learning have traditionally focused on maximising the information gathered about the model parameters, most commonly by optimising the BALD score. We highlight that this can be suboptimal from the perspective of predictive performance. For example, BALD lacks a notion of an input distribution and so is prone to prioritise data of limited relevance. To address this we propose the expected predictive information gain (EPIG), an acquisition function that measures information gain in the space of predictions rather than parameters. We find that using EPIG leads to stronger predictive performance compared with BALD across a range of datasets and models, and thus provides an appealing drop-in replacement.},
	language = {en},
	urldate = {2024-04-04},
	booktitle = {Proceedings of {The} 26th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Smith, Freddie Bickford and Kirsch, Andreas and Farquhar, Sebastian and Gal, Yarin and Foster, Adam and Rainforth, Tom},
	month = apr,
	year = {2023},
	pages = {7331--7348},
}

@misc{hauth_uncertainty_2024,
	title = {Uncertainty {Quantification} of {Graph} {Convolution} {Neural} {Network} {Models} of {Evolving} {Processes}},
	url = {http://arxiv.org/abs/2402.11179},
	doi = {10.48550/arXiv.2402.11179},
	abstract = {The application of neural network models to scientific machine learning tasks has proliferated in recent years. In particular, neural network models have proved to be adept at modeling processes with spatial-temporal complexity. Nevertheless, these highly parameterized models have garnered skepticism in their ability to produce outputs with quantified error bounds over the regimes of interest. Hence there is a need to find uncertainty quantification methods that are suitable for neural networks. In this work we present comparisons of the parametric uncertainty quantification of neural networks modeling complex spatial-temporal processes with Hamiltonian Monte Carlo and Stein variational gradient descent and its projected variant. Specifically we apply these methods to graph convolutional neural network models of evolving systems modeled with recurrent neural network and neural ordinary differential equations architectures. We show that Stein variational inference is a viable alternative to Monte Carlo methods with some clear advantages for complex neural network models. For our exemplars, Stein variational interference gave similar uncertainty profiles through time compared to Hamiltonian Monte Carlo, albeit with generally more generous variance.Projected Stein variational gradient descent also produced similar uncertainty profiles to the non-projected counterpart, but large reductions in the active weight space were confounded by the stability of the neural network predictions and the convoluted likelihood landscape.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Hauth, Jeremiah and Safta, Cosmin and Huan, Xun and Patel, Ravi G. and Jones, Reese E.},
	month = feb,
	year = {2024},
	note = {arXiv:2402.11179 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Physics - Computational Physics},
}

@misc{hauth_uncertainty_2024-1,
	title = {Uncertainty {Quantification} of {Graph} {Convolution} {Neural} {Network} {Models} of {Evolving} {Processes}},
	url = {http://arxiv.org/abs/2402.11179},
	abstract = {The application of neural network models to scientific machine learning tasks has proliferated in recent years. In particular, neural network models have proved to be adept at modeling processes with spatial-temporal complexity. Nevertheless, these highly parameterized models have garnered skepticism in their ability to produce outputs with quantified error bounds over the regimes of interest. Hence there is a need to find uncertainty quantification methods that are suitable for neural networks. In this work we present comparisons of the parametric uncertainty quantification of neural networks modeling complex spatial-temporal processes with Hamiltonian Monte Carlo and Stein variational gradient descent and its projected variant. Specifically we apply these methods to graph convolutional neural network models of evolving systems modeled with recurrent neural network and neural ordinary differential equations architectures. We show that Stein variational inference is a viable alternative to Monte Carlo methods with some clear advantages for complex neural network models. For our exemplars, Stein variational interference gave similar uncertainty profiles through time compared to Hamiltonian Monte Carlo, albeit with generally more generous variance.Projected Stein variational gradient descent also produced similar uncertainty profiles to the non-projected counterpart, but large reductions in the active weight space were confounded by the stability of the neural network predictions and the convoluted likelihood landscape.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Hauth, Jeremiah and Safta, Cosmin and Huan, Xun and Patel, Ravi G. and Jones, Reese E.},
	month = feb,
	year = {2024},
	note = {arXiv:2402.11179 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Physics - Computational Physics},
}

@article{strutz_variational_2024,
	title = {Variational {Bayesian} experimental design for geophysical applications: seismic source location, amplitude versus offset inversion, and estimating {CO2} saturations in a subsurface reservoir},
	volume = {236},
	issn = {0956-540X},
	shorttitle = {Variational {Bayesian} experimental design for geophysical applications},
	url = {https://doi.org/10.1093/gji/ggad492},
	doi = {10.1093/gji/ggad492},
	abstract = {In geophysical surveys or experiments, recorded data are used to constrain properties of the planetary subsurface, oceans, atmosphere or cryosphere. How the experimental data are collected significantly influences which parameters can be resolved and how much confidence can be placed in the results. Bayesian experimental design methods characterize, quantify and maximize expected information post-experiment—an optimization problem. Typical design parameters that can be optimized are source and/or sensor types and locations, and the choice of modelling or data processing methods to be applied to the data. These may all be optimized subject to various physical and cost constraints. This paper introduces variational design methods, and discusses their benefits and limitations in the context of geophysical applications. Variational methods have recently come to prominence due to their importance in machine-learning applications. They can be used to design experiments that best resolve either all model parameters, or the answer to specific questions about the system to be interrogated. The methods are tested in three schematic geophysical applications: (i) estimating a source location given arrival times of radiating energy at sensor locations, (ii) estimating the contrast in seismic velocity across a stratal interface given measurements of the amplitudes of seismic wavefield reflections from that interface, and (iii) designing a survey to best constrain CO2 saturation in a subsurface storage scenario. Variational methods allow the value of expected information from an experiment to be calculated and optimized simultaneously, which results in substantial savings in computational cost. In the context of designing a survey to best constrain CO2 saturation in a subsurface storage scenario, we show that optimal designs may change substantially depending on the particular questions of interest. We also show that one method, so-called DN design, can be effective at substantially lower computational cost than other methods. Overall, this work demonstrates that optimal design methods could be used more widely in Geophysics, as they are in other scientifically advanced fields.},
	number = {3},
	urldate = {2024-04-04},
	journal = {Geophysical Journal International},
	author = {Strutz, Dominik and Curtis, Andrew},
	month = mar,
	year = {2024},
	pages = {1309--1331},
}

@article{alexanderian_optimal_2021,
	title = {Optimal experimental design for infinite-dimensional {Bayesian} inverse problems governed by {PDEs}: a review},
	volume = {37},
	issn = {0266-5611},
	shorttitle = {Optimal experimental design for infinite-dimensional {Bayesian} inverse problems governed by {PDEs}},
	url = {https://dx.doi.org/10.1088/1361-6420/abe10c},
	doi = {10.1088/1361-6420/abe10c},
	abstract = {We present a review of methods for optimal experimental design (OED) for Bayesian inverse problems governed by partial differential equations with infinite-dimensional parameters. The focus is on problems where one seeks to optimize the placement of measurement points, at which data are collected, such that the uncertainty in the estimated parameters is minimized. We present the mathematical foundations of OED in this context and survey the computational methods for the class of OED problems under study. We also outline some directions for future research in this area.},
	language = {en},
	number = {4},
	urldate = {2024-04-04},
	journal = {Inverse Problems},
	author = {Alexanderian, Alen},
	month = mar,
	year = {2021},
	pages = {043001},
}

@misc{zhong_goal-oriented_2024,
	title = {Goal-{Oriented} {Bayesian} {Optimal} {Experimental} {Design} for {Nonlinear} {Models} using {Markov} {Chain} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/2403.18072},
	doi = {10.48550/arXiv.2403.18072},
	abstract = {Optimal experimental design (OED) provides a systematic approach to quantify and maximize the value of experimental data. Under a Bayesian approach, conventional OED maximizes the expected information gain (EIG) on model parameters. However, we are often interested in not the parameters themselves, but predictive quantities of interest (QoIs) that depend on the parameters in a nonlinear manner. We present a computational framework of predictive goal-oriented OED (GO-OED) suitable for nonlinear observation and prediction models, which seeks the experimental design providing the greatest EIG on the QoIs. In particular, we propose a nested Monte Carlo estimator for the QoI EIG, featuring Markov chain Monte Carlo for posterior sampling and kernel density estimation for evaluating the posterior-predictive density and its Kullback-Leibler divergence from the prior-predictive. The GO-OED design is then found by maximizing the EIG over the design space using Bayesian optimization. We demonstrate the effectiveness of the overall nonlinear GO-OED method, and illustrate its differences versus conventional non-GO-OED, through various test problems and an application of sensor placement for source inversion in a convection-diffusion field.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Zhong, Shijie and Shen, Wanggang and Catanach, Tommie and Huan, Xun},
	month = mar,
	year = {2024},
	note = {arXiv:2403.18072 [cs, stat]},
	keywords = {62K05, 62F15, 62B15, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
}

@article{howard_interplanetary_2009,
	title = {Interplanetary {Coronal} {Mass} {Ejections} {Observed} in the {Heliosphere}: 1. {Review} of {Theory}},
	volume = {147},
	issn = {1572-9672},
	url = {https://doi.org/10.1007/s11214-009-9542-5},
	doi = {10.1007/s11214-009-9542-5},
	abstract = {With the recent advancements in interplanetary coronal mass ejection (ICME) imaging it is necessary to understand how heliospheric images may be interpreted, particularly at large elongation angles. Of crucial importance is how the current methods used for coronal mass ejection measurement in coronagraph images must be changed to account for the large elongations involved in the heliosphere. In this review of theory we build up a picture of ICME appearance and evolution at large elongations in terms of how it would appear to an observer near 1 AU from the Sun. We begin by revisiting the basics of Thomson scattering describing how ICMEs are detected, in this we attempt to clarify a number of common misconceptions. We then build up from a single electron to an integrated line of sight, consider the ICME as a collection of lines of sight and describe how a map of ICME appearance may be developed based on its appearance relative to each line of sight. Finally, we discuss how the topology of the ICME affects its observed geometry and kinematic properties, particularly at large elongations. This review is the first of a three-part series of papers, where a review of theory is presented here and a model is developed and used in subsequent papers.},
	number = {1},
	journal = {Space Science Reviews},
	author = {Howard, Timothy A. and Tappin, S. James},
	month = oct,
	year = {2009},
	pages = {31--54},
}

@misc{leger_parametrization_2023,
	title = {Parametrization {Cookbook}: {A} set of {Bijective} {Parametrizations} for using {Machine} {Learning} methods in {Statistical} {Inference}},
	shorttitle = {Parametrization {Cookbook}},
	url = {http://arxiv.org/abs/2301.08297},
	doi = {10.48550/arXiv.2301.08297},
	abstract = {We present in this paper a way to transform a constrained statistical inference problem into an unconstrained one in order to be able to use modern computational methods, such as those based on automatic differentiation, GPU computing, stochastic gradients with mini-batch. Unlike the parametrizations classically used in Machine Learning, the parametrizations introduced here are all bijective and are even diffeomorphisms, thus allowing to keep the important properties from a statistical inference point of view, first of all identifiability. This cookbook presents a set of recipes to use to transform a constrained problem into a unconstrained one. For an easy use of parametrizations, this paper is at the same time a cookbook, and a Python package allowing the use of parametrizations with numpy, but also JAX and PyTorch, as well as a high level and expressive interface allowing to easily describe a parametrization to transform a difficult problem of statistical inference into an easier problem addressable with modern optimization tools.},
	urldate = {2024-03-26},
	publisher = {arXiv},
	author = {Leger, Jean-Benoist},
	month = jan,
	year = {2023},
	note = {arXiv:2301.08297 [stat]},
	keywords = {Statistics - Computation, Statistics - Machine Learning},
}

@article{alam_winograd_2022,
	title = {Winograd {Convolution} for {Deep} {Neural} {Networks}: {Efficient} {Point} {Selection}},
	volume = {21},
	issn = {1539-9087},
	shorttitle = {Winograd {Convolution} for {Deep} {Neural} {Networks}},
	url = {https://dl.acm.org/doi/10.1145/3524069},
	doi = {10.1145/3524069},
	abstract = {Convolutional neural networks (CNNs) have dramatically improved the accuracy of image, video, and audio processing for tasks such as object recognition, image segmentation, and interactive speech systems. CNNs require large amounts of computing resources for both training and inference, primarily because the convolution layers are computationally intensive. Fast convolution algorithms such as Winograd convolution can greatly reduce the computational cost of these layers. However, Winograd convolution has poor numeric properties, such that greater savings in computation cause exponentially increasing floating point errors. A defining feature of each Winograd convolution algorithm is a set of real-value points where polynomials are sampled. The choice of points impacts the numeric accuracy of the algorithm, but the optimal set of points for small convolutions remains unknown. Existing work considers only small integers and simple fractions as candidate points. In this work, we propose a novel approach to point selection using points of the form \{−1𝑐,−𝑐,𝑐,1𝑐\}\{−1c,−c,c,1c\}{\textbackslash}lbrace -{\textbackslash}frac\{1\}\{c\},-c,c,{\textbackslash}frac\{1\}\{c\}{\textbackslash}rbrace using the full range of real-valued numbers for c. We show that groups of this form cause cancellations in the Winograd transform matrices that reduce numeric error. We find empirically that the error for different values of c forms a rough curve across the range of real-value numbers. It is therefore possible to localize the values of c that lead to lower error. We show that it is not necessary to choose integers or simple fractions as evaluation points, and that lower errors can be achieved with non-obvious real-valued points. We study a range of sizes for small convolutions and achieve reduction in error ranging from 2\% to around 59\% for both 1D and 2D convolution, when compared to state of the art. Furthermore, we identify patterns in cases when we select a subset of our proposed points that will always lead to a lower error. Finally, we implement a complete Winograd convolution layer and use it to run state-of-the-art deep convolution neural networks on real datasets and show that our proposed points achieve reduction in error, ranging from 22\% to 63\%, while also showing how an increased Winograd output size can result in execution speed-up for some cases.},
	number = {6},
	urldate = {2024-03-23},
	journal = {ACM Transactions on Embedded Computing Systems},
	author = {Alam, Syed Asad and Anderson, Andrew and Barabasz, Barbara and Gregg, David},
	month = dec,
	year = {2022},
	keywords = {Toom-Cook algorithm, Winograd convolution, deep neural networks},
	pages = {80:1--80:28},
}

@misc{higham_diffusion_2023,
	title = {Diffusion {Models} for {Generative} {Artificial} {Intelligence}: {An} {Introduction} for {Applied} {Mathematicians}},
	shorttitle = {Diffusion {Models} for {Generative} {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2312.14977},
	doi = {10.48550/arXiv.2312.14977},
	abstract = {Generative artificial intelligence (AI) refers to algorithms that create synthetic but realistic output. Diffusion models currently offer state of the art performance in generative AI for images. They also form a key component in more general tools, including text-to-image generators and large language models. Diffusion models work by adding noise to the available training data and then learning how to reverse the process. The reverse operation may then be applied to new random data in order to produce new outputs. We provide a brief introduction to diffusion models for applied mathematicians and statisticians. Our key aims are (a) to present illustrative computational examples, (b) to give a careful derivation of the underlying mathematical formulas involved, and (c) to draw a connection with partial differential equation (PDE) diffusion models. We provide code for the computational experiments. We hope that this topic will be of interest to advanced undergraduate students and postgraduate students. Portions of the material may also provide useful motivational examples for those who teach courses in stochastic processes, inference, machine learning, PDEs or scientific computing.},
	urldate = {2024-02-24},
	publisher = {arXiv},
	author = {Higham, Catherine F. and Higham, Desmond J. and Grindrod, Peter},
	month = dec,
	year = {2023},
	note = {arXiv:2312.14977 [cs]},
	keywords = {68T07, 60J60, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2, I.2.6},
}

@misc{garnelo_neural_2018,
	title = {Neural {Processes}},
	url = {http://arxiv.org/abs/1807.01622},
	doi = {10.48550/arXiv.1807.01622},
	abstract = {A neural network (NN) is a parameterised function that can be tuned via gradient descent to approximate a labelled collection of data with high precision. A Gaussian process (GP), on the other hand, is a probabilistic model that defines a distribution over possible functions, and is updated in light of data via the rules of probabilistic inference. GPs are probabilistic, data-efficient and flexible, however they are also computationally intensive and thus limited in their applicability. We introduce a class of neural latent variable models which we call Neural Processes (NPs), combining the best of both worlds. Like GPs, NPs define distributions over functions, are capable of rapid adaptation to new observations, and can estimate the uncertainty in their predictions. Like NNs, NPs are computationally efficient during training and evaluation but also learn to adapt their priors to data. We demonstrate the performance of NPs on a range of learning tasks, including regression and optimisation, and compare and contrast with related models in the literature.},
	urldate = {2024-02-15},
	publisher = {arXiv},
	author = {Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J. and Eslami, S. M. Ali and Teh, Yee Whye},
	month = jul,
	year = {2018},
	note = {arXiv:1807.01622 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{shchur_neural_2021,
	title = {Neural {Temporal} {Point} {Processes}: {A} {Review}},
	shorttitle = {Neural {Temporal} {Point} {Processes}},
	url = {http://arxiv.org/abs/2104.03528},
	doi = {10.48550/arXiv.2104.03528},
	abstract = {Temporal point processes (TPP) are probabilistic generative models for continuous-time event sequences. Neural TPPs combine the fundamental ideas from point process literature with deep learning approaches, thus enabling construction of flexible and efficient models. The topic of neural TPPs has attracted significant attention in the recent years, leading to the development of numerous new architectures and applications for this class of models. In this review paper we aim to consolidate the existing body of knowledge on neural TPPs. Specifically, we focus on important design choices and general principles for defining neural TPP models. Next, we provide an overview of application areas commonly considered in the literature. We conclude this survey with the list of open challenges and important directions for future work in the field of neural TPPs.},
	urldate = {2024-02-15},
	publisher = {arXiv},
	author = {Shchur, Oleksandr and Türkmen, Ali Caner and Januschowski, Tim and Günnemann, Stephan},
	month = aug,
	year = {2021},
	note = {arXiv:2104.03528 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{westny_stability-informed_2023,
	title = {Stability-{Informed} {Initialization} of {Neural} {Ordinary} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2311.15890},
	doi = {10.48550/arXiv.2311.15890},
	abstract = {This paper addresses the training of Neural Ordinary Differential Equations (neural ODEs), and in particular explores the interplay between numerical integration techniques, stability regions, step size, and initialization techniques. It is shown how the choice of integration technique implicitly regularizes the learned model, and how the solver's corresponding stability region affects training and prediction performance. From this analysis, a stability-informed parameter initialization technique is introduced. The effectiveness of the initialization method is displayed across several learning benchmarks and industrial applications.},
	urldate = {2024-02-06},
	publisher = {arXiv},
	author = {Westny, Theodor and Mohammadi, Arman and Jung, Daniel and Frisk, Erik},
	month = dec,
	year = {2023},
	note = {arXiv:2311.15890 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{gao_generalized_2023,
	title = {Generalized {Bayesian} {Inference} for {Scientific} {Simulators} via {Amortized} {Cost} {Estimation}},
	url = {http://arxiv.org/abs/2305.15208},
	doi = {10.48550/arXiv.2305.15208},
	abstract = {Simulation-based inference (SBI) enables amortized Bayesian inference for simulators with implicit likelihoods. But when we are primarily interested in the quality of predictive simulations, or when the model cannot exactly reproduce the observed data (i.e., is misspecified), targeting the Bayesian posterior may be overly restrictive. Generalized Bayesian Inference (GBI) aims to robustify inference for (misspecified) simulator models, replacing the likelihood-function with a cost function that evaluates the goodness of parameters relative to data. However, GBI methods generally require running multiple simulations to estimate the cost function at each parameter value during inference, making the approach computationally infeasible for even moderately complex simulators. Here, we propose amortized cost estimation (ACE) for GBI to address this challenge: We train a neural network to approximate the cost function, which we define as the expected distance between simulations produced by a parameter and observed data. The trained network can then be used with MCMC to infer GBI posteriors for any observation without running additional simulations. We show that, on several benchmark tasks, ACE accurately predicts cost and provides predictive simulations that are closer to synthetic observations than other SBI methods, especially for misspecified simulators. Finally, we apply ACE to infer parameters of the Hodgkin-Huxley model given real intracellular recordings from the Allen Cell Types Database. ACE identifies better data-matching parameters while being an order of magnitude more simulation-efficient than a standard SBI method. In summary, ACE combines the strengths of SBI methods and GBI to perform robust and simulation-amortized inference for scientific simulators.},
	urldate = {2024-01-31},
	publisher = {arXiv},
	author = {Gao, Richard and Deistler, Michael and Macke, Jakob H.},
	month = nov,
	year = {2023},
	note = {arXiv:2305.15208 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{churchill_flow_2023,
	title = {{FLOW} {MAP} {LEARNING} {FOR} {UNKNOWN} {DYNAMICAL} {SYSTEMS}: {OVERVIEW}, {IMPLEMENTATION}, {AND} {BENCHMARKS}},
	volume = {4},
	issn = {2689-3967, 2689-3975},
	shorttitle = {{FLOW} {MAP} {LEARNING} {FOR} {UNKNOWN} {DYNAMICAL} {SYSTEMS}},
	url = {https://www.dl.begellhouse.com/journals/558048804a15188a,498820861ef102d2,3ff0a7594fae0a96.html},
	doi = {10.1615/JMachLearnModelComput.2023049717},
	abstract = {Flow map learning (FML), in conjunction with deep neural networks (DNNs), has shown promise for data driven modeling of unknown dynamical systems. A remarkable ...},
	language = {English},
	number = {2},
	urldate = {2024-01-30},
	journal = {Journal of Machine Learning for Modeling and Computing},
	author = {Churchill, Victor and Xiu, Dongbin},
	year = {2023},
}

@article{roos_sensitivity_2015,
	title = {Sensitivity {Analysis} for {Bayesian} {Hierarchical} {Models}},
	volume = {10},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-10/issue-2/Sensitivity-Analysis-for-Bayesian-Hierarchical-Models/10.1214/14-BA909.full},
	doi = {10.1214/14-BA909},
	abstract = {Prior sensitivity examination plays an important role in applied Bayesian analyses. This is especially true for Bayesian hierarchical models, where interpretability of the parameters within deeper layers in the hierarchy becomes challenging. In addition, lack of information together with identifiability issues may imply that the prior distributions for such models have an undesired influence on the posterior inference. Despite its importance, informal approaches to prior sensitivity analysis are currently used. They require repetitive re-fits of the model with ad-hoc modified base prior parameter values. Other formal approaches to prior sensitivity analysis suffer from a lack of popularity in practice, mainly due to their high computational cost and absence of software implementation. We propose a novel formal approach to prior sensitivity analysis, which is fast and accurate. It quantifies sensitivity without the need for a model re-fit. Through a series of examples we show how our approach can be used to detect high prior sensitivities of some parameters as well as identifiability issues in possibly over-parametrized Bayesian hierarchical models.},
	number = {2},
	urldate = {2024-01-26},
	journal = {Bayesian Analysis},
	author = {Roos, Małgorzata and Martins, Thiago G. and Held, Leonhard and Rue, Håvard},
	month = jun,
	year = {2015},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {Base prior, Bayesian hierarchical models, Bayesian robustness, Calibration, Hellinger distance, Identifiability, formal local sensitivity measure, overparametrisation},
	pages = {321--349},
}

@article{thomas_likelihood-free_2022,
	title = {Likelihood-{Free} {Inference} by {Ratio} {Estimation}},
	volume = {17},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-17/issue-1/Likelihood-Free-Inference-by-Ratio-Estimation/10.1214/20-BA1238.full},
	doi = {10.1214/20-BA1238},
	abstract = {We consider the problem of parametric statistical inference when likelihood computations are prohibitively expensive but sampling from the model is possible. Several so-called likelihood-free methods have been developed to perform inference in the absence of a likelihood function. The popular synthetic likelihood approach infers the parameters by modelling summary statistics of the data by a Gaussian probability distribution. In another popular approach called approximate Bayesian computation, the inference is performed by identifying parameter values for which the summary statistics of the simulated data are close to those of the observed data. Synthetic likelihood is easier to use as no measure of “closeness” is required but the Gaussianity assumption is often limiting. Moreover, both approaches require judiciously chosen summary statistics. We here present an alternative inference approach that is as easy to use as synthetic likelihood but not as restricted in its assumptions, and that, in a natural way, enables automatic selection of relevant summary statistic from a large set of candidates. The basic idea is to frame the problem of estimating the posterior as a problem of estimating the ratio between the data generating distribution and the marginal distribution. This problem can be solved by logistic regression, and including regularising penalty terms enables automatic selection of the summary statistics relevant to the inference task. We illustrate the general theory on canonical examples and employ it to perform inference for challenging stochastic nonlinear dynamical systems and high-dimensional summary statistics.},
	number = {1},
	urldate = {2024-01-25},
	journal = {Bayesian Analysis},
	author = {Thomas, Owen and Dutta, Ritabrata and Corander, Jukka and Kaski, Samuel and Gutmann, Michael U.},
	month = mar,
	year = {2022},
	keywords = {Approximate Bayesian Computation, density-ratio estimation, likelihood-free inference, logistic regression, probabilistic classification, stochastic dynamical systems, summary statistics selection, synthetic likelihood},
	pages = {1--31},
}

@article{papapicco_neural_2022,
	title = {The {Neural} {Network} shifted-proper orthogonal decomposition: {A} machine learning approach for non-linear reduction of hyperbolic equations},
	volume = {392},
	issn = {0045-7825},
	shorttitle = {The {Neural} {Network} shifted-proper orthogonal decomposition},
	url = {https://www.sciencedirect.com/science/article/pii/S004578252200069X},
	doi = {10.1016/j.cma.2022.114687},
	abstract = {Models with dominant advection always posed a difficult challenge for projection-based reduced order modelling. Many methodologies that have recently been proposed are based on the pre-processing of the full-order solutions to accelerate the Kolmogorov N−width decay thereby obtaining smaller linear subspaces with improved accuracy. These methods however must rely on the knowledge of the characteristic speeds in phase space of the solution, limiting their range of applicability to problems with explicit functional form for the advection field. In this work we approach the problem of automatically detecting the correct pre-processing transformation in a statistical learning framework by implementing a deep-learning architecture. The purely data-driven method allowed us to generalise the existing approaches of linear subspace manipulation to non-linear hyperbolic problems with unknown advection fields. The proposed algorithm has been validated against simple test cases to benchmark its performances and later successfully applied to a multiphase simulation.},
	urldate = {2023-12-25},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Papapicco, Davide and Demo, Nicola and Girfoglio, Michele and Stabile, Giovanni and Rozza, Gianluigi},
	month = mar,
	year = {2022},
	keywords = {Deep Neural Networks (DNNs), Multiphase simulation, Non-linear hyperbolic equations, Reduced order modelling (ROM), Shifted-POD (sPOD)},
	pages = {114687},
}

@misc{hess_data-driven_2022,
	title = {Data-{Driven} {Enhanced} {Model} {Reduction} for {Bifurcating} {Models} in {Computational} {Fluid} {Dynamics}},
	url = {http://arxiv.org/abs/2202.09250},
	doi = {10.48550/arXiv.2202.09250},
	abstract = {We investigate various data-driven methods to enhance projection-based model reduction techniques with the aim of capturing bifurcating solutions. To show the effectiveness of the data-driven enhancements, we focus on the incompressible Navier-Stokes equations and different types of bifurcations. To recover solutions past a Hopf bifurcation, we propose an approach that combines proper orthogonal decomposition with Hankel dynamic mode decomposition. To approximate solutions close to a pitchfork bifurcation, we combine localized reduced models with artificial neural networks. Several numerical examples are shown to demonstrate the feasibility of the presented approaches.},
	urldate = {2023-12-25},
	publisher = {arXiv},
	author = {Hess, Martin W. and Quaini, Annalisa and Rozza, Gianluigi},
	month = jul,
	year = {2022},
	note = {arXiv:2202.09250 [cs, math]},
	keywords = {Mathematics - Numerical Analysis},
}

@article{gilpin_model_2023,
	title = {Model scale versus domain knowledge in statistical forecasting of chaotic systems},
	volume = {5},
	url = {https://link.aps.org/doi/10.1103/PhysRevResearch.5.043252},
	doi = {10.1103/PhysRevResearch.5.043252},
	abstract = {Chaos and unpredictability are traditionally synonymous, yet large-scale machine-learning methods recently have demonstrated a surprising ability to forecast chaotic systems well beyond typical predictability horizons. However, recent works disagree on whether specialized methods grounded in dynamical systems theory, such as reservoir computers or neural ordinary differential equations, outperform general-purpose large-scale learning methods such as transformers or recurrent neural networks. These prior studies perform comparisons on few individually chosen chaotic systems, thereby precluding robust quantification of how statistical modeling choices and dynamical invariants of different chaotic systems jointly determine empirical predictability. Here, we perform the largest to-date comparative study of forecasting methods on the classical problem of forecasting chaos: we benchmark 24 state-of-the-art forecasting methods on a crowdsourced database of 135 low-dimensional systems with 17 forecast metrics. We find that large-scale, domain-agnostic forecasting methods consistently produce predictions that remain accurate up to two dozen Lyapunov times, thereby accessing a long-horizon forecasting regime well beyond classical methods. We find that, in this regime, accuracy decorrelates with classical invariant measures of predictability like the Lyapunov exponent. However, in data-limited settings outside the long-horizon regime, we find that physics-based hybrid methods retain a comparative advantage due to their strong inductive biases.},
	number = {4},
	urldate = {2023-12-22},
	journal = {Physical Review Research},
	author = {Gilpin, William},
	month = dec,
	year = {2023},
	pages = {043252},
}

@misc{salih_burgers_2016,
	title = {Burgers' {Equation}},
	url = {https://www.iist.ac.in/sites/default/files/people/IN08026/Burgers_equation_viscous.pdf},
	urldate = {2023-12-18},
	publisher = {Department of Aerospace Engineering, Indian Institute of Space Science and Technology},
	author = {Salih, A.},
	month = feb,
	year = {2016},
}

@article{gao_analytical_2017,
	title = {An analytical solution for two and three dimensional nonlinear {Burgers}' equation},
	volume = {45},
	issn = {0307-904X},
	url = {https://www.sciencedirect.com/science/article/pii/S0307904X16306710},
	doi = {10.1016/j.apm.2016.12.018},
	abstract = {This paper derives analytical solutions for the two dimensional and the three dimensional Burgers' equation. The two-dimensional and three-dimensional Burgers' equation are defined in a square and a cubic space domain, respectively, and a particular set of boundary and initial conditions is considered. The analytical solution for the two dimensional Burgers' equation is given by the quotient of two infinite series which involve Bessel, exponential, and trigonometric functions. The analytical solution for the three dimensional Burgers' equation is given by the quotient of two infinite series which involve hypergeometric, exponential, trigonometric and power functions. For both cases, the solutions can describe shock wave phenomena for large Reynolds numbers (Re ≥ 100), which is useful for testing numerical methods.},
	urldate = {2023-12-18},
	journal = {Applied Mathematical Modelling},
	author = {Gao, Q. and Zou, M. Y.},
	month = may,
	year = {2017},
	keywords = {Analytical solution, Burgers’ equation, Hopf–Cole transformation, Shock wave},
	pages = {255--270},
}

@article{lang_query_1992,
	title = {Query learning can work poorly when a human oracle is used},
	url = {https://www.academia.edu/6168656/Query_learning_can_work_poorly_when_a_human_oracle_is_used},
	abstract = {Query learning can work poorly when a human oracle is used},
	language = {en},
	urldate = {2023-12-15},
	author = {Lang, Kevin and Baum, Eric},
	month = jan,
	year = {1992},
}

@book{settles_active_2012,
	address = {Cham},
	series = {Synthesis {Lectures} on {Artificial} {Intelligence} and {Machine} {Learning}},
	title = {Active {Learning}},
	isbn = {9783031004322 9783031015601},
	url = {https://link.springer.com/10.1007/978-3-031-01560-1},
	language = {en},
	urldate = {2023-12-07},
	publisher = {Springer International Publishing},
	author = {Settles, Burr},
	year = {2012},
	doi = {10.1007/978-3-031-01560-1},
}

@book{noauthor_statistical_2009,
	edition = {1},
	title = {Statistical {Tolerance} {Regions}: {Theory}, {Applications}, and {Computation}},
	shorttitle = {Statistical {Tolerance} {Regions}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9780470473900},
	urldate = {2023-12-04},
	publisher = {John Wiley \& Sons, Ltd},
	year = {2009},
	doi = {10.1002/9780470473900},
}

@article{ngom_fourier_2021,
	title = {Fourier neural networks as function approximators and differential equation solvers},
	volume = {14},
	copyright = {© 2021 UChicago Argonne, LLC. Statistical Analysis and Data Mining published by Wiley Periodicals LLC.},
	issn = {1932-1872},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sam.11531},
	doi = {10.1002/sam.11531},
	abstract = {We present a Fourier neural network (FNN) that can be mapped directly to the Fourier decomposition. The choice of activation and loss function yields results that replicate a Fourier series expansion closely while preserving a straightforward architecture with a single hidden layer. The simplicity of this network architecture facilitates the integration with any other higher-complexity networks, at a data pre- or postprocessing stage. We validate this FNN on naturally periodic smooth functions and on piecewise continuous periodic functions. We showcase the use of this FNN for modeling or solving partial differential equations with periodic boundary conditions. The main advantages of the current approach are the validity of the solution outside the training region, interpretability of the trained model, and simplicity of use.},
	language = {en},
	number = {6},
	urldate = {2023-11-29},
	journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
	author = {Ngom, Marieme and Marin, Oana},
	year = {2021},
	keywords = {Fourier decomposition, differential equations, neural networks},
	pages = {647--661},
}

@article{bolibar_universal_2023,
	title = {Universal differential equations for glacier ice flow modelling},
	volume = {16},
	issn = {1991-959X},
	url = {https://gmd.copernicus.org/articles/16/6671/2023/},
	doi = {10.5194/gmd-16-6671-2023},
	abstract = {Geoscientific models are facing increasing challenges to exploit growing datasets coming from remote sensing. Universal differential equations (UDEs), aided by differentiable programming, provide a new scientific modelling paradigm enabling both complex functional inversions to potentially discover new physical laws and data assimilation from heterogeneous and sparse observations. We demonstrate an application of UDEs as a proof of concept to learn the creep component of ice flow, i.e. a nonlinear diffusivity differential equation, of a glacier evolution model. By combining a mechanistic model based on a two-dimensional shallow-ice approximation partial differential equation with an embedded neural network, i.e. a UDE, we can learn parts of an equation as nonlinear functions that then can be translated into mathematical expressions. We implemented this modelling framework as ODINN.jl, a package in the Julia programming language, providing high performance, source-to-source automatic differentiation (AD) and seamless integration with tools and global datasets from the Open Global Glacier Model in Python. We demonstrate this concept for 17 different glaciers around the world, for which we successfully recover a prescribed artificial law describing ice creep variability by solving ∼ 500 000 ordinary differential equations in parallel. Furthermore, we investigate which are the best tools in the scientific machine learning ecosystem in Julia to differentiate and optimize large nonlinear diffusivity UDEs. This study represents a proof of concept for a new modelling framework aiming at discovering empirical laws for large-scale glacier processes, such as the variability in ice creep and basal sliding for ice flow, and new hybrid surface mass balance models.},
	language = {English},
	number = {22},
	urldate = {2023-11-28},
	journal = {Geoscientific Model Development},
	author = {Bolibar, Jordi and Sapienza, Facundo and Maussion, Fabien and Lguensat, Redouane and Wouters, Bert and Pérez, Fernando},
	month = nov,
	year = {2023},
	pages = {6671--6687},
}

@incollection{macqueen_methods_1967,
	title = {Some methods for classification and analysis of multivariate observations},
	volume = {5.1},
	url = {https://projecteuclid.org/ebooks/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Fifth-Berkeley-Symposium-on-Mathematical-Statistics-and/chapter/Some-methods-for-classification-and-analysis-of-multivariate-observations/bsmsp/1200512992},
	urldate = {2023-11-27},
	booktitle = {Proceedings of the {Fifth} {Berkeley} {Symposium} on {Mathematical} {Statistics} and {Probability}, {Volume} 1: {Statistics}},
	publisher = {University of California Press},
	author = {MacQueen, J.},
	month = jan,
	year = {1967},
	pages = {281--298},
}

@article{bates_distribution-free_2021,
	title = {Distribution-free, {Risk}-controlling {Prediction} {Sets}},
	volume = {68},
	issn = {0004-5411},
	url = {https://dl.acm.org/doi/10.1145/3478535},
	doi = {10.1145/3478535},
	abstract = {While improving prediction accuracy has been the focus of machine learning in recent years, this alone does not suffice for reliable decision-making. Deploying learning systems in consequential settings also requires calibrating and communicating the uncertainty of predictions. To convey instance-wise uncertainty for prediction tasks, we show how to generate set-valued predictions from a black-box predictor that controls the expected loss on future test points at a user-specified level. Our approach provides explicit finite-sample guarantees for any dataset by using a holdout set to calibrate the size of the prediction sets. This framework enables simple, distribution-free, rigorous error control for many tasks, and we demonstrate it in five large-scale machine learning problems: (1) classification problems where some mistakes are more costly than others; (2) multi-label classification, where each observation has multiple associated labels; (3) classification problems where the labels have a hierarchical structure; (4) image segmentation, where we wish to predict a set of pixels containing an object of interest; and (5) protein structure prediction. Last, we discuss extensions to uncertainty quantification for ranking, metric learning, and distributionally robust learning.},
	number = {6},
	urldate = {2023-11-27},
	journal = {Journal of the ACM},
	author = {Bates, Stephen and Angelopoulos, Anastasios and Lei, Lihua and Malik, Jitendra and Jordan, Michael},
	month = sep,
	year = {2021},
	keywords = {Uncertainty quantification, conformal prediction, predictive uncertainty, set-valued prediction},
	pages = {43:1--43:34},
}

@misc{angelopoulos_image--image_2022,
	title = {Image-to-{Image} {Regression} with {Distribution}-{Free} {Uncertainty} {Quantification} and {Applications} in {Imaging}},
	url = {http://arxiv.org/abs/2202.05265},
	doi = {10.48550/arXiv.2202.05265},
	abstract = {Image-to-image regression is an important learning task, used frequently in biological imaging. Current algorithms, however, do not generally offer statistical guarantees that protect against a model's mistakes and hallucinations. To address this, we develop uncertainty quantification techniques with rigorous statistical guarantees for image-to-image regression problems. In particular, we show how to derive uncertainty intervals around each pixel that are guaranteed to contain the true value with a user-specified confidence probability. Our methods work in conjunction with any base machine learning model, such as a neural network, and endow it with formal mathematical guarantees -- regardless of the true unknown data distribution or choice of model. Furthermore, they are simple to implement and computationally inexpensive. We evaluate our procedure on three image-to-image regression tasks: quantitative phase microscopy, accelerated magnetic resonance imaging, and super-resolution transmission electron microscopy of a Drosophila melanogaster brain.},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Angelopoulos, Anastasios N. and Kohli, Amit P. and Bates, Stephen and Jordan, Michael I. and Malik, Jitendra and Alshaabi, Thayer and Upadhyayula, Srigokul and Romano, Yaniv},
	month = feb,
	year = {2022},
	note = {arXiv:2202.05265 [cs, eess, q-bio, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Quantitative Biology - Quantitative Methods, Statistics - Machine Learning},
}

@misc{takamoto_pdebench_2023,
	title = {{PDEBENCH}: {An} {Extensive} {Benchmark} for {Scientific} {Machine} {Learning}},
	shorttitle = {{PDEBENCH}},
	url = {http://arxiv.org/abs/2210.07182},
	doi = {10.48550/arXiv.2210.07182},
	abstract = {Machine learning-based modeling of physical systems has experienced increased interest in recent years. Despite some impressive progress, there is still a lack of benchmarks for Scientific ML that are easy to use but still challenging and representative of a wide range of problems. We introduce PDEBench, a benchmark suite of time-dependent simulation tasks based on Partial Differential Equations (PDEs). PDEBench comprises both code and data to benchmark the performance of novel machine learning models against both classical numerical simulations and machine learning baselines. Our proposed set of benchmark problems contribute the following unique features: (1) A much wider range of PDEs compared to existing benchmarks, ranging from relatively common examples to more realistic and difficult problems; (2) much larger ready-to-use datasets compared to prior work, comprising multiple simulation runs across a larger number of initial and boundary conditions and PDE parameters; (3) more extensible source codes with user-friendly APIs for data generation and baseline results with popular machine learning models (FNO, U-Net, PINN, Gradient-Based Inverse Method). PDEBench allows researchers to extend the benchmark freely for their own purposes using a standardized API and to compare the performance of new models to existing baseline methods. We also propose new evaluation metrics with the aim to provide a more holistic understanding of learning methods in the context of Scientific ML. With those metrics we identify tasks which are challenging for recent ML methods and propose these tasks as future challenges for the community. The code is available at https://github.com/pdebench/PDEBench.},
	urldate = {2023-11-23},
	publisher = {arXiv},
	author = {Takamoto, Makoto and Praditia, Timothy and Leiteritz, Raphael and MacKinlay, Dan and Alesiani, Francesco and Pflüger, Dirk and Niepert, Mathias},
	month = mar,
	year = {2023},
	note = {arXiv:2210.07182 [physics]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Physics - Fluid Dynamics, Physics - Geophysics},
}

@incollection{kolen_gradient_2009,
	title = {Gradient {Flow} in {Recurrent} {Nets}: {The} {Difficulty} of {Learning} {LongTerm} {Dependencies}},
	isbn = {9780470544037},
	shorttitle = {Gradient {Flow} in {Recurrent} {Nets}},
	url = {https://ieeexplore.ieee.org/document/5264952},
	language = {en},
	urldate = {2023-11-17},
	booktitle = {A {Field} {Guide} to {Dynamical} {Recurrent} {Networks}},
	publisher = {IEEE},
	collaborator = {Kolen, John F. and Kremer, Stefan C.},
	year = {2009},
	doi = {10.1109/9780470544037.ch14},
}

@misc{glusenkamp_unifying_2023,
	title = {Unifying supervised learning and {VAEs} -- coverage, systematics and goodness-of-fit in normalizing-flow based neural network models for astro-particle reconstructions},
	url = {http://arxiv.org/abs/2008.05825},
	doi = {10.48550/arXiv.2008.05825},
	abstract = {Neural-network based predictions of event properties in astro-particle physics are getting more and more common. However, in many cases the result is just utilized as a point prediction. Statistical uncertainties and coverage (1), systematic uncertainties (2) or a goodness-of-fit measure (3) are often not calculated. Here we describe a certain choice of training and network architecture that allows to incorporate all these properties into a single network model. We show that a KL-divergence objective of the joint distribution of data and labels allows to unify supervised learning and variational autoencoders (VAEs) under one umbrella of stochastic variational inference. The unification motivates an extended supervised learning scheme which allows to calculate a goodness-of-fit p-value for the neural network model. Conditional normalizing flows amortized with a neural network are crucial in this construction. We discuss how they allow to rigorously define coverage for posteriors defined jointly on a product space, e.g. \${\textbackslash}mathbb\{R\}{\textasciicircum}n {\textbackslash}times {\textbackslash}mathcal\{S\}{\textasciicircum}m\$, which encompasses posteriors over directions. Finally, systematic uncertainties are naturally included in the variational viewpoint. The proposed extended supervised training with amortized normalizing flows incorporates (1) coverage calculation, (2) systematics and (3) a goodness-of-fit measure in a single machine-learning model. There are no constraints on the shape of the involved distributions (e.g. Gaussianity) for these properties to hold, in fact it works with complex multi-modal distributions defined on product spaces like \${\textbackslash}mathbb\{R\}{\textasciicircum}n {\textbackslash}times {\textbackslash}mathcal\{S\}{\textasciicircum}m\$. We see great potential for exploiting this per-event information in event selections or for fast astronomical alerts which require uncertainty guarantees.},
	urldate = {2023-11-17},
	publisher = {arXiv},
	author = {Glüsenkamp, Thorsten},
	month = oct,
	year = {2023},
	note = {arXiv:2008.05825 [astro-ph, physics:hep-ex, stat]},
	keywords = {Astrophysics - High Energy Astrophysical Phenomena, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, High Energy Physics - Experiment, Statistics - Machine Learning},
}

@article{lam_learning_2023,
	title = {Learning skillful medium-range global weather forecasting},
	volume = {0},
	url = {https://www.science.org/doi/10.1126/science.adi2336},
	doi = {10.1126/science.adi2336},
	abstract = {Global medium-range weather forecasting is critical to decision-making across many social and economic domains. Traditional numerical weather prediction uses increased compute resources to improve forecast accuracy, but does not directly use historical weather data to improve the underlying model. Here, we introduce “GraphCast,” a machine learning-based method trained directly from reanalysis data. It predicts hundreds of weather variables, over 10 days at 0.25° resolution globally, in under one minute. GraphCast significantly outperforms the most accurate operational deterministic systems on 90\% of 1380 verification targets, and its forecasts support better severe event prediction, including tropical cyclones tracking, atmospheric rivers, and extreme temperatures. GraphCast is a key advance in accurate and efficient weather forecasting, and helps realize the promise of machine learning for modeling complex dynamical systems.},
	number = {0},
	urldate = {2023-11-16},
	journal = {Science},
	author = {Lam, Remi and Sanchez-Gonzalez, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Alet, Ferran and Ravuri, Suman and Ewalds, Timo and Eaton-Rosen, Zach and Hu, Weihua and Merose, Alexander and Hoyer, Stephan and Holland, George and Vinyals, Oriol and Stott, Jacklynn and Pritzel, Alexander and Mohamed, Shakir and Battaglia, Peter},
	month = nov,
	year = {2023},
	pages = {eadi2336},
}

@inproceedings{lakshminarayanan_simple_2017,
	title = {Simple and {Scalable} {Predictive} {Uncertainty} {Estimation} using {Deep} {Ensembles}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@inproceedings{pearce_high-quality_2018,
	title = {High-{Quality} {Prediction} {Intervals} for {Deep} {Learning}: {A} {Distribution}-{Free}, {Ensembled} {Approach}},
	shorttitle = {High-{Quality} {Prediction} {Intervals} for {Deep} {Learning}},
	url = {https://proceedings.mlr.press/v80/pearce18a.html},
	abstract = {This paper considers the generation of prediction intervals (PIs) by neural networks for quantifying uncertainty in regression tasks. It is axiomatic that high-quality PIs should be as narrow as possible, whilst capturing a specified portion of data. We derive a loss function directly from this axiom that requires no distributional assumption. We show how its form derives from a likelihood principle, that it can be used with gradient descent, and that model uncertainty is accounted for in ensembled form. Benchmark experiments show the method outperforms current state-of-the-art uncertainty quantification methods, reducing average PI width by over 10\%.},
	language = {en},
	urldate = {2023-11-15},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Pearce, Tim and Brintrup, Alexandra and Zaki, Mohamed and Neely, Andy},
	month = jul,
	year = {2018},
	pages = {4075--4084},
}

@misc{sohl-dickstein_deep_2015,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	url = {http://arxiv.org/abs/1503.03585},
	doi = {10.48550/arXiv.1503.03585},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
	month = nov,
	year = {2015},
	note = {arXiv:1503.03585 [cond-mat, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{Jia2019,
	title = {Neural jump stochastic differential equations},
	volume = {32},
	issn = {10495258},
	abstract = {Many time series are effectively generated by a combination of deterministic continuous flows along with discrete jumps sparked by stochastic events. However, we usually do not have the equation of motion describing the flows, or how they are affected by jumps. To this end, we introduce Neural Jump Stochastic Differential Equations that provide a data-driven approach to learn continuous and discrete dynamic behavior, i.e., hybrid systems that both flow and jump. Our approach extends the framework of Neural Ordinary Differential Equations with a stochastic process term that models discrete events. We then model temporal point processes with a piecewise-continuous latent trajectory, where the discontinuities are caused by stochastic events whose conditional intensity depends on the latent state. We demonstrate the predictive capabilities of our model on a range of synthetic and real-world marked point process datasets, including classical point processes (such as Hawkes processes), awards on Stack Overflow, medical records, and earthquake monitoring.},
	number = {2018},
	journal = {Advances in Neural Information Processing Systems},
	author = {Jia, Junteng and Benson, Austin R.},
	year = {2019},
	note = {arXiv: 1905.10403},
	keywords = {★},
}

@article{reinhart_review_2018,
	title = {A {Review} of {Self}-{Exciting} {Spatio}-{Temporal} {Point} {Processes} and {Their} {Applications}},
	volume = {33},
	issn = {0883-4237},
	url = {https://projecteuclid.org/journals/statistical-science/volume-33/issue-3/A-Review-of-Self-Exciting-Spatio-Temporal-Point-Processes-and/10.1214/17-STS629.full},
	doi = {10.1214/17-STS629},
	abstract = {Self-exciting spatio-temporal point process models predict the rate of events as a function of space, time, and the previous history of events. These models naturally capture triggering and clustering behavior, and have been widely used in fields where spatio-temporal clustering of events is observed, such as earthquake modeling, infectious disease, and crime. In the past several decades, advances have been made in estimation, inference, simulation, and diagnostic tools for self-exciting point process models. In this review, I describe the basic theory, survey related estimation and inference techniques from each field, highlight several key applications, and suggest directions for future research.},
	number = {3},
	urldate = {2023-09-08},
	journal = {https://doi.org/10.1214/17-STS629},
	author = {Reinhart, Alex},
	month = aug,
	year = {2018},
	note = {arXiv: 1708.02647
Publisher: Institute of Mathematical Statistics},
	keywords = {Conditional intensity, Hawkes process, epidemic-type aftershock sequence, stochastic declustering},
	pages = {299--318},
}

@article{Lee2021,
	title = {Parameterized neural ordinary differential equations: {Applications} to computational physics problems},
	volume = {477},
	issn = {14712946},
	doi = {10.1098/rspa.2021.0162},
	abstract = {This work proposes an extension of neural ordinary differential equations (NODEs) by introducing an additional set of ODE input parameters to NODEs. This extension allows NODEs to learn multiple dynamics specified by the input parameter instances. Our extension is inspired by the concept of parameterized ODEs, which are widely investigated in computational science and engineering contexts, where characteristics of the governing equations vary over the input parameters. We apply the proposed parameterized NODEs (PNODEs) for learning latent dynamics of complex dynamical processes that arise in computational physics, which is an essential component for enabling rapid numerical simulations for time-critical physics applications. For this, we propose an encoder-decoder-type framework, which models latent dynamics as PNODEs. We demonstrate the effectiveness of PNODEs on benchmark problems from computational physics.},
	number = {2253},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Lee, Kookjin and Parish, Eric J.},
	year = {2021},
	note = {arXiv: 2010.14685},
	keywords = {autoencoders, deep learning, latent-dynamics learning, model reduction, neural ordinary differential equations, nonlinear manifolds, ★},
}

@article{grathwohl_ffjord_2018,
	title = {{FFJORD}: {Free}-form {Continuous} {Dynamics} for {Scalable} {Reversible} {Generative} {Models}},
	url = {https://arxiv.org/abs/1810.01367v3},
	abstract = {A promising class of generative models maps points from a simple distribution
to a complex distribution through an invertible neural network.
Likelihood-based training of these models requires restricting their
architectures to allow cheap computation of Jacobian determinants.
Alternatively, the Jacobian trace can be used if the transformation is
specified by an ordinary differential equation. In this paper, we use
Hutchinson's trace estimator to give a scalable unbiased estimate of the
log-density. The result is a continuous-time invertible generative model with
unbiased density estimation and one-pass sampling, while allowing unrestricted
neural network architectures. We demonstrate our approach on high-dimensional
density estimation, image generation, and variational inference, achieving the
state-of-the-art among exact likelihood methods with efficient sampling.},
	urldate = {2023-08-11},
	journal = {7th International Conference on Learning Representations, ICLR 2019},
	author = {Grathwohl, Will and Chen, Ricky T.Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.01367
Publisher: International Conference on Learning Representations, ICLR},
}

@article{finlay_how_2020,
	title = {How to {Train} {Your} {Neural} {ODE}: the {World} of {Jacobian} and {Kinetic} {Regularization}},
	url = {https://dl.acm.org/doi/10.5555/3524938.3525234},
	doi = {10.5555/3524938.3525234},
	abstract = {Training neural ODEs on large datasets has not been tractable due to the necessity of allowing the adaptive numerical ODE solver to refine its step size to very small values. In practice this leads to dynamics equivalent to many hundreds or even thousands of layers. In this paper, we overcome this apparent difficulty by introducing a theoretically-grounded combination of both optimal transport and stability regularizations which encourage neural ODEs to prefer simpler dynamics out of all the dynamics that solve a problem well. Simpler dynamics lead to faster convergence and to fewer discretizations of the solver, considerably decreasing wall-clock time without loss in performance. Our approach allows us to train neural ODE-based generative models to the same performance as the unregularized dynamics, with significant reductions in training time. This brings neural ODEs closer to practical relevance in large-scale applications.},
	urldate = {2023-08-05},
	author = {Finlay, Chris and Jacobsen, Jörn-Henrik and Nurbekyan, Levon and Oberman, Adam M},
	year = {2020},
}

@article{scolini_observation-based_2019,
	title = {Observation-based modelling of magnetised coronal mass ejections with {EUHFORIA}},
	volume = {626},
	issn = {14320746},
	doi = {10.1051/0004-6361/201935053},
	abstract = {Context. Coronal mass ejections (CMEs) are the primary source of strong space weather disturbances at Earth. Their geo-effectiveness is largely determined by their dynamic pressure and internal magnetic fields, for which reliable predictions at Earth are not possible with traditional cone CME models. Aims. We study two well-observed Earth-directed CMEs using the EUropean Heliospheric FORecasting Information Asset (EUHFORIA) model, testing for the first time the predictive capabilities of a linear force-free spheromak CME model initialised using parameters derived from remote-sensing observations. Methods. Using observation-based CME input parameters, we performed magnetohydrodynamic simulations of the events with EUHFORIA, using the cone and spheromak CME models. Results. Simulations show that spheromak CMEs propagate faster than cone CMEs when initialised with the same kinematic parameters. We interpret these differences as the result of different Lorentz forces acting within cone and spheromak CMEs, which lead to different CME expansions in the heliosphere. Such discrepancies can be mitigated by initialising spheromak CMEs with a reduced speed corresponding to the radial speed only. Results at Earth provide evidence that the spheromak model improves the predictions of B (Bz) by up to 12-60 (22-40) percentage points compared to a cone model. Considering virtual spacecraft located within ±10° around Earth, B (Bz) predictions reach 45-70\% (58-78\%) of the observed peak values. The spheromak model shows inaccurate predictions of the magnetic field parameters at Earth for CMEs propagating away from the Sun-Earth line. Conclusions. The spheromak model successfully predicts the CME properties and arrival time in the case of strictly Earth-directed events, while modelling CMEs propagating away from the Sun-Earth line requires extra care due to limitations related to the assumed spherical shape. The spatial variability of modelling results and the typical uncertainties in the reconstructed CME direction advocate the need to consider predictions at Earth and at virtual spacecraft located around it.},
	urldate = {2023-06-27},
	journal = {Astronomy and Astrophysics},
	author = {Scolini, C. and Rodriguez, L. and Mierla, M. and Pomoell, J. and Poedts, S.},
	month = jun,
	year = {2019},
	note = {arXiv: 1904.07059
Publisher: EDP Sciences},
	keywords = {Magnetohydrodynamics (MHD), Solar wind, Solar-Terrestrial relations, Sun: coronal mass ejections (CMEs), Sun: heliosphere, Sun: magnetic fields},
}

@article{kidger_neural_2022,
	title = {On {Neural} {Differential} {Equations}},
	url = {https://arxiv.org/abs/2202.02435v1},
	abstract = {The conjoining of dynamical systems and deep learning has become a topic of
great interest. In particular, neural differential equations (NDEs) demonstrate
that neural networks and differential equation are two sides of the same coin.
Traditional parameterised differential equations are a special case. Many
popular neural network architectures, such as residual networks and recurrent
networks, are discretisations. NDEs are suitable for tackling generative problems, dynamical systems, and
time series (particularly in physics, finance, ...) and are thus of interest to
both modern machine learning and traditional mathematical modelling. NDEs offer
high-capacity function approximation, strong priors on model space, the ability
to handle irregular data, memory efficiency, and a wealth of available theory
on both sides. This doctoral thesis provides an in-depth survey of the field. Topics include: neural ordinary differential equations (e.g. for hybrid
neural/mechanistic modelling of physical systems); neural controlled
differential equations (e.g. for learning functions of irregular time series);
and neural stochastic differential equations (e.g. to produce generative models
capable of representing complex stochastic dynamics, or sampling from complex
high-dimensional distributions). Further topics include: numerical methods for NDEs (e.g. reversible
differential equations solvers, backpropagation through differential equations,
Brownian reconstruction); symbolic regression for dynamical systems (e.g. via
regularised evolution); and deep implicit models (e.g. deep equilibrium models,
differentiable optimisation). We anticipate this thesis will be of interest to anyone interested in the
marriage of deep learning with dynamical systems, and hope it will provide a
useful reference for the current state of the art.},
	urldate = {2023-06-25},
	author = {Kidger, Patrick},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.02435},
}

@article{gabriel_stpp_2013,
	title = {stpp: {An} {R} {Package} for {Plotting}, {Simulating} and {Analyzing} {Spatio}-{Temporal} {Point} {Patterns}},
	volume = {53},
	issn = {1548-7660},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v053i02},
	doi = {10.18637/JSS.V053.I02},
	abstract = {stpp is an R package for analyzing, simulating and displaying space-time point patterns. It covers many of the models encountered in applications of point process methods to the study of spatio-temporal phenomena. The package also includes estimators of the space-time inhomogeneous K-function and pair correlation function. stpp is the first dedicated unified computational environment in the area of spatio-temporal point processes. In this paper we describe space-time point processes and introduce the package stpp to new users.},
	number = {2},
	urldate = {2023-06-19},
	journal = {Journal of Statistical Software},
	author = {Gabriel, Edith and Rowlingson, Barry and Diggle, Peter J.},
	month = apr,
	year = {2013},
	note = {Publisher: American Statistical Association},
	keywords = {Epidemiology, Inhomogeneous point patterns, Space-time point processes, Spatial statistics},
	pages = {1--29},
}

@article{barnard_sir-huxtparticle_2023,
	title = {{SIR}-{HUXt}—{A} {Particle} {Filter} {Data} {Assimilation} {Scheme} for {CME} {Time}-{Elongation} {Profiles}},
	volume = {21},
	issn = {1542-7390},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1029/2023SW003487},
	doi = {10.1029/2023SW003487},
	abstract = {2 of 16 numerical models. The objective of DA is to combine information from simulations and observations to provide an optimal estimate of the state of a dynamical system. Heliospheric DA is still a relatively new research topic, but progress is beginning to be made. Lang et al. (2017) explored how the Local Ensemble Transform Kalman filter could be used to assimilate in situ observations of solar wind plasma properties into the ENLIL magnetohydrodynamic (MHD) solar wind model, which demonstrated clear improvements in the representivity of the ENLIL simulations. The Burger Radial Variational Data Assimilation (BRaVDA) scheme was developed in Lang and Owens (2019), in which a variational DA scheme was coupled to the hydrodynamic (HD) HUX solar wind model (Riley \& Lionello, 2011), for the assimilation of observations of the solar wind speed. Experiments with synthetic observations and solar wind speed observations from the STEREO spacecraft showed that BRaVDA reduced the errors in the solar wind speed predictions at Earth. This work was extended by Lang et al. (2021) to the HUXt model, a HD solar wind model with explicit time-dependence (Barnard \& Owens, 2022; M. J. Owens et al., 2020), in which it was shown that over the period 2007-2014, BRaVDA returned a 31\% reduction of the root mean square error in hindcasts of the solar wind speed at Earth. These works have so far focused on the assimilation of in situ observations of solar wind plasma properties, but progress has also been made on the assimilation of remote sensing observations, such as those provided by HIs (Eyles et al., 2008; Howard et al., 2008) and interplanetary scintillation (IPS) (Fallows et al., 2022). For example, Barnard et al. (2020) showed that an ensemble of solar-wind-CME simulations with the HUXt model could be weighted by the time-elongation profiles of CMEs derived from the STEREO HI data. This weighting prioritized ensemble members that more closely matched the observed time-elongation profile, and led to up to 20\% improvements in hindcasts of the CMEs arrival time at Earth. Similarly, Iwai et al. (2021) demonstrated how assimilating IPS observations of 12 halo CMEs into the SUSANOO-CME MHD model led to improvements in the predicted Earth arrival times of these CMEs. Although Barnard et al. (2020) demonstrated that HI data contains useful information on CMEs that can be used to constrain the HUXt solar wind simulations, they did not use formal DA methods. In this work, we present the development of SIR-HUXt, which couples a sequential importance resampling (SIR) particle filter DA scheme with the HUXt solar wind model. SIR-HUXt is constructed to assimilate time-elongation profiles of a CMEs flank, such as those typically extracted from the STEREO-HI data (Barnard et al., 2015, 2017; Davies et al., 2009). This is an important milestone toward the development of DA schemes that can directly assimilate the HI intensity data into solar wind numerical models. We present a first test of SIR-HUXt by using Observing System Simulation Experiments (OSSEs) to investigate the performance of SIR-HUXt for a simple synthetic CME scenario and for a range of observer locations relative to Earth. This article proceeds with Section 2 describing the models and methods we use, including the HUXt numerical model, the background to the SIR algorithm, and on OSSEs. Section 3 presents the results of the OSSEs, and our conclusions are presented in Section 4. 2. Methods and Data 2.1. HUXt HUXt is an open source numerical model of the solar wind, developed in Python (Barnard \& Owens, 2022; M. J. Owens et al., 2020). It is a 1D radial model that uses a reduced-physics approach to produce solar wind simulations that emulate the solar wind flows produced by 3-D MHD models, but at a small fraction of the computational cost. The motivation for developing HUXt is that the models simplicity and computational expense permits the development of certain experiments and techniques that would typically be too expensive with 3-D MHD models. For example, the particle filter DA experiments in this study require ≈10 6 5-day simulations of the inner heliosphere, which is currently an impractical demand of 3-D MHD solar wind models with widely available computing resources. Being based on incompressible hydrodynamics, HUXt solves only for the solar wind flow speed. Consequently, the only boundary condition required is the flow speed at the inner boundary. These boundary conditions can be computed from a wide range of coronal models, including but not limited to; potential field source surface based},
	number = {6},
	urldate = {2023-06-11},
	journal = {Space Weather},
	author = {Barnard, Luke and Owens, Mathew and Scott, Chris and Lang, Matthew and Lockwood, Mike},
	month = jun,
	year = {2023},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {CME forecasting, HUXt, data assimilation, heliospheric imaging},
	pages = {e2023SW003487},
}

@article{Barnard2023,
	title = {{SIR}-{HUXt}—{A} {Particle} {Filter} {Data} {Assimilation} {Scheme} for {CME} {Time}-{Elongation} {Profiles}},
	volume = {21},
	issn = {1542-7390},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1029/2023SW003487},
	doi = {10.1029/2023SW003487},
	abstract = {2 of 16 numerical models. The objective of DA is to combine information from simulations and observations to provide an optimal estimate of the state of a dynamical system. Heliospheric DA is still a relatively new research topic, but progress is beginning to be made. Lang et al. (2017) explored how the Local Ensemble Transform Kalman filter could be used to assimilate in situ observations of solar wind plasma properties into the ENLIL magnetohydrodynamic (MHD) solar wind model, which demonstrated clear improvements in the representivity of the ENLIL simulations. The Burger Radial Variational Data Assimilation (BRaVDA) scheme was developed in Lang and Owens (2019), in which a variational DA scheme was coupled to the hydrodynamic (HD) HUX solar wind model (Riley \& Lionello, 2011), for the assimilation of observations of the solar wind speed. Experiments with synthetic observations and solar wind speed observations from the STEREO spacecraft showed that BRaVDA reduced the errors in the solar wind speed predictions at Earth. This work was extended by Lang et al. (2021) to the HUXt model, a HD solar wind model with explicit time-dependence (Barnard \& Owens, 2022; M. J. Owens et al., 2020), in which it was shown that over the period 2007-2014, BRaVDA returned a 31\% reduction of the root mean square error in hindcasts of the solar wind speed at Earth. These works have so far focused on the assimilation of in situ observations of solar wind plasma properties, but progress has also been made on the assimilation of remote sensing observations, such as those provided by HIs (Eyles et al., 2008; Howard et al., 2008) and interplanetary scintillation (IPS) (Fallows et al., 2022). For example, Barnard et al. (2020) showed that an ensemble of solar-wind-CME simulations with the HUXt model could be weighted by the time-elongation profiles of CMEs derived from the STEREO HI data. This weighting prioritized ensemble members that more closely matched the observed time-elongation profile, and led to up to 20\% improvements in hindcasts of the CMEs arrival time at Earth. Similarly, Iwai et al. (2021) demonstrated how assimilating IPS observations of 12 halo CMEs into the SUSANOO-CME MHD model led to improvements in the predicted Earth arrival times of these CMEs. Although Barnard et al. (2020) demonstrated that HI data contains useful information on CMEs that can be used to constrain the HUXt solar wind simulations, they did not use formal DA methods. In this work, we present the development of SIR-HUXt, which couples a sequential importance resampling (SIR) particle filter DA scheme with the HUXt solar wind model. SIR-HUXt is constructed to assimilate time-elongation profiles of a CMEs flank, such as those typically extracted from the STEREO-HI data (Barnard et al., 2015, 2017; Davies et al., 2009). This is an important milestone toward the development of DA schemes that can directly assimilate the HI intensity data into solar wind numerical models. We present a first test of SIR-HUXt by using Observing System Simulation Experiments (OSSEs) to investigate the performance of SIR-HUXt for a simple synthetic CME scenario and for a range of observer locations relative to Earth. This article proceeds with Section 2 describing the models and methods we use, including the HUXt numerical model, the background to the SIR algorithm, and on OSSEs. Section 3 presents the results of the OSSEs, and our conclusions are presented in Section 4. 2. Methods and Data 2.1. HUXt HUXt is an open source numerical model of the solar wind, developed in Python (Barnard \& Owens, 2022; M. J. Owens et al., 2020). It is a 1D radial model that uses a reduced-physics approach to produce solar wind simulations that emulate the solar wind flows produced by 3-D MHD models, but at a small fraction of the computational cost. The motivation for developing HUXt is that the models simplicity and computational expense permits the development of certain experiments and techniques that would typically be too expensive with 3-D MHD models. For example, the particle filter DA experiments in this study require ≈10 6 5-day simulations of the inner heliosphere, which is currently an impractical demand of 3-D MHD solar wind models with widely available computing resources. Being based on incompressible hydrodynamics, HUXt solves only for the solar wind flow speed. Consequently, the only boundary condition required is the flow speed at the inner boundary. These boundary conditions can be computed from a wide range of coronal models, including but not limited to; potential field source surface based},
	number = {6},
	urldate = {2023-06-14},
	journal = {Space Weather},
	author = {Barnard, Luke and Owens, Mathew and Scott, Chris and Lang, Matthew and Lockwood, Mike},
	month = jun,
	year = {2023},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {CME forecasting, HUXt, data assimilation, heliospheric imaging},
	pages = {e2023SW003487},
}

@article{reinhart_review_2018-1,
	title = {A {Review} of {Self}-{Exciting} {Spatio}-{Temporal} {Point} {Processes} and {Their} {Applications}},
	volume = {33},
	issn = {0883-4237},
	url = {https://projecteuclid.org/journals/statistical-science/volume-33/issue-3/A-Review-of-Self-Exciting-Spatio-Temporal-Point-Processes-and/10.1214/17-STS629.full},
	doi = {10.1214/17-STS629},
	abstract = {Self-exciting spatio-temporal point process models predict the rate of events as a function of space, time, and the previous history of events. These models naturally capture triggering and clustering behavior, and have been widely used in fields where spatio-temporal clustering of events is observed, such as earthquake modeling, infectious disease, and crime. In the past several decades, advances have been made in estimation, inference, simulation, and diagnostic tools for self-exciting point process models. In this review, I describe the basic theory, survey related estimation and inference techniques from each field, highlight several key applications, and suggest directions for future research.},
	number = {3},
	urldate = {2023-06-15},
	journal = {https://doi.org/10.1214/17-STS629},
	author = {Reinhart, Alex},
	month = aug,
	year = {2018},
	note = {arXiv: 1708.02647
Publisher: Institute of Mathematical Statistics},
	keywords = {Conditional intensity, Hawkes process, epidemic-type aftershock sequence, stochastic declustering},
	pages = {299--318},
}

@article{Camporeale2021,
	title = {Accrue: {Accurate} and reliable uncertainty estimate in deterministic models},
	volume = {11},
	issn = {21525099},
	doi = {10.1615/Int.J.UncertaintyQuantification.2021034623},
	abstract = {In this paper we focus on the problem of assigning uncertainties to single-point predictions generated by a deterministic model that outputs a continuous variable. This problem applies to any state-of-the-art physics or engineering models that have a computational cost that does not readily allow running ensembles and estimating the uncertainty associated to single-point predictions. Essentially, we devise a method to easily transform a deterministic prediction into a probabilistic one. We show that for doing so, one has to compromise between the accuracy and the reliability (calibration) of such a probabilistic model. Hence, we introduce a cost function that encodes their trade-off, and we call this new method ACCRUE (ACCurate and Reliable Uncertainty Estimate). We use the continuous rank probability score to measure accuracy and we derive an analytic formula for the reliability, in the case of forecasts of continuous scalar variables expressed in terms of Gaussian distributions. The new ACCRUE cost function is then used to estimate the input-dependent variance, given a black-box “oracle” mean function, by solving a two-objective optimization problem. The simple philosophy behind this strategy is that predictions based on the estimated variances should not only be accurate, but also reliable (i.e., statistically consistent with observations). Conversely, early works based on the minimization of classical cost functions, such as the negative log probability density, cannot simultaneously enforce both accuracy and reliability. We show several examples both with synthetic data, where the underlying hidden noise can accurately be recovered, and with large real-world datasets.},
	number = {4},
	journal = {International Journal for Uncertainty Quantification},
	author = {Camporeale, Enrico and Carè, Algo},
	year = {2021},
	keywords = {Calibration, Deterministic models, Machine learning},
	pages = {81--94},
}

@article{sauer_active_2022,
	title = {Active {Learning} for {Deep} {Gaussian} {Process} {Surrogates}},
	volume = {65},
	issn = {15372723},
	url = {https://doi.org/10.1080/00401706.2021.2008505},
	doi = {10.1080/00401706.2021.2008505},
	abstract = {Deep Gaussian processes (DGPs) are increasingly popular as predictive models in machine learning for their nonstationary flexibility and ability to cope with abrupt regime changes in training data. Here, we explore DGPs as surrogates for computer simulation experiments whose response surfaces exhibit similar characteristics. In particular, we transport a DGP’s automatic warping of the input space and full uncertainty quantification, via a novel elliptical slice sampling Bayesian posterior inferential scheme, through to active learning strategies that distribute runs nonuniformly in the input space—something an ordinary (stationary) GP could not do. Building up the design sequentially in this way allows smaller training sets, limiting both expensive evaluation of the simulator code and mitigating cubic costs of DGP inference. When training data sizes are kept small through careful acquisition, and with parsimonious layout of latent layers, the framework can be both effective and computationally tractable. Our methods are illustrated on simulation data and two real computer experiments of varying input dimensionality. We provide an open source implementation in the deepgp package on CRAN.},
	number = {1},
	urldate = {2023-05-01},
	journal = {Technometrics},
	author = {Sauer, Annie and Gramacy, Robert B. and Higdon, David},
	year = {2022},
	note = {arXiv: 2012.08015
Publisher: American Statistical Association},
	keywords = {Computer model, Elliptical slice sampling, Emulator, Kriging, Sequential design},
	pages = {4--18},
}

@article{sun_improved_2021,
	title = {Improved and {Interpretable} {Solar} {Flare} {Predictions} {With} {Spatial} and {Topological} {Features} of the {Polarity} {Inversion} {Line} {Masked} {Magnetograms}},
	volume = {19},
	issn = {1542-7390},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1029/2021SW002837},
	doi = {10.1029/2021SW002837},
	abstract = {Many current research efforts undertake the solar flare classification task using the Space-weather HMI Active Region Patch (SHARP) parameters as the predictors. The SHARP parameters are scalar quantities based on spatial average or integration of physical quantities derived from the vector magnetic field, which loses information of the two-dimensional spatial distribution of the field and related quantities. In this paper, we construct two new sets of spatial features to expand the feature set used for the flare classification task. The first set uses the idea of topological data analysis to summarize the geometric information of the distributions of various SHARP quantities across active regions. The second set utilizes tools coming from spatial statistics to analyze the vertical magnetic field component Br and summarize its spatial variations and clustering patterns. All features are constructed within regions near the polarity inversion lines (PILs) and classification performances using the new features are compared against those using SHARP parameters (also along the PIL). We found that using the new features can improve the skill scores of the flare classification model and new features tend to have higher feature importance, especially the spatial statistics features. This potentially suggests that even using a single magnetic field component, Br, instead of all SHARP parameters, one can still derive strongly predictive features for flare classification.},
	number = {12},
	urldate = {2023-04-12},
	journal = {Space Weather},
	author = {Sun, Hu and Manchester, Ward and Chen, Yang},
	month = dec,
	year = {2021},
	note = {Publisher: John Wiley \& Sons, Ltd
ISBN: 10.1029/2021},
	pages = {e2021SW002837},
}

@article{Yuan2022,
	title = {Towards out of distribution generalization for problems in mechanics},
	volume = {400},
	issn = {00457825},
	url = {https://doi.org/10.1016/j.cma.2022.115569},
	doi = {10.1016/j.cma.2022.115569},
	abstract = {There has been a massive increase in research interest towards applying data driven methods to problems in mechanics, with a particular emphasis on using data driven methods for predictive modeling and design of materials with novel functionality. While traditional machine learning (ML) methods have enabled many breakthroughs, they rely on the assumption that the training (observed) data and testing (unseen) data are independent and identically distributed (i.i.d). However, when these standard ML approaches are applied to real world mechanics problems with unknown test environments, they can be very sensitive to data distribution shifts, and can break down when evaluated on test datasets that violate the i.i.d. assumption. In contrast, out-of-distribution (OOD) generalization approaches assume that the data contained in test environments are allowed to shift (i.e., violate the i.i.d. assumption). To date, multiple methods have been proposed to improve the OOD generalization of ML methods. However, most of these OOD generalization methods have been focused on classification problems, driven in part by the lack of benchmark datasets available for OOD regression problems. Thus, the efficiency of these OOD generalization methods on regression problems, which are typically more relevant to mechanics research than classification problems, is unknown. To address this, we perform a fundamental study of OOD generalization methods for regression problems in mechanics. Specifically, we identify three OOD generalization problems: covariate shift, mechanism shift, and sampling bias. For each problem, we create two benchmark examples that extend the Mechanical MNIST dataset collection, and we investigate the performance of popular OOD generalization methods on these mechanics-specific regression problems. Our numerical experiments show that in most cases, while the OOD algorithms perform better compared to traditional ML methods on these OOD generalization problems, there is a compelling need to develop more robust OOD methods that can generalize the notion of invariance across multiple OOD scenarios. Overall, we expect that this study, as well as the associated open access benchmark datasets, will enable further development of OOD methods for mechanics specific regression problems.},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Yuan, Lingxiao and Park, Harold S. and Lejeune, Emma},
	year = {2022},
	note = {arXiv: 2206.14917
Publisher: Elsevier B.V.},
	keywords = {Covariate shift, Machine Learning, Mechanism shift, Out of distribution, Regression, Sampling bias},
	pages = {115569},
}

@article{Vaswani2017,
	title = {Attention {Is} {All} {You} {Need}},
	volume = {2017-Decem},
	issn = {10495258},
	url = {https://arxiv.org/abs/1706.03762v5},
	doi = {10.48550/arxiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks in an encoder-decoder configuration. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer, based
solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to be
superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014
English-to-German translation task, improving over the existing best results,
including ensembles by over 2 BLEU. On the WMT 2014 English-to-French
translation task, our model establishes a new single-model state-of-the-art
BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction
of the training costs of the best models from the literature. We show that the
Transformer generalizes well to other tasks by applying it successfully to
English constituency parsing both with large and limited training data.},
	urldate = {2023-01-10},
	journal = {Advances in Neural Information Processing Systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.03762
Publisher: Neural information processing systems foundation},
	keywords = {★},
	pages = {5999--6009},
}

@book{pgm2009,
	title = {Probabilistic {Graphical} {Models}: {Principles} and {Techniques}},
	isbn = {978-0-262-01319-2},
	author = {Koller, Daphne and Friedman, Nir},
	year = {2009},
}

@article{Guo2022,
	title = {Bayesian operator inference for data-driven reduced-order modeling},
	volume = {402},
	issn = {00457825},
	url = {https://doi.org/10.1016/j.cma.2022.115336},
	doi = {10.1016/j.cma.2022.115336},
	abstract = {This work proposes a Bayesian inference method for the reduced-order modeling of time-dependent systems. Informed by the structure of the governing equations, the task of learning a reduced-order model from data is posed as a Bayesian inverse problem with Gaussian prior and likelihood. The resulting posterior distribution characterizes the operators defining the reduced-order model, hence the predictions subsequently issued by the reduced-order model are endowed with uncertainty. The statistical moments of these predictions are estimated via a Monte Carlo sampling of the posterior distribution. Since the reduced models are fast to solve, this sampling is computationally efficient. Furthermore, the proposed Bayesian framework provides a statistical interpretation of the regularization term that is present in the deterministic operator inference problem, and the empirical Bayes approach of maximum marginal likelihood suggests a selection algorithm for the regularization hyperparameters. The proposed method is demonstrated on two examples: the compressible Euler equations with noise-corrupted observations, and a single-injector combustion process.},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Guo, Mengwu and McQuarrie, Shane A. and Willcox, Karen E.},
	year = {2022},
	note = {arXiv: 2204.10829
Publisher: Elsevier B.V.},
	keywords = {Bayesian inversion, Data-driven reduced-order modeling, Operator inference, Single-injector combustion, Tikhonov regularization, Uncertainty quantification, ★},
	pages = {115336},
}

@article{Towne2018,
	title = {Spectral proper orthogonal decomposition and its relationship to dynamic mode decomposition and resolvent analysis},
	volume = {847},
	issn = {14697645},
	doi = {10.1017/jfm.2018.283},
	abstract = {We consider the frequency domain form of proper orthogonal decomposition (POD), called spectral proper orthogonal decomposition (SPOD). Spectral POD is derived from a space-time POD problem for statistically stationary flows and leads to modes that each oscillate at a single frequency. This form of POD goes back to the original work of Lumley (Stochastic Tools in Turbulence, Academic Press, 1970), but has been overshadowed by a space-only form of POD since the 1990s. We clarify the relationship between these two forms of POD and show that SPOD modes represent structures that evolve coherently in space and time, while space-only POD modes in general do not. We also establish a relationship between SPOD and dynamic mode decomposition (DMD); we show that SPOD modes are in fact optimally averaged DMD modes obtained from an ensemble DMD problem for stationary flows. Accordingly, SPOD modes represent structures that are dynamic in the same sense as DMD modes but also optimally account for the statistical variability of turbulent flows. Finally, we establish a connection between SPOD and resolvent analysis. The key observation is that the resolvent-mode expansion coefficients must be regarded as statistical quantities to ensure convergent approximations of the flow statistics. When the expansion coefficients are uncorrelated, we show that SPOD and resolvent modes are identical. Our theoretical results and the overall utility of SPOD are demonstrated using two example problems: the complex Ginzburg-Landau equation and a turbulent jet.},
	journal = {Journal of Fluid Mechanics},
	author = {Towne, Aaron and Schmidt, Oliver T. and Colonius, Tim},
	year = {2018},
	note = {arXiv: 1708.04393},
	keywords = {computational methods, low-dimensional models, turbulent flows, ★},
	pages = {821--867},
}

@book{Howard2013,
	title = {Coronal {Mass} {Ejections}: {An} {Introduction}},
	volume = {53},
	isbn = {978-85-7811-079-6},
	abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-α-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 Å for the interface backbone atoms) increased from 21\% with default Glide SP settings to 58\% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63\% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40\% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
	author = {Howard, Timothy A.},
	year = {2013},
	pmid = {25246403},
	note = {arXiv: 1011.1669v3
Publication Title: Journal of Chemical Information and Modeling
Issue: 9
ISSN: 1098-6596},
	keywords = {icle, ★},
}

@article{Love2020,
	title = {Analyzing {AIA} {Flare} {Observations} {Using} {Convolutional} {Neural} {Networks}},
	volume = {7},
	issn = {2296987X},
	doi = {10.3389/fspas.2020.00034},
	abstract = {In order to efficiently analyse the vast amount of data generated by solar space missions and ground-based instruments, modern machine learning techniques such as decision trees, support vector machines (SVMs) and neural networks can be very useful. In this paper we present initial results from using a convolutional neural network (CNN) to analyse observations from the Atmospheric Imaging Assembly (AIA) in the 1,600Å wavelength. The data is pre-processed to locate flaring regions where flare ribbons are visible in the observations. The CNN is created and trained to automatically analyse the shape and position of the flare ribbons, by identifying whether each image belongs into one of four classes: two-ribbon flare, compact/circular ribbon flare, limb flare, or quiet Sun, with the final class acting as a control for any data included in the training or test sets where flaring regions are not present. The network created can classify flare ribbon observations into any of the four classes with a final accuracy of 94\%. Initial results show that most of the images are correctly classified with the compact flare class being the only class where accuracy drops below 90\% and some observations are wrongly classified as belonging to the limb class.},
	number = {June},
	journal = {Frontiers in Astronomy and Space Sciences},
	author = {Love, Teri and Neukirch, Thomas and Parnell, Clare E.},
	year = {2020},
	keywords = {CNNs, classification, machine learning, ribbons, solar flares},
	pages = {1--8},
}

@article{Bobra2015,
	title = {Solar flare prediction using {SDO}/{HMI} vector magnetic field data with a machine-learning algorithm},
	volume = {798},
	issn = {15384357},
	doi = {10.1088/0004-637X/798/2/135},
	abstract = {We attempt to forecast M- and X-class solar flares using a machine-learning algorithm, called support vector machine (SVM), and four years of data from the Solar Dynamics Observatory's Helioseismic and Magnetic Imager, the first instrument to continuously map the full-disk photospheric vector magnetic field from space. Most flare forecasting efforts described in the literature use either line-of-sight magnetograms or a relatively small number of ground-based vector magnetograms. This is the first time a large data set of vector magnetograms has been used to forecast solar flares. We build a catalog of flaring and non-flaring active regions sampled from a database of 2071 active regions, comprised of 1.5 million active region patches of vector magnetic field data, and characterize each active region by 25 parameters. We then train and test the machine-learning algorithm and we estimate its performances using forecast verification metrics with an emphasis on the true skill statistic (TSS). We obtain relatively high TSS scores and overall predictive abilities. We surmise that this is partly due to fine-tuning the SVM for this purpose and also to an advantageous set of features that can only be calculated from vector magnetic field data. We also apply a feature selection algorithm to determine which of our 25 features are useful for discriminating between flaring and non-flaring active regions and conclude that only a handful are needed for good predictive abilities.},
	number = {2},
	journal = {Astrophysical Journal},
	author = {Bobra, M. G. and Couvidat, S.},
	year = {2015},
	note = {arXiv: 1411.1405},
	keywords = {Sun: activity, Sun: flares, ★},
}

@article{Kraaikamp2015,
	title = {Solar {Demon} - {An} approach to detecting flares, dimmings, and {EUV} waves on {SDO}/{AIA} images},
	volume = {5},
	issn = {21157251},
	doi = {10.1051/swsc/2015019},
	abstract = {Flares, dimmings, and extreme ultraviolet (EUV) waves are three types of eruptive phenomena on the Sun, which are main drivers of space weather. Fast and reliable detection of these phenomena helps augment space weather predictions. In the current paper, we introduce Solar Demon, the first software that detects all three phenomena, using a modular design to exploit synergies. While Solar Demon runs in near real-time on SDO/AIA synoptic quick-look images to provide fast detections of flares, dimmings, and EUV waves for space weather purposes, it also processes new Atmospheric Imaging Assembly (AIA) synoptic science images on a regular basis to build dedicated science quality catalogs. An overview of Solar Demon is given, with a focus on the algorithms for EUV wave detection and characterization. Several first results, such as flare and dimming butterfly diagrams for the rising part of Solar Cycle 24, are presented. The main advantages, challenges, and future prospects for Solar Demon are outlined in the Section 5.},
	journal = {Journal of Space Weather and Space Climate},
	author = {Kraaikamp, Emil and Verbeeck, Cis},
	year = {2015},
	keywords = {Dimmings, EUV waves, Event detection, Flares},
}

@article{Inceoglu2022,
	title = {Identification of {Coronal} {Holes} on {AIA}/{SDO} {Images} {Using} {Unsupervised} {Machine} {Learning}},
	volume = {930},
	issn = {0004-637X},
	url = {http://dx.doi.org/10.3847/1538-4357/ac5f43},
	doi = {10.3847/1538-4357/ac5f43},
	abstract = {Through its magnetic activity, the Sun governs the conditions in Earth’s vicinity, creating space weather events, which have drastic effects on our space- and ground-based technology. One of the most important solar magnetic features creating the space weather is the solar wind that originates from the coronal holes (CHs). The identification of the CHs on the Sun as one of the source regions of the solar wind is therefore crucial to achieve predictive capabilities. In this study, we used an unsupervised machine-learning method, k -means, to pixel-wise cluster the passband images of the Sun taken by the Atmospheric Imaging Assembly on the Solar Dynamics Observatory in 171, 193, and 211 Å in different combinations. Our results show that the pixel-wise k -means clustering together with systematic pre- and postprocessing steps provides compatible results with those from complex methods, such as convolutional neural networks. More importantly, our study shows that there is a need for a CH database where a consensus about the CH boundaries is reached by observers independently. This database then can be used as the “ground truth,” when using a supervised method or just to evaluate the goodness of the models.},
	number = {2},
	journal = {The Astrophysical Journal},
	author = {Inceoglu, Fadil and Shprits, Yuri Y. and Heinemann, Stephan G. and Bianco, Stefano},
	year = {2022},
	note = {arXiv: 2203.10491
Publisher: IOP Publishing},
	keywords = {Detection, Solar coronal holes, Space weather, ★},
	pages = {118},
}

@article{Reiss2015,
	title = {Improvements on coronal hole detection in {SDO}/{AIA} images using supervised classification},
	volume = {5},
	issn = {21157251},
	doi = {10.1051/swsc/2015025},
	abstract = {We demonstrate the use of machine learning algorithms in combination with segmentation techniques in order to distinguish coronal holes and filaments in SDO/AIA EUV images of the Sun. Based on two coronal hole detection techniques (intensity-based thresholding, SPoCA), we prepared datasets of manually labeled coronal hole and filament channel regions present on the Sun during the time range 2011-2013. By mapping the extracted regions from EUV observations onto HMI line-of-sight magnetograms we also include their magnetic characteristics. We computed shape measures from the segmented binary maps as well as first order and second order texture statistics from the segmented regions in the EUV images and magnetograms. These attributes were used for data mining investigations to identify the most performant rule to differentiate between coronal holes and filament channels. We applied several classifiers, namely Support Vector Machine (SVM), Linear Support Vector Machine, Decision Tree, and Random Forest, and found that all classification rules achieve good results in general, with linear SVM providing the best performances (with a true skill statistic of ≈ 0.90). Additional information from magnetic field data systematically improves the performance across all four classifiers for the SPoCA detection. Since the calculation is inexpensive in computing time, this approach is well suited for applications on real-time data. This study demonstrates how a machine learning approach may help improve upon an unsupervised feature extraction method.},
	journal = {Journal of Space Weather and Space Climate},
	author = {Reiss, Martin A. and Hofmeister, Stefan J. and De Visscher, Ruben and Temmer, Manuela and Veronig, Astrid M. and Delouille, Veronique and Mampaey, Benjamin and Ahammer, Helmut},
	year = {2015},
	note = {arXiv: 1506.06623},
	keywords = {Coronal holes, Feature extraction, Filament channels, Solar wind, Supervised Classification, Textural features, ★},
	pages = {1--12},
}

@article{Issan2023,
	title = {Predicting solar wind streams from the inner-heliosphere to {Earth} via shifted operator inference},
	volume = {473},
	issn = {10902716},
	url = {https://doi.org/10.1016/j.jcp.2022.111689},
	doi = {10.1016/j.jcp.2022.111689},
	abstract = {Solar wind conditions are predominantly predicted via three-dimensional numerical magnetohydrodynamic (MHD) models. Despite their ability to produce highly accurate predictions, MHD models require computationally intensive high-dimensional simulations. This renders them inadequate for making time-sensitive predictions and for large-ensemble analysis required in uncertainty quantification. This paper presents a new data-driven reduced-order model (ROM) capability for forecasting heliospheric solar wind speeds. Traditional model reduction methods based on Galerkin projection have difficulties with advection-dominated systems—such as solar winds—since they require a large number of basis functions and can become unstable. A core contribution of this work addresses this challenge by extending the non-intrusive operator inference ROM framework to exploit the translational symmetries present in the solar wind caused by the Sun's rotation. The numerical results show that our method can adequately emulate the MHD simulations and is more accurate than a reduced-physics surrogate model, the Heliospheric Upwind Extrapolation model.},
	journal = {Journal of Computational Physics},
	author = {Issan, Opal and Kramer, Boris},
	year = {2023},
	note = {arXiv: 2203.13372
Publisher: Elsevier Inc.},
	keywords = {Data-driven model reduction, Magnetohydrodynamics, Operator inference, Scientific machine learning, Solar wind modeling, Space weather prediction},
	pages = {111689},
}

@article{Edeling2022,
	title = {On the deep active-subspace method},
	volume = {11},
	number = {November},
	journal = {SIAM Journal on Uncertainty Quantification},
	author = {Edeling, Wouter},
	year = {2023},
	keywords = {46n30, 65j99, 65q30, ams subject classifications, analysis, deep active subspaces, epidemiology, gram-schmidt derivative, high-dimensional uncertainty quantification, sensitivity, ★},
	pages = {62--90},
}

@article{verbeke_quantifying_2022,
	title = {Quantifying errors in {3D} {CME} parameters derived from synthetic data using white-light reconstruction techniques},
	issn = {0273-1177},
	doi = {10.1016/J.ASR.2022.08.056},
	abstract = {Current efforts in space weather forecasting of CMEs have been focused on predicting their arrival time and magnetic structure. To make these predictions, methods have been developed to derive the true CME speed, size, position, and mass, among others. Difficulties in determining the input parameters for CME forecasting models arise from the lack of direct measurements of the coronal magnetic fields and uncertainties in estimating the CME 3D geometric and kinematic parameters after eruption. White-light coronagraph images are usually employed by a variety of CME reconstruction techniques that assume more or less complex geometries. This is the first study from our International Space Science Institute (ISSI) team “Understanding Our Capabilities in Observing and Modeling Coronal Mass Ejections”, in which we explore how subjectivity affects the 3D CME parameters that are obtained from the Graduated Cylindrical Shell (GCS) reconstruction technique, which is widely used in CME research. To be able to quantify such uncertainties, the “true” values that are being fitted should be known, which are impossible to derive from observational data. We have designed two different synthetic scenarios where the “true” geometric parameters are known in order to quantify such uncertainties for the first time. We explore this by using two sets of synthetic data: 1) Using the ray-tracing option from the GCS model software itself, and 2) Using 3D magnetohydrodynamic (MHD) simulation data from the Magnetohydrodynamic Algorithm outside a Sphere code. Our experiment includes different viewing configurations using single and multiple viewpoints. CME reconstructions using a single viewpoint had the largest errors and error ranges overall for both synthetic GCS and simulated MHD white-light data. As the number of viewpoints increased from one to two, the errors decreased by approximately 4° in latitude, 22° in longitude, 14° in tilt, and 10° in half-angle. Our results quantitatively show the critical need for at least two viewpoints to be able to reduce the uncertainty in deriving CME parameters. We did not find a significant decrease in errors when going from two to three viewpoints for our specific hypothetical three spacecraft scenario using synthetic GCS white-light data. As we expected, considering all configurations and numbers of viewpoints, the mean absolute errors in the measured CME parameters are generally significantly higher in the case of the simulated MHD white-light data compared to those from the synthetic white-light images generated by the GCS model. We found the following CME parameter error bars as a starting point for quantifying the minimum error in CME parameters from white-light reconstructions: Δθ (latitude)=6°-3°+2°, Δϕ (longitude)=11°-6°+18°, Δγ (tilt)=25°-7°+8°, Δα(half-angle)=10°-6°+12°, Δh (height)=0.6-0.4+1.2 R⊙, and Δκ (ratio)=0.1-0.02+0.03.},
	urldate = {2023-02-20},
	journal = {Advances in Space Research},
	author = {Verbeke, Christine and Mays, M. Leila and Kay, Christina and Riley, Pete and Palmerio, Erika and Dumbović, Mateja and Mierla, Marilena and Scolini, Camilla and Temmer, Manuela and Paouris, Evangelos and Balmaceda, Laura A. and Cremades, Hebe and Hinterreiter, Jürgen},
	month = aug,
	year = {2022},
	note = {Publisher: Pergamon},
	keywords = {Coronal mass ejections, Remote-sensing observations, Solar corona},
}

@article{chattopadhyay_long-term_2022,
	title = {Long-term stability and generalization of observationally-constrained stochastic data-driven models for geophysical turbulence},
	url = {https://arxiv.org/abs/2205.04601v1},
	doi = {10.48550/arxiv.2205.04601},
	abstract = {Recent years have seen a surge in interest in building deep learning-based
fully data-driven models for weather prediction. Such deep learning models if
trained on observations can mitigate certain biases in current state-of-the-art
weather models, some of which stem from inaccurate representation of
subgrid-scale processes. However, these data-driven models, being
over-parameterized, require a lot of training data which may not be available
from reanalysis (observational data) products. Moreover, an accurate,
noise-free, initial condition to start forecasting with a data-driven weather
model is not available in realistic scenarios. Finally, deterministic
data-driven forecasting models suffer from issues with long-term stability and
unphysical climate drift, which makes these data-driven models unsuitable for
computing climate statistics. Given these challenges, previous studies have
tried to pre-train deep learning-based weather forecasting models on a large
amount of imperfect long-term climate model simulations and then re-train them
on available observational data. In this paper, we propose a convolutional
variational autoencoder-based stochastic data-driven model that is pre-trained
on an imperfect climate model simulation from a 2-layer quasi-geostrophic flow
and re-trained, using transfer learning, on a small number of noisy
observations from a perfect simulation. This re-trained model then performs
stochastic forecasting with a noisy initial condition sampled from the perfect
simulation. We show that our ensemble-based stochastic data-driven model
outperforms a baseline deterministic encoder-decoder-based convolutional model
in terms of short-term skills while remaining stable for long-term climate
simulations yielding accurate climatology.},
	urldate = {2023-01-11},
	author = {Chattopadhyay, Ashesh and Pathak, Jaideep and Nabizadeh, Ebrahim and Bhimji, Wahid and Hassanzadeh, Pedram},
	month = may,
	year = {2022},
	note = {arXiv: 2205.04601},
}

@book{Camporeale2018,
	title = {Machine learning techniques for space weather},
	isbn = {978-0-12-811788-0},
	abstract = {Machine Learning Techniques for Space Weather provides a thorough and accessible presentation of machine learning techniques that can be employed by space weather professionals. Additionally, it presents an overview of real-world applications in space science to the machine learning community, offering a bridge between the fields. As this volume demonstrates, real advances in space weather can be gained using nontraditional approaches that take into account nonlinear and complex dynamics, including information theory, nonlinear auto-regression models, neural networks and clustering algorithms. Offering practical techniques for translating the huge amount of information hidden in data into useful knowledge that allows for better prediction, this book is a unique and important resource for space physicists, space weather professionals and computer scientists in related fields.},
	author = {Camporeale, Enrico and Wing, Simon and Johnson, Jay R.},
	year = {2018},
	doi = {10.1016/C2016-0-01976-9},
	note = {Publication Title: Machine Learning Techniques for Space Weather},
	keywords = {★},
}

@article{kay_osprei_2022,
	title = {{OSPREI}: {A} {Coupled} {Approach} to {Modeling} {CME}-{Driven} {Space} {Weather} {With} {Automatically} {Generated}, {User}-{Friendly} {Outputs}},
	volume = {20},
	issn = {15427390},
	doi = {10.1029/2021SW002914},
	abstract = {Coronal mass ejections (CMEs) drive space weather activity at Earth and throughout the solar system. Current CME-related space weather predictions rely on information reconstructed from coronagraphs, sometimes from only a single viewpoint, to drive a simple interplanetary propagation model, which only gives the arrival time or limited additional information. We present the coupling of three established models into OSPREI (Open Solar Physics Rapid Ensemble Information), a new tool that describes Sun-to-Earth CME behavior, including the location, orientation, size, shape, speed, arrival time, and internal thermal and magnetic properties, on the timescale needed for forecasts. First, Forecasting a CME's Altered Trajectory (ForeCAT) describes the trajectory that a CME takes through the solar corona. Second, ANother Type of Ensemble Arrival Time Results simulates the propagation, including expansion and deformation, of a CME in interplanetary space and determines the evolution of internal properties via conservation laws. Finally, ForeCAT In situ Data Observer produces in situ profiles for a CME's interaction with a synthetic spacecraft. OSPREI includes ensemble modeling by varying each input parameter to probe any uncertainty in their values, yielding probabilities for all outputs. Standardized visualizations are automatically generated, providing easily accessible, essential information for space weather forecasting. We show OSPREI results for a CMEs observed in the corona on 22 April and 09 May 2021. We approach these CME as a forecasting proof-of-concept, using information analogous to what would be available in real time rather than fine-tuning input parameters to achieve a best fit for a detailed scientific study. The OSPREI “prediction” shows good agreement with the arrival time and in situ properties.},
	number = {4},
	urldate = {2022-12-08},
	journal = {Space Weather},
	author = {Kay, C. and Mays, M. L. and Collado-Vega, Y. M.},
	month = apr,
	year = {2022},
	note = {arXiv: 2109.06960
Publisher: John Wiley and Sons Inc},
}

@article{kay_osprei_2022-1,
	title = {{OSPREI}: {A} {Coupled} {Approach} to {Modeling} {CME}-{Driven} {Space} {Weather} {With} {Automatically} {Generated}, {User}-{Friendly} {Outputs}},
	volume = {20},
	issn = {1542-7390},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1029/2021SW002914},
	doi = {10.1029/2021SW002914},
	abstract = {Coronal mass ejections (CMEs) drive space weather activity at Earth and throughout the solar system. Current CME-related space weather predictions rely on information reconstructed from coronagraphs, sometimes from only a single viewpoint, to drive a simple interplanetary propagation model, which only gives the arrival time or limited additional information. We present the coupling of three established models into OSPREI (Open Solar Physics Rapid Ensemble Information), a new tool that describes Sun-to-Earth CME behavior, including the location, orientation, size, shape, speed, arrival time, and internal thermal and magnetic properties, on the timescale needed for forecasts. First, Forecasting a CME's Altered Trajectory (ForeCAT) describes the trajectory that a CME takes through the solar corona. Second, ANother Type of Ensemble Arrival Time Results simulates the propagation, including expansion and deformation, of a CME in interplanetary space and determines the evolution of internal properties via conservation laws. Finally, ForeCAT In situ Data Observer produces in situ profiles for a CME's interaction with a synthetic spacecraft. OSPREI includes ensemble modeling by varying each input parameter to probe any uncertainty in their values, yielding probabilities for all outputs. Standardized visualizations are automatically generated, providing easily accessible, essential information for space weather forecasting. We show OSPREI results for a CMEs observed in the corona on 22 April and 09 May 2021. We approach these CME as a forecasting proof-of-concept, using information analogous to what would be available in real time rather than fine-tuning input parameters to achieve a best fit for a detailed scientific study. The OSPREI “prediction” shows good agreement with the arrival time and in situ properties.},
	number = {4},
	urldate = {2022-12-08},
	journal = {Space Weather},
	author = {Kay, C. and Mays, M. L. and Collado-Vega, Y. M.},
	month = apr,
	year = {2022},
	note = {arXiv: 2109.06960
Publisher: John Wiley \& Sons, Ltd
ISBN: 10.1029/2021},
	pages = {e2021SW002914},
}

@article{huang_global_2006,
	title = {Global {Optimization} of {Stochastic} {Black}-{Box} {Systems} via {Sequential} {Kriging} {Meta}-{Models}},
	volume = {34},
	issn = {1573-2916},
	url = {https://link.springer.com/article/10.1007/s10898-005-2454-3},
	doi = {10.1007/S10898-005-2454-3},
	abstract = {This paper proposes a new method that extends the efficient global optimization to address stochastic black-box systems. The method is based on a kriging meta-model that provides a global prediction of the objective values and a measure of prediction uncertainty at every point. The criterion for the infill sample selection is an augmented expected improvement function with desirable properties for stochastic responses. The method is empirically compared with the revised simplex search, the simultaneous perturbation stochastic approximation, and the DIRECT methods using six test problems from the literature. An application case study on an inventory system is also documented. The results suggest that the proposed method has excellent consistency and efficiency in finding global optimal solutions, and is particularly useful for expensive systems.},
	number = {3},
	urldate = {2022-11-21},
	journal = {Journal of Global Optimization 2006 34:3},
	author = {Huang, D. and Allen, T. T. and Notz, W. I. and Zeng, N.},
	month = mar,
	year = {2006},
	note = {Publisher: Springer},
	keywords = {Computer Science, Operations Research/Decision Theory, Optimization, Real Functions, general},
	pages = {441--466},
}

@article{Meng2015,
	title = {Alfvén wave solar model ({AWSoM}): {Proton} temperature anisotropy and solar wind acceleration},
	volume = {454},
	issn = {13652966},
	doi = {10.1093/mnras/stv2249},
	abstract = {Temperature anisotropy has been frequently observed in the solar corona and the solar wind, yet poorly represented in computational models of the solar wind. Therefore, we have included proton temperature anisotropy in our Alfvén wave solar model (AWSoM). This model solves the magnetohydrodynamic equations augmented with low-frequency Alfvén wave turbulence. The wave reflection due to Alfvén speed gradient and field-aligned vorticity results in turbulent cascade. At the gyroradius scales, the apportioning of the turbulence dissipation into coronal heating of the protons and electrons is through stochastic heating. This paper focuses on the impacts of the proton temperature anisotropy on the solar wind.We apply AWSoM to simulate the steady solar wind from the corona to 1AU using synoptic magnetograms. The Alfvén wave energy density at the inner boundary is prescribed with a uniform Poynting flux per field strength. We present the proton temperature anisotropy distribution, and investigate the firehose instability in the heliosphere from our simulations. In particular, the comparisons between the simulated and observed solar wind properties at 1AU during the ramping-up phase and the maximum of solar cycle 24 imply the importance of addressing the proton temperature anisotropy in solar wind modelling to capture the fast solar wind speed.},
	number = {4},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Meng, X. and Van der Holst, B. and Tóth, G. and Gombosi, T. I.},
	year = {2015},
	keywords = {Methods: Numerical, Solar wind, ★},
	pages = {3697--3709},
}

@article{Hastie2008_StatLearnin,
	title = {Elements of {Statistical} {Learning}},
	issn = {00349437},
	journal = {Springer Series in Statistics},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2008},
}

@article{Sokolov2013,
	title = {Magnetohydrodynamic waves and coronal heating: {Unifying} empirical and mhd turbulence models},
	volume = {764},
	issn = {15384357},
	doi = {10.1088/0004-637X/764/1/23},
	abstract = {We present a new global model of the solar corona, including the low corona, the transition region, and the top of the chromosphere. The realistic three-dimensional magnetic field is simulated using the data from the photospheric magnetic field measurements. The distinctive feature of the new model is incorporating MHD Alfvén wave turbulence. We assume this turbulence and its nonlinear dissipation to be the only momentum and energy source for heating the coronal plasma and driving the solar wind. The difference between the turbulence dissipation efficiency in coronal holes and that in closed field regions is because the nonlinear cascade rate degrades in strongly anisotropic (imbalanced) turbulence in coronal holes (no inward propagating wave), thus resulting in colder coronal holes, from which the fast solar wind originates. The detailed presentation of the theoretical model is illustrated with the synthetic images for multi-wavelength EUV emission compared with the observations from SDO AIA and STEREO EUVI instruments for the Carrington rotation 2107. © 2013. The American Astronomical Society. All rights reserved.},
	number = {1},
	journal = {Astrophysical Journal},
	author = {Sokolov, Igor V. and Van Der Holst, Bart and Oran, Rona and Downs, Cooper and Roussev, Ilia I. and Jin, Meng and Manchester, Ward B. and Evans, Rebekah M. and Gombosi, Tamas I.},
	year = {2013},
	keywords = {Sun: UV radiation, Sun: corona, Sun: transition region, solar wind, ★},
}

@techreport{Ng2012a,
	title = {Multifidelity {Uncertainty} {Quantification} {Using} {Non}-{Intrusive} {Polynomial} {Chaos} and {Stochastic} {Collocation}},
	abstract = {This paper explores the extension of multifidelity modeling concepts to the field of uncertainty quantification. Motivated by local correction functions that enable the provable convergence of a multifidelity optimization approach to an optimal high-fidelity point solution , we extend these ideas to global discrepancy modeling within a stochastic domain and seek convergence of a multifidelity uncertainty quantification process to globally integrated high-fidelity statistics. For constructing stochastic models of both the low fidelity model and the model discrepancy, we employ stochastic expansion methods (nonintrusive polynomial chaos and stochastic collocation) computed from sparse grids, where we seek to employ a coarsely resolved grid for the discrepancy in combination with a more finely resolved grid for the low fidelity model. The resolutions of these grids may be statically defined or determined through uniform and adaptive refinement processes. Adaptive refinement is particularly attractive, as it has the ability to preferentially target stochastic regions where the model discrepancy becomes more complex; i.e., where the predictive capabilities of the low-fidelity model start to break down and greater reliance on the high fidelity model (via the discrepancy) is necessary. These adaptive refinement processes can either be performed separately for the different sparse grids or within a unified multifi-delity algorithm. In particular, we propose an adaptive greedy multifidelity approach in which we extend the generalized sparse grid concept to consider candidate index set refinements drawn from multiple sparse grids. We demonstrate that the multifidelity UQ process converges more rapidly than a single-fidelity UQ in cases where the variance of the discrepancy is reduced relative to the variance of the high fidelity model (resulting in reductions in initial stochastic error) and/or where the spectrum of the expansion coefficients of the model discrepancy decays more rapidly than that of the high-fidelity model (resulting in accelerated convergence rates).},
	author = {Ng, L W T and Eldred, M S},
	year = {2012},
	keywords = {★},
}

@article{Huan2018,
	title = {Compressive sensing with cross-validation and stop-sampling for sparse polynomial chaos expansions},
	volume = {6},
	issn = {21662525},
	doi = {10.1137/17M1141096},
	abstract = {Compressive sensing is a powerful technique for recovering sparse solutions of underdetermined linear systems, which is often encountered in uncertainty quantification analysis of expensive and high-dimensional physical models. We perform numerical investigations employing several compressive sensing solvers that target the unconstrained LASSO formulation, with a focus on linear systems that arise in the construction of polynomial chaos expansions. With core solvers of l1\_ls, SpaRSA, CGIST, FPC\_AS, and ADMM, we develop techniques to mitigate overfitting through an automated selection of regularization constant based on cross-validation, and a heuristic strategy to guide the stop-sampling decision. Practical recommendations on parameter settings for these techniques are provided and discussed. The overall method is applied to a series of numerical examples of increasing complexity, including large eddy simulations of supersonic turbulent jet-in-crossflow involving a 24-dimensional input. Through empirical phase-transition diagrams and convergence plots, we illustrate sparse recovery performance under structures induced by polynomial chaos, accuracy and computational tradeoffs between polynomial bases of different degrees, and practicability of conducting compressive sensing for a realistic, high-dimensional physical application. Across test cases studied in this paper, we find ADMM to have demonstrated empirical advantages through consistent lower errors and faster computational times.},
	number = {2},
	journal = {SIAM-ASA Journal on Uncertainty Quantification},
	author = {Huan, Xun and Safta, Cosmin and Sargsyan, Khachik and Vane, Zachary P. and Lacaze, Guilhem and Oefelein, Joseph C. and Najm, Habib N.},
	year = {2018},
	note = {Publisher: Society for Industrial and Applied Mathematics Publications},
	keywords = {1-regularization, Compressed sensing, LASSO, Sequential compressive sensing, Sparse reconstruction, Sparse regression, Uncertainty quantification, ★},
	pages = {907--936},
}

@book{Constantine2015,
	title = {Active {Subspaces}: {Emerging} {Ideas} for {Dimension} {Reduction} in {Parameter} {Studies}},
	volume = {15},
	isbn = {978-1-61197-385-3},
	abstract = {Scientists and engineers use computer simulations to study relationships between a model's input parameters and its outputs. However, thorough parameter studies are challenging, if not impossible, when the simulation is expensive and the model has several inputs. To enable studies in these instances, the engineer may attempt to reduce the dimension of the model's input parameter space. Active subspaces are an emerging set of dimension reduction tools that identify important directions in the parameter space. This book describes techniques for discovering a model's active subspace and proposes methods for exploiting the reduced dimension to enable otherwise infeasible parameter studies. Readers will find new ideas for dimension reduction, easy-to-implement algorithms, and several examples of active subspaces in action.},
	author = {Constantine, Paul G.},
	year = {2015},
	note = {Issue: 2},
}

@article{xu_bandit-learning_nodate,
	title = {A bandit-learning approach to multifidelity approximation ∗},
	author = {Xu, Yiming and Keshavarzzadeh, Vahid and Kirby, Robert M},
	note = {arXiv: 2103.15342v3},
	keywords = {62-08, 62j05, 65c05, 65n30, ams subject classifications, bandit learning, consistency, linear regression, monte carlo method, multifidelity},
	pages = {1--41},
}

@article{park_deepsdf_2019,
	title = {{DeepSDF}: {Learning} {Continuous} {Signed} {Distance} {Functions} for {Shape} {Representation}},
	url = {https://arxiv.org/abs/1901.05103},
	doi = {10.48550/arxiv.1901.05103},
	abstract = {Computer graphics, 3D computer vision and robotics communities have produced
multiple approaches to representing 3D geometry for rendering and
reconstruction. These provide trade-offs across fidelity, efficiency and
compression capabilities. In this work, we introduce DeepSDF, a learned
continuous Signed Distance Function (SDF) representation of a class of shapes
that enables high quality shape representation, interpolation and completion
from partial and noisy 3D input data. DeepSDF, like its classical counterpart,
represents a shape's surface by a continuous volumetric field: the magnitude of
a point in the field represents the distance to the surface boundary and the
sign indicates whether the region is inside (-) or outside (+) of the shape,
hence our representation implicitly encodes a shape's boundary as the
zero-level-set of the learned function while explicitly representing the
classification of space as being part of the shapes interior or not. While
classical SDF's both in analytical or discretized voxel form typically
represent the surface of a single shape, DeepSDF can represent an entire class
of shapes. Furthermore, we show state-of-the-art performance for learned 3D
shape representation and completion while reducing the model size by an order
of magnitude compared with previous work.},
	urldate = {2022-08-12},
	author = {Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.05103},
}

@article{Osher1988,
	title = {Fronts propagating with curvature-dependent speed: {Algorithms} based on {Hamilton}-{Jacobi} formulations},
	volume = {79},
	issn = {10902716},
	doi = {10.1016/0021-9991(88)90002-2},
	abstract = {We devise new numerical algorithms, called PSC algorithms, for following fronts propagating with curvature-dependent speed. The speed may be an arbitrary function of curvature, and the front also can be passively advected by an underlying flow. These algorithms approximate the equations of motion, which resemble Hamilton-Jacobi equations with parabolic right-hand sides, by using techniques from hyperbolic conservation laws. Non-oscillatory schemes of various orders of accuracy are used to solve the equations, providing methods that accurately capture the formation of sharp gradients and cusps in the moving fronts. The algorithms handle topological merging and breaking naturally, work in any number of space dimensions, and do not require that the moving surface be written as a function. The methods can be also used for more general Hamilton-Jacobi-type problems. We demonstrate our algorithms by computing the solution to a variety of surface motion problems. © 1988.},
	number = {1},
	journal = {Journal of Computational Physics},
	author = {Osher, Stanley and Sethian, James A.},
	year = {1988},
	pages = {12--49},
}

@book{2006,
	title = {Past, present and future of statistical science},
	volume = {1999},
	isbn = {978-1-4822-0498-8},
	abstract = {Past, Present, and Future of Statistical Science was commissioned in 2013 by the Committee of Presidents of Statistical Societies (COPSS) to celebrate its 50th anniversary and the International Year of Statistics. COPSS consists of five charter member statistical societies in North America and is best known for sponsoring prestigious awards in statistics, such as the COPSS Presidents’ award. Through the contributions of a distinguished group of 50 statisticians who are past winners of at least one of the five awards sponsored by COPSS, this volume showcases the breadth and vibrancy of statistics, describes current challenges and new opportunities, highlights the exciting future of statistical science, and provides guidance to future generations of statisticians. The book is not only about statistics and science but also about people and their passion for discovery. Distinguished authors present expository articles on a broad spectrum of topics in statistical education, research, and applications. Topics covered include reminiscences and personal reflections on statistical careers, perspectives on the field and profession, thoughts on the discipline and the future of statistical science, and advice for young statisticians. Many of the articles are accessible not only to professional statisticians and graduate students but also to undergraduate students interested in pursuing statistics as a career and to all those who use statistics in solving real-world problems. A consistent theme of all the articles is the passion for statistics enthusiastically shared by the authors. Their success stories inspire, give a sense of statistics as a discipline, and provide a taste of the exhilaration of discovery, success, and professional accomplishment.},
	author = {Lin, Xihong},
	year = {2006},
	note = {Publication Title: منشورات جامعة دمشق
Issue: December},
	keywords = {★},
}

@article{Xiu,
	title = {The {Wiener}-{Askey} {Polynomial} {Chaos} for {Stochastic} {Differential} {Equations}},
	volume = {24},
	issn = {1064-8275},
	url = {http://epubs.siam.org/doi/10.1137/S1064827501387826},
	doi = {10.1137/S1064827501387826},
	number = {2},
	journal = {SIAM Journal on Scientific Computing},
	author = {Xiu, Dongbin and Karniadakis, George Em},
	month = jan,
	year = {2002},
	pages = {619--644},
}

@article{reiss_forecasting_2020,
	title = {Forecasting the {Ambient} {Solar} {Wind} with {Numerical} {Models}. {II}. {An} {Adaptive} {Prediction} {System} for {Specifying} {Solar} {Wind} {Speed} near the {Sun}},
	volume = {891},
	issn = {1538-4357},
	url = {https://iopscience.iop.org/article/10.3847/1538-4357/ab78a0},
	doi = {10.3847/1538-4357/ab78a0},
	number = {2},
	journal = {The Astrophysical Journal},
	author = {Reiss, Martin A. and MacNeice, Peter J. and Muglach, Karin and Arge, Charles N. and Möstl, Christian and Riley, Pete and Hinterreiter, Jürgen and Bailey, Rachel L. and Weiss, Andreas J. and Owens, Mathew J. and Amerstorfer, Tanja and Amerstorfer, Ute},
	month = mar,
	year = {2020},
	pages = {165},
}

@article{Auder2009,
	title = {Global sensitivity analysis based on entropy},
	volume = {3},
	doi = {10.1201/9781482266481-304},
	abstract = {This paper deals with the sensitivity analysis of model output, using entropy. By the past, variance has been used, and some measures directly related to data distributions have been performed. We expect entropy to give another point of view, refining the other sensitivity analysis, and may be to solve some situations where the other methods were useless. Two different indices based on entropy are presented in this work: those defined by Krzykacz-Hausmann 2001 based on conditional entropy which use directly the definition of Shannon's entropy, and those later suggested by Liu, Chen, and Sudjianto 2006, based on the Kullbak-Leibler entropy, which measure the difference between two probability distributions. After a brief presentation of the two entropy-based indices, two application cases are studied, one analytical function and one industrial computer code. Our goal is to show that some indices are better suited for some specific cases, and that they all enable different but relevant sensitivity analyses to be performed. © 2009 Taylor \& Francis Group.},
	journal = {Safety, Reliability and Risk Analysis: Theory, Methods and Applications - Proceedings of the Joint ESREL and SRA-Europe Conference},
	author = {Auder, Benjamin and Iooss, Bertrand},
	year = {2009},
	note = {ISBN: 9780415485135},
	keywords = {★},
	pages = {2107--2115},
}

@article{Middleton2008,
	title = {Use of information theory with discrete models of continuous systems},
	volume = {37},
	issn = {03081079},
	doi = {10.1080/03081070701250937},
	abstract = {A technique is presented whereby the Shannon entropy can be used to define two parametersthe effective range and the uncertainty indexwhich can be used to quantify consistently the uncertainty in realistic continuous systems by comparing the probability distribution associated with the actual system to a uniform distribution having the same entropy value. Knowing the entropy value of a distribution which describes a system can allow decision-makers to choose more easily between systems in order to achieve a given objective. It also provides a metric for ranking the importance of input parameters for a given system based upon their relative impact upon the total uncertainty of the output of the system. Since the data collected from most engineering systems are actually a set of discrete samples, a method is introduced to calculate the Shannon entropy using spreadsheet software and then calculate the effective range and uncertainty index. The technique is used to choose between two policies for an example construction project and to rank input variables in order of decreasing effect upon the output uncertainty of the project.},
	number = {3},
	journal = {International Journal of General Systems},
	author = {Middleton, Bobby D. and Golay, Michael W.},
	year = {2008},
	note = {ISBN: 0308107070125},
	keywords = {Entropy, Information theory, System dynamics, Systems, Uncertainty},
	pages = {347--371},
}

@article{Gupta2018,
	title = {Revisiting the {Basis} of {Sensitivity} {Analysis} for {Dynamical} {Earth} {System} {Models}},
	volume = {54},
	issn = {19447973},
	doi = {10.1029/2018WR022668},
	abstract = {This paper investigates the problem of global sensitivity analysis (GSA) of Dynamical Earth System Models and proposes a basis for how such analyses should be performed. We argue that (a) performance metric-based approaches to parameter GSA are actually identifiability analyses, (b) the use of a performance metric to assess sensitivity unavoidably distorts the information provided by the model about relative parameter importance, and (c) it is a serious conceptual flaw to interpret the results of such an analysis as being consistent and accurate indications of the sensitivity of the model response to parameter perturbations. Further, because such approaches depend on availability of system state/output observational data, the analysis they provide is necessarily incomplete. Here we frame the GSA problem from first principles, using trajectories of the partial derivatives of model outputs with respect to controlling factors as the theoretical basis for sensitivity, and construct a global sensitivity matrix from which statistical indices of total period time-aggregate parameter importance, and time series of time-varying parameter importance, can be inferred. We demonstrate this framework using the HBV-SASK conceptual hydrologic model applied to the Oldman basin in Canada and show that it disagrees with performance metric-based methods regarding which parameters exert the strongest controls on model behavior. Further, it is highly efficient, requiring less than 1,000 base samples to obtain stable and robust parameter importance assessments for our 10-parameter example.},
	number = {11},
	journal = {Water Resources Research},
	author = {Gupta, Hoshin V. and Razavi, Saman},
	year = {2018},
	keywords = {Parameter importance analysis, dynamical systems, efficiency and robustness, global sensitivity analysis, global sensitivity matrix, time-varying sensitivity, ★},
	pages = {8692--8717},
}

@article{Lemos2022,
	title = {Rediscovering orbital mechanics with machine learning},
	url = {http://arxiv.org/abs/2202.02306},
	abstract = {We present an approach for using machine learning to automatically discover the governing equations and hidden properties of real physical systems from observations. We train a "graph neural network" to simulate the dynamics of our solar system's Sun, planets, and large moons from 30 years of trajectory data. We then use symbolic regression to discover an analytical expression for the force law implicitly learned by the neural network, which our results showed is equivalent to Newton's law of gravitation. The key assumptions that were required were translational and rotational equivariance, and Newton's second and third laws of motion. Our approach correctly discovered the form of the symbolic force law. Furthermore, our approach did not require any assumptions about the masses of planets and moons or physical constants. They, too, were accurately inferred through our methods. Though, of course, the classical law of gravitation has been known since Isaac Newton, our result serves as a validation that our method can discover unknown laws and hidden properties from observed data. More broadly this work represents a key step toward realizing the potential of machine learning for accelerating scientific discovery.},
	author = {Lemos, Pablo and Jeffrey, Niall and Cranmer, Miles and Ho, Shirley and Battaglia, Peter},
	year = {2022},
	note = {arXiv: 2202.02306},
}

@article{Reich2013,
	title = {Downloaded 09 / 03 / 14 to 128 . 117 . 88 . 64 . {Redistribution} subject to {SIAM} license or copyright ; see http://www.siam.org/journals/ojsa.php {A} {NONPARAMETRIC} {ENSEMBLE} {TRANSFORM} {METHOD} {FOR} {Copyright} © by {SIAM} . {Unauthorized} reproduction of this article},
	volume = {35},
	number = {4},
	author = {Reich, Sebastian},
	year = {2013},
	keywords = {10, 1137, 130907367, 62f15, 62m20, 65c05, 86a22, 93e11, ams subject classifications, bayesian inference, doi, linear, monte carlo method, programming, resampling, sequential data assimilation},
	pages = {2013--2024},
}

@article{Dubreuil2014a,
	title = {Construction of bootstrap confidence intervals on sensitivity indices computed by polynomial chaos expansion},
	volume = {121},
	issn = {09518320},
	url = {http://dx.doi.org/10.1016/j.ress.2013.09.011},
	doi = {10.1016/j.ress.2013.09.011},
	abstract = {Sensitivity analysis aims at quantifying influence of input parameters dispersion on the output dispersion of a numerical model. When the model evaluation is time consuming, the computation of Sobol' indices based on Monte Carlo method is not applicable and a surrogate model has to be used. Among all approximation methods, polynomial chaos expansion is one of the most efficient to calculate variance-based sensitivity indices. Indeed, their computation is analytically derived from the expansion coefficients but without error estimators of the meta-model approximation. In order to evaluate the reliability of these indices, we propose to build confidence intervals by bootstrap re-sampling on the experimental design used to estimate the polynomial chaos approximation. Since the evaluation of the sensitivity indices is obtained with confidence intervals, it is possible to find a design of experiments allowing the computation of sensitivity indices with a given accuracy. © 2013 Elsevier Ltd.},
	journal = {Reliability Engineering and System Safety},
	author = {Dubreuil, S. and Berveiller, M. and Petitjean, F. and Salaün, M.},
	year = {2014},
	note = {Publisher: Elsevier},
	keywords = {Bootstrap re-sampling, Polynomial chaos expansion, Sensitivity analysis, ★},
	pages = {263--275},
}

@article{Sachdeva2021,
	title = {Simulating {Solar} {Maximum} {Conditions} {Using} the {Alfvén} {Wave} {Solar} {Atmosphere} {Model} ({AWSoM})},
	volume = {923},
	issn = {0004-637X},
	url = {http://dx.doi.org/10.3847/1538-4357/ac307c},
	doi = {10.3847/1538-4357/ac307c},
	abstract = {To simulate solar coronal mass ejections (CMEs) and predict their time of arrival and geomagnetic impact, it is important to accurately model the background solar wind conditions in which CMEs propagate. We use the Alfvén Wave Solar atmosphere Model (AWSoM) within the the Space Weather Modeling Framework to simulate solar maximum conditions during two Carrington rotations and produce solar wind background conditions comparable to the observations. We describe the inner boundary conditions for AWSoM using the ADAPT global magnetic maps and validate the simulated results with EUV observations in the low corona and measured plasma parameters at L1 as well as at the position of the Solar Terrestrial Relations Observatory spacecraft. This work complements our prior AWSoM validation study for solar minimum conditions and shows that during periods of higher magnetic activity, AWSoM can reproduce the solar plasma conditions (using properly adjusted photospheric Poynting flux) suitable for providing proper initial conditions for launching CMEs.},
	number = {2},
	journal = {The Astrophysical Journal},
	author = {Sachdeva, Nishtha and Tóth, Gábor and Manchester, Ward B. and van der Holst, Bart and Huang, Zhenguang and Sokolov, Igor V. and Zhao, Lulu and Shidi, Qusai Al and Chen, Yuxi and Gombosi, Tamas I. and Henney, Carl J. and Lloveras, Diego G. and Vásquez, Alberto M.},
	year = {2021},
	note = {Publisher: IOP Publishing},
	keywords = {Heliosphere, Solar corona, Solar wind, Space weather},
	pages = {176},
}

@article{Storlie2009,
	title = {Implementation and evaluation of nonparametric regression procedures for sensitivity analysis of computationally demanding models},
	volume = {94},
	issn = {09518320},
	url = {http://dx.doi.org/10.1016/j.ress.2009.05.007},
	doi = {10.1016/j.ress.2009.05.007},
	abstract = {The analysis of many physical and engineering problems involves running complex computational models (simulation models, computer codes). With problems of this type, it is important to understand the relationships between the input variables (whose values are often imprecisely known) and the output. The goal of sensitivity analysis (SA) is to study this relationship and identify the most significant factors or variables affecting the results of the model. In this presentation, an improvement on existing methods for SA of complex computer models is described for use when the model is too computationally expensive for a standard Monte-Carlo analysis. In these situations, a meta-model or surrogate model can be used to estimate the necessary sensitivity index for each input. A sensitivity index is a measure of the variance in the response that is due to the uncertainty in an input. Most existing approaches to this problem either do not work well with a large number of input variables and/or they ignore the error involved in estimating a sensitivity index. Here, a new approach to sensitivity index estimation using meta-models and bootstrap confidence intervals is described that provides solutions to these drawbacks. Further, an efficient yet effective approach to incorporate this methodology into an actual SA is presented. Several simulated and real examples illustrate the utility of this approach. This framework can be extended to uncertainty analysis as well. © 2009 Elsevier Ltd.},
	number = {11},
	journal = {Reliability Engineering and System Safety},
	author = {Storlie, Curtis B. and Swiler, Laura P. and Helton, Jon C. and Sallaberry, Cedric J.},
	year = {2009},
	note = {Publisher: Elsevier},
	keywords = {Bootstrap, Confidence intervals, Meta-model, Nonparametric regression, Sensitivity analysis, Surrogate model, Uncertainty analysis, Variance decomposition, ★},
	pages = {1735--1763},
}

@article{Blatman2010a,
	title = {Efficient computation of global sensitivity indices using sparse polynomial chaos expansions},
	volume = {95},
	issn = {09518320},
	doi = {10.1016/j.ress.2010.06.015},
	abstract = {Global sensitivity analysis aims at quantifying the relative importance of uncertain input variables onto the response of a mathematical model of a physical system. ANOVA-based indices such as the Sobol' indices are well-known in this context. These indices are usually computed by direct Monte Carlo or quasi-Monte Carlo simulation, which may reveal hardly applicable for computationally demanding industrial models. In the present paper, sparse polynomial chaos (PC) expansions are introduced in order to compute sensitivity indices. An adaptive algorithm allows the analyst to build up a PC-based metamodel that only contains the significant terms whereas the PC coefficients are computed by least-square regression using a computer experimental design. The accuracy of the metamodel is assessed by leave-one-out cross validation. Due to the genuine orthogonality properties of the PC basis, ANOVA-based sensitivity indices are post-processed analytically. This paper also develops a bootstrap technique which eventually yields confidence intervals on the results. The approach is illustrated on various application examples up to 21 stochastic dimensions. Accurate results are obtained at a computational cost 23 orders of magnitude smaller than that associated with Monte Carlo simulation. © 2010 Elsevier Ltd. All rights reserved.},
	number = {11},
	journal = {Reliability Engineering and System Safety},
	author = {Blatman, Graud and Sudret, Bruno},
	year = {2010},
	keywords = {ANOVA, Global sensitivity analysis, Sequential experimental design, Sobol' indices, Sparse polynomial chaos, Stepwise regression},
	pages = {1216--1229},
}

@article{Gombosi2021,
	title = {What sustained multi-disciplinary research can achieve: {The} space weather modeling framework},
	volume = {11},
	issn = {21157251},
	doi = {10.1051/swsc/2021020},
	abstract = {Magnetohydrodynamics (MHD)-based global space weather models have mostly been developed and maintained at academic institutions. While the "free spirit"approach of academia enables the rapid emergence and testing of new ideas and methods, the lack of long-Term stability and support makes this arrangement very challenging. This paper describes a successful example of a university-based group, the Center of Space Environment Modeling (CSEM) at the University of Michigan, that developed and maintained the Space Weather Modeling Framework (SWMF) and its core element, the BATS-R-US extended MHD code. It took a quarter of a century to develop this capability and reach its present level of maturity that makes it suitable for research use by the space physics community through the Community Coordinated Modeling Center (CCMC) as well as operational use by the NOAA Space Weather Prediction Center (SWPC).},
	journal = {Journal of Space Weather and Space Climate},
	author = {Gombosi, Tamas I. and Chen, Yuxi and Glocer, Alex and Huang, Zhenguang and Jia, Xianzhe and Liemohn, Michael W. and Manchester, Ward B. and Pulkkinen, Tuija and Sachdeva, Nishtha and Al Shidi, Qusai and Sokolov, Igor V. and Szente, Judit and Tenishev, Valeriy and Toth, Gabor and Van Der Holst, Bart and Welling, Daniel T. and Zhao, Lulu and Zou, Shasha and Gombosi, Tamas I.},
	year = {2021},
	note = {arXiv: 2105.13227},
	keywords = {MHD, Scientific computing, Solar flares and CMEs, Space plasma physics, Space weather},
}

@article{SkamarockW.C.KlempJ.B.DudhiaJ.GillD.O.BarkerD.M.WangW.Powers2005,
	title = {A description of the advanced research {WRF} version 2},
	abstract = {The development of the Weather Research and Forecasting (WRF) modeling system is a multiagency effort intended to provide a next-generation mesoscale forecast model and data assimilation system that will advance both the understanding and prediction of mesoscale weather and accelerate the transfer of research advances into operations. The model is being developed as a collaborative effort ort among the NCAR Mesoscale and Microscale Meteorology (MMM) Division, the National Oceanic and Atmospheric Administration's (NOAA) National Centers for Environmental Prediction (NCEP) and Forecast System Laboratory (FSL), the Department of Defense's Air Force Weather Agency (AFWA) and Naval Research Laboratory (NRL), the Center for Analysis and Prediction of Storms (CAPS) at the University of Oklahoma, and the Federal Aviation Administration (FAA), along with the participation of a number of university scientists. The WRF model is designed to be a flexible, state-of-the-art, portable code that is an efficient in a massively parallel computing environment. A modular single-source code is maintained that can be configured for both research and operations. It offers numerous physics options, thus tapping into the experience of the broad modeling community. Advanced data assimilation systems are being developed and tested in tandem with the model. WRF is maintained and supported as a community model to facilitate wide use, particularly for research and teaching, in the university community. It is suitable for use in a broad spectrum of applications across scales ranging from meters to thousands of kilometers. Such applications include research and operational numerical weather prediction (NWP), data assimilation and parameterized-physics research, downscaling climate simulations, driving air quality models, atmosphere-ocean coupling, and idealized simulations (e.g boundary-layer eddies, convection, baroclinic waves).},
	number = {June},
	author = {Skamarock, W. C., Klemp, J. B., Dudhia, J., Gill, D. O., Barker, D. M., Wang, W., Powers, J. G.},
	year = {2005},
	pages = {100},
}

@article{Coen2013,
	title = {Wrf-fire: {Coupled} weather-wildland fire modeling with the weather research and forecasting model},
	volume = {52},
	issn = {15588424},
	doi = {10.1175/JAMC-D-12-023.1},
	abstract = {A wildland fire-behavior module, named WRF-Fire, was integrated into the Weather Research and Forecasting (WRF) public domain numerical weather prediction model. The fire module is a surface firebehavior model that is two-way coupled with the atmospheric model. Near-surface winds from the atmospheric model are interpolated to a finer fire grid and are used, with fuel properties and local terrain gradients, to determine the fire's spread rate and direction. Fuel consumption releases sensible and latent heat fluxes into the atmospheric model's lowest layers, driving boundary layer circulations. The atmospheric model, configured in turbulence-resolving large-eddy-simulation mode, was used to explore the sensitivity of simulated fire characteristics such as perimeter shape, fire intensity, and spread rate to external factors known to influence fires, such as fuel characteristics and wind speed, and to explain how these external parameters affect the overall fire properties. Through the use of theoretical environmental vertical profiles, a suite of experiments using conditions typical of the daytime convective boundary layer was conducted in which these external parameters were varied around a control experiment. Results showed that simulated fires evolved into the expected bowed shape because of fire-atmosphere feedbacks that control airflow in and near fires. The coupled model reproduced expected differences in fire shapes and heading-region fire intensity among grass, shrub, and forest-litter fuel types; reproduced the expected narrow, rapid spread in higher wind speeds; and reproduced the moderate inhibition of fire spread in higher fuel moistures. The effects of fuel load were more complex: higher fuel loads increased the heat flux and fire-plume strength and thus the inferred fire effects but had limited impact on spread rate. © 2013 American Meteorological Society.},
	number = {1},
	journal = {Journal of Applied Meteorology and Climatology},
	author = {Coen, Janice L. and Cameron, Marques and Michalakes, John and Patton, Edward G. and Riggan, Philip J. and Yedinak, Kara M.},
	year = {2013},
	keywords = {★},
	pages = {16--38},
}

@article{Rackauckas2020,
	title = {Universal {Differential} {Equations} for {Scientific} {Machine} {Learning}},
	issn = {2331-8422},
	url = {http://arxiv.org/abs/2001.04385},
	abstract = {In the context of science, the well-known adage "a picture is worth a thousand words" might well be "a model is worth a thousand datasets." In this manuscript we introduce the SciML software ecosystem as a tool for mixing the information of physical laws and scientific models with data-driven machine learning approaches. We describe a mathematical object, which we denote universal differential equations (UDEs), as the unifying framework connecting the ecosystem. We show how a wide variety of applications, from automatically discovering biological mechanisms to solving high-dimensional Hamilton-Jacobi-Bellman equations, can be phrased and efficiently handled through the UDE formalism and its tooling. We demonstrate the generality of the software tooling to handle stochasticity, delays, and implicit constraints. This funnels the wide variety of SciML applications into a core set of training mechanisms which are highly optimized, stabilized for stiff equations, and compatible with distributed parallelism and GPU accelerators.},
	author = {Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali and Edelman, Alan},
	year = {2020},
	note = {arXiv: 2001.04385},
	pages = {1--55},
}

@article{Gao2021,
	title = {Physics-informed graph neural {Galerkin} networks: {A} unified framework for solving {PDE}-governed forward and inverse problems},
	url = {http://arxiv.org/abs/2107.12146},
	abstract = {Despite the great promise of the physics-informed neural networks (PINNs) in solving forward and inverse problems, several technical challenges are present as roadblocks for more complex and realistic applications. First, most existing PINNs are based on point-wise formulation with fully-connected networks to learn continuous functions, which suffer from poor scalability and hard boundary enforcement. Second, the infinite search space over-complicates the non-convex optimization for network training. Third, although the convolutional neural network (CNN)-based discrete learning can significantly improve training efficiency, CNNs struggle to handle irregular geometries with unstructured meshes. To properly address these challenges, we present a novel discrete PINN framework based on graph convolutional network (GCN) and variational structure of PDE to solve forward and inverse partial differential equations (PDEs) in a unified manner. The use of a piecewise polynomial basis can reduce the dimension of search space and facilitate training and convergence. Without the need of tuning penalty parameters in classic PINNs, the proposed method can strictly impose boundary conditions and assimilate sparse data in both forward and inverse settings. The flexibility of GCNs is leveraged for irregular geometries with unstructured meshes. The effectiveness and merit of the proposed method are demonstrated over a variety of forward and inverse computational mechanics problems governed by both linear and nonlinear PDEs.},
	author = {Gao, Han and Zahr, Matthew J. and Wang, Jian-Xun},
	year = {2021},
	note = {arXiv: 2107.12146},
	keywords = {graph convolutional neural networks, inverse problem, learning, mechanics, partial differential equations, physics-informed machine},
}

@article{Rossi2020,
	title = {Temporal {Graph} {Networks} for {Deep} {Learning} on {Dynamic} {Graphs}},
	url = {http://arxiv.org/abs/2006.10637},
	abstract = {Graph Neural Networks (GNNs) have recently become increasingly popular due to their ability to learn complex systems of relations or interactions arising in a broad spectrum of problems ranging from biology and particle physics to social networks and recommendation systems. Despite the plethora of different models for deep learning on graphs, few approaches have been proposed thus far for dealing with graphs that present some sort of dynamic nature (e.g. evolving features or connectivity over time). In this paper, we present Temporal Graph Networks (TGNs), a generic, efficient framework for deep learning on dynamic graphs represented as sequences of timed events. Thanks to a novel combination of memory modules and graph-based operators, TGNs are able to significantly outperform previous approaches being at the same time more computationally efficient. We furthermore show that several previous models for learning on dynamic graphs can be cast as specific instances of our framework. We perform a detailed ablation study of different components of our framework and devise the best configuration that achieves state-of-the-art performance on several transductive and inductive prediction tasks for dynamic graphs.},
	author = {Rossi, Emanuele and Chamberlain, Ben and Frasca, Fabrizio and Eynard, Davide and Monti, Federico and Bronstein, Michael},
	year = {2020},
	note = {arXiv: 2006.10637},
	pages = {1--16},
}

@article{Perumal2020,
	title = {Comparison of {Recurrent} {Neural} {Network} {Architectures} for {Wildfire} {Spread} {Modelling}},
	volume = {2020-Janua},
	doi = {10.1109/SAUPEC/RobMech/PRASA48453.2020.9078028},
	abstract = {Wildfire modelling is an attempt to reproduce fire behaviour. Through active fire analysis, it is possible to reproduce a dynamical process, such as wildfires, with limited duration time series data. Recurrent neural networks (RNNs) can model dynamic temporal behaviour due to their ability to remember their internal input. In this paper, we compare the Gated Recurrent Unit (GRU) and the Long Short-Term Memory (LSTM) network. We try to determine whether a wildfire continues to burn and given that it does, we aim to predict which one of the 8 cardinal directions the wildfire will spread in. Overall the GRU performs better for longer time series than the LSTM. We have shown that although we are reasonable at predicting the direction in which the wildfire will spread, we are not able to asses if the wildfire continues to burn due to the lack of auxiliary data.},
	number = {May 2020},
	journal = {2020 International SAUPEC/RobMech/PRASA Conference, SAUPEC/RobMech/PRASA 2020},
	author = {Perumal, Rylan and Zyl, Terence L.Van},
	year = {2020},
	note = {arXiv: 2005.13040
ISBN: 9781728141626},
	keywords = {machine learning, recurrent neural networks, wildfire spread modelling},
}

@article{Rozen2021,
	title = {Moser {Flow}: {Divergence}-based {Generative} {Modeling} on {Manifolds}},
	url = {http://arxiv.org/abs/2108.08052},
	abstract = {We are interested in learning generative models for complex geometries described via manifolds, such as spheres, tori, and other implicit surfaces. Current extensions of existing (Euclidean) generative models are restricted to specific geometries and typically suffer from high computational costs. We introduce Moser Flow (MF), a new class of generative models within the family of continuous normalizing flows (CNF). MF also produces a CNF via a solution to the change-of-variable formula, however differently from other CNF methods, its model (learned) density is parameterized as the source (prior) density minus the divergence of a neural network (NN). The divergence is a local, linear differential operator, easy to approximate and calculate on manifolds. Therefore, unlike other CNFs, MF does not require invoking or backpropagating through an ODE solver during training. Furthermore, representing the model density explicitly as the divergence of a NN rather than as a solution of an ODE facilitates learning high fidelity densities. Theoretically, we prove that MF constitutes a universal density approximator under suitable assumptions. Empirically, we demonstrate for the first time the use of flow models for sampling from general curved surfaces and achieve significant improvements in density estimation, sample quality, and training complexity over existing CNFs on challenging synthetic geometries and real-world benchmarks from the earth and climate sciences.},
	number = {2020},
	author = {Rozen, Noam and Grover, Aditya and Nickel, Maximilian and Lipman, Yaron},
	year = {2021},
	note = {arXiv: 2108.08052},
	pages = {1--16},
}

@article{Rajaram2020,
	title = {Non-intrusive parametric reduced order modeling using randomized algorithms},
	volume = {1 PartF},
	doi = {10.2514/6.2020-0417},
	abstract = {This paper demonstrates the creation of purely data-driven, non-intrusive parametric reduced order models (ROMs) for emulation of high-dimensional field outputs using randomized linear algebra techniques. Typically, low-dimensional representations are built using the Proper Orthogonal Decomposition (POD) combined with interpolation/regression in the latent space via machine learning. However, even moderately large simulations can lead to data sets on which the cost of computing the POD becomes intractable due to storage and computational complexity of the numerical procedure. In an attempt to reduce the offline cost, the proposed method demonstrates the application of randomized singular value decomposition (SVD) and sketching-based randomized SVD to compute the POD basis. The predictive capability of ROMs resulting from regular SVD and randomized/sketching-based algorithms are compared with each other to ensure that the decrease in computational cost does not result in a loss in accuracy. Demonstrations on canonical and practical fluid flow problems show that the ROMs resulting from randomized methods are competitive with ROMs that employ the conventional deterministic method. Through this new method, it is hoped that truly large-scale parametric ROMs can be constructed under a significantly limited computational budget.},
	number = {January},
	journal = {AIAA Scitech 2020 Forum},
	author = {Rajaram, Dushhyanth and Puranik, Tejas G. and Perron, Christian and Mavris, Dimitri N.},
	year = {2020},
	note = {ISBN: 9781624105951},
	pages = {1--24},
}

@article{Liu2021,
	title = {Algorithms for {Verifying} {Deep} {Neural} {Networks}},
	volume = {4},
	issn = {2167-3888},
	doi = {10.1561/2400000035},
	abstract = {Deep neural networks are widely used for nonlinear function approximation with applications ranging from computer vision to control. Although these networks involve the composition of simple arithmetic operations, it can be very challenging to verify whether a particular network satisfies certain input-output properties. This article surveys methods that have emerged recently for soundly verifying such properties. These methods borrow insights from reachability analysis, optimization, and search. We discuss fundamental differences and connections between existing algorithms. In addition, we provide pedagogical implementations of existing methods and compare them on a set of benchmark problems.},
	number = {3-4},
	journal = {Foundations and Trends® in Optimization},
	author = {Liu, Changliu and Arnon, Tomer and Lazarus, Christopher and Strong, Christopher and Barrett, Clark and Kochenderfer, Mykel J.},
	year = {2021},
	note = {arXiv: 1903.06758},
	keywords = {★},
	pages = {244--404},
}

@article{Tropp2019,
	title = {Streaming low-rank matrix approximation with an application to scientific simulation},
	volume = {41},
	issn = {10957197},
	doi = {10.1137/18M1201068},
	abstract = {This paper argues that randomized linear sketching is a natural tool for on-the-fly compression of data matrices that arise from large-scale scientific simulations and data collection. The technical contribution consists in a new algorithm for constructing an accurate low-rank approximation of a matrix from streaming data. This method is accompanied by an a priori analysis that allows the user to set algorithm parameters with confidence and an a posteriori error estimator that allows the user to validate the quality of the reconstructed matrix. In comparison to previous techniques, the new method achieves smaller relative approximation errors and is less sensitive to parameter choices. As concrete applications, the paper outlines how the algorithm can be used to compress a Navier-Stokes simulation and a sea surface temperature dataset.},
	number = {4},
	journal = {SIAM Journal on Scientific Computing},
	author = {Tropp, Joel A. and Yurtsever, Alp and Udell, Madeleine and Cevher, Volkan},
	year = {2019},
	note = {arXiv: 1902.08651},
	keywords = {Dimension reduction, Matrix approximation, Numerical linear algebra, Singular value decomposition, Sketching, Streaming, ★},
	pages = {A2430--A2463},
}

@article{Williams2018,
	title = {Unsupervised {Discovery} of {Demixed}, {Low}-{Dimensional} {Neural} {Dynamics} across {Multiple} {Timescales} through {Tensor} {Component} {Analysis}},
	volume = {98},
	issn = {10974199},
	url = {https://doi.org/10.1016/j.neuron.2018.05.015},
	doi = {10.1016/j.neuron.2018.05.015},
	abstract = {Perceptions, thoughts, and actions unfold over millisecond timescales, while learned behaviors can require many days to mature. While recent experimental advances enable large-scale and long-term neural recordings with high temporal fidelity, it remains a formidable challenge to extract unbiased and interpretable descriptions of how rapid single-trial circuit dynamics change slowly over many trials to mediate learning. We demonstrate a simple tensor component analysis (TCA) can meet this challenge by extracting three interconnected, low-dimensional descriptions of neural data: neuron factors, reflecting cell assemblies; temporal factors, reflecting rapid circuit dynamics mediating perceptions, thoughts, and actions within each trial; and trial factors, describing both long-term learning and trial-to-trial changes in cognitive state. We demonstrate the broad applicability of TCA by revealing insights into diverse datasets derived from artificial neural networks, large-scale calcium imaging of rodent prefrontal cortex during maze navigation, and multielectrode recordings of macaque motor cortex during brain machine interface learning. Williams et al. describe an unsupervised method to uncover simple structure in large-scale recordings by extracting distinct cell assemblies with rapid within-trial dynamics, reflecting interpretable aspects of perceptions, actions, and thoughts, and slower across-trial dynamics reflecting learning and internal state changes.},
	number = {6},
	journal = {Neuron},
	author = {Williams, Alex H. and Kim, Tony Hyun and Wang, Forea and Vyas, Saurabh and Ryu, Stephen I. and Shenoy, Krishna V. and Schnitzer, Mark and Kolda, Tamara G. and Ganguli, Surya},
	year = {2018},
	pmid = {29887338},
	note = {Publisher: Elsevier Inc.},
	keywords = {brain machine interfaces, dimensionality reduction, gain modulation, large-scale recordings, learning, motor control, navigation, neural data analysis, recurrent neural networks, single-trial analysis},
	pages = {1099--1115.e8},
}

@article{Kobyzev2020,
	title = {Normalizing {Flows}: {An} {Introduction} and {Review} of {Current} {Methods}},
	issn = {0162-8828},
	doi = {10.1109/tpami.2020.2992934},
	abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kobyzev, Ivan and Prince, Simon and Brubaker, Marcus},
	year = {2020},
	note = {arXiv: 1908.09257},
	pages = {1--1},
}

@article{Gamboa2014,
	title = {Sensitivity analysis for multidimensional and functional outputs},
	volume = {8},
	issn = {19357524},
	doi = {10.1214/14-EJS895},
	abstract = {Let X:=(X{\textless}inf{\textgreater}1{\textless}/inf{\textgreater},..., X{\textless}inf{\textgreater}p{\textless}/inf{\textgreater}) be random objects (the inputs), defined on some probability space (Ω, F, P) and valued in some measurable space E = E{\textless}inf{\textgreater}1{\textless}/inf{\textgreater}×...×E{\textless}inf{\textgreater}p{\textless}/inf{\textgreater}. Further, let Y:= Y =f(X{\textless}inf{\textgreater}1{\textless}/inf{\textgreater},..., X{\textless}inf{\textgreater}p{\textless}/inf{\textgreater}) be the output. Here, f is a measurable function from E to some Hilbert space H (H could be either of finite or infinite dimension). In this work, we give a natural generalization of the Sobol indices (that are classically defined when Y∈R), when the output belongs to H. These indices have very nice properties. First, they are invariant under isometry and scaling. Further they can be, as in dimension 1, easily estimated by using the so-called Pick and Freeze method. We investigate the asymptotic behaviour of such an estimation scheme.},
	number = {1},
	journal = {Electronic Journal of Statistics},
	author = {Gamboa, Fabrice and Janon, Alexandre and Klein, Thierry and Lagnoux, Agnès},
	year = {2014},
	note = {arXiv: 1311.1797},
	keywords = {Concentration inequalities, Quadratic functionals, Semi-parametric efficient estimation, Sensitiv- ity analysis, Sobol indices, Temporal output, Vector output, ★},
	pages = {575--603},
}

@techreport{MunSiew,
	title = {{DCAE}\_DMD model for temporal forecasting in space weather},
	abstract = {Based off of SIAM UQ 2022 presentation - this work focuses on using deep autoencoders in conjunction with basic DMD with control in order to make forecasts of thermospheric density ?? (double check).},
	author = {Mun Siew, Peng and Linares, Richard},
}

@article{Dubreuil2014,
	title = {Construction of bootstrap confidence intervals on sensitivity indices computed by polynomial chaos expansion},
	volume = {121},
	issn = {09518320},
	url = {http://dx.doi.org/10.1016/j.ress.2013.09.011},
	doi = {10.1016/j.ress.2013.09.011},
	abstract = {Sensitivity analysis aims at quantifying influence of input parameters dispersion on the output dispersion of a numerical model. When the model evaluation is time consuming, the computation of Sobol' indices based on Monte Carlo method is not applicable and a surrogate model has to be used. Among all approximation methods, polynomial chaos expansion is one of the most efficient to calculate variance-based sensitivity indices. Indeed, their computation is analytically derived from the expansion coefficients but without error estimators of the meta-model approximation. In order to evaluate the reliability of these indices, we propose to build confidence intervals by bootstrap re-sampling on the experimental design used to estimate the polynomial chaos approximation. Since the evaluation of the sensitivity indices is obtained with confidence intervals, it is possible to find a design of experiments allowing the computation of sensitivity indices with a given accuracy. © 2013 Elsevier Ltd.},
	journal = {Reliability Engineering and System Safety},
	author = {Dubreuil, S. and Berveiller, M. and Petitjean, F. and Salaün, M.},
	year = {2014},
	note = {Publisher: Elsevier},
	keywords = {Bootstrap re-sampling, Polynomial chaos expansion, Sensitivity analysis},
	pages = {263--275},
}

@article{Safta2021,
	title = {Low-{Rank} {Tensor} {Network} {Approximations} for {Earth} {System} {Model}.},
	doi = {10.2172/1854317},
	author = {Safta, Cosmin and Sargsyan, Khachik and Jakeman, John and Gorodetsky, Alex},
	year = {2021},
}

@article{Chami2021,
	title = {{HoroPCA}: {Hyperbolic} {Dimensionality} {Reduction} via {Horospherical} {Projections}},
	url = {http://arxiv.org/abs/2106.03306},
	abstract = {This paper studies Principal Component Analysis (PCA) for data lying in hyperbolic spaces. Given directions, PCA relies on: (1) a parameterization of subspaces spanned by these directions, (2) a method of projection onto subspaces that preserves information in these directions, and (3) an objective to optimize, namely the variance explained by projections. We generalize each of these concepts to the hyperbolic space and propose HoroPCA, a method for hyperbolic dimensionality reduction. By focusing on the core problem of extracting principal directions, HoroPCA theoretically better preserves information in the original data such as distances, compared to previous generalizations of PCA. Empirically, we validate that HoroPCA outperforms existing dimensionality reduction methods, significantly reducing error in distance preservation. As a data whitening method, it improves downstream classification by up to 3.9\% compared to methods that don't use whitening. Finally, we show that HoroPCA can be used to visualize hyperbolic data in two dimensions.},
	author = {Chami, Ines and Gu, Albert and Nguyen, Dat and Ré, Christopher},
	year = {2021},
	note = {arXiv: 2106.03306},
	keywords = {★},
	pages = {1--31},
}

@article{Dunlavy2011,
	title = {Temporal link prediction using matrix and tensor factorizations},
	volume = {5},
	issn = {15564681},
	doi = {10.1145/1921632.1921636},
	abstract = {The data in many disciplines such as social networks, Web analysis, etc. is link-based, and the link structure can be exploited for many different data mining tasks. In this article, we consider the problem of temporal link prediction: Given link data for times 1 through T, can we predict the links at time T +1? If our data has underlying periodic structure, can we predict out even further in time, i.e., links at time T + 2, T + 3, etc.? In this article, we consider bipartite graphs that evolve over time and consider matrix-and tensor-based methods for predicting future links. We present a weight-based method for collapsing multiyear data into a single matrix. We show how the well-known Katz method for link prediction can be extended to bipartite graphs and, moreover, approximated in a scalable way using a truncated singular value decomposition. Using a CANDECOMP/PARAFAC tensor decomposition of the data, we illustrate the usefulness of exploiting the natural three-dimensional structure of temporal link data. Through several numerical experiments, we demonstrate that both matrix-and tensor-based techniques are effective for temporal link prediction despite the inherent difficulty of the problem. Additionally, we show that tensor-based techniques are particularly effective for temporal data with varying periodic patterns. © 2011 ACM.},
	number = {2},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Dunlavy, Daniel M. and Kolda, Tamara G. and Acar, Evrim},
	year = {2011},
	note = {arXiv: 1005.4006},
	keywords = {CANDECOMP, Evolution, Link mining, Link prediction, PARAFAC, Tensor factorization},
	pages = {1--27},
}

@article{Balch2020,
	title = {Fired ({Fire} events delineation): {An} open, flexible algorithm and database of us fire events derived from the modis burned area product (2001–2019)},
	volume = {12},
	issn = {20724292},
	doi = {10.3390/rs12213498},
	abstract = {Harnessing the fire data revolution, i.e., the abundance of information from satellites, government records, social media, and human health sources, now requires complex and challenging data integration approaches. Defining fire events is key to that effort. In order to understand the spatial and temporal characteristics of fire, or the classic fire regime concept, we need to critically define fire events from remote sensing data. Events, fundamentally a geographic concept with delineated spatial and temporal boundaries around a specific phenomenon that is homogenous in some property, are key to understanding fire regimes and more importantly how they are changing. Here, we describe Fire Events Delineation (FIRED), an event-delineation algorithm, that has been used to derive fire events (N = 51,871) from the MODIS MCD64 burned area product for the coterminous US (CONUS) from January 2001 to May 2019. The optimized spatial and temporal parameters to cluster burned area pixels into events were an 11-day window and a 5-pixel (2315 m) distance, when optimized against 13,741 wildfire perimeters in the CONUS from the Monitoring Trends in Burn Severity record. The linear relationship between the size of individual FIRED and Monitoring Trends in Burn Severity (MTBS) events for the CONUS was strong (R2 = 0.92 for all events). Importantly, this algorithm is open-source and flexible, allowing the end user to modify the spatio-temporal threshold or even the underlying algorithm approach as they see fit. We expect the optimized criteria to vary across regions, based on regional distributions of fire event size and rate of spread. We describe the derived metrics provided in a new national database and how they can be used to better understand US fire regimes. The open, flexible FIRED algorithm could be utilized to derive events in any satellite product. We hope that this open science effort will help catalyze a community-driven, data-integration effort (termed OneFire) to build a more complete picture of fire.},
	number = {21},
	journal = {Remote Sensing},
	author = {Balch, Jennifer K. and St. Denis, Lise A. and Mahood, Adam L. and Mietkiewicz, Nathan P. and Williams, Travis M. and McGlinchy, Joe and Cook, Maxwell C.},
	year = {2020},
	keywords = {Data harmonization, Event-builder algorithm, Fire regimes, Open fire science, Satellite fire detections},
	pages = {1--18},
}

@article{Saltelli2010,
	title = {Variance based sensitivity analysis of model output. {Design} and estimator for the total sensitivity index},
	volume = {181},
	issn = {00104655},
	url = {http://dx.doi.org/10.1016/j.cpc.2009.09.018},
	doi = {10.1016/j.cpc.2009.09.018},
	abstract = {Variance based methods have assessed themselves as versatile and effective among the various available techniques for sensitivity analysis of model output. Practitioners can in principle describe the sensitivity pattern of a model Y = f (X1, X2, ..., Xk) with k uncertain input factors via a full decomposition of the variance V of Y into terms depending on the factors and their interactions. More often practitioners are satisfied with computing just k first order effects and k total effects, the latter describing synthetically interactions among input factors. In sensitivity analysis a key concern is the computational cost of the analysis, defined in terms of number of evaluations of f (X1, X2, ..., Xk) needed to complete the analysis, as f (X1, X2, ..., Xk) is often in the form of a numerical model which may take long processing time. While the computational cost is relatively cheap and weakly dependent on k for estimating first order effects, it remains expensive and strictly k-dependent for total effect indices. In the present note we compare existing and new practices for this index and offer recommendations on which to use. © 2009 Elsevier B.V. All rights reserved.},
	number = {2},
	journal = {Computer Physics Communications},
	author = {Saltelli, Andrea and Annoni, Paola and Azzini, Ivano and Campolongo, Francesca and Ratto, Marco and Tarantola, Stefano},
	year = {2010},
	note = {Publisher: Elsevier B.V.},
	keywords = {★},
	pages = {259--270},
}

@article{Ba2018,
	title = {A {Sequential} {Maximum} {Projection} {Design} {Framework} for {Computer} {Experiments}},
	volume = {28},
	url = {https://www.jstor.org/stable/44841929},
	doi = {10.5705/ss.202016.0165},
	abstract = {Many computer experiments involve a large number of input factors, but many of them are inert and only a subset are important. This paper develops a new sequential design framework that can accommodate multiple responses and quickly screen out inert factors so that the final design is space-filling with respect to the active factors. By folding over Latin hypercube designs with sliced structure, this sequential design can have flexible sample size in each stage and also ensure that each stage, as well as the whole combined design, are all approximately Latin hypercube designs. The sequential framework does not require prescribing the total sample size and, under the presence of inert factors, can lead to substantial savings in simulation resources. Even if all factors are important, the proposed sequential design can still achieve a similar overall space-filling property compared to a maximin Latin hypercube design optimized in a single stage.},
	number = {2},
	journal = {Statistica Sinica},
	author = {Ba, Shan and Myers, William and Wang, Dianpeng},
	year = {2018},
}

@article{Blatman2010,
	title = {Efficient computation of global sensitivity indices using sparse polynomial chaos expansions},
	volume = {95},
	issn = {09518320},
	doi = {10.1016/j.ress.2010.06.015},
	abstract = {Global sensitivity analysis aims at quantifying the relative importance of uncertain input variables onto the response of a mathematical model of a physical system. ANOVA-based indices such as the Sobol' indices are well-known in this context. These indices are usually computed by direct Monte Carlo or quasi-Monte Carlo simulation, which may reveal hardly applicable for computationally demanding industrial models. In the present paper, sparse polynomial chaos (PC) expansions are introduced in order to compute sensitivity indices. An adaptive algorithm allows the analyst to build up a PC-based metamodel that only contains the significant terms whereas the PC coefficients are computed by least-square regression using a computer experimental design. The accuracy of the metamodel is assessed by leave-one-out cross validation. Due to the genuine orthogonality properties of the PC basis, ANOVA-based sensitivity indices are post-processed analytically. This paper also develops a bootstrap technique which eventually yields confidence intervals on the results. The approach is illustrated on various application examples up to 21 stochastic dimensions. Accurate results are obtained at a computational cost 23 orders of magnitude smaller than that associated with Monte Carlo simulation. © 2010 Elsevier Ltd. All rights reserved.},
	number = {11},
	journal = {Reliability Engineering and System Safety},
	author = {Blatman, Graud and Sudret, Bruno},
	year = {2010},
	keywords = {ANOVA, Global sensitivity analysis, Sequential experimental design, Sobol' indices, Sparse polynomial chaos, Stepwise regression, ★},
	pages = {1216--1229},
}

@article{Fleeter2020,
	title = {Multilevel and multifidelity uncertainty quantification for cardiovascular hemodynamics},
	volume = {365},
	issn = {00457825},
	url = {https://doi.org/10.1016/j.cma.2020.113030},
	doi = {10.1016/j.cma.2020.113030},
	abstract = {Standard approaches for uncertainty quantification in cardiovascular modeling pose challenges due to the large number of uncertain inputs and the significant computational cost of realistic three-dimensional simulations. We propose an efficient uncertainty quantification framework utilizing a multilevel multifidelity Monte Carlo (MLMF) estimator to improve the accuracy of hemodynamic quantities of interest while maintaining reasonable computational cost. This is achieved by leveraging three cardiovascular model fidelities, each with varying spatial resolution to rigorously quantify the variability in hemodynamic outputs. We employ two low-fidelity models (zero- and one-dimensional) to construct several different estimators. Our goal is to investigate and compare the efficiency of estimators built from combinations of these two low-fidelity model alternatives and our high-fidelity three-dimensional models. We demonstrate this framework on healthy and diseased models of aortic and coronary anatomy, including uncertainties in material property and boundary condition parameters. Our goal is to demonstrate that for this application it is possible to accelerate the convergence of the estimators by utilizing a MLMF paradigm. Therefore, we compare our approach to single fidelity Monte Carlo estimators and to a multilevel Monte Carlo approach based only on three-dimensional simulations, but leveraging multiple spatial resolutions. We demonstrate significant, on the order of 10 to 100 times, reduction in total computational cost with the MLMF estimators. We also examine the differing properties of the MLMF estimators in healthy versus diseased models, as well as global versus local quantities of interest. As expected, global quantities such as outlet pressure and flow show larger reductions than local quantities, such as those relating to wall shear stress, as the latter rely more heavily on the highest fidelity model evaluations. Similarly, healthy models show larger reductions than diseased models. In all cases, our workflow coupling Dakota's MLMF estimators with the SimVascular cardiovascular modeling framework makes uncertainty quantification feasible for constrained computational budgets.},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Fleeter, Casey M. and Geraci, Gianluca and Schiavazzi, Daniele E. and Kahn, Andrew M. and Marsden, Alison L.},
	year = {2020},
	note = {arXiv: 1908.04875
Publisher: Elsevier B.V.},
	keywords = {Cardiovascular modeling, Multifidelity Monte Carlo, Multilevel Monte Carlo, Multilevel multifidelity Monte Carlo, Uncertainty quantification, ★},
	pages = {113030},
}

@article{Cui2016,
	title = {Scalable posterior approximations for large-scale {Bayesian} inverse problems via likelihood-informed parameter and state reduction},
	volume = {315},
	issn = {10902716},
	doi = {10.1016/j.jcp.2016.03.055},
	abstract = {Two major bottlenecks to the solution of large-scale Bayesian inverse problems are the scaling of posterior sampling algorithms to high-dimensional parameter spaces and the computational cost of forward model evaluations. Yet incomplete or noisy data, the state variation and parameter dependence of the forward model, and correlations in the prior collectively provide useful structure that can be exploited for dimension reduction in this setting-both in the parameter space of the inverse problem and in the state space of the forward model. To this end, we show how to jointly construct low-dimensional subspaces of the parameter space and the state space in order to accelerate the Bayesian solution of the inverse problem. As a byproduct of state dimension reduction, we also show how to identify low-dimensional subspaces of the data in problems with high-dimensional observations. These subspaces enable approximation of the posterior as a product of two factors: (i) a projection of the posterior onto a low-dimensional parameter subspace, wherein the original likelihood is replaced by an approximation involving a reduced model; and (ii) the marginal prior distribution on the high-dimensional complement of the parameter subspace. We present and compare several strategies for constructing these subspaces using only a limited number of forward and adjoint model simulations. The resulting posterior approximations can rapidly be characterized using standard sampling techniques, e.g., Markov chain Monte Carlo. Two numerical examples demonstrate the accuracy and efficiency of our approach: inversion of an integral equation in atmospheric remote sensing, where the data dimension is very high; and the inference of a heterogeneous transmissivity field in a groundwater system, which involves a partial differential equation forward model with high dimensional state and parameters.},
	journal = {Journal of Computational Physics},
	author = {Cui, Tiangang and Marzouk, Youssef and Willcox, Karen},
	year = {2016},
	note = {arXiv: 1510.06053},
	keywords = {Bayesian inference, Dimension reduction, Inverse problems, Low-rank approximation, Markov chain Monte Carlo, Model reduction},
	pages = {363--387},
}

@book{Saltelli2008,
	title = {Global {Sensitivity} {Analysis}: {The} {Primer} by {Andrea} {Saltelli}, {Marco} {Ratto}, {Terry} {Andres}, {Francesca} {Campolongo}, {Jessica} {Cariboni}, {Debora} {Gatelli}, {Michaela} {Saisana}, {Stefano} {Tarantola}},
	volume = {76},
	isbn = {978-0-470-05997-5},
	abstract = {Complex mathematical and computational models are used in all areas of society and technology and yet model based science is increasingly contested or refuted, especially when models are applied to controversial themes in domains such as health, the environment or the economy. More stringent standards of proofs are demanded from model-based numbers, especially when these numbers represent potential financial losses, threats to human health or the state of the environment. Quantitative sensitivity analysis is generally agreed to be one such standard. Mathematical models are good at mapping assumptions into inferences. A modeller makes assumptions about laws pertaining to the system, about its status and a plethora of other, often arcane, system variables and internal model settings. To what extent can we rely on the model-based inference when most of these assumptions are fraught with uncertainties? Global Sensitivity Analysis offers an accessible treatment of such problems via quantitative sensitivity analysis, beginning with the first principles and guiding the reader through the full range of recommended practices with a rich set of solved exercises. The text explains the motivation for sensitivity analysis, reviews the required statistical concepts, and provides a guide to potential applications. The book: Provides a self-contained treatment of the subject, allowing readers to learn and practice global sensitivity analysis without further materials. Presents ways to frame the analysis, interpret its results, and avoid potential pitfalls. Features numerous exercises and solved problems to help illustrate the applications. Is authored by leading sensitivity analysis practitioners, combining a range of disciplinary backgrounds. Postgraduate students and practitioners in a wide range of subjects, including statistics, mathematics, engineering, physics, chemistry, environmental sciences, biology, toxicology, actuarial sciences, and econometrics will find much of use here. This book will prove equally valuable to engineers working on risk analysis and to financial analysts concerned with pricing and hedging.},
	author = {Saltelli, Andrea},
	year = {2008},
	doi = {10.1111/j.1751-5823.2008.00062_17.x},
	note = {Publication Title: International Statistical Review
Issue: 3
ISSN: 03067734},
}

@misc{Abramowitz1970,
	title = {Abramowitz\_and\_Stegun.{Pdf}},
	author = {Abramowitz, Milton and Irene, Stegun},
	year = {1970},
	note = {Pages: 470},
	keywords = {★},
}

@article{Foster2021,
	title = {Deep {Adaptive} {Design}: {Amortizing} {Sequential} {Bayesian} {Experimental} {Design}},
	url = {http://arxiv.org/abs/2103.02438},
	abstract = {We introduce Deep Adaptive Design (DAD), a general method for amortizing the cost of performing sequential adaptive experiments using the framework of Bayesian optimal experimental design (BOED). Traditional sequential BOED approaches require substantial computational time at each stage of the experiment. This makes them unsuitable for most real-world applications, where decisions must typically be made quickly. DAD addresses this restriction by learning an amortized design network upfront and then using this to rapidly run (multiple) adaptive experiments at deployment time. This network takes as input the data from previous steps, and outputs the next design using a single forward pass; these design decisions can be made in milliseconds during the live experiment. To train the network, we introduce contrastive information bounds that are suitable objectives for the sequential setting, and propose a customized network architecture that exploits key symmetries. We demonstrate that DAD successfully amortizes the process of experimental design, outperforming alternative strategies on a number of problems.},
	author = {Foster, Adam and Ivanova, Desi R. and Malik, Ilyas and Rainforth, Tom},
	year = {2021},
	note = {arXiv: 2103.02438},
	keywords = {★},
}

@article{Kossen2021,
	title = {Self-{Attention} {Between} {Datapoints}: {Going} {Beyond} {Individual} {Input}-{Output} {Pairs} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2106.02584},
	abstract = {We challenge a common assumption underlying most supervised deep learning: that a model makes a prediction depending only on its parameters and the features of a single input. To this end, we introduce a general-purpose deep learning architecture that takes as input the entire dataset instead of processing one datapoint at a time. Our approach uses self-attention to reason about relationships between datapoints explicitly, which can be seen as realizing non-parametric models using parametric attention mechanisms. However, unlike conventional non-parametric models, we let the model learn end-to-end from the data how to make use of other datapoints for prediction. Empirically, our models solve cross-datapoint lookup and complex reasoning tasks unsolvable by traditional deep learning models. We show highly competitive results on tabular data, early results on CIFAR-10, and give insight into how the model makes use of the interactions between points.},
	author = {Kossen, Jannik and Band, Neil and Lyle, Clare and Gomez, Aidan N. and Rainforth, Tom and Gal, Yarin},
	year = {2021},
	note = {arXiv: 2106.02584},
}

@article{OLeary-Roseberry2020,
	title = {Derivative-{Informed} {Projected} {Neural} {Networks} for {High}-{Dimensional} {Parametric} {Maps} {Governed} by {PDEs}},
	url = {http://arxiv.org/abs/2011.15110},
	abstract = {Many-query problems, arising from uncertainty quantification, Bayesian inversion, Bayesian optimal experimental design, and optimization under uncertainty-require numerous evaluations of a parameter-to-output map. These evaluations become prohibitive if this parametric map is high-dimensional and involves expensive solution of partial differential equations (PDEs). To tackle this challenge, we propose to construct surrogates for high-dimensional PDE-governed parametric maps in the form of projected neural networks that parsimoniously capture the geometry and intrinsic low-dimensionality of these maps. Specifically, we compute Jacobians of these PDE-based maps, and project the high-dimensional parameters onto a low-dimensional derivative-informed active subspace; we also project the possibly high-dimensional outputs onto their principal subspace. This exploits the fact that many high-dimensional PDE-governed parametric maps can be well-approximated in low-dimensional parameter and output subspace. We use the projection basis vectors in the active subspace as well as the principal output subspace to construct the weights for the first and last layers of the neural network, respectively. This frees us to train the weights in only the low-dimensional layers of the neural network. The architecture of the resulting neural network captures to first order, the low-dimensional structure and geometry of the parametric map. We demonstrate that the proposed projected neural network achieves greater generalization accuracy than a full neural network, especially in the limited training data regime afforded by expensive PDE-based parametric maps. Moreover, we show that the number of degrees of freedom of the inner layers of the projected network is independent of the parameter and output dimensions, and high accuracy can be achieved with weight dimension independent of the discretization dimension.},
	author = {O'Leary-Roseberry, Thomas and Villa, Umberto and Chen, Peng and Ghattas, Omar},
	year = {2020},
	note = {arXiv: 2011.15110},
	keywords = {49m41, 65c20, 93e20, 93e35, active subspace, adjoint-based, ams subject classifications, deep learning, derivative-informed dimen-, hessian, neural networks, parametrized pdes, proper orthogonal decomposition, sensitivity, sion reduction, surrogate modeling, uncertainty quantification},
}

@article{Buluc2021,
	title = {Randomized {Algorithms} for {Scientific} {Computing} ({RASC})},
	url = {http://arxiv.org/abs/2104.11079},
	abstract = {Randomized algorithms have propelled advances in artificial intelligence and represent a foundational research area in advancing AI for Science. Future advancements in DOE Office of Science priority areas such as climate science, astrophysics, fusion, advanced materials, combustion, and quantum computing all require randomized algorithms for surmounting challenges of complexity, robustness, and scalability. This report summarizes the outcomes of that workshop, "Randomized Algorithms for Scientific Computing (RASC)," held virtually across four days in December 2020 and January 2021.},
	author = {Buluc, Aydin and Kolda, Tamara G. and Wild, Stefan M. and Anitescu, Mihai and DeGennaro, Anthony and Jakeman, John and Kamath, Chandrika and {Ramakrishnan} and {Kannan} and Lopes, Miles E. and Martinsson, Per-Gunnar and Myers, Kary and Nelson, Jelani and Restrepo, Juan M. and Seshadhri, C. and Vrabie, Draguna and Wohlberg, Brendt and Wright, Stephen J. and Yang, Chao and Zwart, Peter},
	year = {2021},
	note = {arXiv: 2104.11079},
	keywords = {★},
}

@article{Pai2018,
	title = {Patient {Similarity} {Networks} for {Precision} {Medicine}},
	volume = {430},
	issn = {10898638},
	url = {https://doi.org/10.1016/j.jmb.2018.05.037},
	doi = {10.1016/j.jmb.2018.05.037},
	abstract = {Clinical research and practice in the 21st century is poised to be transformed by analysis of computable electronic medical records and population-level genome-scale patient profiles. Genomic data capture genetic and environmental state, providing information on heterogeneity in disease and treatment outcome, but genomic-based clinical risk scores are limited. Achieving the goal of routine precision medicine that takes advantage of these rich genomics data will require computational methods that support heterogeneous data, have excellent predictive performance, and ideally, provide biologically interpretable results. Traditional machine-learning approaches excel at performance, but often have limited interpretability. Patient similarity networks are an emerging paradigm for precision medicine, in which patients are clustered or classified based on their similarities in various features, including genomic profiles. This strategy is analogous to standard medical diagnosis, has excellent performance, is interpretable, and can preserve patient privacy. We review new methods based on patient similarity networks, including Similarity Network Fusion for patient clustering and netDx for patient classification. While these methods are already useful, much work is required to improve their scalability for contemporary genetic cohorts, optimize parameters, and incorporate a wide range of genomics and clinical data. The coming 5 years will provide an opportunity to assess the utility of network-based algorithms for precision medicine.},
	number = {18},
	journal = {Journal of Molecular Biology},
	author = {Pai, Shraddha and Bader, Gary D.},
	year = {2018},
	pmid = {29860027},
	note = {Publisher: The Authors},
	keywords = {genomics, machine learning, networks, patient classifier, precision medicine},
	pages = {2924--2938},
}

@article{Romano2019,
	title = {Conformalized quantile regression},
	issn = {23318422},
	abstract = {Conformal prediction is a technique for constructing prediction intervals that attain valid coverage in finite samples, without making distributional assumptions. Despite this appeal, existing conformal methods can be unnecessarily conservative because they form intervals of constant or weakly varying length across the input space. In this paper we propose a new method that is fully adaptive to heteroscedasticity. It combines conformal prediction with classical quantile regression, inheriting the advantages of both. We establish a theoretical guarantee of valid coverage, supplemented by extensive experiments on popular regression datasets. We compare the efficiency of conformalized quantile regression to other conformal methods, showing that our method tends to produce shorter intervals.},
	journal = {arXiv},
	author = {Romano, Yaniv and Patterson, Evan and Candès, Emmanuel J.},
	year = {2019},
	note = {arXiv: 1905.03222},
}

@article{Ruby2021,
	title = {High-energy-density-physics measurements in implosions using {Bayesian} inference},
	volume = {28},
	issn = {10897674},
	doi = {10.1063/5.0040616},
	abstract = {Convergent high-energy-density (HED) experimental platforms are used to study matter under some of the most extreme conditions that can be produced on Earth, comparable to the interior of stars. There are many challenges in using these systems for fundamental measurements currently being addressed by new analysis methods, such as the combination of a reduced physics model and Bayesian inference, allowing a self-consistent inference of physical quantities with a robust error analysis. These methods in combination with simple (as compared to inertial confinement fusion implosions) implosion platforms, which can be modified to show sensitivity to different physical mechanisms of interest, are used to study the physical properties of matter under extreme conditions. This work discusses a subset of implosion targets for studying opacity effects, electron-ion equilibration, and thermal conductivity and, as an example, a system consisting of a thick-shelled, gas-filled laser-direct-drive implosion is used to show how a reduced model and Bayesian inference can help inform experimental design decisions such as diagnostic choice. It is shown that for this system that a combination of neutron and X-ray self-emission diagnostics is critical for constraining the details of the thermodynamic states in the system and that the conductivity exponent in a Spitzer like framework can be constrained to the 30\% level in deuterium at gigabar conditions. This process can be applied to many HED systems to make underlying model assumptions explicit and facilitate experimental design and analysis.},
	number = {3},
	journal = {Physics of Plasmas},
	author = {Ruby, J. J. and Gaffney, J. A. and Rygg, J. R. and Ping, Y. and Collins, G. W.},
	year = {2021},
	note = {Publisher: AIP Publishing LLC},
}

@article{Hamilton2020,
	title = {Graph {Representation} {Learning} {Hamilton}},
	volume = {14},
	issn = {19394616},
	doi = {10.2200/S01045ED1V01Y202009AIM046},
	abstract = {Graph-structured data is ubiquitous throughout the natural and social sciences, from telecommunication networks to quantum chemistry. Building relational inductive biases into deep learning architectures is crucial for creating systems that can learn, reason, and generalize from this kind of data. Recent years have seen a surge in research on graph representation learning, including techniques for deep graph embeddings, generalizations of convolutional neural networks to graph-structured data, and neural message-passing approaches inspired by belief propagation. These advances in graph representation learning have led to new state-of-the-art results in numerous domains, including chemical synthesis, 3D vision, recommender systems, question answering, and social network analysis. This book provides a synthesis and overview of graph representation learning. It begins with a discussion of the goals of graph representation learning as well as key methodological foundations in graph theory and network analysis. Following this, the book introduces and reviews methods for learning node embeddings, including random-walk-based methods and applications to knowledge graphs. It then provides a technical synthesis and introduction to the highly successful graph neural network (GNN) formalism, which has become a dominant and fast-growing paradigm for deep learning with graph data. The book concludes with a synthesis of recent advancements in deep generative models for graphs a nascent but quickly growing subset of graph representation learning.},
	number = {3},
	journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	author = {Hamilton, William L.},
	year = {2020},
	keywords = {deep learning, geometric deep learning, graph convolutions, graph embeddings, graph neural networks, graph signal processing, knowledge graphs, network analysis, node embeddings, relational data, social networks, spectral graph theory},
	pages = {1--159},
}

@article{Aliabouzar2020,
	title = {Standing wave-assisted acoustic droplet vaporization for single and dual payload release in acoustically-responsive scaffolds},
	volume = {66},
	issn = {18732828},
	doi = {10.1016/j.ultsonch.2020.105109},
	abstract = {© 2020 Elsevier B.V. An ultrasound standing wave field (SWF) has been utilized in many biomedical applications. Here, we demonstrate how a SWF can enhance drug release using acoustic droplet vaporization (ADV) in an acoustically-responsive scaffold (ARS). ARSs are composite fibrin hydrogels containing payload-carrying, monodispersed perfluorocarbon (PFC) emulsions and have been used to stimulate regenerative processes such as angiogenesis. Elevated amplitudes in the SWF significantly enhanced payload release from ARSs containing dextran-loaded emulsions (nominal diameter: 6 μm) compared to the -SWF condition, both at sub- and suprathreshold excitation pressures. At 2.5 MHz and 4 MPa peak rarefactional pressure, the cumulative percentage of payload released from ARSs reached 84.1 ± 5.4\% and 66.1 ± 4.4\% under + SWF and -SWF conditions, respectively, on day 10. A strategy for generating a SWF for an in situ ARS is also presented. For dual-payload release studies, bi-layer ARSs containing a different payload within each layer were exposed to temporally staggered ADV at 3.25 MHz (day 0) and 8.6 MHz (day 4). Sequential payload release was demonstrated using dextran payloads as well as two growth factors relevant to angiogenesis: basic fibroblast growth factor (bFGF) and platelet-derived growth factor BB (PDGF-BB). In addition, bubble growth and fibrin degradation were characterized in the ARSs under +SWF and -SWF conditions. These results highlight the utility of a SWF for modulating single and dual payload release from an ARS and can be used in future therapeutic studies.},
	journal = {Ultrasonics Sonochemistry},
	author = {Aliabouzar, M. and Jivani, A. and Lu, X. and Kripfgans, O.D. and Fowlkes, J.B. and Fabiilli, M.L.},
	year = {2020},
	keywords = {Acoustic droplet vaporization, Controlled release, Fibrin, Growth factors, Perfluorocarbon, Standing waves, Ultrasound},
}

@article{Economon2016a,
	title = {{SU2}: {An} open-source suite for multiphysics simulation and design},
	volume = {54},
	issn = {00011452},
	doi = {10.2514/1.J053813},
	abstract = {This paper presents the main objectives and a description of the SU2 suite, including the novel software architecture and open-source software engineering strategy. SU2 is a computational analysis and design package that has been developed to solve multiphysics analysis and optimization tasks using unstructured mesh topologies. Its unique architecture iswell suited for extensibility to treat partial-differential-equation-based problems not initially envisioned. The common framework adopted enables the rapid implementation of newphysics packages that can be tightly coupled to form a powerful ensemble of analysis tools to address complex problems facing many engineering communities. The framework is demonstrated on a number, solving both the flow and adjoint systems of equations to provide a highfidelity predictive capability and sensitivity information that can be used for optimal shape design using a gradientbased framework, goal-oriented adaptive mesh refinement, or uncertainty quantification.},
	number = {3},
	journal = {AIAA Journal},
	author = {Economon, Thomas D. and Palacios, Francisco and Copeland, Sean R. and Lukaczyk, Trent W. and Alonso, Juan J.},
	year = {2016},
	pages = {828--846},
}

@article{Debusschere2005a,
	title = {Numerical challenges in the use of polynomial chaos representations for stochastic processes},
	volume = {26},
	issn = {10648275},
	doi = {10.1137/S1064827503427741},
	abstract = {This paper gives an overview of the use of polynomial chaos (PC) expansions to represent stochastic processes in numerical simulations. Several methods are presented for performing arithmetic on, as well as for evaluating polynomial and nonpolynomial functions of variables represented by PC expansions. These methods include Taylor series, a newly developed integration method, as well as a sampling-based spectral projection method for nonpolynomial function evaluations. A detailed analysis of the accuracy of the PC representations, and of the different methods for nonpolynomial function evaluations, is performed. It is found that the integration method offers a robust and accurate approach for evaluating nonpolynomial functions, even when very high-order information is present in the PC expansions. © 2004 Society for Industrial and Applied Mathematics.},
	number = {2},
	journal = {SIAM Journal on Scientific Computing},
	author = {Debusschere, Bert J. and Najm, Habib N. and Pébayt, Philippe P. and Knio, Omar M. and Ghanem, Roger G. and Le Maître, Olivier P.},
	year = {2005},
	keywords = {Polynomial chaos, Spectral uncertainty quantification, Stochastic},
	pages = {698--719},
}

@article{Radonic2000,
	title = {Popliteal artery entrapment syndrome: {Diagnosis} and management, with report of three cases},
	volume = {27},
	issn = {07302347},
	abstract = {Popliteal artery entrapment syndrome is an important albeit infrequent cause of serious disability among young adults and athletes with anomalous anatomic relationships between the popliteal artery and surrounding musculotendinous structures. We report our experience with 3 patients, in whom we used duplex ultrasonography, computed tomography, digital subtraction angiography, and conventional arteriography to diagnose popliteal artery entrapment and to grade the severity of dynamic circulatory insufficiency and arterial damage. We used a posterior surgical approach to give the best view of the anatomic structures compressing the popliteal artery. In 2 patients, in whom compression had not yet damaged the arterial wall, operative decompression of the artery by resection of the aberrant muscle was su fficient. In the 3rd patient, operative reconstruction of an occluded segment with autologous vein graft was necessary, in addition to decompression of the vessel and resection of aberrant muscle. The result in each case was complete recovery, with absence of symptoms and with patency verified by Doppler examination. We conclude that clinicians who encounter young patients with progressive lowerlimb arterial insufficiency should be aware of the possibility of popliteal artery entrapment. Early diagnosis through a combined approach (careful physical examination and history-taking, duplex ultrasonography, computerized tomography, and angiography) is necessary for exact diagnosis. The treatment of choice is the surgical creation of normal anatomy within the popliteal fossa.},
	number = {1},
	journal = {Texas Heart Institute Journal},
	author = {Radonić, Vedran and Koplić, Stevan and Giunio, Lovel and Božić, Ivo and Mašković, Josip and Buća, Ante},
	year = {2000},
	keywords = {Angiography, digital subtraction, Popliteal artery entrapment syndrome, Popliteal artery/surgery, Tomography, x-ray computed, Ultrasonography, Doppler, duplex, ★},
	pages = {3--13},
}

@article{Borgonovo2016,
	title = {Sensitivity analysis: {A} review of recent advances},
	volume = {248},
	issn = {03772217},
	url = {http://dx.doi.org/10.1016/j.ejor.2015.06.032},
	doi = {10.1016/j.ejor.2015.06.032},
	abstract = {The solution of several operations research problems requires the creation of a quantitative model. Sensitivity analysis is a crucial step in the model building and result communication process. Through sensitivity analysis we gain essential insights on model behavior, on its structure and on its response to changes in the model inputs. Several interrogations are possible and several sensitivity analysis methods have been developed, giving rise to a vast and growing literature. We present an overview of available methods, structuring them into local and global methods. For local methods, we discuss Tornado diagrams, one way sensitivity functions, differentiation-based methods and scenario decomposition through finite change sensitivity indices, providing a unified view of the associated sensitivity measures. We then analyze global sensitivity methods, first discussing screening methods such as sequential bifurcation and the Morris method. We then address variance-based, moment-independent and value of information-based sensitivity methods. We discuss their formalization in a common rationale and present recent results that permit the estimation of global sensitivity measures by post-processing the sample generated by a traditional Monte Carlo simulation. We then investigate in detail the methodological issues concerning the crucial step of correctly interpreting the results of a sensitivity analysis. A classical example is worked out to illustrate some of the approaches.},
	number = {3},
	journal = {European Journal of Operational Research},
	author = {Borgonovo, Emanuele and Plischke, Elmar},
	year = {2016},
	note = {Publisher: Elsevier Ltd.},
	keywords = {Computer experiments, Sensitivity analysis, Simulation},
	pages = {869--887},
}

@article{Sobol2001,
	title = {Global sensitivity indices for nonlinear mathematical models and their {Monte} {Carlo} estimates},
	volume = {55},
	issn = {03784754},
	doi = {10.1016/S0378-4754(00)00270-6},
	number = {1-3},
	journal = {Mathematics and Computers in Simulation},
	author = {Sobol, I. M.},
	year = {2001},
	keywords = {Mathematical modelling, Monte Carlo method, Quasi-Monte Carlo method, Sensitivity analysis, ★},
	pages = {271--280},
}

@article{Sobol2009,
	title = {Derivative based global sensitivity measures and their link with global sensitivity indices},
	volume = {79},
	issn = {03784754},
	doi = {10.1016/j.matcom.2009.01.023},
	abstract = {A model function f(x1,...,xn) defined in the unit hypercube Hn with Lebesque measure dx = dx1...dxn is considered. If the function is square integrable, global sensitivity indices provide adequate estimates for the influence of individual factors xi or groups of such factors. Alternative estimators that require less computer time can also be used. If the function f is differentiable, functionals depending on ∂f/∂xi have been suggested as estimators for the influence of xi. The Morris importance measure modified by Campolongo, Cariboni and Saltelli μ* is an approximation of the functional μi = ∫Hn fenced(∂ f / ∂ xi) d x. In this paper a similar functional is studiedνi = ∫Hn fenced(frac(∂ f, ∂ xi))2 d xEvidently, μi ≤ sqrt(νi), and νi ≤ C μi if fenced(∂ f / ∂ xi) ≤ C. A link between νi and the sensitivity index Sit o t is established:Sit o t ≤ frac(νi, π2 D)where D is the total variance of f(x1,...,xn). Thus small νi imply small Sit o t, and unessential factors xi (that is xi corresponding to a very small Sit o t) can be detected analyzing computed values ν1,...,νn. However, ranking influential factors xi using these values can give false conclusions. Generalized Sit o t and νi can be applied in situations where the factors x1,...,xn are independent random variables. If xi is a normal random variable with variance σi2, then Sit o t ≤ νi σi2 / D. © 2009 IMACS.},
	number = {10},
	journal = {Mathematics and Computers in Simulation},
	author = {Sobol', I. M. and Kucherenko, S.},
	year = {2009},
	keywords = {Derivative based global sensitivity measure, Global sensitivity index, Morris method, Quasi Monte Carlo method},
	pages = {3009--3017},
}

@article{Shibeshi2005,
	title = {The {Rheology} of {Blood} {Flow} in a {Branched} {Arterial} {System}.},
	volume = {15},
	issn = {1617-8106 (Print)},
	doi = {10.1901/jaba.2005.15-398},
	abstract = {Blood flow rheology is a complex phenomenon. Presently there is no universally agreed upon model to represent the viscous property of blood. However, under the general classification of non-Newtonian models that simulate blood behavior to different degrees of accuracy, there are many variants. The power law, Casson and Carreau models are popular non-Newtonian models and affect hemodynamics quantities under many conditions. In this study, the finite volume method is used to investigate hemodynamics predictions of each of the models. To implement the finite volume method, the computational fluid dynamics software Fluent 6.1 is used. In this numerical study the different hemorheological models are found to predict different results of hemodynamics variables which are known to impact the genesis of atherosclerosis and formation of thrombosis. The axial velocity magnitude percentage difference of up to 2 \% and radial velocity difference up to 90 \% is found at different sections of the T-junction geometry. The size of flow recirculation zones and their associated separation and reattachment point's locations differ for each model. The wall shear stress also experiences up to 12 \% shift in the main tube. A velocity magnitude distribution of the grid cells shows that the Newtonian model is close dynamically to the Casson model while the power law model resembles the Carreau model. ZUSAMMENFASSUNG: Die Rheologie von Blutströmungen ist ein komplexes Phänomen. Gegenwärtig existiert kein allgemein akzeptiertes Modell, um die viskosen Eigenschaften von Blut wiederzugeben. Jedoch gibt es mehrere Varianten unter der allgemeinen Klassifikation von nicht-Newtonschen Modellen, die das Verhalten von Blut mit unterschiedlicher Genauigkeit simulieren. Die Potenzgesetz-, Casson und Carreau-Modelle sind beliebte nicht-New-tonsche Modelle und beeinflussen die hämodynamischen Eigenschaften in vielen Situationen. In dieser Studie wurde die finite Volumenmethode angewandt, um die hämodynamischen Vorhersagen dieser Modelle zu untersuchen. Um die finite Volumenmethode zu implementieren, wurde die Fluiddynamiksoftware Fluent 6.1 verwendet. In dieser numerischen Studie wurde gefunden, dass die unterschiedlichen hämorheologischen Modelle unterschiedliche Resultate für die hämodynamischen Grössen vorhersagen, von denen bekannt ist, dass sie die Entstehung von Arteriosklerose und die Bildung von Thrombose beeinflussen. Es wurde gefunden, dass die relative Differenz der axialen Geschwindigkeit bis zu 2\% und die der radialen Geschwindigkeit bis zu 90\% in unterschiedlichen Abschnitten der T-Verbindung beträgt. Die Grösse der Strömungszirkulationszonen und ihrer dazugehörigen Trennungs- und Vereinigungspunkte differieren für jedes Modell. Die Scherspannung an der Wand erfährt ebenfalls eine Verschiebung im Hauptrohr von bis zu 12\%. Der Verlauf der Geschwindigkeit auf den Gitterzellen zeigt, dass das Newtonsche Modell mit Bezug auf die Dynamik dem Casson-Modell nahe ist, während das Potenzgesetzmodell dem Carreau-Modell ähnlich ist. R\#ENTITYSTARTX000E9;SUM\#ENTITYSTARTX000E9;: La rhéologie de l'écoulement sanguin est un phénomène complexe. Présentement, il n'y a pas de consensus universel sur le modèle qui représente la propriété visqueuse du sang. Cependant, parmi la classification générale des modèles non-Newtoniens qui simulent le comportement du sang avec différents degrés de précision, il y a plusieurs différences. Les lois de puissance, les modèles de Casson et Carreau sont des modèles non-Newtoniens populaires et ont un effet sur les quantités hémodynamiques sous plusieurs conditions. Dans cette étude, la méthode de volume fini est utilisée pour explorer les prédictions hémodynamiques de chacun de ces modèles. Pour implémenter la méthode de volume fini, le logiciel de calcul de dynamique des fluides Fluent 6.1 a été utilisé. Dans cette étude numérique, les différents modèles hémorhéologiques tendent à prédire des résultats différents pour les variables hémodynamiques qui sont reconnues comme ayant un impact sur la genèse de l'artériosclérose et de la thrombose. Une différence jusqu'à 2\% dans l'amplitude de la vélocité axiale et une différence jusqu'à 90\% dans la vélocité radiale sont découverts dans différentes sections d'une géométrie de type jonction en T. La taille des zones de re-circulation d'écoulement et les localisations des points de séparation et de rattachement qui leur sont associées, diffèrent pour chacun des modèles. La contrainte de cisaillement aux parois présente également un déplacement de 12\% dans le tube principal. La distribution de l'amplitude de vitesse dans les cellules du maillage montre que le modèle Newtonien est dynamiquement proche du modèle de Casson tandis que le modèle en loi de puissance ressemble au modèle de Carreau.},
	language = {eng},
	number = {6},
	journal = {Applied rheology (Lappersdorf, Germany : Online)},
	author = {Shibeshi, Shewaferaw S and Collins, William E},
	year = {2005},
	pmid = {16932804},
	keywords = {ShibeshiCollins},
	pages = {398--405},
}

@article{UrregoBlanco2016,
	title = {Journal of geophysical research},
	volume = {175},
	issn = {00280836},
	doi = {10.1002/2015JC011558},
	number = {4449},
	journal = {Journal of Geophysical Research: Oceans},
	author = {Urrego-Blanco, Jorge and Urban, Nathan and Hunke, Elizabeth and Turner, Adrian and Jeffery, Nicole},
	year = {2016},
	pages = {238},
}

@article{Zhang2020a,
	title = {Modern {Monte} {Carlo} methods for efficient uncertainty quantification and propagation: {A} survey},
	issn = {19390068},
	doi = {10.1002/wics.1539},
	abstract = {Uncertainty quantification (UQ) includes the characterization, integration, and propagation of uncertainties that result from stochastic variations and a lack of knowledge or data in the natural world. Monte Carlo (MC) method is a sampling-based approach that has widely used for quantification and propagation of uncertainties. However, the standard MC method is often time-consuming if the simulation-based model is computationally intensive. This article gives an overview of modern MC methods to address the existing challenges of the standard MC in the context of UQ. Specifically, multilevel Monte Carlo (MLMC) extending the concept of control variates achieves a significant reduction of the computational cost by performing most evaluations with low accuracy and corresponding low cost, and relatively few evaluations at high accuracy and corresponding high cost. Multifidelity Monte Carlo (MFMC) accelerates the convergence of standard Monte Carlo by generalizing the control variates with different models having varying fidelities and varying computational costs. Multimodel Monte Carlo method (MMMC), having a different setting of MLMC and MFMC, aims to address the issue of UQ and propagation when data for characterizing probability distributions are limited. Multimodel inference combined with importance sampling is proposed for quantifying and efficiently propagating the uncertainties resulting from small data sets. All of these three modern MC methods achieve a significant improvement of computational efficiency for probabilistic UQ, particularly uncertainty propagation. An algorithm summary and the corresponding code implementation are provided for each of the modern MC methods. The extension and application of these methods are discussed in detail. This article is categorized under: Statistical and Graphical Methods of Data Analysis {\textgreater} Monte Carlo Methods Statistical and Graphical Methods of Data Analysis {\textgreater} Sampling.},
	number = {November},
	journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
	author = {Zhang, Jiaxin},
	year = {2020},
	note = {arXiv: 2011.00680},
	keywords = {Monte Carlo methods, multifidelity Monte Carlo, multilevel Monte Carlo, uncertainty propagation, uncertainty quantification, ★},
}

@article{Meng,
	title = {Multi-fidelity {Bayesian} {Neural} {Networks} : {Algorithms} and},
	author = {Meng, Xuhui and Babaee, Hessam and Karniadakis, George Em},
	note = {arXiv: 2012.13294v1},
	pages = {1--31},
}

@article{Meng2020,
	title = {A composite neural network that learns from multi-fidelity data: {Application} to function approximation and inverse {PDE} problems},
	volume = {401},
	issn = {10902716},
	doi = {10.1016/j.jcp.2019.109020},
	abstract = {Currently the training of neural networks relies on data of comparable accuracy but in real applications only a very small set of high-fidelity data is available while inexpensive lower fidelity data may be plentiful. We propose a new composite neural network (NN) that can be trained based on multi-fidelity data. It is comprised of three NNs, with the first NN trained using the low-fidelity data and coupled to two high-fidelity NNs, one with activation functions and another one without, in order to discover and exploit nonlinear and linear correlations, respectively, between the low-fidelity and the high-fidelity data. We first demonstrate the accuracy of the new multi-fidelity NN for approximating some standard benchmark functions but also a 20-dimensional function that is not easy to approximate with other methods, e.g. Gaussian process regression. Subsequently, we extend the recently developed physics-informed neural networks (PINNs) to be trained with multi-fidelity data sets (MPINNs). MPINNs contain four fully-connected neural networks, where the first one approximates the low-fidelity data, while the second and third construct the correlation between the low- and high-fidelity data and produce the multi-fidelity approximation, which is then used in the last NN that encodes the partial differential equations (PDEs). Specifically, by decomposing the correlation into a linear and nonlinear part, the present model is capable of learning both the linear and complex nonlinear correlations between the low- and high-fidelity data adaptively. By training the MPINNs, we can: (1) obtain the correlation between the low- and high-fidelity data, (2) infer the quantities of interest based on a few scattered data, and (3) identify the unknown parameters in the PDEs. In particular, we employ the MPINNs to learn the hydraulic conductivity field for unsaturated flows as well as the reactive models for reactive transport. The results demonstrate that MPINNs can achieve relatively high accuracy based on a very small set of high-fidelity data. Despite the relatively low dimension and limited number of fidelities (two-fidelity levels) for the benchmark problems in the present study, the proposed model can be readily extended to very high-dimensional regression and classification problems involving multi-fidelity data.},
	number = {1},
	journal = {Journal of Computational Physics},
	author = {Meng, Xuhui and Karniadakis, George Em},
	year = {2020},
	note = {arXiv: 1903.00104},
	keywords = {Adversarial data, Multi-fidelity, Physics-informed neural networks, Porous media, Reactive transport},
	pages = {1--29},
}

@article{mckay_comparison_1979,
	title = {A {Comparison} of {Three} {Methods} for {Selecting} {Values} of {Input} {Variables} in the {Analysis} of {Output} from a {Computer} {Code}},
	volume = {21},
	issn = {0040-1706},
	url = {https://www.jstor.org/stable/1268522},
	doi = {10.2307/1268522},
	abstract = {Two types of sampling plans are examined as alternatives to simple random sampling in Monte Carlo studies. These plans are shown to be improvements over simple random sampling with respect to variance for a class of estimators which includes the sample mean and the empirical distribution function.},
	number = {2},
	urldate = {2022-02-04},
	journal = {Technometrics},
	author = {McKay, M. D. and Beckman, R. J. and Conover, W. J.},
	year = {1979},
	note = {Publisher: [Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]},
	pages = {239--245},
}

@article{comon_downloaded_2008,
	title = {Downloaded 09 / 29 / 21 to 35 . 3 . 222 . 225 {Redistribution} subject to {SIAM} license or copyright ; see https://epubs.siam.org/page/terms {Downloaded} 09 / 29 / 21 to 35 . 3 . 222 . 225 {Redistribution} subject to {SIAM} license or copyright ; see https://epubs},
	volume = {30},
	number = {3},
	author = {Comon, Pierre and Golub, Gene and Lim, Lek-heng and Mourrain, Bernard},
	year = {2008},
	keywords = {15a03, 15a18, 15a21, 15a69, 15a72, ams subject classifications, candecomp, decomposition, generic, maximal symmetric rank, multiway arrays, outer product decomposition, parafac, quantics, symmetric outer product, symmetric rank, symmetric tensor rank, tensor rank, tensors},
	pages = {1254--1279},
}

@article{Surana2016,
	title = {Dynamic tensor time series modeling and analysis},
	doi = {10.1109/CDC.2016.7798500},
	abstract = {In this paper we propose a model reduction and identification approach for multilinear dynamical system (MLDS) driven by noise. Compared to standard linear dynamical system based approaches which fit vector or matrix models to tensor time series, MLDS provides more natural, compact and accurate representation of tensorial data with fewer model parameters. The proposed algorithm for identifying MLDS parameters employs techniques from multilinear subspace learning: mulilinear Principal Component Analysis and multilinear regression. In addition compact array normal distribution is used to represent and estimate model error and output noise.We illustrate the benefits of the proposed approach on some real world datasets.},
	number = {Cdc},
	journal = {2016 IEEE 55th Conference on Decision and Control, CDC 2016},
	author = {Surana, Amit and Patterson, Geoff and Rajapakse, Indika},
	year = {2016},
	note = {Publisher: IEEE
ISBN: 9781509018376},
	keywords = {★},
	pages = {1637--1642},
}

@article{Sidiropoulos2017,
	title = {Tensor {Decomposition} for {Signal} {Processing} and {Machine} {Learning}},
	volume = {65},
	issn = {1053587X},
	doi = {10.1109/TSP.2017.2690524},
	abstract = {Tensors or multiway arrays are functions of three or more indices (i,j,k,⋯)-similar to matrices (two-way arrays), which are functions of two indices (r,c) for (row, column). Tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining, and machine learning. This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. As such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth and depth that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. Some background in applied optimization is useful but not strictly required. The material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning.},
	number = {13},
	journal = {IEEE Transactions on Signal Processing},
	author = {Sidiropoulos, Nicholas D. and De Lathauwer, Lieven and Fu, Xiao and Huang, Kejun and Papalexakis, Evangelos E. and Faloutsos, Christos},
	year = {2017},
	note = {arXiv: 1607.01668
Publisher: IEEE},
	keywords = {Cramér-Rao bound, Gauss-Newton, NP-hard problems, Tensor decomposition, Tucker model, alternating direction method of multipliers, alternating optimization, canonical polyadic decomposition (CPD), classification, collaborative filtering, communications, gradient descent, harmonic retrieval, higher-order singular value decomposition (HOSVD), mixture modeling, multilinear singular value decomposition (MLSVD), parallel factor analysis (PARAFAC), rank, source separation, speech separation, stochastic gradient, subspace learning, tensor factorization, topic modeling, uniqueness},
	pages = {3551--3582},
}

@article{Kolda2009,
	title = {Tensor decompositions and applications},
	volume = {51},
	issn = {00361445},
	doi = {10.1137/07070111X},
	abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N-way array. Decompositions of higher-order tensors (i.e., N-way arrays with N ≥ 3) have applications in psychometrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors. © 2009 Society for Industrial and Applied Mathematics.},
	number = {3},
	journal = {SIAM Review},
	author = {Kolda, Tamara G. and Bader, Brett W.},
	year = {2009},
	keywords = {Canonical decomposition (CANDECOMP), Higher-order principal components analysis (Tucker, Higher-order singular value decomposition (HOSVD), Multilinear algebra, Multiway arrays, Parallel factors (PARAFAC), Tensor decompositions, ★},
	pages = {455--500},
}

@inproceedings{Jivani2021,
	title = {Uncertainty quantification for a turbulent round jet using multifidelity karhunen-loève expansions},
	isbn = {978-1-62410-609-5},
	abstract = {© 2021, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved. Understanding the behavior of turbulent jets under variable environment and uncertain conditions is critical for predicting and mitigating aircraft jet noise. However, uncertainty quantification (UQ) for jet noise, which requires repeated expensive eddy-resolving simulations, is often computationally prohibitive. We thus build surrogate models, in particular Karhunen-Loève expansions (KLEs) for field quantities of interest in three-dimensional turbulent round jets. We build them in a multifidelity manner by combining simulation data from high-fidelity enhanced delayed detached-eddy simulation (EDDES) and low-fidelity Reynolds-averaged Navier-Stokes (RANS), generated under uncertain nozzle exit stagnation pressure and inlet eddy viscosity ratio. Furthermore, we form the KLEs in conjunction with polynomial chaos expansions in order to explicitly associate their randomness to each physical source of uncertainty, and so justifying the combining procedure in the multifidelity construct. We illustrate advantages of the new multifidelity KLE against single-fidelity KLEs, with the former achieving more accurate predictions at locations away from existing high-fidelity training data. With the KLE surrogate, we conduct UQ inexpensively.},
	booktitle = {{AIAA} {Scitech} 2021 {Forum}},
	author = {Jivani, A. and Huan, X. and Safta, C. and Zhou, B.Y. and Gauger, N.R.},
	year = {2021},
}

@article{chen_projected_nodate,
	title = {Projected {Stein} {Variational} {Newton} : {A} {Fast} and {Scalable} {Bayesian} {Inference} {Method} in {High} {Dimensions} {arXiv} : 1901 . 08659v2 [ math . {OC} ] 9 {Feb} 2020},
	author = {Chen, Peng and Wu, Keyi and Chen, Joshua and Leary-roseberry, Thomas O and Ghattas, Omar},
	note = {arXiv: 1901.08659v2},
}

@article{hawkins_bayesian_nodate,
	title = {Bayesian {Tensorized} {Neural} {Networks} with {Automatic} {Rank} {Selection}},
	author = {Hawkins, Cole},
	note = {arXiv: 1905.10478v1},
}

@misc{SMITH2014,
	title = {({Computational} {Science} and {Engineering}) {Ralph} {C}. {Smith} - {Uncertainty} {Quantification}\_ {Theory}, {Implementation}, and {Applications}-{SIAM}-{Society} for {Industrial} and {Applied} {Mathematics} (2013).pdf},
	author = {SMITH, RALPH C.},
	year = {2014},
	note = {Pages: 401},
}

@techreport{Oberkampf2002,
	title = {Verification and validation in computational fluid dynamics},
	abstract = {Verification and validation (V\&V) are the primary means to assess accuracy and reliability in computational simulations. This paper presents an extensive review of the literature in V\&V in computational fluid dynamics (CFD), discusses methods and procedures for assessing V\&V, and develops a number of extensions to existing ideas. The review of the development of V\&V terminology and methodology points out the contributions from members of the operations research, statistics, and CFD communities. Fundamental issues in V\&V are addressed, such as code verification versus solution verification, model validation versus solution validation, the distinction between error and uncertainty, conceptual sources of error and uncertainty, and the relationship between validation and prediction. The fundamental strategy of verification is the identification and quantification of errors in the computational model and its solution. In verification activities, the accuracy of a computational solution is primarily measured relative to two types of highly accurate solutions: analytical solutions and highly accurate numerical solutions. Methods for determining the accuracy of numerical solutions are presented and the importance of software testing during verification activities is emphasized. The fundamental strategy of validation is to assess how accurately the computational results compare with the experimental data, with quantified error and uncertainty estimates for both. This strategy employs a hierarchical methodology that segregates and simplifies the physical and coupling phenomena involved in the complex engineering system of interest. A hypersonic cruise missile is used as an example of how this hierarchical structure is formulated. The discussion of validation assessment also encompasses a number of other important topics. A set of guidelines is proposed for designing and conducting validation experiments, supported by an explanation of how validation experiments are different from traditional experiments and testing. A description is given of a relatively new procedure for estimating experimental uncertainty that has proven more effective at estimating random and correlated bias errors in wind-tunnel experiments than traditional methods. Consistent with the authors' contention that nondeterministic simulations are needed in many validation comparisons, a three-step statistical approach is offered for incorporating experimental uncertainties into the computational analysis. The discussion of validation assessment ends with the topic of validation metrics, where two sample problems are used to demonstrate how such metrics should be constructed. In the spirit of advancing the state of the art in V\&V, the paper concludes with recommendations of topics for future research and with suggestions for needed changes in the implementation of V\&V in production and commercial software. r 2002 Published by Elsevier Science Ltd.},
	author = {Oberkampf, William L and Trucano, Timothy G},
	year = {2002},
	note = {Publication Title: Progress in Aerospace Sciences
Volume: 38},
	pages = {209--272},
}

@article{Vickers2019,
	title = {A simple, step-by-step guide to interpreting decision curve analysis},
	volume = {3},
	issn = {2397-7523},
	doi = {10.1186/s41512-019-0064-7},
	abstract = {Background Decision curve analysis is a method to evaluate prediction models and diagnostic tests that was introduced in a 2006 publication. Decision curves are now commonly reported in the literature, but there remains widespread misunderstanding of and confusion about what they mean. Summary of commentary In this paper, we present a didactic, step-by-step introduction to interpreting a decision curve analysis and answer some common questions about the method. We argue that many of the difficulties with interpreting decision curves can be solved by relabeling the y-axis as "benefit" and the x-axis as "preference." A model or test can be recommended for clinical use if it has the highest level of benefit across a range of clinically reasonable preferences. Conclusion Decision curves are readily interpretable if readers and authors follow a few simple guidelines.},
	number = {1},
	journal = {Diagnostic and Prognostic Research},
	author = {Vickers, Andrew J. and van Calster, Ben and Steyerberg, Ewout W.},
	year = {2019},
	pmid = {31592444},
	note = {Publisher: Diagnostic and Prognostic Research
ISBN: 4151201900647},
	keywords = {Net benefit,Decision curve analysis,Educational pa, decision curve analysis, educational paper, net benefit, ★},
	pages = {1--8},
}

@article{DElia2013,
	title = {Coarse-{Grid} {Sampling} {Interpolatory} {Methods} for {Approximating} {Gaussian} {Random} {Fields}},
	volume = {1},
	issn = {2166-2525},
	doi = {10.1137/120883311},
	number = {1},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {D'Elia, Marta and Gunzburger, Max},
	year = {2013},
	keywords = {60g60, 60g99, 62d05, 62j10, 65d05, 65f15, 65n50, ams subject classifications, cholesky factorization, covariance functions, eve expansion, grid-based sampling, interpolation, karhunen, lo, low-rank approximations, piecewise polynomial ap-, proximation, random fields},
	pages = {270--296},
}

@article{Nazarathy2020,
	title = {Statistics with {Julia}: {Fundamentals} for {Data} {Science}, {Machine} {Learning} and {Artificial} {Intelligence}.},
	author = {Nazarathy, Yoni and Klok, Hayden},
	year = {2020},
	keywords = {★},
}

@book{GhanemHandbookUQ2017,
	title = {Handbook of {Uncertainty} {Quantification}},
	isbn = {978-3-319-12384-4},
	author = {Ghanem, Roger and Higdon, David and Owhadi, Houman},
	year = {2017},
	doi = {10.1007/978-3-319-12385-1},
}

@article{Ramadhan2020a,
	title = {Oceananigans . jl : {Fast} and friendly geophysical fluid dynamics on {GPUs}},
	volume = {5},
	doi = {10.1137/141000671},
	author = {Ramadhan, Ali and Wagner, Gregory Leclaire and Hill, Chris and Churavy, Valentin and Besard, Tim and Souza, Andre and Ferrari, Raffaele and Marshall, John},
	year = {2020},
	pages = {2019--2021},
}

@article{Lacaze2017,
	title = {Large {Eddy} {Simulation} of the {HIFiRE} {Direct} {Connect} {Rig} {Scramjet} {Combustor}},
	doi = {10.2514/6.2017-0142},
	number = {January},
	author = {Lacaze, Guilhem and Vane, Zachary P and Oefelein, Joseph C},
	year = {2017},
}

@book{Santner2018,
	title = {The {Design} and {Analysis} of {Computer} {Experiments}},
	author = {Santner, Thomas and Williams, Brian and Notz, William},
	year = {2018},
	note = {Publication Title: Springer},
}

@article{Queipo2005,
	title = {Surrogate-based analysis and optimization},
	volume = {41},
	issn = {03760421},
	doi = {10.1016/j.paerosci.2005.02.001},
	abstract = {A major challenge to the successful full-scale development of modern aerospace systems is to address competing objectives such as improved performance, reduced costs, and enhanced safety. Accurate, high-fidelity models are typically time consuming and computationally expensive. Furthermore, informed decisions should be made with an understanding of the impact (global sensitivity) of the design variables on the different objectives. In this context, the so-called surrogate-based approach for analysis and optimization can play a very valuable role. The surrogates are constructed using data drawn from high-fidelity models, and provide fast approximations of the objectives and constraints at new design points, thereby making sensitivity and optimization studies feasible. This paper provides a comprehensive discussion of the fundamental issues that arise in surrogate-based analysis and optimization (SBAO), highlighting concepts, methods, techniques, as well as practical implications. The issues addressed include the selection of the loss function and regularization criteria for constructing the surrogates, design of experiments, surrogate selection and construction, sensitivity analysis, convergence, and optimization. The multi-objective optimal design of a liquid rocket injector is presented to highlight the state of the art and to help guide future efforts. © 2005 Elsevier Ltd. All rights reserved.},
	number = {1},
	journal = {Progress in Aerospace Sciences},
	author = {Queipo, Nestor V. and Haftka, Raphael T. and Shyy, Wei and Goel, Tushar and Vaidyanathan, Rajkumar and Kevin Tucker, P.},
	year = {2005},
	keywords = {★},
	pages = {1--28},
}

@article{Ng2016,
	title = {Monte {Carlo} information-reuse approach to aircraft conceptual design optimization under uncertainty},
	volume = {53},
	issn = {15333868},
	doi = {10.2514/1.C033352},
	abstract = {This paper develops a multi-information source formulation for aerospace design under uncertainty problems. As a specific demonstration of the approach, it presents the optimization under uncertainty of an advanced subsonic transport aircraft developed to meet the NASA N + 3 goals and shows how the multi-information source approach enables practical turnaround time for this conceptual aircraft optimization under uncertainty problem. In the conceptual design phase, there are often uncertainties about future developments of the underlying technologies. An aircraft design that is robust to uncertainty is more likely to meet performance requirements as the technologies mature in the intermediate and detailed design phases, reducing the need for expensive redesigns. In the particular example selected here to present the new approach, the multi-information source approach uses an information-reuse estimator that takes advantage of the correlation of the aircraft model in the design space to reduce the number of model evaluations needed to achieve a given standard error in the Monte Carlo estimates of the relevant design statistics (mean and variance). Another contribution of the paper is to extend the approach to reuse information during trade studies that involve solving multiple optimization under uncertainty problems, enabling the analysis of the risk-performance tradeoff in optimal aircraft designs.},
	number = {2},
	journal = {Journal of Aircraft},
	author = {Ng, Leo W.T. and Willcox, Karen E.},
	year = {2016},
	pages = {427--438},
}

@article{Gianluca2017,
	title = {A multifidelity multilevel {Monte} {Carlo} method for uncertainty propagation in aerospace applications},
	doi = {10.2514/6.2017-1951},
	abstract = {The accurate evaluation of the performance of complex engineering devices needs to rely on high-fidelity numerical simulations and the systematic characterization and propagation of uncertainties. Several sources of uncertainty may impact the performance of an engineering device through operative conditions, manufacturing tolerances, and even physical models. In the presence of multiphysics systems the number of the uncertain parameters can be fairly large and their propagation through the numerical codes still remains prohibitive because the overall computational budget often allows for only an handful of such high-fidelity realizations. On the other side, common engineering practice can take advantage from a solid history of development and assessment of so called low-fidelity models which albeit less accurate are often capable to at least capture overall trends and parameter dependencies of the system. In this contribution we address the forward propagation of uncertainty parameters relying on statistical estimators built on sequences of numerical and physical discretizations which are provably convergent to the high-fidelity statistics, while exploiting low-fidelity computational models to increase the reliability and confidence in the numerical predictions. The performances of the approaches are presented by means of two fairly complicated aerospace problems, namely the aero-thermo-structural analysis of a turbofan engine nozzle and a flow through a scramjet-like device.},
	number = {January},
	journal = {19th AIAA Non-Deterministic Approaches Conference, 2017},
	author = {Gianluca, Geraci and Eldred, Michael S. and Iaccarino, Gianluca},
	year = {2017},
	note = {ISBN: 9781624104527},
	pages = {1--24},
}

@article{Peherstorfer2016,
	title = {Optimal {Model} {Management} for {Multifidelity} {Monte} {Carlo} {Estimation}},
	volume = {38},
	number = {5},
	author = {Peherstorfer, Benjamin and Willcox, Karen and Gunzburger, M A X},
	year = {2016},
	keywords = {1, 10, 1137, 15m1046472, 65m22, 65n22, ams subject classifications, doi, in, introduction, long and successful history, model reduction, monte carlo simulation, multifidelity, multilevel techniques have a, surrogate modeling},
}

@article{KennedyOHagan2001,
	title = {Bayesian {Calibration} of {Computer} {Models}},
	volume = {63},
	issn = {1369-7412},
	number = {3},
	journal = {Journal of the Royal Statistical Society},
	author = {Kennedy, Marc and O'Hagan, Anthony},
	year = {2001},
	keywords = {★},
	pages = {425--464},
}

@article{Griffiths2003,
	title = {Determining the optimal cross section of beams},
	volume = {78},
	issn = {17593433},
	doi = {10.4203/ccp.78.36},
	abstract = {Constrained shape discovery and optimisation is a difficult engineering problem. In this paper the problem of finding the optimum cross section of a beam is solved using a suitably crafted Genetic Algorithm (GA). Previous work using GAs for this problem has only managed to evolve satisfactory solutions through applying heuristics that operate directly on the genotype. Such heavy guidance of the GA potentially stifles innovation, can only be applied to situations where the correct answer is known and limits the applicability of the search. This research demonstrates the ability of the GA, in its purest form (unbiased and unguided search) to evolve good, near optimal results. Performing an unbiased search, using only the evolutionary process to search for good solutions, allows the GA to be applied where no heuristic knowledge is available. Advanced 2-dimensional genetic operators, in conjunction with a suitably designed fitness function, allow a productive evolutionary search. The initial test case is the evolution of an optimal beam cross-section, subject to several load cases. It is shown that the methods developed lead to consistently good solutions, despite the complexity of the process. © 2003, Civil-Comp Ltd.},
	journal = {Civil-Comp Proceedings},
	author = {Griffiths, D. R. and Miles, J. C.},
	year = {2003},
	note = {ISBN: 094874992X},
	keywords = {Domain knowledge, Genetic algorithms, Heuristics, Optimisation, Shape discovery, Unguided search},
}

@article{Doost2016,
	title = {Heart blood flow simulation: {A} perspective review},
	volume = {15},
	issn = {1475925X},
	doi = {10.1186/s12938-016-0224-8},
	abstract = {Cardiovascular disease (CVD), the leading cause of death today, incorporates a wide range of cardiovascular system malfunctions that affect heart functionality. It is believed that the hemodynamic loads exerted on the cardiovascular system, the left ventricle (LV) in particular, are the leading cause of CVD initiation and propagation. Moreover, it is believed that the diagnosis and prognosis of CVD at an early stage could reduce its high mortality and morbidity rate. Therefore, a set of robust clinical cardiovascular assessment tools has been introduced to compute the cardiovascular hemodynamics in order to provide useful insights to physicians to recognize indicators leading to CVD and also to aid the diagnosis of CVD. Recently, a combination of computational fluid dynamics (CFD) and different medical imaging tools, image-based CFD (IB-CFD), has been widely employed for cardiovascular functional assessment by providing reliable hemodynamic parameters. Even though the capability of CFD to provide reliable flow dynamics in general fluid mechanics problems has been widely demonstrated for many years, up to now, the clinical implications of the IB-CFD patient-specific LVs have not been applicable due to its limitations and complications. In this paper, we review investigations conducted to numerically simulate patient-specific human LV over the past 15 years using IB-CFD methods. Firstly, we divide different studies according to the different LV types (physiological and different pathological conditions) that have been chosen to reconstruct the geometry, and then discuss their contributions, methodologies, limitations, and findings. In this regard, we have studied CFD simulations of intraventricular flows and related cardiology insights, for (i) Physiological patient-specific LV models, (ii) Pathological heart patient-specific models, including myocardial infarction, dilated cardiomyopathy, hypertrophic cardiomyopathy and hypoplastic left heart syndrome. Finally, we discuss the current stage of the IB-CFD LV simulations in order to mimic realistic hemodynamics of patient-specific LVs. We can conclude that heart flow simulation is on the right track for developing into a useful clinical tool for heart function assessment, by (i) incorporating most of heart structures' (such as heart valves) operations, and (ii) providing useful diagnostic indices based hemodynamic parameters, for routine adoption in clinical usage.},
	number = {1},
	journal = {BioMedical Engineering Online},
	author = {Doost, Siamak N. and Ghista, Dhanjoo and Su, Boyang and Zhong, Liang and Morsi, Yosry S.},
	year = {2016},
	pmid = {27562639},
	note = {Publisher: BioMed Central
ISBN: 1293801602248},
	keywords = {Cardiovascular diseases (CVDs), Computational fluid dynamics (CFD), Fluid structure interaction (FSI), Hemodynamics, Left ventricle (LV)},
	pages = {1--28},
}

@article{Hausfather2020,
	title = {Evaluating the {Performance} of {Past} {Climate} {Model} {Projections}},
	volume = {47},
	issn = {19448007},
	doi = {10.1029/2019GL085378},
	abstract = {Retrospectively comparing future model projections to observations provides a robust and independent test of model skill. Here we analyze the performance of climate models published between 1970 and 2007 in projecting future global mean surface temperature (GMST) changes. Models are compared to observations based on both the change in GMST over time and the change in GMST over the change in external forcing. The latter approach accounts for mismatches in model forcings, a potential source of error in model projections independent of the accuracy of model physics. We find that climate models published over the past five decades were skillful in predicting subsequent GMST changes, with most models examined showing warming consistent with observations, particularly when mismatches between model-projected and observationally estimated forcings were taken into account.},
	number = {1},
	journal = {Geophysical Research Letters},
	author = {Hausfather, Zeke and Drake, Henri F. and Abbott, Tristan and Schmidt, Gavin A.},
	year = {2020},
	pages = {1--10},
}

@article{Ramadhan2020,
	title = {Capturing missing physics in climate model parameterizations using neural differential equations},
	url = {http://arxiv.org/abs/2010.12559},
	abstract = {Even with today's immense computational resources, climate models cannot resolve every cloud in the atmosphere or eddying swirl in the ocean. However, collectively these small-scale turbulent processes play a key role in setting Earth's climate. Climate models attempt to represent unresolved scales via surrogate models known as parameterizations. These have limited fidelity and can exhibit structural deficiencies. Here we demonstrate that neural differential equations (NDEs) may be trained by highly resolved fluid-dynamical models of the scales to be parameterized and those NDEs embedded in an ocean model. They can incorporate conservation laws and are stable in time. We argue that NDEs provide a new route forward to the development of surrogate models for climate science, opening up exciting new opportunities.},
	author = {Ramadhan, Ali and Marshall, John and Souza, Andre and Wagner, Gregory LeClaire and Ponnapati, Manvitha and Rackauckas, Christopher},
	year = {2020},
	note = {arXiv: 2010.12559},
}

@article{Marzouk2009,
	title = {A stochastic collocation approach to {Bayesian} inference in inverse problems},
	volume = {6},
	issn = {19917120},
	doi = {10.4208/cicp.2009.v6.p826},
	abstract = {We present an efficient numerical strategy for the Bayesian solution of inverse problems. Stochastic collocation methods, based on generalized polynomial chaos (gPC), are used to construct a polynomial approximation of the forward solution over the support of the prior distribution. This approximation then defines a surrogate posterior probability density that can be evaluated repeatedly at minimal computational cost. The ability to simulate a large number of samples from the posterior distribution results in very accurate estimates of the inverse solution and its associated uncertainty. Combined with high accuracy of the gPC-based forward solver, the new algorithm can provide great efficiency in practical applications. A rigorous error analysis of the algorithm is conducted, where we establish convergence of the approximate posterior to the true posterior and obtain an estimate of the convergence rate. It is proved that fast (exponential) convergence of the gPC forward solution yields similarly fast (exponential) convergence of the posterior. The numerical strategy and the predicted convergence rates are then demonstrated on nonlinear inverse problems of varying smoothness and dimension. © 2009 Global-Science Press.},
	number = {4},
	journal = {Communications in Computational Physics},
	author = {Marzouk, Youssef and Xiu, Dongbin},
	year = {2009},
	keywords = {Bayesian inference, Generalized polynomial chaos, Inverse problems, Stochastic collocation, Uncertainty quantification},
	pages = {826--847},
}

@article{Gramacy2015,
	title = {Local {Gaussian} {Process} {Approximation} for {Large} {Computer} {Experiments}},
	volume = {24},
	issn = {15372715},
	doi = {10.1080/10618600.2014.914442},
	abstract = {We provide a new approach to approximate emulation of large computer experiments. By focusing expressly on desirable properties of the predictive equations, we derive a family of local sequential design schemes that dynamically define the support of a Gaussian process predictor based on a local subset of the data. We further derive expressions for fast sequential updating of all needed quantities as the local designs are built up iteratively. Then we show how independent application of our local design strategy across the elements of a vast predictive grid facilitates a trivially parallel implementation. The end result is a global predictor able to take advantage of modern multicore architectures, providing a nonstationary modeling feature as a bonus. We demonstrate our method on two examples using designs with thousands of data points, and compare to the method of compactly supported covariances. Supplementary materials for this article are available online.},
	number = {2},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Gramacy, Robert B. and Apley, Daniel W.},
	year = {2015},
	note = {arXiv: 1303.0383},
	keywords = {Active learning, Compactly supported, Covariance, Emulator, Local kriging neighborhoods, Sequential design, Sequential updating, Surrogate model},
	pages = {561--578},
}

@article{Ghanem2019,
	title = {Design optimization of a scramjet under uncertainty using probabilistic learning on manifolds},
	volume = {399},
	issn = {10902716},
	url = {https://doi.org/10.1016/j.jcp.2019.108930},
	doi = {10.1016/j.jcp.2019.108930},
	abstract = {We demonstrate, on a scramjet combustion problem, a constrained probabilistic learning approach that augments physics-based datasets with realizations that adhere to underlying constraints and scatter. The constraints are captured and delineated through diffusion maps, while the scatter is captured and sampled through a projected stochastic differential equation. The objective function and constraints of the optimization problem are then efficiently framed as non-parametric conditional expectations. Different spatial resolutions of a large-eddy simulation filter are used to explore the robustness of the model to the training dataset and to gain insight into the significance of spatial resolution on optimal design.},
	journal = {Journal of Computational Physics},
	author = {Ghanem, R. G. and Soize, C. and Safta, C. and Huan, X. and Lacaze, G. and Oefelein, J. C. and Najm, H. N.},
	year = {2019},
	note = {Publisher: Elsevier Inc.},
	keywords = {Diffusion maps, Machine learning, Optimization under uncertainty, Sampling on manifolds, Scramjet simulations, Uncertainty quantification},
	pages = {108930},
}

@article{Zhou2019a,
	title = {Towards a real-time in-flight ice detection system via computational aeroacoustics and bayesian neural networks},
	abstract = {In-flight rotor icing presents a serious problem in the operation of rotorcraft in cold climates, as complex ice shapes can significantly degrade the aerodynamic performance and handling characteristics of rotorcraft. Reliable real-time detection of ice formation is a critical enabling technology in improving rotorcraft safety. In this work, we propose a novel approach towards developing a real-time in-flight ice detection system using computational aeroacoustics and Bayesian neural networks. In particular, an icing simulation code based on a fully unsteady collection efficiency approach is coupled with the aeroacoustic solver available in the open-source software suite SU2, in order to compute far-field noise signatures corresponding to discrete iced rotor blades in various icing conditions. Additionally, Bayesian neural networks are constructed from the dataset thus generated to enable rapid predictions together with uncertainty information, of aerodynamic performance indicators from acoustic signal, as a first step in developing an in-flight ice detection and warning system.},
	journal = {AIAA Aviation 2019 Forum},
	author = {Zhou, Beckett Y. and Gauger, Nicolas R. and Morelli, Myles and Guardone, Alberto and Hauth, Jeremiah and Huan, Xun},
	year = {2019},
	note = {ISBN: 9781624105890},
	pages = {1--15},
}

@article{Wang2020b,
	title = {A perspective on regression and {Bayesian} approaches for system identification of pattern formation dynamics},
	volume = {10},
	issn = {20950349},
	url = {http://dx.doi.org/10.1016/j.taml.2020.01.028},
	doi = {10.1016/j.taml.2020.01.028},
	abstract = {We present two approaches to system identification, i.e. the identification of partial differential equations (PDEs) from measurement data. The first is a regression-based variational system identification procedure that is advantageous in not requiring repeated forward model solves and has good scalability to large number of differential operators. However it has strict data type requirements needing the ability to directly represent the operators through the available data. The second is a Bayesian inference framework highly valuable for providing uncertainty quantification, and flexible for accommodating sparse and noisy data that may also be indirect quantities of interest. However, it also requires repeated forward solutions of the PDE models which is expensive and hinders scalability. We provide illustrations of results on a model problem for pattern formation dynamics, and discuss merits of the presented methods.},
	number = {3},
	journal = {Theoretical and Applied Mechanics Letters},
	author = {Wang, Zhenlin and Wu, Bowei and Garikipati, Krishna and Huan, Xun},
	year = {2020},
	note = {arXiv: 2001.05646
Publisher: The Authors. Published by Elsevier Ltd on behalf of The Chinese Society of Theoretical and Applied Mechanics},
	keywords = {Bayesian inference, Computational mechanics, Inverse problem, Materials physics, Pattern formation},
	pages = {188--194},
}

@article{Denamiel2019,
	title = {Stochastic {Surrogate} {Model} for {Meteotsunami} {Early} {Warning} {System} in the {Eastern} {Adriatic} {Sea}},
	volume = {124},
	issn = {21699291},
	doi = {10.1029/2019JC015574},
	abstract = {The meteotsunami early warning system prototype using stochastic surrogate approach and running operationally in the eastern Adriatic Sea is presented. First, the atmospheric internal gravity waves (IGWs) driving the meteotsunamis are either forecasted with state-of-the-art deterministic models at least a day in advance or detected through measurements at least 2 hr before the meteotsunami reaches sensitive locations. The extreme sea-level hazard forecast at endangered locations is then derived with an innovative stochastic surrogate model—implemented with generalized polynomial chaos expansion (gPCE) method and synthetic IGWs forcing a barotropic ocean model—used with the input parameters extracted from deterministic model results and/or measurements. The evaluation of the system, both against five historical events and for all the detected potential meteotsunamis since late 2018 when the early warning system prototype became operational, reveals that the meteotsunami hazard is conservatively assessed but often overestimated at some locations. Despite some needed improvements and developments, this study demonstrates that gPCE-based methods can be used for atmospherically driven extreme sea-level hazard assessment and in geosciences in wide.},
	number = {11},
	journal = {Journal of Geophysical Research: Oceans},
	author = {Denamiel, Cléa and Šepić, Jadranka and Huan, Xun and Bolzer, Célia and Vilibić, Ivica},
	year = {2019},
	keywords = {eastern Adriatic, extreme sea-level hazard assessment, meteotsunami early warning system},
	pages = {8485--8499},
}

@article{Bayarri2015,
	title = {Probabilistic quantification of hazards: {A} methodology using small ensembles of physics-based simulations and statistical surrogates},
	volume = {5},
	issn = {21525099},
	doi = {10.1615/Int.J.UncertaintyQuantification.2015011451},
	abstract = {This paper presents a novel approach to assessing the hazard threat to a locale due to a large volcanic avalanche. The methodology combines: (i) mathematical modeling of volcanic mass flows; (ii) field data of avalanche frequency, volume, and runout; (iii) large-scale numerical simulations of flow events; (iv) use of statistical methods to minimize computational costs, and to capture unlikely events; (v) calculation of the probability of a catastrophic flow event over the next T years at a location of interest; and (vi) innovative computational methodology to implement these methods. This unified presentation collects elements that have been separately developed, and incorporates new contributions to the process. The field data and numerical simulations used here are subject to uncertainty from many sources, uncertainties that must be properly accounted for in assessing the hazard. The methodology presented here will be demonstrated with data from the Soufrière Hills Volcano on the island of Montserrat, where there is a relatively complete record of volcanic mass flows from the past 15 years. This methodology can be transferred to other volcanic sites with similar characteristics and where sparse historical data have prevented such high-quality analysis. More generally, the core of this methodology is widely applicable and can be used for other hazard scenarios, such as floods or ash plumes.},
	number = {4},
	journal = {International Journal for Uncertainty Quantification},
	author = {Bayarri, M. J. and Berger, J. O. and Calder, E. S. and Patra, A. K. and Pitman, E. B. and Spiller, E. T. and Wolpert, Robert L.},
	year = {2015},
	keywords = {Computer modeling, Hazard assessment, Statistics, Uncertainty, Volcanic hazards},
	pages = {297--325},
}

@article{Debusschere2005,
	title = {Numerical challenges in the use of polynomial chaos representations for stochastic processes},
	volume = {26},
	issn = {10648275},
	doi = {10.1137/S1064827503427741},
	abstract = {This paper gives an overview of the use of polynomial chaos (PC) expansions to represent stochastic processes in numerical simulations. Several methods are presented for performing arithmetic on, as well as for evaluating polynomial and nonpolynomial functions of variables represented by PC expansions. These methods include Taylor series, a newly developed integration method, as well as a sampling-based spectral projection method for nonpolynomial function evaluations. A detailed analysis of the accuracy of the PC representations, and of the different methods for nonpolynomial function evaluations, is performed. It is found that the integration method offers a robust and accurate approach for evaluating nonpolynomial functions, even when very high-order information is present in the PC expansions. © 2004 Society for Industrial and Applied Mathematics.},
	number = {2},
	journal = {SIAM Journal on Scientific Computing},
	author = {Debusschere, Bert J. and Najm, Habib N. and Pébayt, Philippe P. and Knio, Omar M. and Ghanem, Roger G. and Le Maître, Olivier P.},
	year = {2005},
	keywords = {Polynomial chaos, Spectral uncertainty quantification, Stochastic, ★},
	pages = {698--719},
}

@article{Giles2008,
	title = {Multilevel {Monte} {Carlo} path simulation},
	volume = {56},
	issn = {0030364X},
	doi = {10.1287/opre.1070.0496},
	abstract = {We show that multigrid ideas can be used to reduce the computational complexity of estimating an expected value arising from a stochastic differential equation using Monte Carlo path simulations. In the simplest case of a Lipschitz payoff and a Euler discretisation, the computational cost to achieve an accuracy of O(ε) is reduced from O(ε-3) to O(ε-2(logε)2). The analysis is supported, by numerical results showing significant computational savings. © 2008 INFORMS.},
	number = {3},
	journal = {Operations Research},
	author = {Giles, Michael B.},
	year = {2008},
	keywords = {Analysis of algorithms: Computational complexity, Finance, Simulation: Efficiency. Area of review : Financial, ★},
	pages = {607--617},
}

@article{Economon2016,
	title = {{SU2}: {An} open-source suite for multiphysics simulation and design},
	volume = {54},
	issn = {00011452},
	doi = {10.2514/1.J053813},
	abstract = {This paper presents the main objectives and a description of the SU2 suite, including the novel software architecture and open-source software engineering strategy. SU2 is a computational analysis and design package that has been developed to solve multiphysics analysis and optimization tasks using unstructured mesh topologies. Its unique architecture iswell suited for extensibility to treat partial-differential-equation-based problems not initially envisioned. The common framework adopted enables the rapid implementation of newphysics packages that can be tightly coupled to form a powerful ensemble of analysis tools to address complex problems facing many engineering communities. The framework is demonstrated on a number, solving both the flow and adjoint systems of equations to provide a highfidelity predictive capability and sensitivity information that can be used for optimal shape design using a gradientbased framework, goal-oriented adaptive mesh refinement, or uncertainty quantification.},
	number = {3},
	journal = {AIAA Journal},
	author = {Economon, Thomas D. and Palacios, Francisco and Copeland, Sean R. and Lukaczyk, Trent W. and Alonso, Juan J.},
	year = {2016},
	keywords = {★},
	pages = {828--846},
}

@article{Gorodetsky2020,
	title = {A generalized approximate control variate framework for multifidelity uncertainty quantification},
	volume = {408},
	issn = {10902716},
	url = {https://doi.org/10.1016/j.jcp.2020.109257},
	doi = {10.1016/j.jcp.2020.109257},
	abstract = {We describe and analyze a variance reduction approach for Monte Carlo (MC) sampling that accelerates the estimation of statistics of computationally expensive simulation models using an ensemble of models with lower cost. These lower cost models — which are typically lower fidelity with unknown statistics — are used to reduce the variance in statistical estimators relative to a MC estimator with equivalent cost. We derive the conditions under which our proposed approximate control variate framework recovers existing multifidelity variance reduction schemes as special cases. We demonstrate that existing recursive/nested strategies are suboptimal because they use the additional low-fidelity models only to efficiently estimate the unknown mean of the first low-fidelity model. As a result, they cannot achieve variance reduction beyond that of a control variate estimator that uses a single low-fidelity model with known mean. However, there often exists about an order-of-magnitude gap between the maximum achievable variance reduction using all low-fidelity models and that achieved by a single low-fidelity model with known mean. We show that our proposed approach can exploit this gap to achieve greater variance reduction by using non-recursive sampling schemes. The proposed strategy reduces the total cost of accurately estimating statistics, especially in cases where only low-fidelity simulation models are accessible for additional evaluations. Several analytic examples and an example with a hyperbolic PDE describing elastic wave propagation in heterogeneous media are used to illustrate the main features of the methodology.},
	journal = {Journal of Computational Physics},
	author = {Gorodetsky, Alex A. and Geraci, Gianluca and Eldred, Michael S. and Jakeman, John D.},
	year = {2020},
	note = {arXiv: 1811.04988
Publisher: Elsevier Inc.},
	keywords = {Control variates, Monte Carlo, Multifidelity modeling, Variance reduction},
	pages = {109257},
}

@article{Peherstorfer2018,
	title = {Survey of multifidelity methods in uncertainty propagation, inference, and optimization},
	volume = {60},
	issn = {00361445},
	doi = {10.1137/16M1082469},
	abstract = {In many situations across computational science and engineering, multiple computational models are available that describe a system of interest. These different models have varying evaluation costs and varying fidelities. Typically, a computationally expensive high-fidelity model describes the system with the accuracy required by the current application at hand, while lower-fidelity models are less accurate but computationally cheaper than the high-fidelity model. Outer-loop applications, such as optimization, inference, and uncertainty quantification, require multiple model evaluations at many different inputs, which often leads to computational demands that exceed available resources if only the high-fidelity model is used. This work surveys multifidelity methods that accelerate the solution of outer-loop applications by combining high-fidelity and low-fidelity model evaluations, where the low-fidelity evaluations arise from an explicit low-fidelity model (e.g., a simplified physics approximation, a reduced model, a data-fit surrogate) that approximates the same output quantity as the high-fidelity model. The overall premise of these multifidelity methods is that low-fidelity models are leveraged for speedup while the high-fidelity model is kept in the loop to establish accuracy and/or convergence guarantees. We categorize multifidelity methods according to three classes of strategies: adaptation, fusion, and filtering. The paper reviews multifidelity methods in the outer-loop contexts of uncertainty propagation, inference, and optimization.},
	number = {3},
	journal = {SIAM Review},
	author = {Peherstorfer, Benjamin and Willcox, Karen and Gunzburger, Max},
	year = {2018},
	note = {arXiv: 1806.10761},
	keywords = {Model reduction, Multifidelity, Multifidelity optimization, Multifidelity statistical inference, Multifidelity uncertainty propagation, Multifidelity uncertainty quantification, Surrogate models, ★},
	pages = {550--591},
}

@article{Sun2019,
	title = {Synthesizing simulation and field data of solar irradiance},
	volume = {12},
	issn = {19321872},
	doi = {10.1002/sam.11414},
	abstract = {Predicting the intensity and amount of sunlight as a function of location and time is an essential component in identifying promising locations for economical solar farming. Although weather models and irradiance data are relatively abundant, these have yet, to our knowledge, been hybridized on a continental scale. Rather, much of the emphasis in the literature has been on short-term localized forecasting. This is probably because the amount of data involved in a more global analysis is prohibitive with the canonical toolkit, via the Gaussian process (GP). Here we show how GP surrogate and discrepancy models can be combined to tractably and accurately predict solar irradiance on time-aggregated and daily scales with measurements at thousands of sites across the continental United States. Our results establish short-term accuracy of bias-corrected weather-based simulation of irradiance, when realizations are available in real space-time (eg, in future days), and provide accurate surrogates for smoothing in the more common situation where reliable weather data is not available (eg, in future years).},
	number = {4},
	journal = {Statistical Analysis and Data Mining},
	author = {Sun, Furong and Gramacy, Robert B. and Haaland, Benjamin and Lu, Siyuan and Hwang, Youngdeok},
	year = {2019},
	note = {arXiv: 1806.05131},
	keywords = {approximate kriging, calibration, inverse-variance weighting, nonparametric regression, space-filling design, surrogate modeling, ★},
	pages = {311--324},
}

@article{Renganathan2019,
	title = {Aerodynamic data fusion towards the digital twin paradigm},
	volume = {58},
	issn = {23318422},
	abstract = {We consider the fusion of two aerodynamic data sets originating from differing fidelity physical or computer experiments. We specifically address the fusion of: 1) noisy and incomplete fields from wind tunnel measurements and 2) deterministic but biased fields from numerical simulations. These two data sources are fused in order to estimate the true field that best matches measured quantities that serves as the ground truth. For example, two sources of pressure fields about an aircraft are fused based on measured forces and moments from a wind-tunnel experiment. A fundamental challenge in this problem is that the true field is unknown and can not be estimated with 100\% certainty. We employ a Bayesian framework to infer the true fields conditioned on measured quantities of interest; essentially we perform a statistical correction to the data. The fused data may then be used to construct more accurate surrogate models suitable for early stages of aerospace design. We also introduce an extension of the Proper Orthogonal Decomposition with constraints to solve the same problem. Both methods are demonstrated on fusing the pressure distributions for flow past the RAE2822 airfoil and the Common Research Model wing at transonic conditions. Comparison of both methods reveal that the Bayesian method is more robust when data is scarce while capable of also accounting for uncertainties in the data. Furthermore, given adequate data, the POD based and Bayesian approaches lead to similar results.},
	number = {9},
	journal = {arXiv},
	author = {Renganathan, S. Ashwin and Harada, Kohei and Mavris, Dimitri N.},
	year = {2019},
	keywords = {★},
}

@article{Granados-Ortiz2019,
	title = {On the influence of uncertainty in computational simulations of a high-speed jet flow from an aircraft exhaust},
	volume = {180},
	issn = {00457930},
	url = {https://doi.org/10.1016/j.compfluid.2018.12.003},
	doi = {10.1016/j.compfluid.2018.12.003},
	abstract = {A classic approach to Computational Fluid Dynamics (CFD) is to perform simulations with a fixed set of variables in order to account for parameters and boundary conditions. However, experiments and real-life performance are subject to variability in their conditions. In recent years, the interest of performing simulations under uncertainty is increasing, but this is not yet a common rule, and simulations with lack of information are still taking place. This procedure could be missing details such as whether sources of uncertainty affect dramatic parts in the simulation of the flow. One of the reasons of avoiding to quantify uncertainties is that they usually require to run an unaffordable number of CFD simulations to develop the study. To face this problem, Non-Intrusive Uncertainty Quantification (UQ) has been applied to 3D Reynolds-Averaged Navier-Stokes simulations of an under-expanded jet from an aircraft exhaust with the Spalart-Allmaras turbulent model, in order to assess the impact of inaccuracies and quality in the simulation. To save a large number of computations, sparse grids are used to compute the integrals and built surrogates for UQ. Results show that some regions of the jet plume can be more sensitive than others to variance in both physical and turbulence model parameters. The Spalart-Allmaras turbulent model is demonstrated to have an accurate performance with respect to other turbulent models in RANS, LES and experimental data, and the contribution of a large variance in its parameter is analysed. This investigation explicitly outlines, exhibits and proves the details of the relationship between diverse sources of input uncertainty, the sensitivity of different quantities of interest to said uncertainties and the spatial distribution arising due to their propagation in the simulation of the high-speed jet flow. This analysis represents first numerical study that provides evidence for this heuristic observation.},
	journal = {Computers and Fluids},
	author = {Granados-Ortiz, Francisco Javier and Arroyo, Carlos Pérez and Puigt, Guillaume and Lai, Choi Hong and Airiau, Christophe},
	year = {2019},
	note = {Publisher: Elsevier Ltd},
	keywords = {CFD, Jets, Kriging, Polynomial chaos, RANS, Uncertainty quantification},
	pages = {139--158},
}

@article{Geraci2014,
	title = {Decomposing high-order statistics for sensitivity analysis},
	number = {April 2015},
	journal = {Center for Turbulence Research annual Briefs 2014},
	author = {Geraci, Gianluca and Congedo, Pietro Marco and Iaccarino, Gianluca},
	year = {2014},
	keywords = {★},
}

@article{Wang2020a,
	title = {System inference for the spatio-temporal evolution of infectious diseases: {Michigan} in the time of {COVID}-19},
	volume = {66},
	issn = {14320924},
	url = {https://doi.org/10.1007/s00466-020-01894-2},
	doi = {10.1007/s00466-020-01894-2},
	abstract = {We extend the classical SIR model of infectious disease spread to account for time dependence in the parameters, which also include diffusivities. The temporal dependence accounts for the changing characteristics of testing, quarantine and treatment protocols, while diffusivity incorporates a mobile population. This model has been applied to data on the evolution of the COVID-19 pandemic in the US state of Michigan. For system inference, we use recent advances; specifically our framework for Variational System Identification (Wang et al. in Comput Methods Appl Mech Eng 356:44–74, 2019; arXiv:2001.04816 [cs.CE]) as well as Bayesian machine learning methods.},
	number = {5},
	journal = {Computational Mechanics},
	author = {Wang, Z. and Zhang, X. and Teichert, G. H. and Carrasco-Teja, M. and Garikipati, K.},
	year = {2020},
	note = {arXiv: 2007.00865
Publisher: Springer Berlin Heidelberg},
	keywords = {Compartmental models, Epidemiology, Inverse problems, Neural networks, Optimization, ★},
	pages = {1153--1176},
}

@incollection{doi:10.2514/6.2008-4746,
	title = {Modeling {Swirling} {Jet} {Flows} {Using} a {Hybrid} {RANS}/{LES} {Methodology}},
	url = {https://arc.aiaa.org/doi/abs/10.2514/6.2008-4746},
	booktitle = {44th {AIAA}/{ASME}/{SAE}/{ASEE} {Joint} {Propulsion} {Conference} \&amp; {Exhibit}},
	author = {Chenoweth, James and Kannepalli, Chandrasekhar and Arunajatesan, Srinivasan and Hosangadi, Ashvin},
	doi = {10.2514/6.2008-4746},
}

@article{Udell2019,
	title = {Why {Are} {Big} {Data} {Matrices} {Approximately} {Low} {Rank}?},
	volume = {1},
	doi = {10.1137/18m1183480},
	abstract = {Matrices of (approximate) low rank are pervasive in data science, appearing in recommender systems, movie preferences, topic models, medical records, and genomics. While there is a vast literature on how to exploit low rank structure in these datasets, there is less attention on explaining why the low rank structure appears in the first place. Here, we explain the effectiveness of low rank models in data science by considering a simple generative model for these matrices: we suppose that each row or column is associated to a (possibly high dimensional) bounded latent variable, and entries of the matrix are generated by applying a piecewise analytic function to these latent variables. These matrices are in general full rank. However, we show that we can approximate every entry of an \$m {\textbackslash}times n\$ matrix drawn from this model to within a fixed absolute error by a low rank matrix whose rank grows as \${\textbackslash}mathcal O({\textbackslash}log(m + n))\$. Hence any sufficiently large matrix from such a latent variable model can be approximated, up to a small entrywise error, by a low rank matrix.},
	number = {1},
	journal = {SIAM Journal on Mathematics of Data Science},
	author = {Udell, Madeleine and Townsend, Alex},
	year = {2019},
	note = {arXiv: 1705.07474},
	keywords = {1, 10, 1137, 15a03, 18m1183480, 68p99, ams subject classifications, big data, doi, introduction, johnson--lindenstrauss lemma, low rank matrices, low rank matrices appear, numerous tech-, throughout the sciences},
	pages = {144--160},
}

@article{Champion2019,
	title = {Data-driven discovery of coordinates and governing equations},
	volume = {116},
	issn = {10916490},
	doi = {10.1073/pnas.1906995116},
	abstract = {The discovery of governing equations from scientific data has the potential to transform data-rich fields that lack well-characterized quantitative descriptions. Advances in sparse regression are currently enabling the tractable identification of both the structure and parameters of a nonlinear dynamical system from data. The resulting models have the fewest terms necessary to describe the dynamics, balancing model complexity with descriptive ability, and thus promoting interpretability and generalizability. This provides an algorithmic approach to Occam’s razor for model discovery. However, this approach fundamentally relies on an effective coordinate system in which the dynamics have a simple representation. In this work, we design a custom deep autoencoder network to discover a coordinate transformation into a reduced space where the dynamics may be sparsely represented. Thus, we simultaneously learn the governing equations and the associated coordinate system. We demonstrate this approach on several example high-dimensional systems with low-dimensional behavior. The resulting modeling framework combines the strengths of deep neural networks for flexible representation and sparse identification of nonlinear dynamics (SINDy) for parsimonious models. This method places the discovery of coordinates and models on an equal footing.},
	number = {45},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Champion, Kathleen and Lusch, Bethany and Nathan Kutz, J. and Brunton, Steven L.},
	year = {2019},
	pmid = {31636218},
	note = {arXiv: 1904.02107},
	keywords = {Deep learning, Dynamical systems, Machine learning, Model discovery},
	pages = {22445--22451},
}

@article{Donoho2000,
	title = {High-dimensional data analysis: {The} curses and blessings of dimensionality},
	abstract = {The coming century is surely the century of data. A combination of blind faith and serious purpose makes our society invest massively in the collection and processing of data of all kinds, on scales unimaginable until recently. Hyperspectral Imagery, Internet Portals, Financial tick-by-tick data, and DNA Microarrays are just a few of the better- known sources, feeding data in torrential streams into scientific and business databases worldwide. In traditional statistical data analysis, we think of observations of instances of par- ticular phenomena (e.g. instance ↔ human being), these observations being a vector of values we measured on several variables (e.g. blood pressure, weight, height, ...). In traditional statistical methodology, we assumed many observations and a few, well- chosen variables. The trend today is towards more observations but even more so, to radically larger numbers of variables – voracious, automatic, systematic collection of hyper-informative detail about each observed instance. We are seeing examples where the observations gathered on individual instances are curves, or spectra, or images, or even movies, so that a single observation has dimensions in the thousands or billions, while there are only tens or hundreds of instances available for study. Classical methods are simply not designed to cope with this kind of explosive growth of dimensionality of the observation vector. We can say with complete confidence that in the coming cen- tury, high-dimensional data analysis will be a very significant activity, and completely new methods of high-dimensional data analysis will be developed; we just don’t know what they are yet. Mathematicians are ideally prepared for appreciating the abstract issues involved in finding patterns in such high-dimensional data. Two of the most influential prin- ciples in the coming century will be principles originally discovered and cultivated by mathematicians: the blessings of dimensionality and the curse of dimensionality. The curse of dimensionality is a phrase used by several subfields in the mathematical sciences; I use it here to refer to the apparent intractability of systematically searching through a high-dimensional space, the apparent intractability of accurately approxi- mating a general high-dimensional function, the apparent intractability of integrating a high-dimensional function. The blessings of dimensionality are less widely noted, but they include the concen- tration of measure phenomenon (so-called in the geometry of Banach spaces), which means that certain random fluctuations are very well controlled in high dimensions and the success of asymptotic methods, used widely in mathematical statistics and statistical physics, which suggest that statements about very high-dimensional settings may be made where moderate dimensions would be too complicated. There is a large body of interesting work going on in the mathematical sciences, both to attack the curse of dimensionality in specific ways, and to extend the benefits of dimensionality. I will mention work in high-dimensional approximation theory, in probability theory, and in mathematical statistics. I expect to see in the coming decades many further mathematical elaborations to our inventory of Blessings and Curses, and I expect such contributions to have a broad impact on society’s ability to extract meaning from the massive datasets it has decided to compile. In my talk, I will also draw on my personal research experiences which suggest to me (1) there are substantial chances that by interpreting ongoing development in high-dimensional data analysis, mathematicians can become aware of new problems in harmonic analysis; and (2) that many of the problems of data analysis even in fairly low dimensions are unsolved and are similar to problems in mathematics which have only recently been attacked, and for which only the merest beginnings have been made. Both fields can progress together.},
	journal = {AMS Math Challenges Lecture},
	author = {Donoho, Dl L and {others}},
	year = {2000},
	keywords = {Data Mining. Multivariate Data Analysis. Principal},
	pages = {1--32},
}

@article{Sesia2020,
	title = {A comparison of some conformal quantile regression methods},
	volume = {9},
	issn = {2049-1573},
	doi = {10.1002/sta4.261},
	abstract = {We compare two recently proposed methods that combine ideas from conformal inference and quantile regression to produce locally adaptive and marginally valid prediction intervals under sample exchangeability (Romano et al., 2019; Kivaranovic et al., 2019). First, we prove that these two approaches are asymptotically efficient in large samples, under some additional assumptions. Then we compare them empirically on simulated and real data. Our results demonstrate that the method in Romano et al. (2019) typically yields tighter prediction intervals in finite samples. Finally, we discuss how to tune these procedures by fixing the relative proportions of observations used for training and conformalization.},
	number = {1},
	journal = {Stat},
	author = {Sesia, Matteo and Candès, Emmanuel J.},
	year = {2020},
	note = {arXiv: 1909.05433},
	pages = {1--20},
}

@article{McInnes2018,
	title = {{UMAP}: {Uniform} manifold approximation and projection for dimension reduction},
	issn = {23318422},
	abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAPis constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
	journal = {arXiv},
	author = {McInnes, Leland and Healy, John and Melville, James},
	year = {2018},
	note = {arXiv: 1802.03426},
}

@article{Joseph2015,
	title = {Maximum projection designs for computer experiments},
	volume = {102},
	issn = {14643510},
	doi = {10.1093/biomet/asv002},
	abstract = {Space-filling properties are important in designing computer experiments. The traditional maximin and minimax distance designs consider only space-filling in the full-dimensional space; this can result in poor projections onto lower-dimensional spaces, which is undesirable when only a few factors are active. Restricting maximin distance design to the class of Latin hypercubes can improve one-dimensional projections but cannot guarantee good space-filling properties in larger subspaces. We propose designs that maximize space-filling properties on projections to all subsets of factors. We call our designs maximum projection designs. Our design criterion can be computed at no more cost than a design criterion that ignores projection properties.},
	number = {2},
	journal = {Biometrika},
	author = {Joseph, V. Roshan and Gul, Evren and Ba, Shan},
	year = {2015},
	keywords = {Experimental design, Gaussian process, Latin hypercube design, Screening design, Space-filling .design},
	pages = {371--380},
}

@article{Jung2018,
	title = {Design {Space} {Exploration} for {Powertrain} {Electrification} using {Gaussian} {Processes}},
	volume = {2018-June},
	issn = {07431619},
	doi = {10.23919/ACC.2018.8430899},
	abstract = {Design space exploration of hybrid electric vehicles is an important multi-objective global optimization problem. One of the main objectives is to minimize fuel consumption while maintaining satisfactory driveability performance and vehicle cost. The design problem often includes multiple design options, including different driveline architectures and component sizes, where different candidates have various trade-offs between different, in many cases contradictory, performance requirements. Thus, there is no global optimum but a set of Pareto-optimal solutions to be explored. The objective functions can be expensive to evaluate, due to time-consuming simulations, which requires careful selection of which candidates to evaluate. A design space exploration algorithm is proposed for finding the set of Pareto-optimal solutions when the design search space includes multiple design options. As a case study, powertrain optimization is performed for a medium-sized series hybrid electric delivery truck.},
	journal = {Proceedings of the American Control Conference},
	author = {Jung, Daniel and Ahmed, Qadeer and Rizzoni, Giorgio},
	year = {2018},
	note = {Publisher: AACC
ISBN: 9781538654286},
	pages = {846--851},
}

@article{Callaham2019,
	title = {Robust flow reconstruction from limited measurements via sparse representation},
	volume = {4},
	issn = {2469990X},
	url = {https://doi.org/10.1103/PhysRevFluids.4.103907},
	doi = {10.1103/PhysRevFluids.4.103907},
	abstract = {In many applications it is important to estimate a fluid flow field from limited and possibly corrupt measurements. Current methods in flow estimation often use least squares regression to reconstruct the flow field, finding the minimum-energy solution that is consistent with the measured data. However, this approach may be prone to overfitting and sensitive to noise. To address these challenges we instead seek a sparse representation of the data in a library of examples. Sparse representation has been widely used for image recognition and reconstruction, and it is well-suited to structured data with limited, corrupt measurements. We explore sparse representation for flow reconstruction on a variety of fluid data sets with a wide range of complexity, including vortex shedding past a cylinder at low Reynolds number, a mixing layer, and two geophysical flows. In addition, we compare several measurement strategies and consider various types of noise and corruption over a range of intensities. We find that sparse representation has considerably improved the estimation accuracy and robustness to noise and corruption compared with least squares methods. We also introduce a sparse estimation procedure on local spatial patches for complex multiscale flows that preclude a global sparse representation. Based on these results, sparse representation is a promising framework for extracting useful information from complex flow fields with realistic measurements.},
	number = {10},
	journal = {Physical Review Fluids},
	author = {Callaham, Jared L. and Maeda, Kazuki and Brunton, Steven L.},
	year = {2019},
	note = {arXiv: 1810.06723
Publisher: American Physical Society},
	keywords = {doi:10.1103/PhysRevFluids.4.103907 url:https://doi, ★},
	pages = {103907},
}

@article{Wang2020,
	title = {Double robust principal component analysis},
	volume = {391},
	issn = {18728286},
	doi = {10.1016/j.neucom.2020.01.097},
	abstract = {Robust Principal Component Analysis (RPCA) aiming to recover underlying clean data with low-rank structure from the corrupted data, is a powerful tool in machine learning and data mining. However, in many real-world applications where new data (i.e., out-of-samples) in the testing phase can be unseen in the training procedure, (1) RPCA which is a transductive method can be naturally incapable of handing out-of-samples, and (2) violently applying RPCA into this applications does not explicitly consider the relationships between reconstruction error and low-rank representation. To tackle these problems, in this paper, we propose a Double Robust Principal Component Analysis to deal with the out-of-sample problems, which is termed as DRPCA. More specifically, we integrate a reconstruction error into the criterion function of RPCA. Our proposed model can then benefit from (1) the robustness of principal components to outliers and missing values, (2) the bridge between reconstruction error and low-rank representation, (3) low-rank clean data extraction from new datum by a linear transform. To this end, extensive experiments on several datasets demonstrate its superiority, when comparing with the state-of-the-art models, in several clustering and low-rank recovery tasks.},
	journal = {Neurocomputing},
	author = {Wang, Qianqian and Gao, Quan Xue and Sun, Gan and Ding, Chris},
	year = {2020},
	note = {arXiv: 0912.3599v1},
	keywords = {Double, Low-rank representation, Robust principal component analysis},
	pages = {119--128},
}

@article{Zhang2020,
	title = {Basic {Framework} and {Main} {Methods} of {Uncertainty} {Quantification}},
	volume = {2020},
	author = {Zhang, Juan and Yin, Junping and Wang, Ruili},
	year = {2020},
	keywords = {★},
}

@article{Scherl2020,
	title = {Robust principal component analysis for modal decomposition of corrupt fluid flows},
	volume = {5},
	issn = {2469990X},
	doi = {10.1103/PhysRevFluids.5.054401},
	abstract = {Modal analysis techniques are used to identify patterns and develop reduced-order models in a variety of fluid applications. However, experimentally acquired flow fields may be corrupted with incorrect and missing entries, which may degrade modal decomposition. Here we use robust principal component analysis (RPCA) to improve the quality of flow-field data by leveraging global coherent structures to identify and replace spurious data points. RPCA is a robust variant of principal component analysis, also known as proper orthogonal decomposition in fluids, that decomposes a data matrix into the sum of a low-rank matrix containing coherent structures and a sparse matrix of outliers and corrupt entries. We apply RPCA filtering to a range of fluid simulations and experiments of varying complexities and assess the accuracy of low-rank structure recovery. First, we analyze direct numerical simulations of flow past a circular cylinder at Reynolds number 100 with artificial outliers, alongside similar particle image velocimetry (PIV) measurements at Reynolds number 413. Next, we apply RPCA filtering to a turbulent channel flow simulation from the Johns Hopkins Turbulence database, demonstrating that dominant coherent structures are preserved in the low-rank matrix. Finally, we investigate PIV measurements behind a two-bladed cross-flow turbine that exhibits both broadband and coherent phenomena. In all cases, we find that RPCA filtering extracts dominant coherent structures and identifies and fills in incorrect or missing measurements. The performance is particularly striking when flow fields are analyzed using dynamic mode decomposition, which is sensitive to noise and outliers.},
	number = {5},
	journal = {Physical Review Fluids},
	author = {Scherl, Isabel and Strom, Benjamin and Shang, Jessica K. and Williams, Owen and Polagye, Brian L. and Brunton, Steven L.},
	year = {2020},
	note = {arXiv: 1905.07062},
	keywords = {★},
}

@article{Schulz2018,
	title = {A tutorial on {Gaussian} process regression: {Modelling}, exploring, and exploiting functions},
	volume = {85},
	issn = {10960880},
	url = {https://doi.org/10.1016/j.jmp.2018.03.001},
	doi = {10.1016/j.jmp.2018.03.001},
	abstract = {This tutorial introduces the reader to Gaussian process regression as an expressive tool to model, actively explore and exploit unknown functions. Gaussian process regression is a powerful, non-parametric Bayesian approach towards regression problems that can be utilized in exploration and exploitation scenarios. This tutorial aims to provide an accessible introduction to these techniques. We will introduce Gaussian processes which generate distributions over functions used for Bayesian non-parametric regression, and demonstrate their use in applications and didactic examples including simple regression problems, a demonstration of kernel-encoded prior assumptions and compositions, a pure exploration scenario within an optimal design framework, and a bandit-like exploration–exploitation scenario where the goal is to recommend movies. Beyond that, we describe a situation modelling risk-averse exploration in which an additional constraint (not to sample below a certain threshold) needs to be accounted for. Lastly, we summarize recent psychological experiments utilizing Gaussian processes. Software and literature pointers are also provided.},
	journal = {Journal of Mathematical Psychology},
	author = {Schulz, Eric and Speekenbrink, Maarten and Krause, Andreas},
	year = {2018},
	note = {Publisher: Elsevier Inc.},
	keywords = {Active learning, Bandit problems, Exploration–exploitation, Gaussian process regression, ★},
	pages = {1--16},
}

@article{Urquhart2020,
	title = {Surrogate-based optimisation using adaptively scaled radial basis functions},
	volume = {88},
	issn = {15684946},
	url = {https://doi.org/10.1016/j.asoc.2019.106050},
	doi = {10.1016/j.asoc.2019.106050},
	abstract = {Aerodynamic shape optimisation is widely used in several applications, such as road vehicles, aircraft and trains. This paper investigates the performance of two surrogate-based optimisation methods; a Proper Orthogonal Decomposition-based method and a force-based surrogate model. The generic passenger vehicle DrivAer is used as a test case where the predictive capability of the surrogate in terms of aerodynamic drag is presented. The Proper Orthogonal Decomposition-based method uses simulation results from topologically different meshes by interpolating all solutions to a common mesh for which the decomposition is calculated. Both the Proper Orthogonal Decomposition- and force-based approaches make use of Radial Basis Function interpolation. The Radial Basis Function hyperparameters are optimised using differential evolution. Additionally, the axis scaling is treated as a hyperparameter, which reduces the interpolation error by more than 50\% for the investigated test case. It is shown that the force-based approach performs better than the Proper Orthogonal Decomposition method, especially at low sample counts, both with and without adaptive scaling. The sample points, from which the surrogate model is built, are determined using an optimised Latin Hypercube sampling plan. The Latin Hypercube sampling plan is extended to include both continuous and categorical values, which further improve the surrogate's predictive capability when categorical design parameters, such as on/off parameters, are included in the design space. The performance of the force-based surrogate model is compared with four other gradient-free optimisation techniques: Random Sample, Differential Evolution, Nelder–Mead and Bayesian Optimisation. The surrogate model performed as good as, or better than these algorithms, for 17 out of the 18 investigated benchmark problems.},
	journal = {Applied Soft Computing Journal},
	author = {Urquhart, Magnus and Ljungskog, Emil and Sebben, Simone},
	year = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {Aerodynamics, Bayesian optimisation, Benchmarking, Black box optimisation, Evolutionary algorithm, Global optimisation, Gradient-free, Latin Hypercube Sampling, Optimisation, Proper Orthogonal Decomposition, Radial Basis Function interpolation, Surrogate model},
	pages = {106050},
}

@article{Machine2019,
	title = {I {Mproving} {Machine} {Classification} {Using} {Human}},
	author = {Machine, Mproving and Using, Classification and Uncertainty, Human},
	year = {2019},
	pages = {1--10},
}

@article{Li2018,
	title = {Visualizing the loss landscape of neural nets},
	volume = {2018-Decem},
	issn = {10495258},
	abstract = {Neural network training relies on our ability to find “good” minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple “filter normalization” method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
	number = {Nips 2018},
	journal = {Advances in Neural Information Processing Systems},
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	year = {2018},
	note = {arXiv: 1712.09913},
	pages = {6389--6399},
}

@article{Passi2019,
	title = {Problem formulation and fairness},
	doi = {10.1145/3287560.3287567},
	abstract = {Formulating data science problems is an uncertain and difficult process. It requires various forms of discretionary work to translate high-level objectives or strategic goals into tractable problems, necessitating, among other things, the identification of appropriate target variables and proxies. While these choices are rarely self-evident, normative assessments of data science projects often take them for granted, even though different translations can raise profoundly different ethical concerns. Whether we consider a data science project fair often has as much to do with the formulation of the problem as any property of the resulting model. Building on six months of ethnographic fieldwork with a corporate data science team-and channeling ideas from sociology and history of science, critical data studies, and early writing on knowledge discovery in databases-we describe the complex set of actors and activities involved in problem formulation. Our research demonstrates that the specification and operationalization of the problem are always negotiated and elastic, and rarely worked out with explicit normative considerations in mind. In so doing, we show that careful accounts of everyday data science work can help us better understand how and why data science problems are posed in certain ways-and why specific formulations prevail in practice, even in the face of what might seem like normatively preferable alternatives. We conclude by discussing the implications of our findings, arguing that effective normative interventions will require attending to the practical work of problem formulation.},
	journal = {FAT* 2019 - Proceedings of the 2019 Conference on Fairness, Accountability, and Transparency},
	author = {Passi, Samir and Barocas, Solon},
	year = {2019},
	note = {ISBN: 9781450361255},
	keywords = {Data Science, Fairness, Machine Learning, Problem Formulation, Target Variable},
	pages = {39--48},
}

@article{Olver2020,
	title = {Fast {Algorithms} using {Orthogonal} {Polynomials}},
	issn = {22965017},
	doi = {10.1007/978-3-030-47048-7_4},
	abstract = {We review recent advances in algorithms for quadrature, transforms, differen- tial equations and singular integral equations using orthogonal polynomials. Quadrature based on asymptotics has facilitated optimal complexity quadrat- ure rules, allowing for efficient computation of quadrature rules with millions of nodes. Transforms based on rank structures in change-of-basis operators allow for quasi-optimal complexity, including in multivariate settings such as on triangles and for spherical harmonics. Ordinary and partial differential equations can be solved via sparse linear algebra when set up using ortho- gonal polynomials as a basis, provided that care is taken with the weights of orthogonality. A similar idea, together with low-rank approximation, gives an efficient method for solving singular integral equations. These techniques can be combined to produce high-performance codes for a wide range of problems that appear in applications. Downloaded},
	journal = {Applied and Numerical Harmonic Analysis},
	author = {Olver, Sheehan and Slevinsky, Richard and Townsend, Alex},
	year = {2020},
	pages = {121--191},
}

@article{Martinsson2020,
	title = {Randomized numerical linear algebra: {Foundations} \& {Algorithms}},
	doi = {10.1017/S0962492920000021},
	abstract = {This survey describes probabilistic algorithms for linear algebra computations, such as factorizing matrices and solving linear systems. It focuses on techniques that have a proven track record for real-world problem instances. The paper treats both the theoretical foundations of the subject and the practical computational issues. Topics covered include norm estimation; matrix approximation by sampling; structured and unstructured random embeddings; linear regression problems; low-rank approximation; subspace iteration and Krylov methods; error estimation and adaptivity; interpolatory and CUR factorizations; Nyström approximation of positive-semidefinite matrices; single view (“streaming”) algorithms; full rank-revealing factorizations; solvers for linear systems; and approximation of kernel matrices that arise in machine learning and in scientific computing.},
	journal = {arXiv},
	author = {Martinsson, Per Gunnar and Tropp, Joel A.},
	year = {2020},
	note = {arXiv: 2002.01387},
	pages = {403--572},
}

@article{Xiu2002,
	title = {{THE} {WIENER} – {ASKEY} {POLYNOMIAL} {CHAOS} {FOR} {STOCHASTIC} orthogonal polynomials},
	volume = {24},
	number = {2},
	journal = {Society},
	author = {Xiu, Dongbin and Karniadakis, George E M},
	year = {2002},
	keywords = {65c20, 65c30, ams subject classifications, askey scheme, equations, galerkin projection, orthogonal polynomials, pii, polynomial chaos, s1064827501387826, spectral methods, stochastic differential},
	pages = {619--644},
}

@article{Xiao2019,
	title = {Quantification of model uncertainty in {RANS} simulations: {A} review},
	volume = {108},
	issn = {03760421},
	url = {https://doi.org/10.1016/j.paerosci.2018.10.001},
	doi = {10.1016/j.paerosci.2018.10.001},
	abstract = {In computational fluid dynamics simulations of industrial flows, models based on the Reynolds-averaged Navier–Stokes (RANS) equations are expected to play an important role in decades to come. However, model uncertainties are still a major obstacle for the predictive capability of RANS simulations. This review examines both the parametric and structural uncertainties in turbulence models. We review recent literature on data-free (uncertainty propagation) and data-driven (statistical inference) approaches for quantifying and reducing model uncertainties in RANS simulations. Moreover, the fundamentals of uncertainty propagation and Bayesian inference are introduced in the context of RANS model uncertainty quantification. Finally, the literature on uncertainties in scale-resolving simulations is briefly reviewed with particular emphasis on large eddy simulations.},
	number = {October 2018},
	journal = {Progress in Aerospace Sciences},
	author = {Xiao, Heng and Cinnella, Paola},
	year = {2019},
	note = {arXiv: 1806.10434
Publisher: Elsevier Ltd},
	keywords = {Bayesian inference, Machine learning, Model-form uncertainty, Reynolds-averaged Navier–Stokes equations, Turbulence modeling},
	pages = {1--31},
}

@article{Rumpfkeil2020,
	title = {Multi-fidelity, gradient-enhanced, and locally optimized sparse polynomial chaos and kriging surrogate models applied to benchmark problems},
	volume = {1 PartF},
	doi = {10.2514/6.2020-0677},
	abstract = {In this paper, multi-fidelity, gradient-enhanced, and locally optimized sparse polynomial chaos expansion (PCE) and kriging surrogate models are constructed in lieu of solely using computationally expensive high-fidelity engineering analyses. Once an accurate surrogate model is built, it can be used for evaluating a large number of designs for design space exploration or for uncertainty quantification. To demonstrate that accurate multi-fidelity, gradient-enhanced, and locally optimized surrogate models can be obtained at lower computational cost than basic high-fidelity ones, a number of benchmark problems are employed. These include polynomial and non-polynomial analytical functions of arbitrary dimension, a heterogeneous non-polynomial analytical function in one, two, and three dimensions as well as a coupled spring-mass-system leading to a linear system of ODEs of arbitrary dimension.},
	number = {January},
	journal = {AIAA Scitech 2020 Forum},
	author = {Rumpfkeil, Markus P. and Beran, Philip S.},
	year = {2020},
	note = {ISBN: 9781624105951},
	keywords = {★},
	pages = {1--23},
}

@article{Shahane2019,
	title = {Uncertainty quantification in three dimensional natural convection using polynomial chaos expansion and deep neural networks},
	volume = {139},
	issn = {00179310},
	doi = {10.1016/j.ijheatmasstransfer.2019.05.014},
	abstract = {This paper analyzes the effects of input uncertainties on the outputs of a three dimensional natural convection problem in a differentially heated cubical enclosure. Two different cases are considered for parameter uncertainty propagation and global sensitivity analysis. In case A, stochastic variation is introduced in the two non-dimensional parameters (Rayleigh and Prandtl numbers) with an assumption that the boundary temperature is uniform. Being a two dimensional stochastic problem, the polynomial chaos expansion (PCE) method is used as a surrogate model. Case B deals with non-uniform stochasticity in the boundary temperature. Instead of the traditional Gaussian process model with the Karhunen-Loève expansion, a novel approach is successfully implemented to model uncertainty in the boundary condition. The boundary is divided into multiple domains and the temperature imposed on each domain is assumed to be an independent and identically distributed (i.i.d) random variable. Deep neural networks are trained with the boundary temperatures as inputs and Nusselt number, internal temperature or velocities as outputs. The number of domains which is essentially the stochastic dimension is 4, 8, 16 or 32. Rigorous training and testing process shows that the neural network is able to approximate the outputs to a reasonable accuracy. For a high stochastic dimension such as 32, it is computationally expensive to fit the PCE. This paper demonstrates a novel way of using the deep neural network as a surrogate modeling method for uncertainty quantification with the number of simulations much fewer than that required for fitting the PCE, thus, saving the computational cost.},
	journal = {International Journal of Heat and Mass Transfer},
	author = {Shahane, Shantanu and Aluru, Narayana R. and Vanka, Surya Pratap},
	year = {2019},
	note = {arXiv: 1810.11934},
	keywords = {Deep neural networks, Natural convection, Polynomial chaos expansion, Uncertainty quantification},
	pages = {613--631},
}

@article{Moreland2005,
	title = {Diverging {Color} {Maps} for {Scientific} {Visualization} ( {Expanded} )},
	author = {Moreland, Kenneth},
	year = {2005},
}

@article{Li2020,
	title = {Fourier {Neural} {Operator} for {Parametric} {Partial} {Differential} {Equations}},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	year = {2020},
	note = {arXiv: 2010.08895v1},
}

@article{carlberg_network_nodate,
	title = {The network uncertainty quantification method for propagating uncertainties in component-based systems},
	author = {Carlberg, Kevin and Guzzetti, Sofia and Khalil, Mohammad and Sargsyan, Khachik},
	note = {arXiv: 1908.11476v1},
	keywords = {1, 35r60, 49m20, 49m27, 60h15, 60h35, 65c20, 65n55, ams subject classifications, and engineering, anderson acceleration, domain decomposition, introduction, many systems in science, network, ranging from power grids, relaxation methods, to gas, uncertainty propagation, uncertainty quantification, ★},
	pages = {1--34},
}

@misc{karhunen_representationofsignalsnonlinearpca_karhunenpdf_nodate,
	title = {{RepresentationOfSignalsNonLinearPCA}\_Karhunen.pdf},
	author = {Karhunen, J},
	keywords = {★},
}

@article{scholz_gene_2005,
	title = {Gene expression {Non}-linear {PCA} : a missing data approach},
	volume = {21},
	doi = {10.1093/bioinformatics/bti634},
	number = {20},
	author = {Scholz, Matthias and Kaplan, Fatma and Guy, Charles L and Kopka, Joachim and Selbig, Joachim},
	year = {2005},
	keywords = {★},
	pages = {3887--3895},
}

@article{hasegawa_machine-learning-based_2020,
	title = {Machine-learning-based reduced-order modeling for unsteady flows around bluff bodies of various shapes},
	volume = {34},
	issn = {1432-2250},
	url = {https://doi.org/10.1007/s00162-020-00528-w},
	doi = {10.1007/s00162-020-00528-w},
	number = {4},
	journal = {Theoretical and Computational Fluid Dynamics},
	author = {Hasegawa, Kazuto and Fukami, Kai and Murata, Takaaki and Fukagata, Koji},
	year = {2020},
	note = {Publisher: Springer Berlin Heidelberg},
	keywords = {Reduced-order modeling,Machine learning,Unsteady w, machine learning, reduced-order modeling, unsteady wake},
	pages = {367--383},
}

@article{bates_experiments_2011,
	title = {Experiments {Using} a {Permutation} {Genetic} {Algorithm}},
	number = {April 2004},
	author = {Bates, Stuart J and Sienz, Johann and Toropov, Vassili V and Latin, Eglais and Doe, Hypercube and Experiments, Design Of and Doe, Latin Hypercube and Latin, Optimal and Doe, Hypercube and Lecturer, Senior and Aerospace, Programme and Design, Principal and Specialist, Optimization},
	year = {2011},
	pages = {1--7},
}

@article{urquhart_surrogate-based_2020,
	title = {Surrogate-based optimisation using adaptively scaled radial basis functions},
	volume = {88},
	issn = {1568-4946},
	url = {https://doi.org/10.1016/j.asoc.2019.106050},
	doi = {10.1016/j.asoc.2019.106050},
	journal = {Applied Soft Computing Journal},
	author = {Urquhart, Magnus and Ljungskog, Emil and Sebben, Simone},
	year = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {bayesian optimisation, black box optimisation, evolutionary algorithm, ★},
	pages = {106050},
}

@article{Mckay1979,
	title = {A {Comparison} of {Three} {Methods} for {Selecting} {Values} of {Input} {Variables} in the {Analysis} of {Output} from a {Computer} {Code}},
	volume = {21},
	doi = {10.1080/00401706.1979.10489755},
	number = {2},
	author = {Mckay, M D and Beckman, R J and Conover, W J},
	year = {1979},
	pages = {239--245},
}

@article{gerritsma_time-dependent_2010,
	title = {Time-dependent generalized polynomial chaos},
	volume = {229},
	issn = {0021-9991},
	url = {http://dx.doi.org/10.1016/j.jcp.2010.07.020},
	doi = {10.1016/j.jcp.2010.07.020},
	number = {22},
	journal = {Journal of Computational Physics},
	author = {Gerritsma, Marc and Steen, Jan-bart Van Der and Vos, Peter and Karniadakis, George},
	year = {2010},
	note = {Publisher: Elsevier Inc.},
	pages = {8333--8363},
}

@article{Huan2018b,
	title = {Global {Sensitivity} {Analysis} and {Estimation} of {Model} {Error} , {Toward} {Uncertainty} {Quantification} in {Scramjet} {Computations}},
	volume = {56},
	doi = {10.2514/1.J056278},
	number = {3},
	author = {Huan, Xun and Safta, Cosmin and Sargsyan, Khachik},
	year = {2018},
}

@article{Chastaing2012,
	title = {Generalized {Hoeffding}-{Sobol} decomposition for dependent variables - application to sensitivity analysis},
	volume = {6},
	issn = {19357524},
	doi = {10.1214/12-EJS749},
	abstract = {In this paper, we consider a regression model built on dependent variables. This regression modelizes an input output relationship. Under boundedness type assumptions on the joint density function of the input variables, we show that a generalized Hoeffding-Sobol decomposition is available. This leads to new indices measuring the sensitivity of the output with respect to the input variables. We also study and discuss the estimation of these new indices.},
	journal = {Electronic Journal of Statistics},
	author = {Chastaing, Gaelle and Gamboa, Fabrice and Prieur, Clémentine},
	year = {2012},
	note = {arXiv: 1112.1788},
	keywords = {Dependent variables, Hoeffding decomposition, Sensitivity index, Sobol decomposition, ★},
	pages = {2420--2448},
}

@article{Cousin2019,
	title = {On the consistency of {Sobol} indices with respect to stochastic ordering of model parameters},
	volume = {23},
	issn = {12623318},
	doi = {10.1051/ps/2018001},
	abstract = {In the past decade, Sobol's variance decomposition has been used as a tool to assess how the output of a model is affected by the uncertainty on its input parameters. We show some links between global sensitivity analysis and stochastic ordering theory. More specifically, we study the influence of inputs' distributions on Sobol indices in relation with stochastic orders. This gives an argument in favor of using Sobol's indices in uncertainty quantification, as one indicator among others.},
	journal = {ESAIM - Probability and Statistics},
	author = {Cousin, A. and Janon, A. and Maume-Deschamps, V. and Niang, I.},
	year = {2019},
	note = {arXiv: 1407.5565},
	keywords = {Sensitivity analysis, Sobol indices, Stochastic orders, ★},
	pages = {387--408},
}

@article{Johnson2012,
	title = {Hybrid space-filling designs for computer experiments},
	doi = {10.1007/978-3-7908-2846-7-19},
	abstract = {Computer models play an increasingly important role in engineering design and in the study of complex systems, where physical experiments on the real system or even a prototype are prohibitively expensive. Both deterministic and stochastic computer models are used in these situations. A deterministic computer model is a set of complex equations whose solution depends on the input conditions and the levels of design factors or parameters but not on random elements. Examples include finite element models and computational fluid dynamics models. Spacefilling designs are usually employed to study these deterministic computer models and often the modeling strategy involves fitting a spatial correlation or Kriging model (the Gaussian stochastic process model) to the data, because this model interpolates the experimental data exactly.We provide a survey of these designs and the modeling strategy, and propose a new type of hybrid space-filling design. The new design is a hybrid consisting of design points from a traditional space-filling design augmented by runs from a near saturated I-optimal design for a polynomial. We illustrate the construction of these designs with examples, and demonstrate their performance in response prediction for several situations. A comparison with standard space-filling designs is provided. © Springer-Verlag Berlin Heidelberg 2012.},
	journal = {Frontiers in Statistical Quality Control 10},
	author = {Johnson, Rachel T. and Montgomery, Douglas C. and Kennedy, Kathryn S.},
	year = {2012},
	keywords = {Gaussian process model, Linear regression, Optimal design, Response surface},
	pages = {287--301},
}

@inproceedings{Sanchez2015,
	title = {Work {Smarter}, {Not} {Harder}: {A} {Tutorial} {On} {Designing} and {Conducting} {Simulation} {Experiments}},
	isbn = {978-1-4673-9743-8},
	abstract = {Simulation models are integral to modern scientific research, national defense, industry and manufacturing, and in public policy debates. These models tend to be extremely complex, often with thousands of factors and many sources of uncertainty. To understand the impact of these factors and their interactions on model outcomes requires efficient, high-dimensional design of experiments. Unfortunately, all too often, many large-scale simulation models continue to be explored in ad hoc ways. This suggests that more simulation researchers and practitioners need to be aware of the power of experimental design in order to get the most from their simulation studies. In this tutorial, we demonstrate the basic concepts important for design and conducting simulation experiments, and provide references to other resources for those wishing to learn more. This tutorial (an update of previous WSC tutorials) will prepare you to make your next simulation study a simulation experiment.},
	booktitle = {2015 {Winter} {Simulation} {Conference} ({WSC})., 2015},
	author = {Sanchez, Susan and Wan, Hong},
	year = {2015},
	pages = {779--790},
}

@article{Joseph2016,
	title = {Space-filling designs for computer experiments: {A} review},
	volume = {28},
	issn = {15324222},
	doi = {10.1080/08982112.2015.1100447},
	abstract = {Improving the quality of a product/process using a computer simulator is a much less expensive option than the real physical testing. However, simulation using computationally intensive computer models can be time consuming and, therefore, directly doing the optimization on the computer simulator can be infeasible. Experimental design and statistical modeling techniques can be used to overcome this problem. This article reviews experimental designs known as space-filling designs that are suitable for computer simulations. In the article, a special emphasis is given for a recently developed space-filling design called maximum projection design. Its advantages are illustrated using a simulation conducted for optimizing a milling process.},
	number = {1},
	journal = {Quality Engineering},
	author = {Joseph, V. Roshan},
	year = {2016},
	keywords = {design of experiments, kriging, optimization, simulation},
	pages = {28--35},
}

@article{Guo2014,
	title = {Time-dependent global sensitivity analysis for long-term degeneracy model using polynomial chaos},
	volume = {2014},
	issn = {16878140},
	doi = {10.1155/2014/719825},
	abstract = {Global sensitivity is used to quantify the influence of uncertain model inputs on the output variability of static models in general. However, very few approaches can be applied for the sensitivity analysis of long-term degeneracy models, as far as time-dependent reliability is concerned. The reason is that the static sensitivity may not reflect the completed sensitivity during the entire life circle. This paper presents time-dependent global sensitivity analysis for long-term degeneracy models based on polynomial chaos expansion (PCE). Sobol' indices are employed as the time-dependent global sensitivity since they provide accurate information on the selected uncertain inputs. In order to compute Sobol' indices more efficiently, this paper proposes a moving least squares (MLS) method to obtain the time-dependent PCE coefficients with acceptable simulation effort. Then Sobol' indices can be calculated analytically as a postprocessing of the time-dependent PCE coefficients with almost no additional cost. A test case is used to show how to conduct the proposed method, then this approach is applied to an engineering case, and the time-dependent global sensitivity is obtained for the long-term degeneracy mechanism model. © 2014 Jianbin Guo et al.},
	journal = {Advances in Mechanical Engineering},
	author = {Guo, Jianbin and Du, Shaohua and Wang, Yao and Zeng, Shengkui},
	year = {2014},
}

@article{arge_air_2013,
	title = {Air {Force} {Data} {Assimilative} {Photospheric} {Flux} {Transport} ( {ADAPT} ) {Model} {Air} {Force} {Data} {Assimilative} {Photospheric} {Flux} {Transport} ( {ADAPT} ) {Model}},
	volume = {343},
	number = {April 2010},
	author = {Arge, C Nick and Henney, Carl J and Koller, Josef and Compeau, C Rich and Young, Shawn and Mackenzie, David and Fay, Alex and John, W and Arge, C Nick and Henney, Carl J and Koller, Josef and Compeau, C Rich and Mackenzie, David and Fay, Alex and Harvey, John W},
	year = {2013},
	note = {ISBN: 9780735407596},
	keywords = {50, 60, 95, 96, bh, ci, corona, data assimilation, heliosphere and by extension, mz, pacs, pc, q, solar flux transport, solar magnetic fields, solar photosphere, the solar wind-magnetosphere interaction, the state of the},
}

@article{Lincoln1967,
	title = {Geomagnetic indices},
	volume = {11},
	issn = {00746142},
	doi = {10.1016/B978-0-12-480301-5.50009-4},
	number = {P1},
	journal = {International Geophysics},
	author = {Menvielle, Michel and Iyemori, Toshihiko},
	year = {1967},
	pages = {67--100},
}

@article{ManchesterIV2004,
	title = {Modeling a space weather event from the {Sun} to the {Earth}: {CME} generation and interplanetary propagation},
	volume = {109},
	issn = {21699402},
	doi = {10.1029/2003JA010150},
	abstract = {We present a three-dimensional (3-D) numerical ideal magnetohydrodynamics (MHD) model describing the time-dependent expulsion of a coronal mass ejection (CME) from the solar corona propagating to 1 astronomical unit (AU). The simulations are performed using the Block Adaptive Tree Solar-Wind Roe Upwind Scheme (BATS-R-US) code. We begin by developing a global steady-state model of the corona that possesses high-latitude coronal holes and a helmet streamer structure with a current sheet at the equator. The Archimedean spiral topology of the interplanetary magnetic field is reproduced along with fast and slow speed solar wind. Within this model system, we drive a CME to erupt by the introduction of a Gibson-Low magnetic flux rope that is anchored at both ends in the photosphere and embedded in the helmet streamer in an initial state of force imbalance. The flux rope rapidly expands and is ejected from the corona with maximum speeds in excess of 1000 km/s. Physics-based adaptive mesh refinement (AMR) allows us to capture the structure of the CME focused on a particular Sun-Earth line with high spatial resolution given to the bow shock ahead of the flux rope as well as to the current sheet behind. The CME produces a large magnetic cloud at 1 AU ({\textgreater}100 R⊙) in which Bz undergoes a full rotation from north to south with an amplitude of 20 nT. In a companion paper, we find that the CME is very effective in generating strong geomagnetic activity at the Earth in two ways. First, through the strong sustained southward Bz (lasting more than 10 hours) and, second, by a pressure increase associated with the CME-driven shock that compresses the magnetosphere. Copyright 2004 by the American Geophysical Union.},
	number = {A2},
	journal = {Journal of Geophysical Research: Space Physics},
	author = {Manchester IV, Ward B. and Gombosi, Tamas I. and Roussev, Ilia and Ridley, Aaron and De Zeeuw, Darren L. and Sokolov, I. V. and Powell, Kenneth G. and Tóth, Gábor},
	year = {2004},
	keywords = {Coronal mass ejection, Magnetohydrodynamics, Space weather},
}

@article{Singer2001,
	title = {Space weather forecasting: {A} grand challenge},
	volume = {125},
	issn = {23288779},
	doi = {10.1029/GM125p0023},
	abstract = {Space Environment Center (SEC) is the United States' official source of space weather alerts, warnings, and forecasts. Forecasts are used to support activities that are impacted by space weather such as electric power transmission, satellite operations, humans in space, navigation, and communication. This article presents a brief review of current space weather forecasting capabilities, and then focuses on the science, the models, the data, the new technologies, and the process for transitioning research into operations that is needed to meet the challenge to improve space weather forecasting in the new millennium. Forecasting critical parameters such as the interplanetary magnetic field at the magnetopause, and critical events such as coronal mass ejections are two examples of challenges to the research, observation, and modeling communities. Major improvements in space weather forecasting will be achieved when these, as well as other, challenges are met. The forecasting challenge is also discussed in the context of the goals of the US National Space Weather Program (NSWP) and other international activities.},
	journal = {Geophysical Monograph Series},
	author = {Singer, H. J. and Heckman, G. R. and Hirman, J. W.},
	year = {2001},
	note = {ISBN: 9781118668351},
	pages = {23--29},
}

@article{Sachdeva2019,
	title = {Validation of the {Alfvén} {Wave} {Solar} {Atmosphere} {Model} ({AWSoM}) with {Observations} from the {Low} {Corona} to 1 au},
	volume = {887},
	issn = {0004-637X},
	url = {http://dx.doi.org/10.3847/1538-4357/ab4f5e},
	doi = {10.3847/1538-4357/ab4f5e},
	abstract = {We perform a validation study of the latest version of the Alfv{\textbackslash}'\{e\}n Wave Solar atmosphere Model (AWSoM) within the Space Weather Modeling Framework (SWMF). To do so, we compare the simulation results of the model with a comprehensive suite of observations for Carrington rotations representative of the solar minimum conditions extending from the solar corona to the heliosphere up to the Earth. In the low corona (\$r {\textless} 1.25\$ {\textbackslash}Rs), we compare with EUV images from both STEREO-A/EUVI and SDO/AIA and to three-dimensional (3-D) tomographic reconstructions of the electron temperature and density based on these same data. We also compare the model to tomographic reconstructions of the electron density from SOHO/LASCO observations (\$2.55 {\textless} r {\textless} 6.0\${\textbackslash}Rs). In the heliosphere, we compare model predictions of solar wind speed with velocity reconstructions from InterPlanetary Scintillation (IPS) observations. For comparison with observations near the Earth, we use OMNI data. Our results show that the improved AWSoM model performs well in quantitative agreement with the observations between the inner corona and 1 AU. The model now reproduces the fast solar wind speed in the polar regions. Near the Earth, our model shows good agreement with observations of solar wind velocity, proton temperature and density. AWSoM offers an extensive application to study the solar corona and larger heliosphere in concert with current and future solar missions as well as being well suited for space weather predictions.},
	number = {1},
	journal = {The Astrophysical Journal},
	author = {Sachdeva, Nishtha and Holst, Bart van der and Manchester, Ward B. and Tóth, Gabor and Chen, Yuxi and Lloveras, Diego G. and Vásquez, Alberto M. and Lamy, Philippe and Wojak, Julien and Jackson, Bernard V. and Yu, Hsiu-Shan and Henney, Carl J.},
	year = {2019},
	note = {arXiv: 1910.08110
Publisher: IOP Publishing},
	keywords = {Solar corona,Magnetohydrodynamics,Solar coronal wa},
	pages = {83},
}

@article{VanDerHolst2014,
	title = {Alfvén wave solar model ({AWSoM}): {Coronal} heating},
	volume = {782},
	issn = {15384357},
	doi = {10.1088/0004-637X/782/2/81},
	abstract = {We present a new version of the Alfvén wave solar model, a global model from the upper chromosphere to the corona and the heliosphere. The coronal heating and solar wind acceleration are addressed with low-frequency Alfvén wave turbulence. The injection of Alfvén wave energy at the inner boundary is such that the Poynting flux is proportional to the magnetic field strength. The three-dimensional magnetic field topology is simulated using data from photospheric magnetic field measurements. This model does not impose open-closed magnetic field boundaries; those develop self-consistently. The physics include the following. (1) The model employs three different temperatures, namely the isotropic electron temperature and the parallel and perpendicular ion temperatures. The firehose, mirror, and ion-cyclotron instabilities due to the developing ion temperature anisotropy are accounted for. (2) The Alfvén waves are partially reflected by the Alfvén speed gradient and the vorticity along the field lines. The resulting counter-propagating waves are responsible for the nonlinear turbulent cascade. The balanced turbulence due to uncorrelated waves near the apex of the closed field lines and the resulting elevated temperatures are addressed. (3) To apportion the wave dissipation to the three temperatures, we employ the results of the theories of linear wave damping and nonlinear stochastic heating. (4) We have incorporated the collisional and collisionless electron heat conduction. We compare the simulated multi-wavelength extreme ultraviolet images of CR2107 with the observations from STEREO/EUVI and the Solar Dynamics Observatory/AIA instruments. We demonstrate that the reflection due to strong magnetic fields in the proximity of active regions sufficiently intensifies the dissipation and observable emission. © 2014. The American Astronomical Society. All rights reserved..},
	number = {2},
	journal = {Astrophysical Journal},
	author = {Van Der Holst, B. and Sokolov, I. V. and Meng, X. and Jin, M. and Manchester, W. B. and Tóth, G. and Gombosi, T. I.},
	year = {2014},
	keywords = {interplanetary medium, magnetohydrodynamics, methods, mhd, numerical, solar wind},
}

@article{Marin2012,
	title = {Approximate {Bayesian} computational methods},
	volume = {22},
	issn = {09603174},
	doi = {10.1007/s11222-011-9288-2},
	abstract = {Approximate Bayesian Computation (ABC) methods, also known as likelihood-free techniques, have appeared in the past ten years as the most satisfactory approach to intractable likelihood problems, first in genetics then in a broader spectrum of applications. However, these methods suffer to some degree from calibration difficulties that make them rather volatile in their implementation and thus render them suspicious to the users of more traditional Monte Carlo methods. In this survey, we study the various improvements and extensions brought on the original ABC algorithm in recent years. © 2011 Springer Science+Business Media, LLC.},
	number = {6},
	journal = {Statistics and Computing},
	author = {Marin, Jean Michel and Pudlo, Pierre and Robert, Christian P. and Ryder, Robin J.},
	year = {2012},
	note = {arXiv: 1101.0955
ISBN: 1122201192},
	keywords = {ABC methodology, Bayesian model choice, Bayesian statistics, DIYABC, Likelihood-free methods},
	pages = {1167--1180},
}

@article{Jin2017,
	title = {Data-{Constrained} {Coronal} {Mass} {Ejections} in a {Global} {Magnetohydrodynamics} {Model}},
	volume = {834},
	issn = {0004-637X},
	url = {http://dx.doi.org/10.3847/1538-4357/834/2/173},
	doi = {10.3847/1538-4357/834/2/173},
	abstract = {We present a first-principles-based coronal mass ejection (CME) model suitable for both scientific and operational purposes by combining a global magnetohydrodynamics (MHD) solar wind model with a flux rope-driven CME model. Realistic CME events are simulated self-consistently with high fidelity and forecasting capability by constraining initial flux rope parameters with observational data from GONG, SOHO/LASCO, and STEREO/COR. We automate this process so that minimum manual intervention is required in specifying the CME initial state. With the newly developed data-driven Eruptive Event Generator Gibson-Low (EEGGL), we present a method to derive Gibson-Low (GL) flux rope parameters through a handful of observational quantities so that the modeled CMEs can propagate with the desired CME speeds near the Sun. A test result with CMEs launched with different Carrington rotation magnetograms are shown. Our study shows a promising result for using the first-principles-based MHD global model as a forecasting tool, which is capable of predicting the CME direction of propagation, arrival time, and ICME magnetic field at 1 AU (see companion paper by Jin et al. 2016b).},
	number = {2},
	journal = {The Astrophysical Journal},
	author = {Jin, M. and Manchester, W. B. and van der Holst, B. and Sokolov, I. and Tóth, G. and Mullinix, R. E. and Taktakishvili, A. and Chulaki, A. and Gombosi, T. I.},
	year = {2017},
	note = {Publisher: IOP Publishing},
	keywords = {Sun: corona, Sun: coronal mass ejections (CMEs), cmes, corona, coronal mass ejections, interplanetary medium, magnetohydrodynamics, magnetohydrodynamics (MHD), methods, methods: numerical, mhd, numerical, solar wind, sun},
	pages = {173},
}

@book{OHagan2006,
	title = {Uncertain {Judgements}},
	isbn = {978-0-470-02999-2},
	author = {O'Hagan, Anthony},
	year = {2006},
	doi = {10.1002/9780470683019.scard},
	note = {Publication Title: Comparing Clinical Measurement Methods},
}

@article{Raissi2019,
	title = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {10902716},
	url = {https://doi.org/10.1016/j.jcp.2018.10.045},
	doi = {10.1016/j.jcp.2018.10.045},
	abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.},
	journal = {Journal of Computational Physics},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
	year = {2019},
	note = {Publisher: Elsevier Inc.},
	keywords = {Data-driven scientific computing, Machine learning, Nonlinear dynamics, Predictive modeling, Runge–Kutta methods},
	pages = {686--707},
}

@article{Huan2018a,
	title = {Global sensitivity analysis and estimation of model error, toward uncertainty quantification in scramjet computations},
	volume = {56},
	issn = {00011452},
	doi = {10.2514/1.J056278},
	abstract = {The development of scramjet engines is an important research area for advancing hypersonic and orbital flights. Progress toward optimal engine designs requires accurate flow simulations together with uncertainty quantification. However, performing uncertainty quantification for scramjet simulations is challenging due to the large number of uncertainparameters involvedandthe high computational costofflow simulations. These difficulties are addressedin this paper by developing practical uncertainty quantification algorithms and computational methods, and deploying themin the current studyto large-eddy simulations ofajet incrossflow inside a simplified HIFiRE Direct Connect Rig scramjet combustor. First, global sensitivity analysis is conducted to identify influential uncertain input parameters, which can help reduce the system's stochastic dimension. Second, because models of different fidelity are used in the overall uncertainty quantification assessment, a framework for quantifying and propagating the uncertainty due to model error is presented. These methods are demonstrated on a nonreacting jet-in-crossflow test problem in a simplified scramjet geometry, with parameter space up to 24 dimensions, using static and dynamic treatments of the turbulence subgrid model, and with two-dimensional and three-dimensional geometries.},
	number = {3},
	journal = {AIAA Journal},
	author = {Huan, Xun and Safta, Cosmin and Geraci, Gianluca and Eldred, Michael S. and Vane, Zachary P. and Lacaze, Guilhem and Oefelein, Joseph C. and Najm, Habib N.},
	year = {2018},
	note = {arXiv: 1707.09478},
	pages = {1170--1184},
}

@article{Jiao2020,
	title = {Solar {Flare} {Intensity} {Prediction} {With} {Machine} {Learning} {Models}},
	volume = {18},
	issn = {15427390},
	doi = {10.1029/2020SW002440},
	abstract = {We develop a mixed long short-term memory (LSTM) regression model to predict the maximum solar flare intensity within a 24-hr time window 0–24, 6–30, 12–36, and 24–48 hr ahead of time using 6, 12, 24, and 48 hr of data (predictors) for each Helioseismic and Magnetic Imager (HMI) Active Region Patch (HARP). The model makes use of (1) the Space-Weather HMI Active Region Patch (SHARP) parameters as predictors and (2) the exact flare intensities instead of class labels recorded in the Geostationary Operational Environmental Satellites (GOES) data set, which serves as the source of the response variables. Compared to solar flare classification, the model offers us more detailed information about the exact maximum flux level, that is, intensity, for each occurrence of a flare. We also consider classification models built on top of the regression model and obtain better results in solar flare classifications as compared to Chen et al. (2019, https://doi.org/10.1029/2019SW002214). Our results suggest that the most efficient time period for predicting the solar activity is within 24 hr before the prediction time using the SHARP parameters and the LSTM model.},
	number = {7},
	journal = {Space Weather},
	author = {Jiao, Zhenbang and Sun, Hu and Wang, Xiantong and Manchester, Ward and Gombosi, Tamas and Hero, Alfred and Chen, Yang},
	year = {2020},
	note = {arXiv: 1912.06120},
	pages = {1--28},
}

@article{Toth2012,
	title = {Adaptive numerical algorithms in space weather modeling},
	volume = {231},
	issn = {10902716},
	doi = {10.1016/j.jcp.2011.02.006},
	abstract = {Space weather describes the various processes in the Sun-Earth system that present danger to human health and technology. The goal of space weather forecasting is to provide an opportunity to mitigate these negative effects. Physics-based space weather modeling is characterized by disparate temporal and spatial scales as well as by different relevant physics in different domains. A multi-physics system can be modeled by a software framework comprising several components. Each component corresponds to a physics domain, and each component is represented by one or more numerical models. The publicly available Space Weather Modeling Framework (SWMF) can execute and couple together several components distributed over a parallel machine in a flexible and efficient manner. The framework also allows resolving disparate spatial and temporal scales with independent spatial and temporal discretizations in the various models.Several of the computationally most expensive domains of the framework are modeled by the Block-Adaptive Tree Solarwind Roe-type Upwind Scheme (BATS-R-US) code that can solve various forms of the magnetohydrodynamic (MHD) equations, including Hall, semi-relativistic, multi-species and multi-fluid MHD, anisotropic pressure, radiative transport and heat conduction. Modeling disparate scales within BATS-R-US is achieved by a block-adaptive mesh both in Cartesian and generalized coordinates. Most recently we have created a new core for BATS-R-US: the Block-Adaptive Tree Library (BATL) that provides a general toolkit for creating, load balancing and message passing in a 1, 2 or 3 dimensional block-adaptive grid. We describe the algorithms of BATL and demonstrate its efficiency and scaling properties for various problems.BATS-R-US uses several time-integration schemes to address multiple time-scales: explicit time stepping with fixed or local time steps, partially steady-state evolution, point-implicit, semi-implicit, explicit/implicit, and fully implicit numerical schemes. Depending on the application, we find that different time stepping methods are optimal. Several of the time integration schemes exploit the block-based granularity of the grid structure.The framework and the adaptive algorithms enable physics-based space weather modeling and even short-term forecasting. © 2011 Elsevier Inc.},
	number = {3},
	journal = {Journal of Computational Physics},
	author = {Tóth, Gábor and van der Holst, Bart and Sokolov, Igor V. and De Zeeuw, Darren L. and Gombosi, Tamas I. and Fang, Fang and Manchester, Ward B. and Meng, Xing and Najib, Dalal and Powell, Kenneth G. and Stout, Quentin F. and Glocer, Alex and Ma, Ying Juan and Opher, Merav},
	year = {2012},
	keywords = {65D99 Numerical approximation, 77A05 Magnetohydrodynamics},
	pages = {870--903},
}

@article{Jakeman2020,
	title = {Adaptive multi-index collocation for uncertainty quantification and sensitivity analysis},
	volume = {121},
	issn = {10970207},
	doi = {10.1002/nme.6268},
	abstract = {In this paper, we present an adaptive algorithm to construct response surface approximations of high-fidelity models using a hierarchy of lower fidelity models. Our algorithm is based on multi-index stochastic collocation and automatically balances physical discretization error and response surface error to construct an approximation of model outputs. This surrogate can be used for uncertainty quantification (UQ) and sensitivity analysis (SA) at a fraction of the cost of a purely high-fidelity approach. We demonstrate the effectiveness of our algorithm on a canonical test problem from the UQ literature and a complex multiphysics model that simulates the performance of an integrated nozzle for an unmanned aerospace vehicle. We find that, when the input-output response is sufficiently smooth, our algorithm produces approximations that can be over two orders of magnitude more accurate than single fidelity approximations for a fixed computational budget.},
	number = {6},
	journal = {International Journal for Numerical Methods in Engineering},
	author = {Jakeman, John D. and Eldred, Michael S. and Geraci, Gianluca and Gorodetsky, Alex},
	year = {2020},
	note = {arXiv: 1909.13845},
	keywords = {decision making, modeling, multifidelity, sensitivity analysis, simulation, uncertainty quantification, validation, ★},
	pages = {1314--1343},
}

@article{NasehiTehrani2012,
	title = {L1 regularization method in electrical impedance tomography by using the {L1}-curve ({Pareto} frontier curve)},
	volume = {36},
	issn = {0307904X},
	url = {http://dx.doi.org/10.1016/j.apm.2011.07.055},
	doi = {10.1016/j.apm.2011.07.055},
	abstract = {Electrical impedance tomography (EIT), as an inverse problem, aims to calculate the internal conductivity distribution at the interior of an object from current-voltage measurements on its boundary. Many inverse problems are ill-posed, since the measurement data are limited and imperfect. To overcome ill-posedness in EIT, two main types of regularization techniques are widely used. One is categorized as the projection methods, such as truncated singular value decomposition (SVD or TSVD). The other categorized as penalty methods, such as Tikhonov regularization, and total variation methods. For both of these methods, a good regularization parameter should yield a fair balance between the perturbation error and regularized solution. In this paper a new method combining the least absolute shrinkage and selection operator (LASSO) and the basis pursuit denoising (BPDN) is introduced for EIT. For choosing the optimum regularization we use the L1-curve (Pareto frontier curve) which is similar to the L-curve used in optimising L2-norm problems. In the L1-curve we use the L1-norm of the solution instead of the L2 norm. The results are compared with the TSVD regularization method where the best regularization parameters are selected by observing the Picard condition and minimizing generalized cross validation (GCV) function. We show that this method yields a good regularization parameter corresponding to a regularized solution. Also, in situations where little is known about the noise level σ, it is also useful to visualize the L1-curve in order to understand the trade-offs between the norms of the residual and the solution. This method gives us a means to control the sparsity and filtering of the ill-posed EIT problem. Tracing this curve for the optimum solution can decrease the number of iterations by three times in comparison with using LASSO or BPDN separately. © 2011 Elsevier Inc.},
	number = {3},
	journal = {Applied Mathematical Modelling},
	author = {Nasehi Tehrani, J. and McEwan, A. and Jin, C. and van Schaik, A.},
	year = {2012},
	note = {Publisher: Elsevier Inc.},
	keywords = {Electrical impedance tomography, L1-curve (Pareto frontier curve), Regularization},
	pages = {1095--1105},
}

@article{Peherstorfer2019,
	title = {Multifidelity {Monte} {Carlo} estimation with adaptive low-fidelity models},
	volume = {7},
	issn = {21662525},
	doi = {10.1137/17M1159208},
	abstract = {Multifidelity Monte Carlo (MFMC) estimation combines low- and high-fidelity models to speed up the estimation of statistics of the high-fidelity model outputs. MFMC optimally samples the lowand high-fidelity models such that the MFMC estimator has minimal mean-squared error (MSE) for a given computational budget. In the setup of MFMC, the low-fidelity models are static; i.e., they are given and fixed and cannot be changed and adapted. We introduce the adaptive MFMC (AMFMC) method that splits the computational budget between adapting the low-fidelity models to improve their approximation quality and sampling the low- and high-fidelity models to reduce the MSE of the estimator. Our AMFMC approach derives the quasi-optimal balance between adaptation and sampling in the sense that our approach minimizes an upper bound of the MSE, instead of the error directly. We show that the quasi-optimal number of adaptations of the low-fidelity models is bounded even in the limit of an infinite budget. This shows that adapting low-fidelity models in MFMC beyond a certain approximation accuracy is unnecessary and can even be wasteful. Our AMFMC approach trades off adaptation and sampling and so avoids overadaptation of the low- fidelity models. Besides the costs of adapting low-fidelity models, our AMFMC approach can also take into account the costs of the initial construction of the low-fidelity models ("offline costs"), which is critical if low-fidelity models are computationally expensive to build such as reduced models and data-fit surrogate models. Numerical results demonstrate that our adaptive approach can achieve orders of magnitude speedups compared to MFMC estimators with static low-fidelity models and compared to Monte Carlo estimators that use the high-fidelity model alone.},
	number = {2},
	journal = {SIAM-ASA Journal on Uncertainty Quantification},
	author = {Peherstorfer, Benjamin},
	year = {2019},
	keywords = {Model reduction, Monte Carlo, Multifidelity, Multilevel, Surrogate models, Uncertainty quantification, ★},
	pages = {579--603},
}

@article{Sinsbeck2015,
	title = {Impact of data assimilation on cost-accuracy tradeoff in multifidelity models},
	volume = {3},
	issn = {21662525},
	doi = {10.1137/141001743},
	abstract = {Observable phenomena can often be described by alternative models with different degrees of fidelity. Such models typically contain uncertain parameters and forcings, rendering predictions of the state variables uncertain as well. Within the probabilistic framework, solutions of these models are given in terms of their probability density functions (PDFs). In the presence of data, the latter can be treated as prior distributions. Uncertainty and assimilation of measurements into model predictions, e.g., via Bayesian updating of solution PDFs, pose a question of model selection: Given a significant difference in computational cost, is a lower-fidelity model preferable to its higher-fidelity counterpart? We investigate this question in the context of multiphase flow in heterogeneous porous media whose hydraulic properties are uncertain. While low-fidelity (reduced-complexity) models introduce a model error, their moderate computational cost makes it possible to generate more realizations, which reduces the (e.g., Monte Carlo) sampling error. These two errors determine the model with the smallest total error. Our analysis suggests that assimilation of measurements of a quantity of interest (a medium's saturation, in our example) influences both types of errors, increasing the probability that the predictive accuracy of a reduced-complexity model exceeds that of its higher-fidelity counterpart.},
	number = {1},
	journal = {SIAM-ASA Journal on Uncertainty Quantification},
	author = {Sinsbeck, Michael and Tartakovsky, Daniel M.},
	year = {2015},
	keywords = {Porous media, Reduced complexity, Stochastic, Subsurface, Uncertainty quantification, Unsaturated, ★},
	pages = {954--968},
}

@article{Heas2020,
	title = {Selecting {Reduced} {Models} in the {Cross}-{Entropy} {Method}},
	volume = {8},
	doi = {10.1137/18m1192500},
	abstract = {This paper deals with the estimation of rare event probabilities using importance sampling (IS), where an \{{\textbackslash}it optimal\} proposal distribution is computed with the cross-entropy (CE) method. Although, IS optimised with the CE method leads to an efficient reduction of the estimator variance, this approach remains unaffordable for problems where the repeated evaluation of the score function represents a too intensive computational effort. This is often the case for score functions related to the solution of a partial differential equation (PDE) with random inputs. This work proposes to alleviate computation by adapting a score function approximation along the CE optimisation process. The score function approximation is obtained by selecting the surrogate of lowest dimensionality, whose accuracy guarantees to pass the current CE optimisation stage. The adaptation of the surrogate relies on certified upper bounds on the error norm. An asymptotic analysis provides some theoretical guarantees on the efficiency and convergence of the proposed algorithm. Numerical results demonstrate the gain brought by the adaptive method in the context of pollution alerts and a system modelled by a PDE.},
	number = {2},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Héas, P.},
	year = {2020},
	note = {arXiv: 1806.01916},
	keywords = {10, 1137, 18m1192500, 60-08, 65c05, 65n30, 68q87, ams subject classifications, certified, cross-entropy optimization, doi, error bound, importance sampling, partial differential equation, rare event simulation, reduced basis},
	pages = {511--538},
}

@book{Gorodetsky2020a,
	title = {A generalized approximate control variate framework for multifidelity uncertainty quantification},
	volume = {408},
	isbn = {0-02-199912-0},
	abstract = {We describe and analyze a variance reduction approach for Monte Carlo (MC) sampling that accelerates the estimation of statistics of computationally expensive simulation models using an ensemble of models with lower cost. These lower cost models — which are typically lower fidelity with unknown statistics — are used to reduce the variance in statistical estimators relative to a MC estimator with equivalent cost. We derive the conditions under which our proposed approximate control variate framework recovers existing multifidelity variance reduction schemes as special cases. We demonstrate that existing recursive/nested strategies are suboptimal because they use the additional low-fidelity models only to efficiently estimate the unknown mean of the first low-fidelity model. As a result, they cannot achieve variance reduction beyond that of a control variate estimator that uses a single low-fidelity model with known mean. However, there often exists about an order-of-magnitude gap between the maximum achievable variance reduction using all low-fidelity models and that achieved by a single low-fidelity model with known mean. We show that our proposed approach can exploit this gap to achieve greater variance reduction by using non-recursive sampling schemes. The proposed strategy reduces the total cost of accurately estimating statistics, especially in cases where only low-fidelity simulation models are accessible for additional evaluations. Several analytic examples and an example with a hyperbolic PDE describing elastic wave propagation in heterogeneous media are used to illustrate the main features of the methodology.},
	author = {Gorodetsky, Alex A. and Geraci, Gianluca and Eldred, Michael S. and Jakeman, John D.},
	year = {2020},
	doi = {10.1016/j.jcp.2020.109257},
	note = {arXiv: 1811.04988
Publication Title: Journal of Computational Physics
ISSN: 10902716},
	keywords = {Control variates, Monte Carlo, Multifidelity modeling, Variance reduction, ★},
}

@article{Hui2015,
	title = {Tuning {Parameter} {Selection} for the {Adaptive} {Lasso} {Using} {ERIC}},
	volume = {110},
	issn = {1537274X},
	doi = {10.1080/01621459.2014.951444},
	abstract = {The adaptive Lasso is a commonly applied penalty for variable selection in regression modeling. Like all penalties though, its performance depends critically on the choice of the tuning parameter. One method for choosing the tuning parameter is via information criteria, such as those based on AIC and BIC. However, these criteria were developed for use with unpenalized maximum likelihood estimators, and it is not clear that they take into account the effects of penalization. In this article, we propose the extended regularized information criterion (ERIC) for choosing the tuning parameter in adaptive Lasso regression. ERIC extends the BIC to account for the effect of applying the adaptive Lasso on the bias-variance tradeoff. This leads to a criterion whose penalty for model complexity is itself a function of the tuning parameter. We show the tuning parameter chosen by ERIC is selection consistent when the number of variables grows with sample size, and that this consistency holds in a wider range of contexts compared to using BIC to choose the tuning parameter. Simulation show that ERIC can significantly outperform BIC and other information criteria proposed (for choosing the tuning parameter) in selecting the true model. For ultra high-dimensional data (p {\textgreater} n), we consider a two-stage approach combining sure independence screening with adaptive Lasso regression using ERIC, which is selection consistent and performs strongly in simulation. Supplementary materials for this article are available online.},
	number = {509},
	journal = {Journal of the American Statistical Association},
	author = {Hui, Francis K.C. and Warton, David I. and Foster, Scott D.},
	year = {2015},
	keywords = {BIC, Consistency, High-dimensional data, Information criteria, Penalized likelihood, Regularization parameter, Variable selection},
	pages = {262--269},
}

@article{Sun2013,
	title = {Consistent selection of tuning parameters via variable selection stability},
	volume = {14},
	issn = {15324435},
	abstract = {Penalized regression models are popularly used in high-dimensional data analysis to conduct variable selection and model fitting simultaneously. Whereas success has been widely reported in literature, their performances largely depend on the tuning parameters that balance the trade-off between model fitting and model sparsity. Existing tuning criteria mainly follow the route of minimizing the estimated prediction error or maximizing the posterior model probability, such as cross validation, AIC and BIC. This article introduces a general tuning parameter selection criterion based on variable selection stability. The key idea is to select the tuning parameters so that the resultant penalized regression model is stable in variable selection. The asymptotic selection consistency is established for both fixed and diverging dimensions. Its effectiveness is also demonstrated in a variety of simulated examples as well as an application to the prostate cancer data. © 2013 Wei Sun, Junhui Wang and Yixin Fang.},
	journal = {Journal of Machine Learning Research},
	author = {Sun, Wei and Wang, Junhui and Fang, Yixin},
	year = {2013},
	note = {arXiv: 1208.3380},
	keywords = {Kappa coefficient, Penalized regression, Selection consistency, Stability, Tuning},
	pages = {3419--3440},
}

@article{Battles2004,
	title = {An extension of {MATLAB} to continuous functions and operators},
	volume = {25},
	issn = {10648275},
	doi = {10.1137/S1064827503430126},
	abstract = {An object-oriented MATLAB system is described for performing numerical linear algebra on continuous functions and operators rather than the usual discrete vectors and matrices. About eighty MATLAB functions from plot and sum to svd and cond have been overloaded so that one can work with our \& quot;chebfun" objects using almost exactly the usual MATLAB syntax. All functions live on [-1, 1] and are represented by values at sufficiently many Chebyshev points for the polynomial interpolant to be accurate to close to machine precision. Each of our overloaded operations raises questions about the proper generalization of familiar notions to the continuous context and about appropriate methods of interpolation, differentiation, integration, zerofinding, or transforms. Applications in approximation theory and numerical analysis are explored, and possible extensions for more substantial problems of scientific computing are mentioned.},
	number = {5},
	journal = {SIAM Journal on Scientific Computing},
	author = {Battles, Zachary and Trefethen, Lloyd N.},
	year = {2004},
	keywords = {Barycentric formula, Chebyshev points, FFT, Interpolation, MATLAB, Spectral methods, ★},
	pages = {1743--1770},
}

@article{Cortiella2020,
	title = {Sparse {Identification} of {Nonlinear} {Dynamical} {Systems} via {Reweighted} \${\textbackslash}ell\_1\$-regularized {Least} {Squares}},
	url = {http://arxiv.org/abs/2005.13232},
	abstract = {This work proposes an iterative sparse-regularized regression method to recover governing equations of nonlinear dynamical systems from noisy state measurements. The method is inspired by the Sparse Identification of Nonlinear Dynamics (SINDy) approach of \{{\textbackslash}it [Brunton et al., PNAS, 113 (15) (2016) 3932-3937]\}, which relies on two main assumptions: the state variables are known \{{\textbackslash}it a priori\} and the governing equations lend themselves to sparse, linear expansions in a (nonlinear) basis of the state variables. The aim of this work is to improve the accuracy and robustness of SINDy in the presence of state measurement noise. To this end, a reweighted \${\textbackslash}ell\_1\$-regularized least squares solver is developed, wherein the regularization parameter is selected from the corner point of a Pareto curve. The idea behind using weighted \${\textbackslash}ell\_1\$-norm for regularization -- instead of the standard \${\textbackslash}ell\_1\$-norm -- is to better promote sparsity in the recovery of the governing equations and, in turn, mitigate the effect of noise in the state variables. We also present a method to recover single physical constraints from state measurements. Through several examples of well-known nonlinear dynamical systems, we demonstrate empirically the accuracy and robustness of the reweighted \${\textbackslash}ell\_1\$-regularized least squares strategy with respect to state measurement noise, thus illustrating its viability for a wide range of potential applications.},
	author = {Cortiella, Alexandre and Park, Kwang-Chun and Doostan, Alireza},
	year = {2020},
	note = {arXiv: 2005.13232},
	keywords = {basis pursuit denoising, bpdn, nonlinear system identification, pareto curve, reweighted, sindy, sparse regression, ℓ 1 -regularization},
	pages = {1--33},
}

@article{Widman2002,
	title = {The {L}-curve and its use in the numerical treatment of inverse problems},
	volume = {35},
	issn = {15320464},
	doi = {10.1016/s1532-0464(02)00008-4},
	abstract = {The L-curve is a log-log plot of the norm of a regularized solution versus the norm of the corresponding residual norm. It is a convenient graphical tool for displaying the trade-off between the size of a regularized solution and its fit to the given data, as the regularization parameter varies. The L-curve thus gives insight into the regularizing properties of the underlying regularization method, and it is an aid in choosing an appropriate regularization parameter for the given data. In this chapter we summarize the main properties of the L-curve, and demonstrate by examples its usefulness and its limitations both as an analysis tool and as a method for choosing the regularization parameter. 1 Introduction Practically all regularization methods for computing stable solutions to inverse problems involve a trade-off between the "size" of the regularized solution and the quality of the fit that it provides to the given data. What distinguishes the various regularization methods is how...},
	number = {1},
	journal = {Journal of Biomedical Informatics},
	author = {Hansen, P C},
	year = {2002},
	keywords = {★},
	pages = {51},
}

@article{Hansen1993,
	title = {The {Use} of the {L}-{Curve} in the {Regularization} of {Discrete} {Ill}-{Posed} {Problems}},
	volume = {14},
	doi = {10.1137/0914086},
	number = {6},
	journal = {SIAM Journal on Scientific Computing},
	author = {Hansen, P C and O'Leary, D P},
	year = {1993},
	keywords = {discrepancy principle, generalized cross validation, ill-posed problems, in many applications such, introduction, l-curve, parameter choice, regularization, seismography},
	pages = {1487--1503},
}

@article{Donoho2015,
	title = {50 {Years} of {Data} {Science}},
	abstract = {More than 50 years ago, John Tukey called for a reformation of academic statistics. In 'The Future of Data Analysis', he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or 'data analysis'. Ten to twenty years ago, John Chambers, Bill Cleveland and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland even suggested the catchy name "Data Science" for his envisioned field.},
	author = {Donoho, David},
	year = {2015},
	pages = {1--41},
}

@article{Ghanem2017a,
	title = {Handbook of uncertainty quantification},
	doi = {10.1007/978-3-319-12385-1},
	abstract = {The topic of Uncertainty Quantification (UQ) has witnessed massive developments in response to the promise of achieving risk mitigation through scientific prediction. It has led to the integration of ideas from mathematics, statistics and engineering being used to lend credence to predictive assessments of risk but also to design actions (by engineers, scientists and investors) that are consistent with risk aversion. The objective of this Handbook is to facilitate the dissemination of the forefront of UQ ideas to their audiences. We recognize that these audiences are varied, with interests ranging from theory to application, and from research to development and even execution.},
	journal = {Handbook of Uncertainty Quantification},
	author = {Ghanem, Roger and Owhadi, Houman and Higdon, David},
	year = {2017},
	note = {ISBN: 9783319123851},
	keywords = {bayesian inference, global sensitivity analysis, markov chain monte carlo, polynomial chaos, quadrature, surrogate modeling, uncertainty propagation, uq toolkit, uqtk},
	pages = {1--2053},
}

@inproceedings{10.1007/978-3-319-52995-0_6,
	address = {Cham},
	title = {Single-{Stream} {Round} {Jet} at {M} = 0.9},
	isbn = {978-3-319-52995-0},
	abstract = {Results from three partners obtained using different non-zonal Grey Area Mitigation (GAM) approaches in different CFD codes are compared for an isothermal, single-stream, static round jet at \$\$ M = 0.9 \$\$and \$\$ Re\_\{D\} = 1.1 {\textbackslash}times 10{\textasciicircum}\{6\} \$\$. A systematic grid resolution study, using up to four different resolutions, shows encouraging convergence with grid refinement of the mean flow and (for one partner) the far-field noise towards experimental data. Excellent results are achieved using the novel methods for an important flow which was previously difficult to handle using standard DES methods.},
	booktitle = {{Go4Hybrid}: {Grey} {Area} {Mitigation} for {Hybrid} {RANS}-{LES} {Methods}},
	publisher = {Springer International Publishing},
	author = {Fuchs, M and Mockett, C and Shur, M and Strelets, M and Kok, J C},
	editor = {Mockett, Charles and Haase, Werner and Schwamborn, Dieter},
	year = {2018},
	pages = {125--137},
}

@incollection{Debusschere2017,
	address = {Cham},
	title = {Uncertainty {Quantification} {Toolkit} ({UQTk})},
	isbn = {978-3-319-12385-1},
	url = {https://doi.org/10.1007/978-3-319-12385-1_56},
	abstract = {The UQ Toolkit (UQTk) is a collection of tools for uncertainty quantification, ranging from intrusive and nonintrusive forward propagation of uncertainty to inverse problems and sensitivity analysis. This chapter first outlines the UQTk design philosophy, followed by an overview of the available methods and the way they are implemented in UQTk. The second part of this chapter is a detailed example that illustrates a UQ workflow from surrogate construction, and calibration, to forward propagation and attribution.},
	booktitle = {Handbook of {Uncertainty} {Quantification}},
	publisher = {Springer International Publishing},
	author = {Debusschere, Bert and Sargsyan, Khachik and Safta, Cosmin and Chowdhary, Kenny},
	editor = {Ghanem, Roger and Higdon, David and Owhadi, Houman},
	year = {2017},
	doi = {10.1007/978-3-319-12385-1_56},
	pages = {1807--1827},
}

@article{Ng2012,
	title = {Multifidelity uncertainty quantification using non-intrusive polynomial chaos and stochastic collocation},
	issn = {02734508},
	doi = {10.2514/6.2012-1852},
	abstract = {This paper explores the extension of multifidelity modeling concepts to the field of uncertainty quantification. Motivated by local correction functions that enable the provable convergence of a multifidelity optimization approach to an optimal high-fidelity point solution, we extend these ideas to global discrepancy modeling within a stochastic domain and seek convergence of a multifidelity uncertainty quantification process to globally integrated high-fidelity statistics. For constructing stochastic models of both the low fidelity model and the model discrepancy, we employ stochastic expansion methods (nonintrusive polynomial chaos and stochastic collocation) computed from sparse grids, where we seek to employ a coarsely resolved grid for the discrepancy in combination with a more finely resolved grid for the low fidelity model. The resolutions of these grids may be statically defined or determined through uniform and adaptive refinement processes. Adaptive refinement is particularly attractive, as it has the ability to preferentially target stochastic regions where the model discrepancy becomes more complex; i.e., where the predictive capabilities of the low-fidelity model start to break down and greater reliance on the high fidelity model (via the discrepancy) is necessary. These adaptive refinement processes can either be performed separately for the different sparse grids or within a unified multifidelity algorithm. In particular, we propose an adaptive greedy multifidelity approach in which we extend the generalized sparse grid concept to consider candidate index set refinements drawn from multiple sparse grids. We demonstrate that the multifidelity UQ process converges more rapidly than a single-fidelity UQ in cases where the variance of the discrepancy is reduced relative to the variance of the high fidelity model (resulting in reductions in initial stochastic error) and/or where the spectrum of the expansion coefficients of the model discrepancy decays more rapidly than that of the high-fidelity model (resulting in accelerated convergence rates). © 2012 AIAA.},
	number = {April},
	journal = {Collection of Technical Papers - AIAA/ASME/ASCE/AHS/ASC Structures, Structural Dynamics and Materials Conference},
	author = {Ng, L. W.T. and Eldred, M. S.},
	year = {2012},
	note = {ISBN: 9781600869372},
	pages = {1--17},
}

@book{Ghanem2017,
	title = {Handbook of uncertainty quantification},
	isbn = {978-3-319-12385-1},
	abstract = {The topic of Uncertainty Quantification (UQ) has witnessed massive developments in response to the promise of achieving risk mitigation through scientific prediction. It has led to the integration of ideas from mathematics, statistics and engineering being used to lend credence to predictive assessments of risk but also to design actions (by engineers, scientists and investors) that are consistent with risk aversion. The objective of this Handbook is to facilitate the dissemination of the forefront of UQ ideas to their audiences. We recognize that these audiences are varied, with interests ranging from theory to application, and from research to development and even execution.},
	author = {Ghanem, Roger and Owhadi, Houman and Higdon, David},
	year = {2017},
	doi = {10.1007/978-3-319-12385-1},
	note = {Publication Title: Handbook of Uncertainty Quantification},
}

@incollection{Eldred2017,
	address = {Cham},
	title = {Multifidelity {Uncertainty} {Quantification} {Using} {Spectral} {Stochastic} {Discrepancy} {Models}},
	isbn = {978-3-319-12385-1},
	url = {https://doi.org/10.1007/978-3-319-12385-1_25},
	abstract = {When faced with a restrictive evaluation budget that is typical of today's high-fidelity simulation models, the effective exploitation of lower-fidelity alternatives within the uncertainty quantification (UQ) process becomes critically important. Herein, we explore the use of multifidelity modeling within UQ, for which we rigorously combine information from multiple simulation-based models within a hierarchy of fidelity, in seeking accurate high-fidelity statistics at lower computational cost. Motivated by correction functions that enable the provable convergence of a multifidelity optimization approach to an optimal high-fidelity point solution, we extend these ideas to discrepancy modeling within a stochastic domain and seek convergence of a multifidelity uncertainty quantification process to globally integrated high-fidelity statistics. For constructing stochastic models of both the low-fidelity model and the model discrepancy, we employ stochastic expansion methods (non-intrusive polynomial chaos and stochastic collocation) computed by integration/interpolation on structured sparse grids or regularized regression on unstructured grids. We seek to employ a coarsely resolved grid for the discrepancy in combination with a more finely resolved grid for the low-fidelity model. The resolutions of these grids may be defined statically or determined through uniform and adaptive refinement processes. Adaptive refinement is particularly attractive, as it has the ability to preferentially target stochastic regions where the model discrepancy becomes more complex, i.e., where the predictive capabilities of the low-fidelity model start to break down and greater reliance on the high-fidelity model (via the discrepancy) is necessary. These adaptive refinement processes can either be performed separately for the different grids or within a coordinated multifidelity algorithm. In particular, we present an adaptive greedy multifidelity approach in which we extend the generalized sparse grid concept to consider candidate index set refinements drawn from multiple sparse grids, as governed by induced changes in the statistical quantities of interest and normalized by relative computational cost. Through a series of numerical experiments using statically defined sparse grids, adaptive multifidelity sparse grids, and multifidelity compressed sensing, we demonstrate that the multifidelity UQ process converges more rapidly than a single-fidelity UQ in cases where the variance of the discrepancy is reduced relative to the variance of the high-fidelity model (resulting in reductions in initial stochastic error), where the spectrum of the expansion coefficients of the model discrepancy decays more rapidly than that of the high-fidelity model (resulting in accelerated convergence rates), and/or where the discrepancy is more sparse than the high-fidelity model (requiring the recovery of fewer significant terms).},
	booktitle = {Handbook of {Uncertainty} {Quantification}},
	publisher = {Springer International Publishing},
	author = {Eldred, Michael S and Ng, Leo W T and Barone, Matthew F and Domino, Stefan P},
	editor = {Ghanem, Roger and Higdon, David and Owhadi, Houman},
	year = {2017},
	doi = {10.1007/978-3-319-12385-1_25},
	pages = {991--1036},
}

@incollection{Sargsyan2017,
	address = {Cham},
	title = {Surrogate {Models} for {Uncertainty} {Propagation} and {Sensitivity} {Analysis}},
	isbn = {978-3-319-12385-1},
	url = {https://doi.org/10.1007/978-3-319-12385-1_22},
	abstract = {For computationally intensive tasks such as design optimization, global sensitivity analysis, or parameter estimation, a model of interest needs to be evaluated multiple times exploring potential parameter ranges or design conditions. If a single simulation of the computational model is expensive, it is common to employ a precomputed surrogate approximation instead. The construction of an appropriate surrogate does still require a number of training evaluations of the original model. Typically, more function evaluations lead to more accurate surrogates, and therefore a careful accuracy-vs-efficiency tradeoff needs to take place for a given computational task. This chapter specifically focuses on polynomial chaos surrogates that are well suited for forward uncertainty propagation tasks, discusses a few construction mechanisms for such surrogates, and demonstrates the computational gain on select test functions.},
	booktitle = {Handbook of {Uncertainty} {Quantification}},
	publisher = {Springer International Publishing},
	author = {Sargsyan, Khachik},
	editor = {Ghanem, Roger and Higdon, David and Owhadi, Houman},
	year = {2017},
	doi = {10.1007/978-3-319-12385-1_22},
	pages = {673--698},
}

@article{Fernandez-Godino2016,
	title = {Review of multi-fidelity models},
	url = {http://arxiv.org/abs/1609.07196},
	abstract = {Simulations are often computationally expensive and the need for multiple realizations, as in uncertainty quantification or optimization, makes surrogate models an attractive option. For expensive high-fidelity models (HFMs), however, even performing the number of simulations needed for fitting a surrogate may be too expensive. Inexpensive but less accurate low-fidelity models (LFMs) are often also available. Multi-fidelity models (MFMs) combine HFMs and LFMs in order to achieve accuracy at a reasonable cost. With the increasing popularity of MFMs in mind, the aim of this paper is to summarize the state-of-the-art of MFM trends. For this purpose, publications in this field are classified based on application, surrogate selection if any, the difference between fidelities, the method used to combine these fidelities, the field of application and the year published. Available methods of combining fidelities are also reviewed, focusing our attention especially on multi-fidelity surrogate models in which fidelities are combined inside a surrogate model. Computation time savings are usually the reason for using MFMs, hence it is important to properly report the achieved savings. Unfortunately, we find that many papers do not present sufficient information to determine these savings. Therefore, the paper also includes guidelines for authors to present their MFM savings in a way that is useful to future MFM users. Based on papers that provided enough information, we find that time savings are highly problem dependent and that MFM methods we surveyed provided time savings up to 90\%. Keywords: Multi-fidelity, Variable-complexity, Variable-fidelity, Surrogate models, Optimization, Uncertainty quantification, Review, Survey},
	author = {Fernández-Godino, M. Giselle and Park, Chanyoung and Kim, Nam-Ho and Haftka, Raphael T.},
	year = {2016},
	note = {arXiv: 1609.07196},
	keywords = {fl 32611, gainesville, models, multi-fidelity, optimization, review, surrogate, survey, uncertainty quantification, university of florida, variable-complexity, variable-fidelity, ★},
}

@book{Maitre2010,
	title = {Specral {Methods} for {Uncertainty} {Quantification}: with {Applications} to {Computational} {Fluid} {Dynamics}},
	isbn = {978-90-481-3519-6},
	publisher = {Springer Netherlands, Houten, Netherlands},
	author = {Le Maitre, O.P. and Knio, O.M.},
	year = {2010},
	doi = {10.1007/978-90-481-3520-2},
	keywords = {★},
}

@article{Nemirovski2006,
	title = {Downloaded 09 / 21 / 14 to 210 . 32 . 178 . 82 . {Redistribution} subject to {SIAM} license or copyright ; see http://www.siam.org/journals/ojsa.php},
	volume = {45},
	number = {2},
	author = {Equation, Space-fractional Diffusion and Ervin, Vincent J and Heuer, Norbert and Roop, John Paul},
	year = {2007},
	keywords = {050642757, 1, 10, 1137, 65n30, ams subject classification, anomalous diffusion, doi, finite element approximation, in this paper we, introduction, nonlinear parabolic equation, study the numerical approximation, to time},
	pages = {572--591},
}

@article{GiselleFernandez-Godino2019,
	title = {Issues in deciding whether to use multifidelity surrogates},
	volume = {57},
	issn = {00011452},
	doi = {10.2514/1.J057750},
	abstract = {Multifidelity surrogates are essential in cases where it is not affordable to have more than a few high-fidelity samples, but it is affordable to have as many low-fidelity samples as needed. In these cases, given a good correlation between the models, the performance of multifidelity models can be outstanding. The first objective of this paper is to discuss progress in creating accurate multifidelity surrogates when they are essential. A more ambiguous situation exists when it may be possible to afford enough high-fidelity samples to construct an accurate surrogate model. In that case, the question is whether a multifidelity surrogate will afford a substantial cost reduction for comparable accuracy. Our the second objective is to see if there are any indications under what circumstances this substantial cost reduction is realized. From the literature, it appears that it is hard to get an idea, in terms of cost savings, of when it is useful to invest the additional effort of creating and using multifidelity surrogates. It is observed that in some cases the inclusion of low-fidelity samples along with the high-fidelity samples in building multifidelity surrogates led to less accurate surrogates than just using the available high-fidelity samples.},
	number = {5},
	journal = {AIAA Journal},
	author = {Giselle Fernández-Godino, M. and Park, Chanyoung and Kim, Nam H. and Haftka, Raphael T.},
	year = {2019},
	pages = {2039--2054},
}

@article{Biehler2019,
	title = {Multifidelity approaches for uncertainty quantification},
	volume = {42},
	issn = {09367195},
	doi = {10.1002/gamm.201900008},
	abstract = {The aim of this paper is to give an overview of different multifidelity uncertainty quantification (UQ) schemes. Therefore, different views on multifidelity UQ approaches from a frequentist, Bayesian, and possibilistic perspective are provided and recent developments are discussed. Differences as well as similarities between the methods are highlighted and strategies to construct low-fidelity models are explained. In addition, two state-of-the-art examples to showcase the capabilities of these methods and the tremendous reduction of computational costs that can be achieved when using these approaches are provided.},
	number = {2},
	journal = {GAMM Mitteilungen},
	author = {Biehler, Jonas and Mäck, Markus and Nitzler, Jonas and Hanss, Michael and Koutsourelakis, Phaedon Stelios and Wall, Wolfgang A.},
	month = may,
	year = {2019},
	note = {Publisher: Wiley-VCH Verlag},
	keywords = {Bayesian, multifidelity, possibilistic, uncertainty quantification},
}

@article{Najm2009,
	title = {Uncertainty {Quantification} and {Polynomial} {Chaos} {Techniques} in {Computational} {Fluid} {Dynamics}},
	volume = {41},
	issn = {0066-4189},
	doi = {10.1146/annurev.fluid.010908.165248},
	abstract = {The quantification of uncertainty in computational fluid dynamics (CFD) predictions is both a significant challenge and an important goal. Probabilistic uncertainty quantification (UQ) methods have been used to propagate uncertainty from model inputs to outputs when input ...},
	number = {1},
	journal = {Annual Review of Fluid Mechanics},
	author = {Najm, Habib N.},
	month = jan,
	year = {2009},
	note = {Publisher: Annual Reviews},
	keywords = {★},
	pages = {35--52},
}

@article{Boyd2010,
	title = {Distributed optimization and statistical learning via the alternating direction method of multipliers},
	volume = {3},
	issn = {19358237},
	doi = {10.1561/2200000016},
	abstract = {printed},
	number = {1},
	journal = {Foundations and Trends in Machine Learning},
	author = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
	year = {2010},
	pages = {1--122},
}

@inproceedings{Huan2019,
	title = {Uncertainty propagation using conditional random fields in large-eddy simulations of scramjet computations},
	isbn = {978-1-62410-578-4},
	doi = {10.2514/6.2019-0724},
	booktitle = {{AIAA} {Scitech} 2019 {Forum}},
	publisher = {American Institute of Aeronautics and Astronautics Inc, AIAA},
	author = {Huan, Xun and Safta, Cosmin and Vane, Zachary P. and Lacaze, Guilhem and Oefelein, Joseph C. and Najm, Habib N.},
	year = {2019},
}

@article{Wilke2019,
	title = {Variable-fidelity methodology for the aerodynamic optimization of helicopter rotors},
	volume = {57},
	issn = {00011452},
	doi = {10.2514/1.J056486},
	abstract = {The design of helicopter rotor blades is a challenging task. On the one hand, there are the demanding simulations, which are a multidisciplinary endeavor. On the other hand, tools for parametric studies or optimizations require many simulations, making the design process even more costly. In the rotorcraft community, two routes for the numerical optimization task are observed. The first route is based upon local gradient search algorithms, which exploit low-fidelity tools or adjoint-based computational fluid dynamics (CFD) simulations. The second route is surrogate-based optimization in combination with high-fidelity CFD simulations. These surrogate-based optimizations can be further accelerated, when knowledge from low-fidelity models is used. This paper presents a framework that is developed for the multi-objective aerodynamic optimization of helicopter rotor blades including surrogate models based on different fidelities. The individual components necessary for performing a variable-fidelity, multi-objective optimization are reviewed before being applied. A novel technique to deal with unsuccessful simulations referred to as a failure map is additionally presented. The gain of the variable-fidelity optimizations in contrast to the single-fidelity optimizations is quantified, and a reduction in computational resources of up to 69\% is observed. The failure map requires 78\% less resources in contrast to the classical failure handling.},
	number = {8},
	journal = {AIAA Journal},
	author = {Wilke, Gunther},
	year = {2019},
	pages = {3145--3158},
}

@article{Lauwers2007,
	title = {One is enough!},
	volume = {28},
	issn = {16107438},
	doi = {10.1177/2158244011428647},
	abstract = {We postulate that multi-wheel statically-stable mobile robots for operation in human environments are an evolutionary dead end. Robots of this class tall enough to interact meaningfully with people must have low centers of gravity, overly wide bases of support, and very low accelerations to avoid tipping over. Accordingly, we are developing an inverse of this type of mobile robot that is the height, width, and weight of a person, having a high center of gravity, that balances dynamically on a single spherical wheel. Unlike balancing 2-wheel platforms which must turn before driving in some direction, the single-wheel robot can move directly in any direction. We present the overall design, actuator mechanism based on an inverse mouse-ball drive, control system, and initial results including dynamic balancing, station keeping, and point-to-point motion.},
	number = {July},
	journal = {Springer Tracts in Advanced Robotics},
	author = {Lauwers, Tom and Kantor, George and Hollis, Ralph},
	year = {2007},
	pages = {114--127},
}

@article{Davenport2016,
	title = {An {Overview} of {Low}-{Rank} {Matrix} {Recovery} from {Incomplete} {Observations}},
	volume = {10},
	issn = {19324553},
	doi = {10.1109/JSTSP.2016.2539100},
	abstract = {Low-rank matrices play a fundamental role in modeling and computational methods for signal processing and machine learning. In many applications where low-rank matrices arise, these matrices cannot be fully sampled or directly observed, and one encounters the problem of recovering the matrix given only incomplete and indirect observations. This paper provides an overview of modern techniques for exploiting low-rank structure to perform matrix recovery in these settings, providing a survey of recent advances in this rapidly-developing field. Specific attention is paid to the algorithms most commonly used in practice, the existing theoretical guarantees for these algorithms, and representative practical applications of these techniques.},
	number = {4},
	journal = {IEEE Journal on Selected Topics in Signal Processing},
	author = {Davenport, Mark A. and Romberg, Justin},
	year = {2016},
	note = {arXiv: 1601.06422
Publisher: IEEE},
	pages = {608--622},
}

@article{Boutsidis2008,
	title = {{SVD} based initialization: {A} head start for nonnegative matrix factorization},
	volume = {41},
	issn = {00313203},
	doi = {10.1016/j.patcog.2007.09.010},
	abstract = {We describe Nonnegative Double Singular Value Decomposition (NNDSVD), a new method designed to enhance the initialization stage of nonnegative matrix factorization (NMF). NNDSVD can readily be combined with existing NMF algorithms. The basic algorithm contains no randomization and is based on two SVD processes, one approximating the data matrix, the other approximating positive sections of the resulting partial SVD factors utilizing an algebraic property of unit rank matrices. Simple practical variants for NMF with dense factors are described. NNDSVD is also well suited to initialize NMF algorithms with sparse factors. Many numerical examples suggest that NNDSVD leads to rapid reduction of the approximation error of many NMF algorithms. © 2007 Elsevier Ltd. All rights reserved.},
	number = {4},
	journal = {Pattern Recognition},
	author = {Boutsidis, C. and Gallopoulos, E.},
	year = {2008},
	keywords = {Low rank, NMF, Nonnegative matrix factorization, Perron-Frobenius, SVD, Singular value decomposition, Sparse NMF, Sparse factorization, Structured initialization},
	pages = {1350--1362},
}

@article{Prasadan2019,
	title = {Time {Series} {Source} {Separation} using {Dynamic} {Mode} {Decomposition}},
	url = {http://arxiv.org/abs/1903.01310%0Ahttp://dx.doi.org/10.5281/zenodo.2656681},
	doi = {10.5281/zenodo.2656681},
	abstract = {The Dynamic Mode Decomposition (DMD) extracted dynamic modes are the non-orthogonal eigenvectors of the matrix that best approximates the one-step temporal evolution of the multivariate samples. In the context of dynamical system analysis, the extracted dynamic modes are a generalization of global stability modes. We apply DMD to a data matrix whose rows are linearly independent, additive mixtures of latent time series. We show that when the latent time series are uncorrelated at a lag of one time-step then, in the large sample limit, the recovered dynamic modes will approximate, up to a column-wise normalization, the columns of the mixing matrix. Thus, DMD is a time series blind source separation algorithm in disguise, but is different from closely related second order algorithms such as the Second-Order Blind Identification (SOBI) method and the Algorithm for Multiple Unknown Signals Extraction (AMUSE). All can unmix mixed stationary, ergodic Gaussian time series in a way that kurtosis-based Independent Components Analysis (ICA) fundamentally cannot. We use our insights on single lag DMD to develop a higher-lag extension, analyze the finite sample performance with and without randomly missing data, and identify settings where the higher lag variant can outperform the conventional single lag variant. We validate our results with numerical simulations, and highlight how DMD can be used in change point detection.},
	author = {Prasadan, Arvind and Nadakuditi, Raj Rao},
	year = {2019},
	note = {arXiv: 1903.01310},
}

@article{Takeishi2017,
	title = {Bayesian dynamic mode decomposition},
	issn = {10450823},
	doi = {10.24963/ijcai.2017/392},
	abstract = {Dynamic mode decomposition (DMD) is a datadriven method for calculating a modal representation of a nonlinear dynamical system, and it has been utilized in various fields of science and engineering. In this paper, we propose Bayesian DMD, which provides a principled way to transfer the advantages of the Bayesian formulation into DMD. To this end, we first develop a probabilistic model corresponding to DMD, and then, provide the Gibbs sampler for the posterior inference in Bayesian DMD. Moreover, as a specific example, we discuss the case of using a sparsity-promoting prior for an automatic determination of the number of dynamic modes. We investigate the empirical performance of Bayesian DMD using synthetic and real-world datasets.},
	journal = {IJCAI International Joint Conference on Artificial Intelligence},
	author = {Takeishi, Naoya and Kawahara, Yoshinobu and Tabei, Yasuo and Yairi, Takehisa},
	year = {2017},
	note = {ISBN: 9780999241103},
	keywords = {Machine Learning: Time-series/Data Streams, Machine Learning: Unsupervised Learning, Uncertainty in AI: Uncertainty in AI},
	pages = {2814--2821},
}

@article{Levy2001,
	title = {Efficient sequential {Karhunen}-{Loeve} basis extraction},
	volume = {2},
	doi = {10.1109/ICCV.2001.937701},
	abstract = {An approach to reduce computational effort, relying on the relatively small dimension of the partial Karhunen-Loeve (KL) basis is suggested. An algorithm that does not require to store the entire set of input images before proceeding to the calculation of the KL basis is proposed. The algorithm is named Sequential Karhunen Loeve algorithm (SKL).},
	number = {8},
	journal = {Proceedings of the IEEE International Conference on Computer Vision},
	author = {Levy, Avraham and Lindenbaum, Michael},
	year = {2001},
	pages = {739},
}

@article{Gavish2014,
	title = {The optimal hard threshold for singular values is 4/√3},
	volume = {60},
	issn = {00189448},
	doi = {10.1109/TIT.2014.2323359},
	abstract = {We consider recovery of low-rank matrices from noisy data by hard thresholding of singular values, in which empirical singular values below a threshold λ are set to 0. We study the asymptotic mean squared error (AMSE) in a framework, where the matrix size is large compared with the rank of the matrix to be recovered, and the signal-to-noise ratio of the low-rank piece stays constant. The AMSE-optimal choice of hard threshold, in the case of n-by- n matrix in white noise of level σ, is simply (4/√3) √nσ ≈ 2.309 √nσ when σ is known, or simply 2.858 · ymed when σ is unknown, where ymed is the median empirical singular value. For nonsquare, m by n matrices with m ≠ n the thresholding coefficients 4/√3 and 2.858 are replaced with different provided constants that depend on m/n. Asymptotically, this thresholding rule adapts to unknown rank and unknown noise level in an optimal manner: it is always better than hard thresholding at any other value, and is always better than ideal truncated singular value decomposition (TSVD), which truncates at the true rank of the low-rank matrix we are trying to recover. Hard thresholding at the recommended value to recover an n-by-n matrix of rank r guarantees an AMSE at most 3 nrσ2. In comparison, the guarantees provided by TSVD, optimally tuned singular value soft thresholding and the best guarantee achievable by any shrinkage of the data singular values are 5 nrσ2, 6 nrσ2, and 2 nrσ2, respectively. The recommended value for hard threshold also offers, among hard thresholds, the best possible AMSE guarantees for recovering matrices with bounded nuclear norm. Empirical evidence suggests that performance improvement over TSVD and other popular shrinkage rules can be substantial, for different noise distributions, even in relatively small n. © 2014 IEEE.},
	number = {8},
	journal = {IEEE Transactions on Information Theory},
	author = {Gavish, Matan and Donoho, David L.},
	year = {2014},
	note = {arXiv: 1305.5870},
	keywords = {Singular values shrinkage, bulk edge, low-rank matrix denoising, optimal threshold, quarter circle law, scree plot elbow truncation, unique admissible},
	pages = {5040--5053},
}

@article{Alexanderian2015,
	title = {A brief note on the {Karhunen}-{Loeve} expansion},
	url = {http://arxiv.org/abs/1509.07526},
	abstract = {We provide a detailed derivation of the Karhunen-Lo{\textbackslash}`eve expansion of a stochastic process. We also discuss briefly Gaussian processes, and provide a simple numerical study for the purpose of illustration.},
	author = {Alexanderian, Alen},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.07526},
}

@techreport{Gerbrands1981,
	title = {{ON} {THE} {RELATIONSHIPS} {BETWEEN} {SVD}, {KLT} {AND} {PCA}},
	abstract = {In recent literature on digital image processing much attention is devoted to the singular value decomposition (SVD) of a matrix. Many authors refer to the Karhunen-Loeve transform (KLT) and principal components analysis (PCA) while treating the SVD. In this paper we give definitions of the three transforms and investigate their relationships. It is shown that in the context of multivariate statistical analysis and statistical pattern recognition the three transforms are very similar if a specific estimate of the column covariance matrix is used. In the context of two-dimensional image processing this similarity still holds if one single matrix is considered. In that approach the use of the names KLT and PCA is rather inappropriate and confusing. If the matrix is considered to be a realization of a two-dimensional random process, the SVD and the two statistically defined transforms differ substantially. Image processing Statistical analysis Statistical pattern recognition Orthogonal image transforms Singular value decomposition Karhunen-Loeve transform Principal components},
	author = {Gerbrands, Jan J},
	year = {1981},
	note = {Volume: 14},
	pages = {375--381},
}

@article{OHagan2011,
	title = {Polynomial {Chaos}: {A} {Tutorial} and {Critique} from a {Statisticianís} {Perspective}},
	abstract = {This article is written in the spirit of helping recent e¤orts to build bridges between the community of researchers in …elds such as applied mathematics and engineering, where the term UQ began, and the commu-nity of statisticians who work on problems of uncertainty in the predictions of mechanistic models. It is addressed to researchers and practitioners in both communities. The …rst purpose of the article is to explain polynomial chaos, one of the key tools of the …rst community, in terms that will be readily understood by a statistician in the second community. The second purpose is to explain to researchers in the …rst community some aspects of PC, both in theory and in practice, that a statistician might regard as de…ciencies or limitations.},
	journal = {Tonyohagan.Co.Uk},
	author = {O'Hagan, Anthony},
	year = {2011},
	keywords = {★},
	pages = {1--16},
}

@article{urzay_supersonic_2018,
	title = {Supersonic {Combustion} in {Air}-{Breathing} {Propulsion} {Systems} for {Hypersonic} {Flight}},
	volume = {50},
	url = {https://doi.org/10.1146/annurev-fluid-122316-},
	doi = {10.1146/annurev-fluid-122316},
	abstract = {Great efforts have been dedicated during the last decades to the research and development of hypersonic aircrafts that can fly at several times the speed of sound. These aerospace vehicles have revolutionary applications in national security as advanced hypersonic weapons, in space exploration as reusable stages for access to low Earth orbit, and in commercial aviation as fast long-range methods for air transportation of passengers around the globe. This review addresses the topic of supersonic combustion, which represents the central physical process that enables scramjet hypersonic propulsion systems to accelerate aircrafts to ultra-high speeds. The description focuses on recent experimental flights and ground-based research programs and highlights associated fundamental flow physics, subgrid-scale model development, and full-system numerical simulations. 593},
	journal = {Annu. Rev. Fluid Mech},
	author = {Urzay, Javier},
	year = {2018},
	keywords = {compressible flows, high-speed chemical propulsion, hypersonics, scramjets, sound barrier, turbulent combustion},
	pages = {593--627},
}

@techreport{Ubaru2016,
	title = {Fast methods for estimating the {Numerical} rank of large matrices},
	abstract = {We present two computationally inexpensive techniques for estimating the numerical rank of a matrix, combining powerful tools from computational linear algebra. These techniques exploit three key ingredients. The first is to approximate the projector on the non-null invariant subspace of the matrix by using a polynomial filter. Two types of filters are discussed, one based on Her-mite interpolation and the other based on Cheby-shev expansions. The second ingredient employs stochastic trace estimators to compute the rank of this wanted eigen-projector, which yields the desired rank of the matrix. In order to obtain a good filter, it is necessary to detect a gap between the eigenvalues that correspond to noise and the relevant eigenvalues that correspond to the non-null invariant subspace. The third ingredient of the proposed approaches exploits the idea of spectral density, popular in physics, and the Lanczos spectroscopic method to locate this gap.},
	author = {Ubaru, Shashanka and Saad, Yousef},
	year = {2016},
	keywords = {★},
}

@article{Yan2018,
	title = {Gaussian processes and polynomial chaos expansion for regression problem: {Linkage} via the {RKHS} and comparison via the {KL} divergence},
	volume = {20},
	issn = {10994300},
	doi = {10.3390/e20030191},
	abstract = {In this paper, we examine two widely-used approaches, the polynomial chaos expansion (PCE) and Gaussian process (GP) regression, for the development of surrogate models. The theoretical differences between the PCE and GP approximations are discussed. A state-of-the-art PCE approach is constructed based on high precision quadrature points; however, the need for truncation may result in potential precision loss; the GP approach performs well on small datasets and allows a fine and precise trade-off between fitting the data and smoothing, but its overall performance depends largely on the training dataset. The reproducing kernel Hilbert space (RKHS) and Mercer's theorem are introduced to form a linkage between the two methods. The theorem has proven that the two surrogates can be embedded in two isomorphic RKHS, by which we propose a novel method named Gaussian process on polynomial chaos basis (GPCB) that incorporates the PCE and GP. A theoretical comparison is made between the PCE and GPCB with the help of the Kullback-Leibler divergence. We present that the GPCB is as stable and accurate as the PCE method. Furthermore, the GPCB is a one-step Bayesian method that chooses the best subset of RKHS in which the true function should lie, while the PCE method requires an adaptive procedure. Simulations of 1D and 2D benchmark functions show that GPCB outperforms both the PCE and classical GP methods. In order to solve high dimensional problems, a random sample scheme with a constructive design (i.e., tensor product of quadrature points) is proposed to generate a valid training dataset for the GPCB method. This approach utilizes the nature of the high numerical accuracy underlying the quadrature points while ensuring the computational feasibility. Finally, the experimental results show that our sample strategy has a higher accuracy than classical experimental designs; meanwhile, it is suitable for solving high dimensional problems.},
	number = {3},
	journal = {Entropy},
	author = {Yan, Liang and Duan, Xiaojun and Liu, Bowen and Xu, Jin},
	year = {2018},
	keywords = {Experimental design, Gaussian process, Kullback-Leibler divergence, Polynomial chaos expansion, Reproducing kernel Hilbert space, ★},
}

@article{Uw2017,
	title = {Spectral {Clustering}},
	doi = {10.1007/978-1-4899-7687-1_100437},
	journal = {Encyclopedia of Machine Learning and Data Mining},
	author = {Uw, S},
	year = {2017},
	pages = {1167--1167},
}

@article{Brunton2016,
	title = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
	volume = {113},
	issn = {10916490},
	doi = {10.1073/pnas.1517384113},
	abstract = {Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.},
	number = {15},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan and Bialek, William},
	year = {2016},
	note = {arXiv: 1509.03580},
	keywords = {Dynamical systems, Machine learning, Optimization, Sparse regression, System identification, ★},
	pages = {3932--3937},
}

@article{VonLuxburg2007,
	title = {A tutorial on spectral clustering},
	volume = {17},
	issn = {09603174},
	doi = {10.1007/s11222-007-9033-z},
	abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed. © 2007 Springer Science+Business Media, LLC.},
	number = {4},
	journal = {Statistics and Computing},
	author = {Von Luxburg, Ulrike},
	year = {2007},
	note = {arXiv: 0711.0189},
	keywords = {Graph Laplacian, Spectral clustering},
	pages = {395--416},
}

@techreport{Srivastava2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Salakhutdinov, Ruslan},
	year = {2014},
	note = {Publication Title: Journal of Machine Learning Research
Volume: 15},
	keywords = {deep learning, model combination, neural networks, regularization, ★},
	pages = {1929--1958},
}

@techreport{Angelil,
	title = {Bubble {Evolution} and {Properties} in {Homogeneous} {Nucleation} {Simulations}},
	abstract = {We analyze the properties of naturally formed nano-bubbles in Lennard-Jones molecular dynamics simulations of liquid-to-vapor nucleation in the boiling and the cavitation regimes. The large computational volumes provide a realistic environment at unchanging average temperature and liquid pressure, which allows us to accurately measure properties of bubbles from their inception as stable, critically sized bubbles, to their continued growth into the constant speed regime. Bubble gas densities are up to 50\% lower than the equilibrium vapor densities at the liquid temperature, yet quite close to the gas equilibrium density at the lower gas temperatures measured in the simulations: The latent heat of transformation results in bubble gas temperatures up to 25\% below those of the surrounding bulk liquid. In the case of rapid bubble growth-typical for the cavitation regime-compression of the liquid outside the bubble leads to local temperature increases of up to 5\%, likely significant enough to alter the surface tension as well as the local viscosity. The liquid-vapor bubble interface is thinner than expected from planar coexistence simulations by up to 50\%. Bubbles near the critical size are extremely non-spherical, yet they quickly become spherical as they grow. The Rayleigh-Plesset description of bubble-growth gives good agreement in the cavitation regime.},
	author = {Angélil, Raymond and Diemand, Jürg and Tanaka, Kyoko K and Tanaka, Hidekazu},
	note = {arXiv: 1411.4782v2
Volume: 36},
}

@article{Zhou2019,
	title = {Towards {Real}-{Time} {In}-{Flight} {Ice} {Detection} {Systems} via {Computational} {Aeroacoustics} and {Machine} {Learning}},
	doi = {10.2514/6.2019-3103},
	number = {June},
	author = {Zhou, Beckett Yx and Gauger, Nicolas R. and Hauth, Jeremiah and Huan, Xun and Morelli, Myles and Guardone, Alberto},
	year = {2019},
	pages = {1--15},
}

@article{Petz2001,
	title = {Entropy, von {Neumann} and the von {Neumann} {Entropy}},
	doi = {10.1007/978-94-017-2012-0_7},
	abstract = {This paper is an introduction to the von Neumann entropy in a historic approach. Von Neumann's gedanken experiment is repeated, which led him to the formula of thermodynamic entropy of a statistical operator. In the analysis of his ideas we stress the role of superselection sectors and summarize von Neumann's knowledge about quantum mechanical entropy. The final part of the paper is devoted to important developments discovered long after von Neumann's work. Subadditivity and the interpretation of the von Neumann entropy as channel capacity are among those.},
	journal = {John von Neumann and the Foundations of Quantum Physics},
	author = {Petz, Dénes},
	year = {2001},
	note = {arXiv: math-ph/0102013},
	pages = {83--96},
}

@article{LeMagoarou2016,
	title = {Flexible {Multilayer} {Sparse} {Approximations} of {Matrices} and {Applications}},
	volume = {10},
	issn = {19324553},
	doi = {10.1109/JSTSP.2016.2543461},
	abstract = {The computational cost of many signal processing and machine learning techniques is often dominated by the cost of applying certain linear operators to high-dimensional vectors. This paper introduces an algorithm aimed at reducing the complexity of applying linear operators in high dimension by approximately factorizing the corresponding matrix into few sparse factors. The approach relies on recent advances in nonconvex optimization. It is first explained and analyzed in details and then demonstrated experimentally on various problems including dictionary learning for image denoising and the approximation of large matrices arising in inverse problems.},
	number = {4},
	journal = {IEEE Journal on Selected Topics in Signal Processing},
	author = {Le Magoarou, Luc and Gribonval, Remi},
	year = {2016},
	note = {arXiv: 1506.07300
ISBN: 2011277906},
	keywords = {Sparse representations, dictionary learning, fast algorithms, image denoising, inverse problems, low complexity, ★},
	pages = {688--700},
}

@article{Tu2014,
	title = {On dynamic mode decomposition: {Theory} and applications},
	volume = {1},
	issn = {21582505},
	doi = {10.3934/jcd.2014.1.391},
	abstract = {Originally introduced in the fluid mechanics community, dynamic mode decomposition (DMD) has emerged as a powerful tool for analyzing the dynamics of nonlinear systems. However, existing DMD theory deals primarily with sequential time series for which the measurement dimension is much larger than the number of measurements taken. We present a theoretical framework in which we define DMD as the eigendecomposition of an approximating linear operator. This generalizes DMD to a larger class of datasets, including nonsequential time series. We demonstrate the utility of this approach by presenting novel sampling strategies that increase computational effciency and mitigate the effects of noise, respectively. We also introduce the concept of linear consistency, which helps explain the potential pitfalls of applying DMD to rank-deficient datasets, illustrating with examples. Such computations are not considered in the existing literature but can be understood using our more general framework. In addition, we show that our theory strengthens the connections between DMD and Koopman operator theory. It also establishes connections between DMD and other techniques, including the eigensystem realization algorithm (ERA), a system identification method, and linear inverse modeling (LIM), a method from climate science. We show that under certain conditions, DMD is equivalent to LIM.},
	number = {2},
	journal = {Journal of Computational Dynamics},
	author = {Tu, Jonathan H. and Rowley, Clarence W. and Luchtenburg, Dirk M. and Brunton, Steven L. and Kutz, J. Nathan},
	year = {2014},
	note = {arXiv: 1312.0041},
	keywords = {Dynamic mode decomposition, Koopman operator, Reduced-order models, Spectral analysis, Time series analysis},
	pages = {391--421},
}

@article{PatricHeas2017,
	title = {{OPTIMAL} {LOW}-{RANK} {DYNAMIC} {MODE} {DECOMPOSITION} {Patrick} {H} ´ eas and {C} ´ edric {Herzet} {INRIA} {Centre} {Rennes} - {Bretagne} {Atlantique} , {Campus} universitaire de {Beaulieu} , 35000 {Rennes} , {France}},
	number = {3},
	journal = {IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2017},
	author = {Patric Heas, Cedric Herzet{\textbackslash}},
	year = {2017},
	note = {ISBN: 9781509041176},
	keywords = {★},
	pages = {4456--4460},
}

@techreport{Gal2016,
	title = {Uncertainty in {Deep} {Learning}},
	author = {Gal, Yarin},
	year = {2016},
}

@techreport{Heckerman1996a,
	title = {A {Tutorial} on {Learning} {With} {Bayesian} {Networks}},
	abstract = {A Bayesian network is a graphical model that encodes probabilistic relationships among variables of interest. When used in conjunction with statistical techniques, the graphical model has several advantages for data analysis. One, because the model encodes dependencies among all variables, it readily handles situations where some data entries are missing. Two, a Bayesian network can be used to learn causal relationships, and hence can be used to gain understanding about a problem domain and to predict the consequences of intervention. Three, because the model has both a causal and probabilistic semantics, it is an ideal representation for combining prior knowledge (which often comes in causal form) and data. Four, Bayesian statistical methods in conjunction with Bayesian networks offer an efficient and principled approach for avoiding the overfitting of data. In this paper, we discuss methods for constructing Bayesian networks from prior knowledge and summarize Bayesian statistical methods for using data to improve these models. With regard to the latter task, we describe methods for learning both the parameters and structure of a Bayesian network, including techniques for learning with incomplete data. In addition, we relate Bayesian-network methods for learning to techniques for supervised and unsupervised learning. We illustrate the graphical-modeling approach using a real-world case study.},
	author = {Heckerman, David},
	year = {1996},
	keywords = {★},
}

@article{Liu2016,
	title = {Stein variational gradient descent: {A} general purpose {Bayesian} inference algorithm},
	issn = {10495258},
	abstract = {We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Liu, Qiang and Wang, Dilin},
	year = {2016},
	note = {arXiv: 1608.04471},
	pages = {2378--2386},
}

@article{Larsen,
	title = {Lanczos {Bidiagonalization} with {Partial} {Reorthogonalization} - svds},
	author = {Larsen, Rasmus},
}

@article{Huang2017,
	title = {Fast {Ambiguity} {Resolution} for {Pulsar}-{Based} {Navigation} by {Means} of {Hypothesis} {Testing}},
	volume = {53},
	issn = {00189251},
	doi = {10.1109/TAES.2017.2649698},
	abstract = {The problem of the global navigation satellite system carrier-phase ambiguity resolution (AR) has drawn a lot of attention and to some extent been well solved in the past. However, the pulse-phase AR in the pulsar-based navigation has not been fully investigated due to its special feature that the source pulsars are far away enough to be considered stationary relative to the observers. In this paper, the indeterminate AR problem is proposed to describe this AR situation by a group of indeterminate measurement equations and a new method is developed that enables faster estimation of pulse-phase ambiguities. This method is based on the hypothesis testing that helps one to construct the ambiguity acceptance space at a certain significance level. The acceptance space is then reformulated as a linear form via singular value decomposition that is much easier to search for. Besides, the search algorithm is redesigned that uses particle swarm optimization to quickly find an initial solution and employs a new parameter to compress the search space. As a result, the ambiguity search can be performed much more efficiently especially at big problem sizes.},
	number = {1},
	journal = {IEEE Transactions on Aerospace and Electronic Systems},
	author = {Huang, Liangwei and Lin, Qingqing and Zhang, Xinyuan and Shuai, Ping},
	year = {2017},
	keywords = {Acceptance space, ambiguity resolution (AR), hypothesis testing, matching search, pulsar-based navigation (PNAV)},
	pages = {137--147},
}

@article{Lin2016,
	title = {Densities of {Large} {Matrices} ∗},
	volume = {58},
	number = {1},
	author = {Lin, Lin},
	year = {2016},
	keywords = {10, 1137, 130934283, 15a18, 65f15, ams subject classifications, approximation of distribu-, density of states, doi, large scale sparse matrix, quantum mechanics, spectral density, tion, ★},
	pages = {34--65},
}

@article{Fleeter2019,
	title = {Multilevel and multifidelity uncertainty quantification for cardiovascular hemodynamics},
	url = {http://arxiv.org/abs/1908.04875},
	abstract = {Standard approaches for uncertainty quantification (UQ) in cardiovascular modeling pose challenges due to the large number of uncertain inputs and the significant computational cost of realistic 3D simulations. We propose an efficient UQ framework utilizing a multilevel multifidelity Monte Carlo (MLMF) estimator to improve the accuracy of hemodynamic quantities of interest while maintaining reasonable computational cost. This is achieved by leveraging three cardiovascular model fidelities, each with varying spatial resolution to rigorously quantify the variability in hemodynamic outputs. Our goal is to investigate and compare the efficiency of estimators built from two low-fidelity model alternatives and our high-fidelity 3D models. We demonstrate this framework on healthy and diseased models of aortic and coronary anatomy, including uncertainties in material property and boundary condition parameters. Our goal is to demonstrate that for this application it is possible to accelerate the convergence of the estimators by utilizing a MLMF paradigm. Therefore, we compare our approach to single fidelity Monte Carlo estimators and to a multilevel Monte Carlo approach based only on 3D simulations, but leveraging multiple spatial resolutions. We demonstrate significant, on the order of 10 to 100 times, reduction in total computational cost with the MLMF estimators. We also examine the differing properties of the MLMF estimators in healthy versus diseased models, as well as global versus local quantities of interest. As expected, healthy models and global quantities show larger reductions than diseased models and local quantities as the latter rely more heavily on the highest fidelity model evaluations. In all cases, our workflow coupling Dakota MLMF estimators with the SimVascular cardiovascular workflow make UQ feasible for constrained computational budgets.},
	author = {Fleeter, Casey M. and Geraci, Gianluca and Schiavazzi, Daniele E. and Kahn, Andrew M. and Marsden, Alison L.},
	year = {2019},
	note = {arXiv: 1908.04875},
	keywords = {cardiovascular modeling, monte carlo, multifidelity, multilevel monte carlo, multilevel multifidelity monte carlo, uncertainty quantification, ★},
}

@article{Quick2019,
	title = {Correction: {Multifidelity} {Uncertainty} {Quantification} with {Applications} in {Wind} {Turbine} {Aerodynamics}},
	doi = {10.2514/6.2019-0542.c1},
	abstract = {The propagation of input uncertainty through engineering models allows designers to better understand the range of possible outcomes resulting from design decisions. This could lead to greater trust between modelers and stakeholders in the wind energy industry. In this study, we apply multilevel-multifidelity Monte Carlo sampling to flow over an airfoil, assuming uncertainty in the inflow conditions, and characterize the associated computational savings compared to standard Monte Carlo approaches. The truth model is provided by an airfoil simulation with a very fine computational time step, and auxiliary lower-level models are provided by simulations with coarser time steps. Reynolds-averaged Navier Stokes and detached eddy simulations are used to obtain two different model fidelities. The primary quantity of interest for this analysis is the lift force, which is examined for a range of angles of attack. We launch an initial set of "trial" samples to determine the optimal allocation of model evaluations, and these trial evaluations are used to inform a larger sampling effort. Using the multilevel-multifidelity approach, we achieve roughly an order of magnitude variance reduction in expected lift as compared to the standard Monte Carlo approach with an equivalent computational cost.},
	number = {January},
	author = {Quick, Julian and Hamlington, Peter E. and King, Ryan and Sprague, Michael A.},
	year = {2019},
}

@article{Sculley2015,
	title = {Hidden technical debt in machine learning systems},
	volume = {2015-Janua},
	issn = {10495258},
	abstract = {Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean François and Dennison, Dan},
	year = {2015},
	pages = {2503--2511},
}

@article{VillanuevaZacarias2018,
	title = {A framework to guide the selection and configuration of machine-learning-based data analytics solutions in manufacturing},
	volume = {72},
	issn = {22128271},
	url = {https://doi.org/10.1016/j.procir.2018.03.215},
	doi = {10.1016/j.procir.2018.03.215},
	abstract = {Users in manufacturing willing to apply machine-learning-based (ML-based) data analytics face challenges related to data quality or to the selection and configuration of proper ML algorithms. Current approaches are either purely empirical or reliant on technical data. This makes understanding and comparing candidate solutions difficult, and also ignores the way it impacts the real application problem. In this paper, we propose a framework to generate analytics solutions based on a systematic profiling of all aspects involved. With it, users can visually and systematically explore relevant alternatives for their specific scenario, and obtain recommendations in terms of costs, productivity, results quality, or execution time.},
	journal = {Procedia CIRP},
	author = {Villanueva Zacarias, Alejandro Gabriel and Reimann, Peter and Mitschang, Bernhard},
	year = {2018},
	note = {Publisher: Elsevier B.V.
ISBN: 4971168578402},
	keywords = {data analytics, generative design, learning algorithms, machine learning},
	pages = {153--158},
}

@article{Blundell2015,
	title = {Weight {Uncertainty} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1505.05424},
	abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	month = may,
	year = {2015},
	note = {arXiv: 1505.05424},
}

@inproceedings{Mohr2004,
	title = {Cell-centred multigrid revisited},
	volume = {7},
	doi = {10.1007/s00791-004-0137-0},
	abstract = {In this paper we treat the cell-centred multigrid approach, which distinguishes itself from the classical vertexcentred multigrid by a non-nested hierarchy of grid nodes and the use of constant, problem-independent transfer operators even in complicated situations. We demonstrate, that the tool of Local Fourier Analysis can also be profitably applied in this setting. We consider in detail the standard transfer operators from literature and their respective polynomial and Fourier orders, paying special attention to the combination of piecewise constant interpolation and its adjoint. Furthermore, we give several numerical examples for model problems and an application from biomedical engineering. © Springer-Verlag 2004.},
	booktitle = {Computing and {Visualization} in {Science}},
	publisher = {Springer Verlag},
	author = {Mohr, Marcus and Wienands, Roman},
	year = {2004},
	note = {Issue: 3-4
ISSN: 14330369},
	pages = {129--140},
}

@article{Edelman2005,
	title = {Random matrix theory},
	volume = {14},
	issn = {0962-4929},
	url = {https://www.cambridge.org/core/product/identifier/S0962492904000236/type/journal_article},
	doi = {10.1017/S0962492904000236},
	abstract = {{\textless}p{\textgreater}Random matrix theory is now a big subject with applications in many disciplines of science, engineering and finance. This article is a survey specifically oriented towards the needs and interests of a numerical analyst. This survey includes some original material not found anywhere else. We include the important mathematics which is a very modern development, as well as the computational software that is transforming the theory into useful practice.{\textless}/p{\textgreater}},
	journal = {Acta Numerica},
	author = {Edelman, Alan and Rao, N. Raj},
	month = may,
	year = {2005},
	pages = {233--297},
}

@article{Carrassi2018,
	title = {Data assimilation in the geosciences: {An} overview of methods, issues, and perspectives},
	volume = {9},
	issn = {17577799},
	doi = {10.1002/wcc.535},
	abstract = {We commonly refer to state-estimation theory in geosciences as data assimilation. This term encompasses the entire sequence of operations that, starting from the observations of a system, and from additional statistical and dynamical information (such as a dynamical evolution model), provides an estimate of its state. Data assimilation is standard practice in numerical weather prediction, but its application is becoming widespread in many other areas of climate, atmosphere, ocean and environment modeling; in all circumstances where one intends to estimate the state of a large dynamical system based on limited information. While the complexity of data assimilation, and of the methods thereof, stands on its interdisciplinary nature across statistics, dynamical systems and numerical optimization, when applied to geosciences an additional difficulty arises by the continually increasing sophistication of the environmental models. Thus, in spite of data assimilation being nowadays ubiquitous in geosciences, it has so far remained a topic mostly reserved to experts. We aim this overview article at geoscientists with a background in mathematical and physical modeling, who are interested in the rapid development of data assimilation and its growing domains of application in environmental science, but so far have not delved into its conceptual and methodological complexities.},
	number = {5},
	journal = {Wiley Interdisciplinary Reviews: Climate Change},
	author = {Carrassi, Alberto and Bocquet, Marc and Bertino, Laurent and Evensen, Geir},
	month = sep,
	year = {2018},
	note = {Publisher: Wiley-Blackwell},
	keywords = {Bayesian methods, data assimilation, ensemble methods, environmental prediction},
}

@incollection{Nikkar2013,
	title = {Energy {Stable} {High} {Order} {Finite} {Difference} {Methods} for {Hyperbolic} {Equations} in {Moving} {Coordinate} {Systems}},
	url = {https://doi.org/10.2514/6.2013-2579},
	booktitle = {21st {AIAA} {Computational} {Fluid} {Dynamics} {Conference}},
	publisher = {American Institute of Aeronautics and Astronautics},
	author = {Nikkar, Samira and Nordstrom, Jan},
	month = jun,
	year = {2013},
	doi = {10.2514/6.2013-2579},
	note = {Series Title: Fluid Dynamics and Co-located Conferences},
}

@techreport{noauthor_dimensionality_nodate,
	title = {Dimensionality {Reduction}},
}

@article{fredriksson_model-based_2010,
	title = {Model-based quantitative laser {Doppler} flowmetry in skin},
	volume = {15},
	issn = {1083-3668},
	doi = {10.1117/1.3484746},
	abstract = {Laser Doppler flowmetry (LDF) can be used for assessing the microcirculatory perfusion. However, conventional LDF (cLDF) gives only a relative perfusion estimate for an unknown measurement volume, with no information about the blood flow speed distribution. To overcome these limitations, a model-based analysis method for quantitative LDF (qLDF) is proposed. The method uses inverse Monte Carlo technique with an adaptive three-layer skin model. By analyzing the optimal model where measured and simulated LDF spectra detected at two different source-detector separations match, the absolute microcirculatory perfusion for a specified speed region in a predefined volume is determined. qLDF displayed errors{\textless}12\% when evaluated using simulations of physiologically relevant variations in the layer structure, in the optical properties of static tissue, and in blood absorption. Inhomogeneous models containing small blood vessels, hair, and sweat glands displayed errors{\textless}5\%. Evaluation models containing single larger blood vessels displayed significant errors but could be dismissed by residual analysis. In vivo measurements using local heat provocation displayed a higher perfusion increase with qLDF than cLDF, due to nonlinear effects in the latter. The qLDF showed that the perfusion increase occurred due to an increased amount of red blood cells with a speed{\textgreater}1 mm∕s.},
	number = {5},
	journal = {Journal of Biomedical Optics},
	author = {Fredriksson, Ingemar},
	month = sep,
	year = {2010},
	note = {Publisher: SPIE-Intl Soc Optical Eng},
	pages = {057002},
}

@article{fredriksson_machine_2019,
	title = {Machine learning in multiexposure laser speckle contrast imaging can replace conventional laser {Doppler} flowmetry},
	volume = {24},
	issn = {1560-2281},
	doi = {10.1117/1.jbo.24.1.016001},
	abstract = {Laser speckle contrast imaging (LSCI) enables video rate imaging of
blood flow. However, its relation to tissue blood perfusion is nonlinear
and depends strongly on exposure time. By contrast, the perfusion
estimate from the slower laser Doppler flowmetry (LDF) technique has a
relationship to blood perfusion that is better understood. Multiexposure
LSCI (MELSCI) enables a perfusion estimate closer to the actual
perfusion than that using a single exposure time. We present and
evaluate a method that utilizes contrasts from seven exposure times
between 1 and 64 ms to calculate a perfusion estimate that resembles the
perfusion estimate from LDF. The method is based on artificial neural
networks (ANN) for fast and accurate processing of MELSCI contrasts to
perfusion. The networks are trained using modeling of Doppler histograms
and speckle contrasts from tissue models. The importance of accounting
for noise is demonstrated. Results show that by using ANN, MELSCI data
can be processed to LDF perfusion with high accuracy, with a correlation
coefficient R = 1.000 for noise-free data, R = 0.993 when a moderate
degree of noise is present, and R = 0.995 for in vivo data from an
occlusion-release experiment. (C) The Authors. Published by SPIE under a
Creative Commons Attribution 4.0 Unported License.},
	number = {01},
	journal = {Journal of Biomedical Optics},
	author = {Fredriksson, Ingemar and Hultman, Martin and Strömberg, Tomas and Larsson, Marcus},
	month = jan,
	year = {2019},
	note = {Publisher: SPIE-Intl Soc Optical Eng},
	pages = {1},
}

@article{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02531},
}

@article{wang_gaussian_2016,
	title = {Gaussian process surrogates for failure detection: {A} {Bayesian} experimental design approach},
	volume = {313},
	issn = {10902716},
	doi = {10.1016/j.jcp.2016.02.053},
	abstract = {An important task of uncertainty quantification is to identify the probability of undesired events, in particular, system failures, caused by various sources of uncertainties. In this work we consider the construction of Gaussian process surrogates for failure detection and failure probability estimation. In particular, we consider the situation that the underlying computer models are extremely expensive, and in this setting, determining the sampling points in the state space is of essential importance. We formulate the problem as an optimal experimental design for Bayesian inferences of the limit state (i.e., the failure boundary) and propose an efficient numerical scheme to solve the resulting optimization problem. In particular, the proposed limit-state inference method is capable of determining multiple sampling points at a time, and thus it is well suited for problems where multiple computer simulations can be performed in parallel. The accuracy and performance of the proposed method is demonstrated by both academic and practical examples.},
	journal = {Journal of Computational Physics},
	author = {Wang, Hongqiao and Lin, Guang and Li, Jinglai},
	month = may,
	year = {2016},
	note = {Publisher: Academic Press Inc.},
	keywords = {Bayesian inference, Experimental design, Failure detection, Gaussian processes, Monte Carlo, Response surfaces, Uncertainty quantification},
	pages = {247--259},
}

@techreport{sapsis_output-weighted_2019,
	title = {Output-weighted optimal sampling for {Bayesian} regression and rare event statistics using few samples},
	abstract = {For many important problems the quantity of interest (or output) is an unknown function of the parameter space (or input), which is a random vector with known statistics. Since the dependence of the output on this random vector is unknown, the challenge is to identify its statistics, using the minimum number of function evaluations. This is a problem that can been seen in the context of active learning or optimal experimental design. We employ Bayesian regression to represent the derived model uncertainty due to finite and small number of input-output pairs. In this context we evaluate existing methods for optimal sample selection, such as model error minimization and mutual information maximization. We show that the commonly employed criteria in the literature do not take into account the output values of the existing input-output pairs. To overcome this deficiency we introduce a new criterion that explicitly takes into account the values of the output for the existing samples and adaptively selects inputs from regions or dimensions of the parameter space which have important contribution to the output. The new method allows for application to a large number of input variables, paving the way for optimal experimental design in very high-dimensions.},
	author = {Sapsis, Themistoklis P},
	year = {2019},
	keywords = {Active learning, Bayesian regression, Opti-mal sampling, Optimal experimental design, Rare extreme events},
}

@techreport{gal_dropout_2016,
	title = {Dropout as a {Bayesian} {Approximation}: {Representing} {Model} {Uncertainty} in {Deep} {Learning} {Zoubin} {Ghahramani}},
	url = {http://yarin.co.},
	abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison , Bayesian models offer a mathematically grounded framework to reason about model uncertainty , but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs-extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predic-tive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
	author = {Gal, Yarin and Uk, Zg201@cam Ac},
	year = {2016},
	keywords = {★},
}

@techreport{noauthor_karhunen-loeve_2008,
	title = {Karhunen-{Loeve} {Expansions} and their {Applications} {Limin} {Wang}},
	year = {2008},
}

@article{niiyama_murine_2009,
	title = {Murine model of hindlimb ischemia},
	issn = {1940087X},
	doi = {10.3791/1035},
	abstract = {In the United States, peripheral arterial disease (PAD) affects about 10 million individuals, and is also prevalent worldwide. Medical therapies forsymptomatic relief are limited. Surgical or endovascular interventions are useful for some individuals, but long-term results are often disappointing.As a result, there is a need for developing new therapies to treat PAD. The murine hindlimb ischemia preparation is a model of PAD, and is usefulfor testing new therapies. When compared to other models of tissue ischemia such as coronary or cerebral artery ligation, femoral artery ligationprovides for a simpler model of ischemic tissue. Other advantages of this model are the ease of access to the femoral artery and low mortality rate.In this video, we demonstrate the methodology for the murine model of unilateral hindimb ischemia. The specific materials and procedures forcreating and evaluating the model will be described, including the assessment of limb perfusion by laser Doppler imaging. This protocol can also beutilized for the transplantation and non-invasive tracking of cells, which is demonstrated by Huang et al. © 2009 Journal of Visualized Experiments.},
	number = {23},
	journal = {Journal of Visualized Experiments},
	author = {Niiyama, Hiroshi and Huang, Ngan F. and Rollins, Mark D. and Cooke, John P.},
	year = {2009},
	note = {Publisher: Journal of Visualized Experiments},
}

@techreport{bharai_steady_1982,
	title = {{STEADY} {FLOW} {IN} {A} {MODEL} {OF} {THE} {HUMAN} {CAROTID} {BIFURCATION}. {PART} {I}-{FLOW} {VISUALIZATION}*},
	abstract = {The geometry of a typical adult human carotid bifurcation, complete with the sinus, was established from a study of a large number of angiograms. A rigid model was constructed from glass and investigations were performed under steady flow conditions using flow visualization techniques over a range of upstream Reynolds numbers and flow division ratios through the branches representative of physiologic conditions expected in the human vasculature. The study reveals a complex flow field in which secondary flows play an important role. The separation regions occuring at the outer corners of the branching are also zones of Iow wall shear stress but are not regions of recirculation. The apex and side walls appear to be subjected to much higher shear stress. Comparison with pathologic data on Iocalization of atherosclerotic lesions indicates that zones susceptible to disease experience low or oscillatory shear stress while regions subject to higher shear are free of deposits.},
	author = {Bharai, B K and Mabon, R F and Giddens, D P},
	year = {1982},
	note = {Publication Title: J. Biomechanics
Volume: 15
Issue: 5},
	pages = {349--362},
}

@techreport{ku_pulsatile_nodate,
	title = {Pulsatile {Flow} and {Atherosclerosis} in the {Human} {Carotid} {Bifurcation} {Positive} {Correlation} between {Plaque} {Location} and {Low} and {Oscillating} {Shear} {Stress}},
	abstract = {Fluid velocities were measured by laser Doppler velocimetry under conditions of pulsatile flow in a scale model of the human carotid bifurcation. Flow velocity and wall shear stress at five axial and four circumferential positions were compared with intimal plaque thickness at corresponding locations in carotid bifurcations obtained from cadavers. Velocities and wall shear stresses during diastole were similar to those found previously under steady flow conditions, but these quantities oscillated in both magnitude and direction during the systolic phase. At the inner wall of the internal carotid sinus, in the region of the flow divider, wall shear stress was highest (systole = 41 dynes/cm 2 , diastole = 10 dynes/cm 2 , mean = 17 dynes/cm 2) and remained unidirectional during systole. Intimal thickening in this location was minimal. At the outer wall of the carotid sinus where intimal plaques were thickest, mean shear stress was low (-0. 5 dynes/cm 2) but the instantaneous shear stress oscillated between -7 and + 4 dynes/cm 2. Along the side walls of the sinus, intimal plaque thickness was greater than in the region of the flow divider and circumferential oscillations of shear stress were prominent. With all 20 axial and circumferential measurement locations considered, strong correlations were found between intimal thickness and the reciprocal of maximum shear stress (r = 0.90, p {\textless} 0.0005) or the reciprocal of mean shear stress (r = 0.82, p {\textless} 0.001). An index which takes into account oscillations of wall shear also correlated strongly with intimal thickness (r-0.82, p {\textless} 0.001). When only the inner wall and outer wall positions were taken into account, correlations of lesion thickness with the inverse of maximum wall shear and mean wall shear were 0.94 (p {\textless} 0.001) and 0.95 (p {\textless} 0.001), respectively, and with the oscillatory shear index, 0.93 (p {\textless} 0.001). These studies confirm earlier findings under steady flow conditions that plaques tend to form in areas of low, rather than high, shear stress, but indicate in addition that marked oscillations in the direction of wall shear may enhance atherogenesis. (Arteriosclerosis 5:293-302, May/June 1985) T he role of specific hemodynamic variables in the initiation and development of atherosclerotic plaques in human arteries can be assessed by correlating flow field measurements with the distribution of intimal lesions about branch ostia and bifurcations. flow velocity profiles in situ do not provide sufficient spatial resolution to describe the complex flow patterns at such locations, nor are current imaging techniques adequate for precise localization of early non-stenosing lesions. With the use of appropriate geometric and scaling parameters, rigid glass or plastic models can be used to visualize flow profiles and measure flow velocities representative of those occurring in human vessels. Postmortem human arteries are suitable for deter-minations of corresponding plaque location and size if appropriate pressure-fixation procedures are used. Previous comparison of steady flow measurements in scale models of the human carotid bifurcation with plaque deposition in a corresponding series of autopsy specimens 1 revealed that early lesions occurred principally in regions of flow separation, low wall shear stress, and departure from unidirectional 293},
	author = {Ku, David N and Giddens, Don P and Zarins, Christopher K and Glagov, Seymour},
}

@inproceedings{shaik_numerical_2006,
	title = {Numerical flow simulations of blood in arteries},
	volume = {5},
	isbn = {1-56347-807-2},
	doi = {10.2514/6.2006-294},
	abstract = {The objectives of this investigation are 1) to differentiate the effect of Newtonian and non-Newtonian fluid flow considering the three dimensional models of the artery, 2) to investigate the effect of arterial geometry by considering two models of the arterial bifurcation: (model 1) daughter and parent artery diameters are the same and (model 2) different arterial diameters for both parent and daughter arteries, and 3) to correlate the development of the atherosclerosis in the arteries for both steady and pulsatile flow simulations. In the computations, the non-Newtonian behavior of blood was described using the Carreau-Yasuda model. The predicted effect and the velocity pattern of non-Newtonian model was in qualitative agreement with the experimental measurements available in the literature. The effect of non-Newtonian flow on the shear stress was calculated on both the inner and the outer wall of the two arteries. The areas of extremes in shear stress (low or high), which correlate with the development of atherosclerosis in the arteries are identified for both steady and pulsatile flow cases. Pulsatile flow simulations resulted in showing the variance in the shear stresses at different time levels of the pulse cycle.},
	booktitle = {Collection of {Technical} {Papers} - 44th {AIAA} {Aerospace} {Sciences} {Meeting}},
	author = {Shaik, Eleyas and Hoffmann, Klaus A. and Dietiker, Jean Francois},
	year = {2006},
	pages = {3498--3511},
}

@techreport{rogowska_5_2008,
	title = {5 {Overview} and {Fundamentals} of {Medical} {Image} {Segmentation}},
	author = {Rogowska, Jadwiga},
	year = {2008},
	doi = {10.1016/B978-0-12-373904-9.50013-1},
	note = {Publication Title: HANDBOOK OF MEDICAL IMAGE PROCESSING AND ANALYSIS},
	keywords = {★},
	pages = {73--90},
}

@techreport{wu_study_2012,
	title = {Study on {De}-noising {Extraction} and {Digitization} {Methods} of {Blood} {Vessel}},
	author = {Wu, Cong},
	year = {2012},
}

@article{humeau-heurtier_microvascular_2015,
	title = {Microvascular blood flow monitoring with laser speckle contrast imaging using the generalized differences algorithm},
	volume = {98},
	issn = {10959319},
	doi = {10.1016/j.mvr.2014.12.003},
	abstract = {Laser speckle contrast imaging (LSCI) is a full-field optical technique to monitor microvascular blood flow with high spatial and temporal resolutions. It is used in many medical fields such as dermatology, vascular medicine, or neurosciences. However, LSCI leads to a large amount of data: image sampling frequency is often of several Hz and recordings usually last several minutes. Therefore, clinicians often perform regions of interest in which a spatial averaging of blood flow is performed and the result is followed with time. Unfortunately, this leads to a poor spatial resolution for the analyzed data. At the same time, a higher spatial resolution for the perfusion maps is wanted. To get over this dilemma we propose a new post-acquisition visual representation for LSCI perfusion data using the so-called generalized differences (GD) algorithm. From a stack of perfusion images, the procedure leads to a new single image with the same spatial resolution as the original images and this new image reflects perfusion changes. The algorithm is herein applied on simulated stacks of images and on experimental LSCI perfusion data acquired in three different situations with a commercialized laser speckle contrast imager. The results show that the GD algorithm provides a new way of visualizing LSCI perfusion data.},
	journal = {Microvascular Research},
	author = {Humeau-Heurtier, Anne and Mahé, Guillaume and Abraham, Pierre},
	month = mar,
	year = {2015},
	note = {Publisher: Academic Press Inc.},
	keywords = {Blood flow, Image analysis, Laser speckle, Medical and biological imaging, Microcirculation},
	pages = {54--61},
}

@article{gnyawali_retooling_2017,
	title = {Retooling {Laser} {Speckle} {Contrast} {Analysis} {Algorithm} to {Enhance} {Non}-{Invasive} {High} {Resolution} {Laser} {Speckle} {Functional} {Imaging} of {Cutaneous} {Microcirculation}},
	volume = {7},
	issn = {20452322},
	doi = {10.1038/srep41048},
	abstract = {Cutaneous microvasculopathy complicates wound healing. Functional assessment of gated individual dermal microvessels is therefore of outstanding interest. Functional performance of laser speckle contrast imaging (LSCI) systems is compromised by motion artefacts. To address such weakness, post-processing of stacked images is reported. We report the first post-processing of binary raw data from a high-resolution LSCI camera. Sharp images of low-flowing microvessels were enabled by introducing inverse variance in conjunction with speckle contrast in Matlab-based program code. Extended moving window averaging enhanced signal-To-noise ratio. Functional quantitative study of blood flow kinetics was performed on single gated microvessels using a free hand tool. Based on detection of flow in low-flow microvessels, a new sharp contrast image was derived. Thus, this work presents the first distinct image with quantitative microperfusion data from gated human foot microvasculature. This versatile platform is applicable to study a wide range of tissue systems including fine vascular network in murine brain without craniotomy as well as that in the murine dorsal skin. Importantly, the algorithm reported herein is hardware agnostic and is capable of post-processing binary raw data from any camera source to improve the sensitivity of functional flow data above and beyond standard limits of the optical system.},
	journal = {Scientific Reports},
	author = {Gnyawali, Surya C. and Blum, Kevin and Pal, Durba and Ghatak, Subhadip and Khanna, Savita and Roy, Sashwati and Sen, Chandan K.},
	month = jan,
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	keywords = {★},
}

@article{kochi_characterization_2013,
	title = {Characterization of the arterial anatomy of the murine hindlimb: {Functional} role in the design and understanding of ischemia models},
	volume = {8},
	issn = {19326203},
	doi = {10.1371/journal.pone.0084047},
	abstract = {Rationale: Appropriate ischemia models are required for successful studies of therapeutic angiogenesis. While collateral routes are known to be present within the innate vasculature, there are no reports describing the detailed vascular anatomy of the murine hindlimb. In addition, differences in the descriptions of anatomical names and locations in the literature impede understanding of the circulation and the design of hindlimb ischemia models. To understand better the collateral circulation in the whole hindlimb, clarification of all the feeding arteries of the hindlimb is required. Objective: The aim of this study is to reveal the detailed arterial anatomy and collateral routes in murine hindlimb to enable the appropriate design of therapeutic angiogenesis studies and to facilitate understanding of the circulation in ischemia models. Methods and Results: Arterial anatomy in the murine hindlimb was investigated by contrast-enhanced X-ray imaging and surgical dissection. The observed anatomy is shown in photographic images and in a schema. Previously unnoticed but relatively large arteries were observed in deep, cranial and lateral parts of the thigh. The data indicates that there are three collateral routes through the medial thigh, quadriceps femoris, and the biceps femoris muscles. Furthermore, anatomical variations were found at the origins of the three feeding arteries. Conclusions: The detailed arterial anatomy of murine hindlimb and collateral routes deduced from the anatomy are described. Limitations on designs of ischemia models in view of anatomical variations are proposed. These observations will contribute to the development of animal studies of therapeutic angiogenesis using murine hindlimb ischemia models.},
	number = {12},
	journal = {PLoS ONE},
	author = {Kochi, Takashi and Imai, Yoshimichi and Takeda, Atsushi and Watanabe, Yukiko and Mori, Shiro and Tachi, Masahiro and Kodama, Tetsuya},
	month = dec,
	year = {2013},
	keywords = {★},
}

@article{briers_laser_nodate,
	title = {Laser speckle contrast imaging: theoretical and practical limitations},
	url = {https://www.spiedigitallibrary.org/terms-of-use},
	doi = {10.1117/1},
	abstract = {When laser light illuminates a diffuse object, it produces a random interference effect known as a speckle pattern. If there is movement in the object, the speckles fluctuate in intensity. These fluctuations can provide information about the movement. A simple way of accessing this information is to image the speckle pattern with an exposure time longer than the shortest speckle fluctuation time scale-the fluctuations cause a blurring of the speckle, leading to a reduction in the local speckle contrast. Thus, velocity distributions are coded as speckle contrast variations. The same information can be obtained by using the Doppler effect, but producing a two-dimensional Doppler map requires either scanning of the laser beam or imaging with a high-speed camera: laser speckle contrast imaging (LSCI) avoids the need to scan and can be performed with a normal CCD-or CMOS-camera. LSCI is used primarily to map flow systems, especially blood flow. The development of LSCI is reviewed and its limitations and problems are investigated.},
	author = {Briers, David and Duncan, Donald D and Hirst, Evan and Kirkpatrick, Sean J and Larsson, Marcus and Steenbergen, Wiendelt and Stromberg, Tomas and Thompson, Oliver B},
	keywords = {blood flow, laser Doppler, laser speckle, medical imaging, perfusion Paper 130209R, time-varying speckle},
}

@techreport{abbeel_apprenticeship_nodate,
	title = {Apprenticeship {Learning} via {Inverse} {Reinforcement} {Learning}},
	abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is ex-pressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the ex-pert's unknown reward function.},
	author = {Abbeel, Pieter and Ng, Andrew Y},
	keywords = {★},
}

@article{tompson_accelerating_2016,
	title = {Accelerating {Eulerian} {Fluid} {Simulation} {With} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1607.03597},
	abstract = {Efficient simulation of the Navier-Stokes equations for fluid flow is a long standing problem in applied mathematics, for which state-of-the-art methods require large compute resources. In this work, we propose a data-driven approach that leverages the approximation power of deep-learning with the precision of standard solvers to obtain fast and highly realistic simulations. Our method solves the incompressible Euler equations using the standard operator splitting method, in which a large sparse linear system with many free parameters must be solved. We use a Convolutional Network with a highly tailored architecture, trained using a novel unsupervised learning framework to solve the linear system. We present real-time 2D and 3D simulations that outperform recently proposed data-driven methods; the obtained results are realistic and show good generalization properties.},
	author = {Tompson, Jonathan and Schlachter, Kristofer and Sprechmann, Pablo and Perlin, Ken},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.03597},
	keywords = {★},
}

@article{liu_reinforcement_2017,
	title = {Reinforcement {Learning} {Optimized} {Look}-{Ahead} {Energy} {Management} of a {Parallel} {Hybrid} {Electric} {Vehicle}},
	volume = {22},
	issn = {10834435},
	doi = {10.1109/TMECH.2017.2707338},
	abstract = {This paper presents a predictive energy management strategy for a parallel hybrid electric vehicle (HEV) based on velocity prediction and reinforcement learning (RL). The design procedure starts with modeling the parallel HEV as a systematic control-oriented model and defining a cost function. Fuzzy encoding and nearest neighbor approaches are proposed to achieve velocity prediction, and a finite-state Markov chain is exploited to learn transition probabilities of power demand. To determine the optimal control behaviors and power distribution between two energy sources, a novel RL-based energy management strategy is introduced. For comparison purposes, the two velocity prediction processes are examined by RL using the same realistic driving cycle. The look-ahead energy management strategy is contrasted with shortsighted and dynamic programming based counterparts, and further validated by hardware-in-the-loop test. The results demonstrate that the RL-optimized control is able to significantly reduce fuel consumption and computational time.},
	number = {4},
	journal = {IEEE/ASME Transactions on Mechatronics},
	author = {Liu, Teng and Hu, Xiaosong and Li, Shengbo Eben and Cao, Dongpu},
	month = aug,
	year = {2017},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Energy management, Markov chain (MC), hybrid electric vehicle (HEV), predictive control, reinforcement learning (RL)},
	pages = {1497--1507},
}

@article{rodwell_characteristics_2013,
	title = {Characteristics of occasional poor medium-range weather forecasts for {Europe}},
	volume = {94},
	issn = {00030007},
	doi = {10.1175/BAMS-D-12-00099.1},
	number = {9},
	journal = {Bulletin of the American Meteorological Society},
	author = {Rodwell, Mark J. and Magnusson, Linus and Bauer, Peter and Bechtold, Peter and Bonavita, Massimo and Cardinali, Carla and Diamantakis, Michail and Earnshaw, Paul and Garcia-Mendez, Antonio and Isaksen, Lars and Källén, Erland and Klocke, Daniel and Lopez, Philippe and McNally, Tony and Persson, Anders and Prates, Fernando and Wedi, Nils},
	month = sep,
	year = {2013},
	pages = {1393--1405},
}

@article{derakhti_predicting_2018,
	title = {Predicting the breaking strength of gravity water waves in deep and intermediate depth},
	volume = {848},
	issn = {14697645},
	doi = {10.1017/jfm.2018.352},
	abstract = {We revisit the classical but as yet unresolved problem of predicting the strength of breaking 2-D and 3-D gravity water waves, as quantified by the amount of wave energy dissipated per breaking event. Following Duncan (J. Fluid Mech., vol. 126, 1983, pp. 507-520), the wave energy dissipation rate per unit length of breaking crest may be related to the fifth moment of the wave speed and the non-dimensional breaking strength parameter b. We use a finite-volume Navier-Stokes solver with large-eddy simulation resolution and volume-of-fluid surface reconstruction (Derakhti \& Kirby, J. Fluid Mech., vol. 761, 2014a, pp. 464-506; J. Fluid Mech., vol. 790, 2016, pp. 553-581) to simulate nonlinear wave evolution, with a strong focus on breaking onset and postbreaking behaviour for representative cases of wave packets with breaking due to dispersive focusing and modulational instability. The present study uses these results to investigate the relationship between the breaking strength parameter b and the breaking onset parameter B proposed recently by Barthelemy et al. (J. Fluid Mech., vol. 841, 2018, pp. 463-488). The latter, formed from the local energy flux normalized by the local energy density and the local crest speed, simplifies, on the wave surface, to the ratio of fluid speed to crest speed. Following a wave crest, when B exceeds a generic threshold value at the wave crest (Barthelemy et al. 2018), breaking is imminent. We find a robust relationship between the breaking strength parameter b and the rate of change of breaking onset parameter dB/dt at the wave crest, as it transitions through the generic breaking onset threshold (B ∼ 0:85), scaled by the local period of the breaking wave. This result significantly refines previous efforts to express b in terms of a wave packet steepness parameter, which is difficult to define robustly and which does not provide a generically accurate forecast of the energy dissipated by breaking.},
	journal = {Journal of Fluid Mechanics},
	author = {Derakhti, Morteza and Banner, Michael L. and Kirby, James T.},
	month = aug,
	year = {2018},
	note = {Publisher: Cambridge University Press},
	keywords = {air/sea interactions, surface gravity waves, wave breaking},
	pages = {R2},
}

@article{gnyawali_retooling_2017-1,
	title = {Retooling {Laser} {Speckle} {Contrast} {Analysis} {Algorithm} to {Enhance} {Non}-{Invasive} {High} {Resolution} {Laser} {Speckle} {Functional} {Imaging} of {Cutaneous} {Microcirculation}},
	volume = {7},
	issn = {20452322},
	doi = {10.1038/srep41048},
	abstract = {Cutaneous microvasculopathy complicates wound healing. Functional assessment of gated individual dermal microvessels is therefore of outstanding interest. Functional performance of laser speckle contrast imaging (LSCI) systems is compromised by motion artefacts. To address such weakness, post-processing of stacked images is reported. We report the first post-processing of binary raw data from a high-resolution LSCI camera. Sharp images of low-flowing microvessels were enabled by introducing inverse variance in conjunction with speckle contrast in Matlab-based program code. Extended moving window averaging enhanced signal-To-noise ratio. Functional quantitative study of blood flow kinetics was performed on single gated microvessels using a free hand tool. Based on detection of flow in low-flow microvessels, a new sharp contrast image was derived. Thus, this work presents the first distinct image with quantitative microperfusion data from gated human foot microvasculature. This versatile platform is applicable to study a wide range of tissue systems including fine vascular network in murine brain without craniotomy as well as that in the murine dorsal skin. Importantly, the algorithm reported herein is hardware agnostic and is capable of post-processing binary raw data from any camera source to improve the sensitivity of functional flow data above and beyond standard limits of the optical system.},
	journal = {Scientific Reports},
	author = {Gnyawali, Surya C. and Blum, Kevin and Pal, Durba and Ghatak, Subhadip and Khanna, Savita and Roy, Sashwati and Sen, Chandan K.},
	month = jan,
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	keywords = {★},
}

@article{monteiro_rodrigues_observations_2018,
	title = {Observations on the perfusion recovery of regenerative angiogenesis in an ischemic limb model under hyperoxia},
	volume = {6},
	issn = {2051817X},
	doi = {10.14814/phy2.13736},
	abstract = {This study combines two well-known vascular research models, hyperoxia and hind limb ischemia, aiming to better characterize capacities of the hyperoxia challenge. We studied two groups of C57/BL6 male mice, a control (C) and a hind limb ischemia (HLI) group. Perfusion from both limbs was recorded in all animals by laser Doppler techniques under an oxygen (O2) saturated atmosphere, once for control and, during 35 days for the HLI group. We used a third set of normoxic animals for HLI morphometric control. The expected variability of responses was higher for the younger animals. In the HLI group, capillary density normalized at Day 21 as expected, but not microcirculatory physiology. In the operated limb, perfusion decreased dramatically following surgery (Day 4), as a slight reduction in the non-operated limb was also noted. Consistently, the response to hyperoxia was an increased perfusion in the ischemic limb and decreased perfusion in the contralateral limb. Only at Day 35, both limbs exhibited similar flows, although noticeably lower than Day 0. These observations help to understand some of the functional variability attributed to the hyperoxia model, by showing (i) differences in the circulation of the limb pairs to readjust a new perfusion set-point even after ischemia, an original finding implying that (ii) data from both limbs should be recorded when performing distal measurements in vivo. Our data demonstrate that the new vessels following HLI are not functionally normal, and this also affects the non-operated limb. These findings confirm the discriminative capacities of the hyperoxia challenge and suggest its potential utility to study other pathologies with vascular impact.},
	number = {12},
	journal = {Physiological Reports},
	author = {Monteiro Rodrigues, Luis and Silva, Henrique and Ferreira, Hugo and Renault, Marie Ange and Gadeau, Alain Pierre},
	month = jun,
	year = {2018},
	note = {Publisher: American Physiological Society},
	keywords = {Hind limb ischemia, hyperoxia, laser Doppler, mice, perfusion balance between limbs},
}

@article{seo_numerical_2013,
	title = {Numerical simulations of blood flow in arterial bifurcation models},
	volume = {25},
	issn = {1226119X},
	doi = {10.1007/s13367-013-0016-7},
	abstract = {In the study, two different arterial bifurcation model geometries were used in the flow simulation. The model 1 is assumed the internal carotid artery (ICA) and the external carotid artery (ECA) branches of the bifurcation aligned in parallel to each other, while the model 2 is the typical carotid geometry. In the computation the Non-Newtonian behavior of blood was described using Carreau model. Generally, in the comparison between Newtonian and Non-Newtonian results good agreement was observed in the velocity profiles, while some discrepancies were found in the temporal wall shear stress (WSS) distributions as well as pressure profiles due to the shear thinning behavior. The temporal evolution of WSS periodically increases and decreases closely that of the inlet velocity waveform. It was also observed that the reversed flow region in the ICA of model 2 is 2.5 times larger than that of model 1. As a result, the variation of the flow characteristics can be dependent on the geometry as well as the arterial bifurcation geometry plays an important role in the development of atherosclerosis. © 2013 The Korean Society of Rheology and Springer.},
	number = {3},
	journal = {Korea Australia Rheology Journal},
	author = {Seo, Taewon},
	month = aug,
	year = {2013},
	keywords = {Atherosclerosis, Bifurcation model, Blood flow, Carotid sinus, Wall shear stress, ★},
	pages = {153--161},
}

@misc{noauthor_error_interpolation_nodate,
	title = {error\_interpolation},
}

@article{Petcu2013,
	title = {The one‐dimensional shallow water equations with transparent boundary conditions},
	volume = {36},
	url = {http://umich.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwnZ1LT9tAEIBHiF6KEG9a84j2QBEcTL3rR9bHKCWqEBFSBefVbryWkEigsSPorT-hv7G_pDO7tgU5oV7iw07kOJ6Zndnd-QaAL_kDQxzp-zrkF4tp8YAfzSJamOSUu0jXuuHbtRiM0qtRfLkCsi2M8bCIbvWNzMQ5b7J2bapmk198nU71BfEsyROjcVOYdDPs_},
	doi = {10.1002/mma.1482},
	number = {15},
	journal = {Mathematical Methods in the Applied Sciences},
	author = {Petcu, Madalina and Temam, Roger},
	year = {2013},
	note = {Publisher: WILEY-BLACKWELL
ISBN: 0170-4214},
	pages = {1979--1994},
}

@article{ADAMY2010235,
	title = {A multilevel method for finite volume discretization of the two-dimensional nonlinear shallow-water equations},
	volume = {33},
	issn = {14635003},
	url = {http://www.sciencedirect.com/science/article/pii/S1463500310000156},
	doi = {10.1016/j.ocemod.2010.02.006},
	abstract = {In this article we propose and implement a multilevel method to simulate the solution of the two-dimensional nonlinear shallow-water equations. The multilevel method is based on a central-upwind finite volume scheme and uses new incremental unknowns which enable to preserve the numerical conservation of the scheme. The method is tested and analyzed on two and three levels of discretization on different test cases and turns out to furnish a good solution of the problems while saving CPU time. © 2010 Elsevier Ltd.},
	number = {3-4},
	journal = {Ocean Modelling},
	author = {Adamy, K. and Bousquet, A. and Faure, S. and Laminie, J. and Temam, R.},
	year = {2010},
	keywords = {Finite volume methods, Incremental unknowns, Multilevel methods, Shallow-water problem},
	pages = {235--256},
}

@article{ozgen,
	title = {Urban flood modeling using shallow water equations with depth-dependent anisotropic porosity},
	volume = {541},
	journal = {Journal of Hydrology},
	author = {Özgen, Ilhan and Zhao, Jiaheng and Liang, Dongfang and Hinkelmann, Reinhard},
	year = {2016},
	note = {Publisher: Elsevier B.V
ISBN: 0022-1694},
	pages = {1165--1184},
}

@article{BOUSQUET201366,
	title = {Multilevel finite volume methods and boundary conditions for geophysical flows},
	volume = {74},
	issn = {0045-7930},
	url = {http://www.sciencedirect.com/science/article/pii/S0045793013000108},
	doi = {https://doi.org/10.1016/j.compfluid.2013.01.001},
	abstract = {This review article concerns multi-level methods for finite volume discretizations. They are presented in the context of the non-viscous shallow water equations in space dimension one and two, although the ideas are of more general validity. The second, partly related topic, addressed in this article is that of boundary conditions also presented in the context of the inviscid shallow water equations. Beside algorithmic presentation, the article contains the discussion of some numerical stability issues and the results of some numerical simulations. The multi-level method is applied numerically to the equations of shallow water using a central-upwind scheme for finite volumes and we prove the stability for the linear shallow water equations in different situations. The question of the boundary conditions is illustrated for the two-dimensional equations in the reference case of the so-called Rossby soliton.},
	journal = {Computers \& Fluids},
	author = {Bousquet, Arthur and Marion, Martine and Petcu, Madalina and Temam, Roger},
	year = {2013},
	keywords = {Central-upwind, Finite volume, Multi-level method, Shallow water equations, Transparent boundary conditions},
	pages = {66--90},
}

@article{Faure2005,
	title = {Finite {Volume} {Discretization} and {Multilevel} {Methods} in {Flow} {Problems}},
	volume = {25},
	issn = {1573-7691},
	url = {https://doi.org/10.1007/s10915-004-4642-6},
	doi = {10.1007/s10915-004-4642-6},
	abstract = {This article is intended as a preliminary report on the implementation of a finite volume multilevel scheme for the discretization of the incompressible Navier--Stokes equations. As is well known the use of staggered grids (e.g. MAC grids, Perić et al. Comput. Fluids, 16(4), 389--403, (1988)) is a serious impediment for the implementation of multilevel schemes in the context of finite differences. This difficulty is circumvented here by the use of a colocated finite volume discretization (Faure et al. (2004a) Submitted, Perić et al. Comput. Fluids, 16(4), 389--403, (1988)), for which the algebra of multilevel methods is much simpler than in the context of MAC type finite differences. The general ideas and the numerical simulations are presented in this article in the simplified context of a two-dimensional Burgers equations; the two-, and three-dimensional Navier--Stokes equations introducing new difficulties related to the incompressibility condition and the time discretization, will be considered elsewhere (see Faure et al. (2004a) Submitted and Faure et al. (2004b), in preparation).},
	number = {1},
	journal = {Journal of Scientific Computing},
	author = {Faure, S and Laminie, J and Temam, R},
	month = oct,
	year = {2005},
	pages = {231--261},
}

@article{DUBOIS2005660,
	title = {Multilevel schemes for the shallow water equations},
	volume = {207},
	issn = {0021-9991},
	doi = {https://doi.org/10.1016/j.jcp.2005.01.025},
	abstract = {In this paper, we study a number of multilevel schemes for the numerical solution of the shallow water equations; new schemes and new perspectives of known schemes are examined. We consider the case of periodic boundary conditions. Spatial discretization is obtained using a Fourier spectral Galerkin method. For the time integration, two strategies are studied. The first one is based on scale separation, and we choose the time scheme (explicit or semi-implicit) as a function of the spatial scales (multilevel schemes). The second approach is based on a splitting of the operators, and we choose the time integration method as a function of the operator considered (multistep or fractional schemes). The numerical results obtained are compared with the explicit reference scheme (Leap–Frog scheme), and with the semi-implicit scheme (Leap–Frog scheme with Crank–Nicholson scheme for the gravity terms), both computed with a similar mesh. The drawback of the explicit reference scheme being the numerical stability constraint on the time step, and the drawback of the semi-implicit scheme being the dispersive error, the aim with the new schemes is to obtain schemes with less dispersive error than the semi-implicit scheme, and with better stability properties than the explicit reference scheme. The numerical results obtained show that the schemes proposed allow one to reduce the dispersive error and to increase the numerical stability at reduced cost.},
	number = {2},
	journal = {Journal of Computational Physics},
	author = {Dubois, T and Jauberteau, F and Temam, R M and Tribbia, J},
	year = {2005},
	keywords = {Fractional step methods, Gravity waves, Inertial waves, Multilevel methods, Shallow water equations},
	pages = {660--694},
}

@techreport{yeung_principal_2001,
	title = {Principal component analysis for clustering gene expression data},
	url = {http://www.cs.washington.edu/homes/kayee/pca},
	abstract = {Motivation: There is a great need to develop analytical methodology to analyze and to exploit the information contained in gene expression data. Because of the large number of genes and the complexity of biological networks, clustering is a useful exploratory technique for analysis of gene expression data. Other classical techniques, such as principal component analysis (PCA), have also been applied to analyze gene expression data. Using different data analysis techniques and different clustering algorithms to analyze the same data set can lead to very different conclusions. Our goal is to study the effectiveness of principal components (PCs) in capturing cluster structure. Specifically, using both real and synthetic gene expression data sets, we compared the quality of clusters obtained from the original data to the quality of clusters obtained after projecting onto subsets of the principal component axes. Results: Our empirical study showed that clustering with the PCs instead of the original variables does not necessarily improve, and often degrades, cluster quality. In particular, the first few PCs (which contain most of the variation in the data) do not necessarily capture most of the cluster structure. We also showed that clustering with PCs has different impact on different algorithms and different similarity metrics. Overall, we would not recommend PCA before clustering except in special circumstances.},
	author = {Yeung, K Y and Ruzzo, W L},
	year = {2001},
	note = {Publication Title: BIOINFORMATICS
Volume: 17
Issue: 9},
	pages = {763--774},
}

@techreport{wold_principal_nodate,
	title = {Principal {Component} {Analysis}},
	author = {Wold, Svante and Esbensen, Kim and Geladi, Paul},
}
